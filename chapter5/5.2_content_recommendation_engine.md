# 5.2 å†…å®¹æ¨èå¼•æ“

## é¡¹ç›®æ¦‚è¿°

å†…å®¹æ¨èå¼•æ“æ˜¯ç°ä»£äº’è”ç½‘å¹³å°çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œé€šè¿‡åˆ†æç”¨æˆ·è¡Œä¸ºå’Œå†…å®¹ç‰¹å¾ï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„å†…å®¹æ¨èã€‚æœ¬é¡¹ç›®å°†æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ¨èç³»ç»Ÿï¼Œé›†æˆå¤šç§æ¨èç®—æ³•ï¼Œå®ç°å®æ—¶ä¸ªæ€§åŒ–æ¨èã€‚

### ğŸ¯ é¡¹ç›®ç›®æ ‡
- æ„å»ºå¤šç­–ç•¥èåˆçš„æ¨èå¼•æ“
- å®ç°å®æ—¶ç”¨æˆ·è¡Œä¸ºåˆ†æå’Œå»ºæ¨¡
- è§£å†³å†·å¯åŠ¨å’Œæ•°æ®ç¨€ç–é—®é¢˜
- æä¾›æ¨èè§£é‡Šæ€§å’ŒA/Bæµ‹è¯•èƒ½åŠ›
- æ”¯æŒå¤§è§„æ¨¡å¹¶å‘æ¨èè¯·æ±‚

### ğŸ“Š é¢„æœŸæ•ˆæœ
- **æ¨èå‡†ç¡®ç‡**: > 85%
- **ç”¨æˆ·ç‚¹å‡»ç‡**: æå‡30%+
- **ç”¨æˆ·åœç•™æ—¶é—´**: å¢åŠ 25%+
- **ç³»ç»Ÿå“åº”æ—¶é—´**: < 100ms

## 5.2.1 æ¨èç³»ç»Ÿæ¶æ„è®¾è®¡

### ä¸šåŠ¡åœºæ™¯åˆ†æ

**æ ¸å¿ƒæ¨èåœºæ™¯**:
1. **é¦–é¡µæ¨è**: åŸºäºç”¨æˆ·å…´è¶£çš„ä¸ªæ€§åŒ–å†…å®¹æµ
2. **ç›¸å…³æ¨è**: åŸºäºå½“å‰æµè§ˆå†…å®¹çš„ç›¸ä¼¼æ¨è
3. **çƒ­é—¨æ¨è**: åŸºäºå…¨å±€çƒ­åº¦çš„è¶‹åŠ¿å†…å®¹
4. **åˆ†ç±»æ¨è**: åŸºäºç”¨æˆ·åå¥½çš„åˆ†ç±»å†…å®¹

**ç”¨æˆ·ç±»å‹åˆ†æ**:
- **æ–°ç”¨æˆ·**: ç¼ºä¹å†å²è¡Œä¸ºæ•°æ®ï¼Œéœ€è¦å†·å¯åŠ¨ç­–ç•¥
- **æ´»è·ƒç”¨æˆ·**: æœ‰ä¸°å¯Œè¡Œä¸ºæ•°æ®ï¼Œå¯è¿›è¡Œç²¾å‡†æ¨è
- **å›æµç”¨æˆ·**: é•¿æœŸæœªæ´»è·ƒï¼Œéœ€è¦é‡æ–°æ¿€æ´»
- **é«˜ä»·å€¼ç”¨æˆ·**: ä»˜è´¹æˆ–é«˜äº’åŠ¨ç”¨æˆ·ï¼Œéœ€è¦ç‰¹æ®Šå…³æ³¨

### ç³»ç»Ÿæ¶æ„è®¾è®¡

```mermaid
graph TB
    A[ç”¨æˆ·è¯·æ±‚] --> B[APIç½‘å…³]
    B --> C[æ¨èæœåŠ¡]
    C --> D[ç”¨æˆ·ç”»åƒæœåŠ¡]
    C --> E[å†…å®¹ç‰¹å¾æœåŠ¡]
    C --> F[å¬å›æœåŠ¡]
    C --> G[æ’åºæœåŠ¡]
    C --> H[å¤šæ ·æ€§ä¼˜åŒ–]
    
    D --> I[ç”¨æˆ·è¡Œä¸ºåˆ†æ]
    D --> J[ç”¨æˆ·æ ‡ç­¾ç³»ç»Ÿ]
    E --> K[å†…å®¹å‘é‡åŒ–]
    E --> L[å†…å®¹æ ‡ç­¾æå–]
    
    F --> M[ååŒè¿‡æ»¤å¬å›]
    F --> N[å†…å®¹å¬å›]
    F --> O[çƒ­é—¨å¬å›]
    F --> P[æ·±åº¦å­¦ä¹ å¬å›]
    
    G --> Q[ç‰¹å¾å·¥ç¨‹]
    G --> R[æ’åºæ¨¡å‹]
    
    I --> S[å®æ—¶è¡Œä¸ºæµ]
    S --> T[Kafka]
    T --> U[Spark Streaming]
    U --> V[Redisç¼“å­˜]
    
    K --> W[å‘é‡æ•°æ®åº“]
    J --> X[ç”¨æˆ·ç‰¹å¾åº“]
    L --> Y[å†…å®¹ç‰¹å¾åº“]
```

**æ ¸å¿ƒæ¨¡å—è¯´æ˜**:
- **å¬å›å±‚**: ä»æµ·é‡å†…å®¹ä¸­å¿«é€Ÿå¬å›å€™é€‰é›†
- **æ’åºå±‚**: å¯¹å¬å›ç»“æœè¿›è¡Œç²¾å‡†æ’åº
- **ç”¨æˆ·ç”»åƒ**: æ„å»ºå¤šç»´åº¦ç”¨æˆ·å…´è¶£æ¨¡å‹
- **å†…å®¹ç†è§£**: æå–å†…å®¹ç‰¹å¾å’Œè¯­ä¹‰è¡¨ç¤º
- **å®æ—¶è®¡ç®—**: å¤„ç†ç”¨æˆ·å®æ—¶è¡Œä¸ºå’Œæ›´æ–°æ¨è

### æŠ€æœ¯é€‰å‹

**æ¨èç®—æ³•**:
- **ååŒè¿‡æ»¤**: UserCF, ItemCF, Matrix Factorization
- **æ·±åº¦å­¦ä¹ **: DeepFM, Wide&Deep, DIN, DSSM
- **å†…å®¹æ¨è**: TF-IDF, Word2Vec, BERT
- **å›¾ç¥ç»ç½‘ç»œ**: GraphSAGE, LightGCN

**æŠ€æœ¯æ ˆ**:
- **åç«¯**: Python, FastAPI, Celery
- **æœºå™¨å­¦ä¹ **: TensorFlow, PyTorch, Scikit-learn
- **æ•°æ®å­˜å‚¨**: PostgreSQL, Redis, Elasticsearch
- **å®æ—¶è®¡ç®—**: Apache Kafka, Spark Streaming
- **å‘é‡æ£€ç´¢**: Faiss, Annoy, Milvus

## 5.2.2 ç”¨æˆ·ç”»åƒå’Œè¡Œä¸ºåˆ†æ

### ç”¨æˆ·ç”»åƒæ„å»º

```python
# user_profile.py
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
import redis
import json

@dataclass
class UserProfile:
    """ç”¨æˆ·ç”»åƒæ•°æ®ç»“æ„"""
    user_id: str
    demographics: Dict[str, Any] = field(default_factory=dict)
    interests: Dict[str, float] = field(default_factory=dict)
    behavior_patterns: Dict[str, Any] = field(default_factory=dict)
    preferences: Dict[str, Any] = field(default_factory=dict)
    activity_level: str = "normal"  # low, normal, high
    user_type: str = "regular"  # new, regular, premium, inactive
    last_updated: datetime = field(default_factory=datetime.now)

@dataclass
class UserBehavior:
    """ç”¨æˆ·è¡Œä¸ºæ•°æ®"""
    user_id: str
    item_id: str
    behavior_type: str  # view, click, like, share, comment, purchase
    timestamp: datetime
    duration: Optional[float] = None
    rating: Optional[float] = None
    context: Dict[str, Any] = field(default_factory=dict)

class UserProfileService:
    """ç”¨æˆ·ç”»åƒæœåŠ¡"""
    
    def __init__(self, redis_client: redis.Redis, db_config: Dict):
        self.redis_client = redis_client
        self.db_config = db_config
        
        # å…´è¶£è¡°å‡å‚æ•°
        self.interest_decay_rate = 0.95
        self.behavior_weights = {
            'view': 1.0,
            'click': 2.0,
            'like': 3.0,
            'share': 4.0,
            'comment': 5.0,
            'purchase': 10.0
        }
    
    def update_user_profile(self, user_id: str, behaviors: List[UserBehavior]) -> UserProfile:
        """æ›´æ–°ç”¨æˆ·ç”»åƒ"""
        # è·å–ç°æœ‰ç”»åƒ
        profile = self.get_user_profile(user_id)
        if not profile:
            profile = UserProfile(user_id=user_id)
        
        # æ›´æ–°å…´è¶£æ ‡ç­¾
        self._update_interests(profile, behaviors)
        
        # æ›´æ–°è¡Œä¸ºæ¨¡å¼
        self._update_behavior_patterns(profile, behaviors)
        
        # æ›´æ–°ç”¨æˆ·åå¥½
        self._update_preferences(profile, behaviors)
        
        # è®¡ç®—æ´»è·ƒåº¦
        self._calculate_activity_level(profile, behaviors)
        
        # ç¡®å®šç”¨æˆ·ç±»å‹
        self._determine_user_type(profile)
        
        # ä¿å­˜ç”»åƒ
        self._save_user_profile(profile)
        
        return profile
    
    def _update_interests(self, profile: UserProfile, behaviors: List[UserBehavior]):
        """æ›´æ–°ç”¨æˆ·å…´è¶£"""
        # åº”ç”¨æ—¶é—´è¡°å‡
        current_time = datetime.now()
        for interest, score in profile.interests.items():
            days_passed = (current_time - profile.last_updated).days
            decayed_score = score * (self.interest_decay_rate ** days_passed)
            profile.interests[interest] = max(0.1, decayed_score)
        
        # åŸºäºæ–°è¡Œä¸ºæ›´æ–°å…´è¶£
        interest_updates = defaultdict(float)
        
        for behavior in behaviors:
            # è·å–å†…å®¹æ ‡ç­¾ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            content_tags = self._get_content_tags(behavior.item_id)
            weight = self.behavior_weights.get(behavior.behavior_type, 1.0)
            
            # æ—¶é—´æƒé‡ï¼ˆè¶Šæ–°çš„è¡Œä¸ºæƒé‡è¶Šé«˜ï¼‰
            time_weight = self._calculate_time_weight(behavior.timestamp)
            
            for tag in content_tags:
                interest_updates[tag] += weight * time_weight
        
        # åˆå¹¶å…´è¶£æ›´æ–°
        for interest, update_score in interest_updates.items():
            current_score = profile.interests.get(interest, 0.0)
            profile.interests[interest] = min(10.0, current_score + update_score)
        
        # å½’ä¸€åŒ–å…´è¶£åˆ†æ•°
        self._normalize_interests(profile)
    
    def _update_behavior_patterns(self, profile: UserProfile, behaviors: List[UserBehavior]):
        """æ›´æ–°è¡Œä¸ºæ¨¡å¼"""
        if not behaviors:
            return
        
        # æ´»è·ƒæ—¶é—´åˆ†æ
        active_hours = [b.timestamp.hour for b in behaviors]
        hour_distribution = Counter(active_hours)
        
        # è¡Œä¸ºç±»å‹åˆ†æ
        behavior_types = [b.behavior_type for b in behaviors]
        behavior_distribution = Counter(behavior_types)
        
        # ä¼šè¯é•¿åº¦åˆ†æ
        session_durations = [b.duration for b in behaviors if b.duration]
        avg_session_duration = np.mean(session_durations) if session_durations else 0
        
        profile.behavior_patterns.update({
            'active_hours': dict(hour_distribution),
            'behavior_distribution': dict(behavior_distribution),
            'avg_session_duration': avg_session_duration,
            'total_behaviors': len(behaviors),
            'behavior_frequency': len(behaviors) / max(1, (datetime.now() - min(b.timestamp for b in behaviors)).days)
        })
    
    def _update_preferences(self, profile: UserProfile, behaviors: List[UserBehavior]):
        """æ›´æ–°ç”¨æˆ·åå¥½"""
        # å†…å®¹ç±»å‹åå¥½
        content_types = [self._get_content_type(b.item_id) for b in behaviors]
        type_preferences = Counter(content_types)
        
        # å†…å®¹é•¿åº¦åå¥½
        content_lengths = [self._get_content_length(b.item_id) for b in behaviors]
        avg_preferred_length = np.mean([l for l in content_lengths if l > 0])
        
        # äº’åŠ¨åå¥½
        interaction_behaviors = ['like', 'share', 'comment']
        interaction_rate = sum(1 for b in behaviors if b.behavior_type in interaction_behaviors) / len(behaviors)
        
        profile.preferences.update({
            'content_type_preferences': dict(type_preferences),
            'preferred_content_length': avg_preferred_length,
            'interaction_rate': interaction_rate,
            'exploration_rate': self._calculate_exploration_rate(behaviors)
        })
    
    def _calculate_activity_level(self, profile: UserProfile, behaviors: List[UserBehavior]):
        """è®¡ç®—ç”¨æˆ·æ´»è·ƒåº¦"""
        if not behaviors:
            profile.activity_level = "low"
            return
        
        # æœ€è¿‘7å¤©çš„è¡Œä¸ºæ•°é‡
        recent_behaviors = [
            b for b in behaviors 
            if (datetime.now() - b.timestamp).days <= 7
        ]
        
        daily_avg_behaviors = len(recent_behaviors) / 7
        
        if daily_avg_behaviors >= 10:
            profile.activity_level = "high"
        elif daily_avg_behaviors >= 3:
            profile.activity_level = "normal"
        else:
            profile.activity_level = "low"
    
    def _determine_user_type(self, profile: UserProfile):
        """ç¡®å®šç”¨æˆ·ç±»å‹"""
        total_behaviors = profile.behavior_patterns.get('total_behaviors', 0)
        days_since_registration = (datetime.now() - profile.last_updated).days
        
        if total_behaviors == 0:
            profile.user_type = "new"
        elif days_since_registration > 30 and profile.activity_level == "low":
            profile.user_type = "inactive"
        elif profile.preferences.get('interaction_rate', 0) > 0.3:
            profile.user_type = "premium"
        else:
            profile.user_type = "regular"
    
    def get_user_profile(self, user_id: str) -> Optional[UserProfile]:
        """è·å–ç”¨æˆ·ç”»åƒ"""
        try:
            data = self.redis_client.get(f"user_profile:{user_id}")
            if data:
                profile_dict = json.loads(data)
                return self._dict_to_profile(profile_dict)
            return None
        except Exception as e:
            print(f"è·å–ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
            return None
    
    def _save_user_profile(self, profile: UserProfile):
        """ä¿å­˜ç”¨æˆ·ç”»åƒ"""
        profile.last_updated = datetime.now()
        profile_dict = self._profile_to_dict(profile)
        
        # ä¿å­˜åˆ°Redisï¼ˆ24å°æ—¶è¿‡æœŸï¼‰
        self.redis_client.setex(
            f"user_profile:{profile.user_id}",
            86400,
            json.dumps(profile_dict, default=str)
        )
    
    def _get_content_tags(self, item_id: str) -> List[str]:
        """è·å–å†…å®¹æ ‡ç­¾ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…å®ç°ä¸­åº”è¯¥ä»å†…å®¹æ•°æ®åº“è·å–
        return ["technology", "science", "entertainment"]  # ç¤ºä¾‹æ ‡ç­¾
    
    def _get_content_type(self, item_id: str) -> str:
        """è·å–å†…å®¹ç±»å‹"""
        # å®é™…å®ç°ä¸­åº”è¯¥ä»å†…å®¹æ•°æ®åº“è·å–
        return "article"  # ç¤ºä¾‹ç±»å‹
    
    def _get_content_length(self, item_id: str) -> int:
        """è·å–å†…å®¹é•¿åº¦"""
        # å®é™…å®ç°ä¸­åº”è¯¥ä»å†…å®¹æ•°æ®åº“è·å–
        return 1000  # ç¤ºä¾‹é•¿åº¦
    
    def _calculate_time_weight(self, timestamp: datetime) -> float:
        """è®¡ç®—æ—¶é—´æƒé‡"""
        days_ago = (datetime.now() - timestamp).days
        return max(0.1, 1.0 - days_ago * 0.1)
    
    def _calculate_exploration_rate(self, behaviors: List[UserBehavior]) -> float:
        """è®¡ç®—æ¢ç´¢ç‡ï¼ˆç”¨æˆ·å°è¯•æ–°å†…å®¹çš„å€¾å‘ï¼‰"""
        if len(behaviors) < 10:
            return 0.5  # é»˜è®¤å€¼
        
        # è®¡ç®—å†…å®¹å¤šæ ·æ€§
        unique_items = len(set(b.item_id for b in behaviors))
        total_behaviors = len(behaviors)
        
        return unique_items / total_behaviors
    
    def _normalize_interests(self, profile: UserProfile):
        """å½’ä¸€åŒ–å…´è¶£åˆ†æ•°"""
        if not profile.interests:
            return
        
        total_score = sum(profile.interests.values())
        if total_score > 0:
            for interest in profile.interests:
                profile.interests[interest] = profile.interests[interest] / total_score
    
    def _profile_to_dict(self, profile: UserProfile) -> Dict:
        """è½¬æ¢ç”»åƒä¸ºå­—å…¸"""
        return {
            'user_id': profile.user_id,
            'demographics': profile.demographics,
            'interests': profile.interests,
            'behavior_patterns': profile.behavior_patterns,
            'preferences': profile.preferences,
            'activity_level': profile.activity_level,
            'user_type': profile.user_type,
            'last_updated': profile.last_updated.isoformat()
        }
    
    def _dict_to_profile(self, data: Dict) -> UserProfile:
        """ä»å­—å…¸è½¬æ¢ä¸ºç”»åƒå¯¹è±¡"""
        return UserProfile(
            user_id=data['user_id'],
            demographics=data.get('demographics', {}),
            interests=data.get('interests', {}),
            behavior_patterns=data.get('behavior_patterns', {}),
            preferences=data.get('preferences', {}),
            activity_level=data.get('activity_level', 'normal'),
            user_type=data.get('user_type', 'regular'),
            last_updated=datetime.fromisoformat(data['last_updated'])
        )
```

### å®æ—¶è¡Œä¸ºå¤„ç†

```python
# behavior_processor.py
from typing import List, Dict, Any
from kafka import KafkaConsumer, KafkaProducer
import json
from datetime import datetime
import asyncio
from dataclasses import asdict

class BehaviorProcessor:
    """å®æ—¶è¡Œä¸ºå¤„ç†å™¨"""
    
    def __init__(self, kafka_config: Dict, user_profile_service):
        self.kafka_config = kafka_config
        self.user_profile_service = user_profile_service
        
        # Kafkaæ¶ˆè´¹è€…
        self.consumer = KafkaConsumer(
            'user_behaviors',
            bootstrap_servers=kafka_config['bootstrap_servers'],
            value_deserializer=lambda x: json.loads(x.decode('utf-8')),
            group_id='behavior_processor'
        )
        
        # Kafkaç”Ÿäº§è€…
        self.producer = KafkaProducer(
            bootstrap_servers=kafka_config['bootstrap_servers'],
            value_serializer=lambda x: json.dumps(x, default=str).encode('utf-8')
        )
    
    async def start_processing(self):
        """å¼€å§‹å¤„ç†å®æ—¶è¡Œä¸ºæ•°æ®"""
        print("å¼€å§‹å¤„ç†å®æ—¶è¡Œä¸ºæ•°æ®...")
        
        for message in self.consumer:
            try:
                behavior_data = message.value
                await self.process_behavior(behavior_data)
            except Exception as e:
                print(f"å¤„ç†è¡Œä¸ºæ•°æ®å¤±è´¥: {e}")
    
    async def process_behavior(self, behavior_data: Dict):
        """å¤„ç†å•ä¸ªè¡Œä¸ºäº‹ä»¶"""
        # è§£æè¡Œä¸ºæ•°æ®
        behavior = UserBehavior(
            user_id=behavior_data['user_id'],
            item_id=behavior_data['item_id'],
            behavior_type=behavior_data['behavior_type'],
            timestamp=datetime.fromisoformat(behavior_data['timestamp']),
            duration=behavior_data.get('duration'),
            rating=behavior_data.get('rating'),
            context=behavior_data.get('context', {})
        )
        
        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        profile = self.user_profile_service.update_user_profile(
            behavior.user_id, [behavior]
        )
        
        # è§¦å‘å®æ—¶æ¨èæ›´æ–°
        await self.trigger_recommendation_update(behavior, profile)
        
        # å‘é€å¤„ç†å®Œæˆäº‹ä»¶
        self.producer.send('behavior_processed', {
            'user_id': behavior.user_id,
            'behavior_type': behavior.behavior_type,
            'processed_at': datetime.now().isoformat()
        })
    
    async def trigger_recommendation_update(self, behavior: UserBehavior, profile: UserProfile):
        """è§¦å‘æ¨èæ›´æ–°"""
        # æ ¹æ®è¡Œä¸ºç±»å‹å†³å®šæ›´æ–°ç­–ç•¥
        if behavior.behavior_type in ['like', 'share', 'purchase']:
            # é«˜ä»·å€¼è¡Œä¸ºï¼Œç«‹å³æ›´æ–°æ¨è
            await self.update_user_recommendations(behavior.user_id, profile)
        elif behavior.behavior_type == 'view':
            # æµè§ˆè¡Œä¸ºï¼Œæ‰¹é‡æ›´æ–°
            await self.schedule_batch_update(behavior.user_id)
    
    async def update_user_recommendations(self, user_id: str, profile: UserProfile):
        """æ›´æ–°ç”¨æˆ·æ¨è"""
        # å‘é€æ¨èæ›´æ–°è¯·æ±‚
        self.producer.send('recommendation_update', {
            'user_id': user_id,
            'profile': asdict(profile),
            'update_type': 'immediate'
        })
    
    async def schedule_batch_update(self, user_id: str):
        """å®‰æ’æ‰¹é‡æ›´æ–°"""
        self.producer.send('recommendation_update', {
            'user_id': user_id,
            'update_type': 'batch'
        })
```

## 5.2.3 å¤šç­–ç•¥æ¨èç®—æ³•å®ç°

### ååŒè¿‡æ»¤æ¨è

```python
# collaborative_filtering.py
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

@dataclass
class RecommendationResult:
    """æ¨èç»“æœ"""
    item_id: str
    score: float
    reason: str
    algorithm: str

class CollaborativeFiltering:
    """ååŒè¿‡æ»¤æ¨èç®—æ³•"""
    
    def __init__(self, min_interactions: int = 5):
        self.min_interactions = min_interactions
        self.user_item_matrix = None
        self.item_user_matrix = None
        self.user_similarity = None
        self.item_similarity = None
        self.user_means = None
    
    def fit(self, interactions_df: pd.DataFrame):
        """è®­ç»ƒååŒè¿‡æ»¤æ¨¡å‹"""
        # è¿‡æ»¤ä½é¢‘ç”¨æˆ·å’Œç‰©å“
        user_counts = interactions_df['user_id'].value_counts()
        item_counts = interactions_df['item_id'].value_counts()
        
        valid_users = user_counts[user_counts >= self.min_interactions].index
        valid_items = item_counts[item_counts >= self.min_interactions].index
        
        filtered_df = interactions_df[
            (interactions_df['user_id'].isin(valid_users)) &
            (interactions_df['item_id'].isin(valid_items))
        ]
        
        # æ„å»ºç”¨æˆ·-ç‰©å“çŸ©é˜µ
        self.user_item_matrix = filtered_df.pivot_table(
            index='user_id',
            columns='item_id',
            values='rating',
            fill_value=0
        )
        
        # æ„å»ºç‰©å“-ç”¨æˆ·çŸ©é˜µ
        self.item_user_matrix = self.user_item_matrix.T
        
        # è®¡ç®—ç”¨æˆ·å¹³å‡è¯„åˆ†
        self.user_means = self.user_item_matrix.mean(axis=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        self._compute_similarities()
    
    def _compute_similarities(self):
        """è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ"""
        # ç”¨æˆ·ç›¸ä¼¼åº¦ï¼ˆåŸºäºçš®å°”é€Šç›¸å…³ç³»æ•°ï¼‰
        user_matrix_centered = self.user_item_matrix.sub(self.user_means, axis=0).fillna(0)
        self.user_similarity = cosine_similarity(user_matrix_centered)
        
        # ç‰©å“ç›¸ä¼¼åº¦
        self.item_similarity = cosine_similarity(self.item_user_matrix.fillna(0))
    
    def user_based_recommend(self, user_id: str, n_recommendations: int = 10) -> List[RecommendationResult]:
        """åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤æ¨è"""
        if user_id not in self.user_item_matrix.index:
            return []
        
        user_idx = self.user_item_matrix.index.get_loc(user_id)
        user_ratings = self.user_item_matrix.iloc[user_idx]
        user_mean = self.user_means.iloc[user_idx]
        
        # æ‰¾åˆ°ç›¸ä¼¼ç”¨æˆ·
        user_similarities = self.user_similarity[user_idx]
        similar_users_idx = np.argsort(user_similarities)[::-1][1:51]  # å‰50ä¸ªç›¸ä¼¼ç”¨æˆ·
        
        # é¢„æµ‹è¯„åˆ†
        predictions = {}
        for item_id in self.user_item_matrix.columns:
            if user_ratings[item_id] > 0:  # å·²è¯„åˆ†ç‰©å“è·³è¿‡
                continue
            
            numerator = 0
            denominator = 0
            
            for similar_user_idx in similar_users_idx:
                similarity = user_similarities[similar_user_idx]
                if similarity <= 0:
                    continue
                
                similar_user_rating = self.user_item_matrix.iloc[similar_user_idx][item_id]
                if similar_user_rating > 0:
                    similar_user_mean = self.user_means.iloc[similar_user_idx]
                    numerator += similarity * (similar_user_rating - similar_user_mean)
                    denominator += abs(similarity)
            
            if denominator > 0:
                predicted_rating = user_mean + numerator / denominator
                predictions[item_id] = predicted_rating
        
        # æ’åºå¹¶è¿”å›æ¨èç»“æœ
        sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)
        
        recommendations = []
        for item_id, score in sorted_predictions[:n_recommendations]:
            recommendations.append(RecommendationResult(
                item_id=item_id,
                score=score,
                reason="åŸºäºç›¸ä¼¼ç”¨æˆ·çš„åå¥½",
                algorithm="user_based_cf"
            ))
        
        return recommendations
    
    def item_based_recommend(self, user_id: str, n_recommendations: int = 10) -> List[RecommendationResult]:
        """åŸºäºç‰©å“çš„ååŒè¿‡æ»¤æ¨è"""
        if user_id not in self.user_item_matrix.index:
            return []
        
        user_ratings = self.user_item_matrix.loc[user_id]
        rated_items = user_ratings[user_ratings > 0].index.tolist()
        
        if not rated_items:
            return []
        
        # è®¡ç®—å€™é€‰ç‰©å“çš„é¢„æµ‹è¯„åˆ†
        predictions = {}
        
        for candidate_item in self.user_item_matrix.columns:
            if candidate_item in rated_items:
                continue
            
            candidate_idx = self.item_user_matrix.index.get_loc(candidate_item)
            item_similarities = self.item_similarity[candidate_idx]
            
            numerator = 0
            denominator = 0
            
            for rated_item in rated_items:
                if rated_item not in self.item_user_matrix.index:
                    continue
                
                rated_item_idx = self.item_user_matrix.index.get_loc(rated_item)
                similarity = item_similarities[rated_item_idx]
                
                if similarity > 0:
                    user_rating = user_ratings[rated_item]
                    numerator += similarity * user_rating
                    denominator += abs(similarity)
            
            if denominator > 0:
                predicted_rating = numerator / denominator
                predictions[candidate_item] = predicted_rating
        
        # æ’åºå¹¶è¿”å›æ¨èç»“æœ
        sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)
        
        recommendations = []
        for item_id, score in sorted_predictions[:n_recommendations]:
            recommendations.append(RecommendationResult(
                item_id=item_id,
                score=score,
                reason="åŸºäºç›¸ä¼¼ç‰©å“çš„æ¨è",
                algorithm="item_based_cf"
            ))
        
        return recommendations
```

### æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹

```python
# deep_learning_models.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import Dict, List, Tuple

class DeepFMModel(nn.Module):
    """DeepFMæ¨èæ¨¡å‹"""
    
    def __init__(self, feature_dims: Dict[str, int], embedding_dim: int = 64, 
                 hidden_dims: List[int] = [256, 128, 64]):
        super(DeepFMModel, self).__init__()
        
        self.feature_dims = feature_dims
        self.embedding_dim = embedding_dim
        
        # ç‰¹å¾åµŒå…¥å±‚
        self.embeddings = nn.ModuleDict({
            name: nn.Embedding(dim, embedding_dim)
            for name, dim in feature_dims.items()
        })
        
        # FMéƒ¨åˆ†
        self.fm_first_order = nn.ModuleDict({
            name: nn.Embedding(dim, 1)
            for name, dim in feature_dims.items()
        })
        
        # Deepéƒ¨åˆ†
        total_embedding_dim = len(feature_dims) * embedding_dim
        deep_layers = []
        
        input_dim = total_embedding_dim
        for hidden_dim in hidden_dims:
            deep_layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            input_dim = hidden_dim
        
        deep_layers.append(nn.Linear(input_dim, 1))
        self.deep_layers = nn.Sequential(*deep_layers)
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(2, 1)  # FM + Deep
    
    def forward(self, features: Dict[str, torch.Tensor]) -> torch.Tensor:
        # FMä¸€é˜¶é¡¹
        fm_first_order_output = torch.sum(torch.cat([
            self.fm_first_order[name](features[name])
            for name in self.feature_dims.keys()
        ], dim=1), dim=1, keepdim=True)
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = [self.embeddings[name](features[name]) for name in self.feature_dims.keys()]
        embeddings_concat = torch.cat(embeddings, dim=1)  # [batch_size, num_features * embedding_dim]
        
        # FMäºŒé˜¶é¡¹
        square_of_sum = torch.sum(embeddings_concat, dim=1) ** 2
        sum_of_square = torch.sum(embeddings_concat ** 2, dim=1)
        fm_second_order_output = 0.5 * torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)
        
        # Deepéƒ¨åˆ†
        deep_input = embeddings_concat.view(embeddings_concat.size(0), -1)
        deep_output = self.deep_layers(deep_input)
        
        # ç»„åˆFMå’ŒDeep
        fm_output = fm_first_order_output + fm_second_order_output
        combined_output = torch.cat([fm_output, deep_output], dim=1)
        
        # æœ€ç»ˆè¾“å‡º
        output = torch.sigmoid(self.output_layer(combined_output))
        
        return output.squeeze()

class DINModel(nn.Module):
    """Deep Interest Network (DIN) æ¨¡å‹"""
    
    def __init__(self, item_num: int, cate_num: int, embedding_dim: int = 64):
        super(DINModel, self).__init__()
        
        self.embedding_dim = embedding_dim
        
        # åµŒå…¥å±‚
        self.item_embedding = nn.Embedding(item_num, embedding_dim)
        self.cate_embedding = nn.Embedding(cate_num, embedding_dim)
        
        # æ³¨æ„åŠ›ç½‘ç»œ
        self.attention_layers = nn.Sequential(
            nn.Linear(embedding_dim * 4, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
        
        # é¢„æµ‹ç½‘ç»œ
        self.prediction_layers = nn.Sequential(
            nn.Linear(embedding_dim * 3, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
    
    def forward(self, user_behavior_items: torch.Tensor, user_behavior_cates: torch.Tensor,
                target_item: torch.Tensor, target_cate: torch.Tensor) -> torch.Tensor:
        
        # è·å–åµŒå…¥
        behavior_item_emb = self.item_embedding(user_behavior_items)  # [batch_size, seq_len, emb_dim]
        behavior_cate_emb = self.cate_embedding(user_behavior_cates)  # [batch_size, seq_len, emb_dim]
        target_item_emb = self.item_embedding(target_item)  # [batch_size, emb_dim]
        target_cate_emb = self.cate_embedding(target_cate)  # [batch_size, emb_dim]
        
        # è¡Œä¸ºåºåˆ—åµŒå…¥
        behavior_emb = behavior_item_emb + behavior_cate_emb  # [batch_size, seq_len, emb_dim]
        target_emb = target_item_emb + target_cate_emb  # [batch_size, emb_dim]
        
        # æ³¨æ„åŠ›æœºåˆ¶
        target_emb_expanded = target_emb.unsqueeze(1).expand_as(behavior_emb)  # [batch_size, seq_len, emb_dim]
        
        # æ„å»ºæ³¨æ„åŠ›è¾“å…¥
        attention_input = torch.cat([
            behavior_emb,
            target_emb_expanded,
            behavior_emb * target_emb_expanded,
            behavior_emb - target_emb_expanded
        ], dim=-1)  # [batch_size, seq_len, emb_dim * 4]
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_scores = self.attention_layers(attention_input).squeeze(-1)  # [batch_size, seq_len]
        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, seq_len]
        
        # åŠ æƒèšåˆç”¨æˆ·å…´è¶£
        user_interest = torch.sum(behavior_emb * attention_weights.unsqueeze(-1), dim=1)  # [batch_size, emb_dim]
        
        # é¢„æµ‹
        prediction_input = torch.cat([user_interest, target_emb, user_interest * target_emb], dim=-1)
        output = self.prediction_layers(prediction_input)
        
        return output.squeeze()

class RecommendationDataset(Dataset):
    """æ¨èç³»ç»Ÿæ•°æ®é›†"""
    
    def __init__(self, interactions_df, item_features_df, user_features_df):
        self.interactions = interactions_df
        self.item_features = item_features_df
        self.user_features = user_features_df
    
    def __len__(self):
        return len(self.interactions)
    
    def __getitem__(self, idx):
        interaction = self.interactions.iloc[idx]
        
        user_id = interaction['user_id']
        item_id = interaction['item_id']
        rating = interaction['rating']
        
        # è·å–ç”¨æˆ·ç‰¹å¾
        user_features = self.user_features[self.user_features['user_id'] == user_id].iloc[0]
        
        # è·å–ç‰©å“ç‰¹å¾
        item_features = self.item_features[self.item_features['item_id'] == item_id].iloc[0]
        
        return {
            'user_id': torch.tensor(user_id, dtype=torch.long),
            'item_id': torch.tensor(item_id, dtype=torch.long),
            'user_age': torch.tensor(user_features['age'], dtype=torch.long),
            'user_gender': torch.tensor(user_features['gender'], dtype=torch.long),
            'item_category': torch.tensor(item_features['category'], dtype=torch.long),
            'rating': torch.tensor(rating, dtype=torch.float)
        }

class DeepRecommendationService:
    """æ·±åº¦å­¦ä¹ æ¨èæœåŠ¡"""
    
    def __init__(self, model_config: Dict):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_config = model_config
        self.models = {}
        
    def train_deepfm(self, train_loader: DataLoader, val_loader: DataLoader, 
                     feature_dims: Dict[str, int], epochs: int = 50):
        """è®­ç»ƒDeepFMæ¨¡å‹"""
        model = DeepFMModel(feature_dims).to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.BCELoss()
        
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            model.train()
            train_loss = 0
            
            for batch in train_loader:
                optimizer.zero_grad()
                
                features = {k: v.to(self.device) for k, v in batch.items() if k != 'rating'}
                targets = batch['rating'].to(self.device)
                
                outputs = model(features)
                loss = criterion(outputs, targets)
                
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # éªŒè¯
            model.eval()
            val_loss = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    features = {k: v.to(self.device) for k, v in batch.items() if k != 'rating'}
                    targets = batch['rating'].to(self.device)
                    
                    outputs = model(features)
                    loss = criterion(outputs, targets)
                    
                    val_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(model.state_dict(), 'best_deepfm_model.pth')
        
        self.models['deepfm'] = model
        return model
    
    def predict(self, model_name: str, features: Dict[str, torch.Tensor]) -> float:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if model_name not in self.models:
            raise ValueError(f"æ¨¡å‹ {model_name} æœªæ‰¾åˆ°")
        
        model = self.models[model_name]
        model.eval()
        
        with torch.no_grad():
            features = {k: v.to(self.device) for k, v in features.items()}
            output = model(features)
            return output.cpu().numpy()
    
    def get_recommendations(self, user_id: str, candidate_items: List[str], 
                          model_name: str = 'deepfm', top_k: int = 10) -> List[RecommendationResult]:
        """è·å–æ·±åº¦å­¦ä¹ æ¨¡å‹æ¨èç»“æœ"""
        recommendations = []
        
        for item_id in candidate_items:
            # æ„å»ºç‰¹å¾ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè·å–ç‰¹å¾ï¼‰
            features = self._build_features(user_id, item_id)
            
            # é¢„æµ‹è¯„åˆ†
            score = self.predict(model_name, features)
            
            recommendations.append(RecommendationResult(
                item_id=item_id,
                score=float(score),
                reason="åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹",
                algorithm=model_name
            ))
        
        # æ’åºå¹¶è¿”å›top-k
        recommendations.sort(key=lambda x: x.score, reverse=True)
        return recommendations[:top_k]
    
    def _build_features(self, user_id: str, item_id: str) -> Dict[str, torch.Tensor]:
        """æ„å»ºæ¨¡å‹è¾“å…¥ç‰¹å¾"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ç‰¹å¾å·¥ç¨‹æ¥å®ç°
        # ç¤ºä¾‹ç‰¹å¾æ„å»º
        return {
            'user_id': torch.tensor([int(user_id)], dtype=torch.long),
            'item_id': torch.tensor([int(item_id)], dtype=torch.long),
            'user_age': torch.tensor([25], dtype=torch.long),  # ç¤ºä¾‹å€¼
            'user_gender': torch.tensor([1], dtype=torch.long),  # ç¤ºä¾‹å€¼
            'item_category': torch.tensor([3], dtype=torch.long)  # ç¤ºä¾‹å€¼
        }
```

## 5.2.4 æ¨èæœåŠ¡é›†æˆä¸APIè®¾è®¡

### å¤šç­–ç•¥èåˆæ¨èå¼•æ“

```python
# recommendation_engine.py
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import asyncio
import numpy as np
from collections import defaultdict
import redis
import json

@dataclass
class RecommendationRequest:
    """æ¨èè¯·æ±‚"""
    user_id: str
    scenario: str  # homepage, related, category, search
    context: Dict[str, Any] = None
    num_recommendations: int = 10
    exclude_items: List[str] = None
    include_reasons: bool = True

@dataclass
class RecommendationResponse:
    """æ¨èå“åº”"""
    user_id: str
    recommendations: List[RecommendationResult]
    total_candidates: int
    processing_time_ms: float
    algorithms_used: List[str]
    timestamp: datetime

class HybridRecommendationEngine:
    """æ··åˆæ¨èå¼•æ“"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.redis_client = redis.Redis(**config['redis'])
        
        # åˆå§‹åŒ–å„ç§æ¨èç®—æ³•
        self.collaborative_filtering = CollaborativeFiltering()
        self.deep_learning_service = DeepRecommendationService(config['deep_learning'])
        self.content_based_recommender = ContentBasedRecommender()
        self.popularity_recommender = PopularityRecommender()
        
        # ç®—æ³•æƒé‡é…ç½®
        self.algorithm_weights = config.get('algorithm_weights', {
            'collaborative_filtering': 0.3,
            'deep_learning': 0.4,
            'content_based': 0.2,
            'popularity': 0.1
        })
        
        # å¤šæ ·æ€§å‚æ•°
        self.diversity_lambda = config.get('diversity_lambda', 0.1)
    
    async def get_recommendations(self, request: RecommendationRequest) -> RecommendationResponse:
        """è·å–æ··åˆæ¨èç»“æœ"""
        start_time = datetime.now()
        
        # è·å–ç”¨æˆ·ç”»åƒ
        user_profile = await self._get_user_profile(request.user_id)
        
        # æ ¹æ®åœºæ™¯é€‰æ‹©æ¨èç­–ç•¥
        algorithms_to_use = self._select_algorithms(request.scenario, user_profile)
        
        # å¹¶è¡Œæ‰§è¡Œå¤šç§æ¨èç®—æ³•
        algorithm_results = await self._run_algorithms_parallel(
            request, user_profile, algorithms_to_use
        )
        
        # èåˆæ¨èç»“æœ
        fused_recommendations = self._fuse_recommendations(
            algorithm_results, request.num_recommendations
        )
        
        # å¤šæ ·æ€§ä¼˜åŒ–
        diversified_recommendations = self._apply_diversity(
            fused_recommendations, request.num_recommendations
        )
        
        # è¿‡æ»¤å·²æ’é™¤çš„ç‰©å“
        if request.exclude_items:
            diversified_recommendations = [
                rec for rec in diversified_recommendations 
                if rec.item_id not in request.exclude_items
            ]
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (datetime.now() - start_time).total_seconds() * 1000
        
        # ç¼“å­˜æ¨èç»“æœ
        await self._cache_recommendations(request.user_id, diversified_recommendations)
        
        return RecommendationResponse(
            user_id=request.user_id,
            recommendations=diversified_recommendations[:request.num_recommendations],
            total_candidates=sum(len(results) for results in algorithm_results.values()),
            processing_time_ms=processing_time,
            algorithms_used=list(algorithms_to_use.keys()),
            timestamp=datetime.now()
        )
    
    def _select_algorithms(self, scenario: str, user_profile: Optional[UserProfile]) -> Dict[str, float]:
        """æ ¹æ®åœºæ™¯å’Œç”¨æˆ·ç”»åƒé€‰æ‹©ç®—æ³•"""
        base_weights = self.algorithm_weights.copy()
        
        # æ ¹æ®åœºæ™¯è°ƒæ•´æƒé‡
        if scenario == 'homepage':
            # é¦–é¡µæ¨èï¼Œå¹³è¡¡ä¸ªæ€§åŒ–å’Œå¤šæ ·æ€§
            pass
        elif scenario == 'related':
            # ç›¸å…³æ¨èï¼Œå¢åŠ å†…å®¹ç›¸ä¼¼æ€§æƒé‡
            base_weights['content_based'] *= 1.5
            base_weights['collaborative_filtering'] *= 0.8
        elif scenario == 'category':
            # åˆ†ç±»æ¨èï¼Œå¢åŠ å†…å®¹æƒé‡
            base_weights['content_based'] *= 1.3
        elif scenario == 'search':
            # æœç´¢æ¨èï¼Œä¸»è¦åŸºäºå†…å®¹
            base_weights['content_based'] *= 2.0
            base_weights['deep_learning'] *= 0.7
        
        # æ ¹æ®ç”¨æˆ·ç±»å‹è°ƒæ•´æƒé‡
        if user_profile:
            if user_profile.user_type == 'new':
                # æ–°ç”¨æˆ·ï¼Œå¢åŠ çƒ­é—¨æ¨èæƒé‡
                base_weights['popularity'] *= 2.0
                base_weights['collaborative_filtering'] *= 0.5
            elif user_profile.activity_level == 'high':
                # é«˜æ´»è·ƒç”¨æˆ·ï¼Œå¢åŠ ä¸ªæ€§åŒ–æƒé‡
                base_weights['deep_learning'] *= 1.2
                base_weights['collaborative_filtering'] *= 1.1
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(base_weights.values())
        return {k: v/total_weight for k, v in base_weights.items()}
    
    async def _run_algorithms_parallel(self, request: RecommendationRequest, 
                                     user_profile: Optional[UserProfile],
                                     algorithms: Dict[str, float]) -> Dict[str, List[RecommendationResult]]:
        """å¹¶è¡Œè¿è¡Œå¤šç§æ¨èç®—æ³•"""
        tasks = []
        
        if 'collaborative_filtering' in algorithms:
            tasks.append(self._run_collaborative_filtering(request.user_id, request.num_recommendations * 2))
        
        if 'deep_learning' in algorithms:
            tasks.append(self._run_deep_learning(request.user_id, request.num_recommendations * 2))
        
        if 'content_based' in algorithms:
            tasks.append(self._run_content_based(request, user_profile, request.num_recommendations * 2))
        
        if 'popularity' in algorithms:
            tasks.append(self._run_popularity_based(request.num_recommendations * 2))
        
        # å¹¶è¡Œæ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»„ç»‡ç»“æœ
        algorithm_results = {}
        algorithm_names = [name for name in algorithms.keys()]
        
        for i, result in enumerate(results):
            if not isinstance(result, Exception) and i < len(algorithm_names):
                algorithm_results[algorithm_names[i]] = result
            else:
                print(f"ç®—æ³• {algorithm_names[i] if i < len(algorithm_names) else 'unknown'} æ‰§è¡Œå¤±è´¥: {result}")
        
        return algorithm_results
    
    def _fuse_recommendations(self, algorithm_results: Dict[str, List[RecommendationResult]], 
                            num_recommendations: int) -> List[RecommendationResult]:
        """èåˆå¤šç§ç®—æ³•çš„æ¨èç»“æœ"""
        # æ”¶é›†æ‰€æœ‰æ¨èç‰©å“
        item_scores = defaultdict(list)
        
        for algorithm, recommendations in algorithm_results.items():
            weight = self.algorithm_weights.get(algorithm, 0.1)
            
            for rec in recommendations:
                item_scores[rec.item_id].append({
                    'score': rec.score * weight,
                    'algorithm': algorithm,
                    'reason': rec.reason
                })
        
        # è®¡ç®—èåˆåˆ†æ•°
        fused_recommendations = []
        
        for item_id, scores in item_scores.items():
            # åŠ æƒå¹³å‡åˆ†æ•°
            total_score = sum(s['score'] for s in scores)
            algorithms_used = [s['algorithm'] for s in scores]
            reasons = [s['reason'] for s in scores]
            
            fused_recommendations.append(RecommendationResult(
                item_id=item_id,
                score=total_score,
                reason=f"èåˆæ¨è: {', '.join(set(reasons))}",
                algorithm=f"hybrid({'+'.join(set(algorithms_used))})"
            ))
        
        # æŒ‰åˆ†æ•°æ’åº
        fused_recommendations.sort(key=lambda x: x.score, reverse=True)
        
        return fused_recommendations[:num_recommendations * 2]  # è¿”å›æ›´å¤šå€™é€‰ç”¨äºå¤šæ ·æ€§ä¼˜åŒ–
    
    def _apply_diversity(self, recommendations: List[RecommendationResult], 
                        num_recommendations: int) -> List[RecommendationResult]:
        """åº”ç”¨å¤šæ ·æ€§ä¼˜åŒ–"""
        if len(recommendations) <= num_recommendations:
            return recommendations
        
        # MMR (Maximal Marginal Relevance) ç®—æ³•
        selected = []
        remaining = recommendations.copy()
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªï¼ˆåˆ†æ•°æœ€é«˜çš„ï¼‰
        selected.append(remaining.pop(0))
        
        while len(selected) < num_recommendations and remaining:
            best_mmr_score = -1
            best_idx = 0
            
            for i, candidate in enumerate(remaining):
                # è®¡ç®—ä¸å·²é€‰æ‹©ç‰©å“çš„æœ€å¤§ç›¸ä¼¼åº¦
                max_similarity = 0
                for selected_item in selected:
                    similarity = self._calculate_item_similarity(candidate.item_id, selected_item.item_id)
                    max_similarity = max(max_similarity, similarity)
                
                # MMRåˆ†æ•° = Î» * ç›¸å…³æ€§ - (1-Î») * ç›¸ä¼¼åº¦
                mmr_score = (self.diversity_lambda * candidate.score - 
                           (1 - self.diversity_lambda) * max_similarity)
                
                if mmr_score > best_mmr_score:
                    best_mmr_score = mmr_score
                    best_idx = i
            
            selected.append(remaining.pop(best_idx))
        
        return selected
    
    def _calculate_item_similarity(self, item1: str, item2: str) -> float:
        """è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…å®ç°ä¸­åº”è¯¥åŸºäºç‰©å“ç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦
        # è¿™é‡Œè¿”å›éšæœºå€¼ä½œä¸ºç¤ºä¾‹
        return np.random.random() * 0.5  # 0-0.5ä¹‹é—´çš„ç›¸ä¼¼åº¦
    
    async def _get_user_profile(self, user_id: str) -> Optional[UserProfile]:
        """è·å–ç”¨æˆ·ç”»åƒ"""
        try:
            data = self.redis_client.get(f"user_profile:{user_id}")
            if data:
                profile_dict = json.loads(data)
                return self._dict_to_profile(profile_dict)
            return None
        except Exception as e:
            print(f"è·å–ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
            return None
    
    async def _cache_recommendations(self, user_id: str, recommendations: List[RecommendationResult]):
        """ç¼“å­˜æ¨èç»“æœ"""
        try:
            cache_data = {
                'recommendations': [asdict(rec) for rec in recommendations],
                'timestamp': datetime.now().isoformat()
            }
            
            # ç¼“å­˜1å°æ—¶
            self.redis_client.setex(
                f"recommendations:{user_id}",
                3600,
                json.dumps(cache_data, default=str)
            )
        except Exception as e:
            print(f"ç¼“å­˜æ¨èç»“æœå¤±è´¥: {e}")
    
    # å„ç§æ¨èç®—æ³•çš„å¼‚æ­¥åŒ…è£…æ–¹æ³•
    async def _run_collaborative_filtering(self, user_id: str, num_recs: int) -> List[RecommendationResult]:
        """è¿è¡ŒååŒè¿‡æ»¤ç®—æ³•"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„ååŒè¿‡æ»¤ç®—æ³•
            return self.collaborative_filtering.user_based_recommend(user_id, num_recs)
        except Exception as e:
            print(f"ååŒè¿‡æ»¤æ¨èå¤±è´¥: {e}")
            return []
    
    async def _run_deep_learning(self, user_id: str, num_recs: int) -> List[RecommendationResult]:
        """è¿è¡Œæ·±åº¦å­¦ä¹ ç®—æ³•"""
        try:
            # è·å–å€™é€‰ç‰©å“
            candidate_items = await self._get_candidate_items(user_id, num_recs * 5)
            return self.deep_learning_service.get_recommendations(user_id, candidate_items, 'deepfm', num_recs)
        except Exception as e:
            print(f"æ·±åº¦å­¦ä¹ æ¨èå¤±è´¥: {e}")
            return []
    
    async def _run_content_based(self, request: RecommendationRequest, 
                               user_profile: Optional[UserProfile], num_recs: int) -> List[RecommendationResult]:
        """è¿è¡ŒåŸºäºå†…å®¹çš„æ¨è"""
        try:
            return await self.content_based_recommender.recommend(request.user_id, user_profile, num_recs)
        except Exception as e:
            print(f"åŸºäºå†…å®¹çš„æ¨èå¤±è´¥: {e}")
            return []
    
    async def _run_popularity_based(self, num_recs: int) -> List[RecommendationResult]:
        """è¿è¡ŒåŸºäºçƒ­é—¨åº¦çš„æ¨è"""
        try:
            return await self.popularity_recommender.get_popular_items(num_recs)
        except Exception as e:
            print(f"çƒ­é—¨æ¨èå¤±è´¥: {e}")
            return []
    
    async def _get_candidate_items(self, user_id: str, num_candidates: int) -> List[str]:
        """è·å–å€™é€‰ç‰©å“"""
        # å®é™…å®ç°ä¸­åº”è¯¥ä»æ•°æ®åº“è·å–å€™é€‰ç‰©å“
        return [f"item_{i}" for i in range(num_candidates)]  # ç¤ºä¾‹å€™é€‰ç‰©å“
```

### FastAPIæ¨èæœåŠ¡æ¥å£

```python
# recommendation_api.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import asyncio
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="å†…å®¹æ¨èå¼•æ“API", version="1.0.0")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class RecommendationRequestModel(BaseModel):
    user_id: str
    scenario: str = "homepage"
    context: Optional[Dict[str, Any]] = None
    num_recommendations: int = 10
    exclude_items: Optional[List[str]] = None
    include_reasons: bool = True

class FeedbackModel(BaseModel):
    user_id: str
    item_id: str
    feedback_type: str  # click, like, dislike, share, purchase
    timestamp: Optional[datetime] = None
    context: Optional[Dict[str, Any]] = None

class ABTestModel(BaseModel):
    user_id: str
    experiment_id: str
    variant: str
    recommendations: List[str]

# å“åº”æ¨¡å‹
class RecommendationItemModel(BaseModel):
    item_id: str
    score: float
    reason: str
    algorithm: str

class RecommendationResponseModel(BaseModel):
    user_id: str
    recommendations: List[RecommendationItemModel]
    total_candidates: int
    processing_time_ms: float
    algorithms_used: List[str]
    timestamp: datetime

# å…¨å±€æ¨èå¼•æ“å®ä¾‹
recommendation_engine = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–æ¨èå¼•æ“"""
    global recommendation_engine
    
    config = {
        'redis': {
            'host': 'localhost',
            'port': 6379,
            'db': 0
        },
        'deep_learning': {
            'model_path': './models/'
        },
        'algorithm_weights': {
            'collaborative_filtering': 0.3,
            'deep_learning': 0.4,
            'content_based': 0.2,
            'popularity': 0.1
        },
        'diversity_lambda': 0.1
    }
    
    recommendation_engine = HybridRecommendationEngine(config)
    logger.info("æ¨èå¼•æ“åˆå§‹åŒ–å®Œæˆ")

@app.post("/recommendations", response_model=RecommendationResponseModel)
async def get_recommendations(request: RecommendationRequestModel):
    """è·å–ä¸ªæ€§åŒ–æ¨è"""
    try:
        # è½¬æ¢è¯·æ±‚æ ¼å¼
        rec_request = RecommendationRequest(
            user_id=request.user_id,
            scenario=request.scenario,
            context=request.context or {},
            num_recommendations=request.num_recommendations,
            exclude_items=request.exclude_items or [],
            include_reasons=request.include_reasons
        )
        
        # è·å–æ¨èç»“æœ
        response = await recommendation_engine.get_recommendations(rec_request)
        
        # è½¬æ¢å“åº”æ ¼å¼
        return RecommendationResponseModel(
            user_id=response.user_id,
            recommendations=[
                RecommendationItemModel(
                    item_id=rec.item_id,
                    score=rec.score,
                    reason=rec.reason,
                    algorithm=rec.algorithm
                ) for rec in response.recommendations
            ],
            total_candidates=response.total_candidates,
            processing_time_ms=response.processing_time_ms,
            algorithms_used=response.algorithms_used,
            timestamp=response.timestamp
        )
        
    except Exception as e:
        logger.error(f"æ¨èè¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/feedback")
async def record_feedback(feedback: FeedbackModel, background_tasks: BackgroundTasks):
    """è®°å½•ç”¨æˆ·åé¦ˆ"""
    try:
        # å¼‚æ­¥å¤„ç†åé¦ˆæ•°æ®
        background_tasks.add_task(process_feedback, feedback)
        
        return {"status": "success", "message": "åé¦ˆå·²è®°å½•"}
        
    except Exception as e:
        logger.error(f"è®°å½•åé¦ˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/user/{user_id}/profile")
async def get_user_profile(user_id: str):
    """è·å–ç”¨æˆ·ç”»åƒ"""
    try:
        profile = await recommendation_engine._get_user_profile(user_id)
        
        if not profile:
            raise HTTPException(status_code=404, detail="ç”¨æˆ·ç”»åƒæœªæ‰¾åˆ°")
        
        return {
            "user_id": profile.user_id,
            "interests": profile.interests,
            "behavior_patterns": profile.behavior_patterns,
            "preferences": profile.preferences,
            "activity_level": profile.activity_level,
            "user_type": profile.user_type,
            "last_updated": profile.last_updated
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ab-test")
async def record_ab_test(ab_test: ABTestModel):
    """è®°å½•A/Bæµ‹è¯•æ•°æ®"""
    try:
        # è®°å½•A/Bæµ‹è¯•æ•°æ®ç”¨äºåç»­åˆ†æ
        logger.info(f"A/Bæµ‹è¯•è®°å½•: ç”¨æˆ·{ab_test.user_id}, å®éªŒ{ab_test.experiment_id}, å˜ä½“{ab_test.variant}")
        
        return {"status": "success", "message": "A/Bæµ‹è¯•æ•°æ®å·²è®°å½•"}
        
    except Exception as e:
        logger.error(f"è®°å½•A/Bæµ‹è¯•æ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "timestamp": datetime.now(),
        "service": "recommendation-engine"
    }

@app.get("/metrics")
async def get_metrics():
    """è·å–æ¨èç³»ç»ŸæŒ‡æ ‡"""
    try:
        # è¿™é‡Œåº”è¯¥è¿”å›å®é™…çš„ç³»ç»ŸæŒ‡æ ‡
        return {
            "total_users": 10000,
            "total_items": 50000,
            "daily_recommendations": 100000,
            "avg_response_time_ms": 85,
            "recommendation_accuracy": 0.87,
            "user_engagement_rate": 0.34
        }
        
    except Exception as e:
        logger.error(f"è·å–æŒ‡æ ‡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_feedback(feedback: FeedbackModel):
    """å¤„ç†ç”¨æˆ·åé¦ˆï¼ˆåå°ä»»åŠ¡ï¼‰"""
    try:
        # åˆ›å»ºè¡Œä¸ºäº‹ä»¶
        behavior = UserBehavior(
            user_id=feedback.user_id,
            item_id=feedback.item_id,
            behavior_type=feedback.feedback_type,
            timestamp=feedback.timestamp or datetime.now(),
            context=feedback.context or {}
        )
        
        # æ›´æ–°ç”¨æˆ·ç”»åƒ
        # è¿™é‡Œåº”è¯¥è°ƒç”¨ç”¨æˆ·ç”»åƒæœåŠ¡æ›´æ–°
        logger.info(f"å¤„ç†ç”¨æˆ·åé¦ˆ: {feedback.user_id} -> {feedback.item_id} ({feedback.feedback_type})")
        
    except Exception as e:
        logger.error(f"å¤„ç†åé¦ˆå¤±è´¥: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## 5.2.5 æ¨èç³»ç»Ÿè¯„ä¼°ä¸ä¼˜åŒ–

### è¯„ä¼°æŒ‡æ ‡å®ç°

```python
# evaluation_metrics.py
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Set
from collections import defaultdict
import math

class RecommendationEvaluator:
    """æ¨èç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics_history = []
    
    def evaluate_recommendations(self, 
                               recommendations: Dict[str, List[str]],  # user_id -> [item_ids]
                               ground_truth: Dict[str, List[str]],     # user_id -> [actual_item_ids]
                               k_values: List[int] = [5, 10, 20]) -> Dict[str, float]:
        """è¯„ä¼°æ¨èç»“æœ"""
        
        metrics = {}
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        for k in k_values:
            metrics[f'precision@{k}'] = self.precision_at_k(recommendations, ground_truth, k)
            metrics[f'recall@{k}'] = self.recall_at_k(recommendations, ground_truth, k)
            metrics[f'f1@{k}'] = self.f1_at_k(recommendations, ground_truth, k)
            metrics[f'ndcg@{k}'] = self.ndcg_at_k(recommendations, ground_truth, k)
        
        # è®¡ç®—è¦†ç›–ç‡å’Œå¤šæ ·æ€§
        metrics['coverage'] = self.catalog_coverage(recommendations, ground_truth)
        metrics['diversity'] = self.intra_list_diversity(recommendations)
        metrics['novelty'] = self.novelty(recommendations, ground_truth)
        
        # è®°å½•è¯„ä¼°å†å²
        self.metrics_history.append({
            'timestamp': pd.Timestamp.now(),
            'metrics': metrics
        })
        
        return metrics
    
    def precision_at_k(self, recommendations: Dict[str, List[str]], 
                      ground_truth: Dict[str, List[str]], k: int) -> float:
        """è®¡ç®—Precision@K"""
        precisions = []
        
        for user_id in recommendations:
            if user_id not in ground_truth:
                continue
            
            rec_items = set(recommendations[user_id][:k])
            true_items = set(ground_truth[user_id])
            
            if len(rec_items) == 0:
                precisions.append(0.0)
            else:
                precision = len(rec_items & true_items) / len(rec_items)
                precisions.append(precision)
        
        return np.mean(precisions) if precisions else 0.0
    
    def recall_at_k(self, recommendations: Dict[str, List[str]], 
                   ground_truth: Dict[str, List[str]], k: int) -> float:
        """è®¡ç®—Recall@K"""
        recalls = []
        
        for user_id in recommendations:
            if user_id not in ground_truth:
                continue
            
            rec_items = set(recommendations[user_id][:k])
            true_items = set(ground_truth[user_id])
            
            if len(true_items) == 0:
                recalls.append(0.0)
            else:
                recall = len(rec_items & true_items) / len(true_items)
                recalls.append(recall)
        
        return np.mean(recalls) if recalls else 0.0
    
    def f1_at_k(self, recommendations: Dict[str, List[str]], 
               ground_truth: Dict[str, List[str]], k: int) -> float:
        """è®¡ç®—F1@K"""
        precision = self.precision_at_k(recommendations, ground_truth, k)
        recall = self.recall_at_k(recommendations, ground_truth, k)
        
        if precision + recall == 0:
            return 0.0
        
        return 2 * precision * recall / (precision + recall)
    
    def ndcg_at_k(self, recommendations: Dict[str, List[str]], 
                 ground_truth: Dict[str, List[str]], k: int) -> float:
        """è®¡ç®—NDCG@K"""
        ndcgs = []
        
        for user_id in recommendations:
            if user_id not in ground_truth:
                continue
            
            rec_items = recommendations[user_id][:k]
            true_items = set(ground_truth[user_id])
            
            # è®¡ç®—DCG
            dcg = 0.0
            for i, item in enumerate(rec_items):
                if item in true_items:
                    dcg += 1.0 / math.log2(i + 2)  # i+2 because log2(1) = 0
            
            # è®¡ç®—IDCG
            idcg = 0.0
            for i in range(min(len(true_items), k)):
                idcg += 1.0 / math.log2(i + 2)
            
            # è®¡ç®—NDCG
            if idcg > 0:
                ndcgs.append(dcg / idcg)
            else:
                ndcgs.append(0.0)
        
        return np.mean(ndcgs) if ndcgs else 0.0
    
    def catalog_coverage(self, recommendations: Dict[str, List[str]], 
                        ground_truth: Dict[str, List[str]]) -> float:
        """è®¡ç®—ç›®å½•è¦†ç›–ç‡"""
        # æ‰€æœ‰æ¨èçš„ç‰©å“
        recommended_items = set()
        for user_recs in recommendations.values():
            recommended_items.update(user_recs)
        
        # æ‰€æœ‰å¯èƒ½çš„ç‰©å“
        all_items = set()
        for user_items in ground_truth.values():
            all_items.update(user_items)
        for user_recs in recommendations.values():
            all_items.update(user_recs)
        
        if len(all_items) == 0:
            return 0.0
        
        return len(recommended_items) / len(all_items)
    
    def intra_list_diversity(self, recommendations: Dict[str, List[str]]) -> float:
        """è®¡ç®—æ¨èåˆ—è¡¨å†…å¤šæ ·æ€§"""
        diversities = []
        
        for user_id, rec_items in recommendations.items():
            if len(rec_items) <= 1:
                diversities.append(0.0)
                continue
            
            # è®¡ç®—ç‰©å“é—´çš„å¹³å‡è·ç¦»ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºä¸åŒç‰©å“çš„æ¯”ä¾‹ï¼‰
            unique_items = len(set(rec_items))
            diversity = unique_items / len(rec_items)
            diversities.append(diversity)
        
        return np.mean(diversities) if diversities else 0.0
    
    def novelty(self, recommendations: Dict[str, List[str]], 
               ground_truth: Dict[str, List[str]]) -> float:
        """è®¡ç®—æ–°é¢–æ€§"""
        # è®¡ç®—ç‰©å“æµè¡Œåº¦
        item_popularity = defaultdict(int)
        total_interactions = 0
        
        for user_items in ground_truth.values():
            for item in user_items:
                item_popularity[item] += 1
                total_interactions += 1
        
        # è®¡ç®—æ¨èçš„æ–°é¢–æ€§
        novelties = []
        
        for user_id, rec_items in recommendations.items():
            user_novelty = 0.0
            
            for item in rec_items:
                popularity = item_popularity.get(item, 0)
                if total_interactions > 0:
                    # æ–°é¢–æ€§ = -log2(popularity / total_interactions)
                    if popularity > 0:
                        novelty_score = -math.log2(popularity / total_interactions)
                    else:
                        novelty_score = math.log2(total_interactions)  # å®Œå…¨æ–°é¢–çš„ç‰©å“
                    user_novelty += novelty_score
            
            if len(rec_items) > 0:
                novelties.append(user_novelty / len(rec_items))
        
        return np.mean(novelties) if novelties else 0.0
    
    def generate_evaluation_report(self, metrics: Dict[str, float]) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = "\n=== æ¨èç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š ===\n\n"
        
        # å‡†ç¡®æ€§æŒ‡æ ‡
        report += "ğŸ“Š å‡†ç¡®æ€§æŒ‡æ ‡:\n"
        for k in [5, 10, 20]:
            if f'precision@{k}' in metrics:
                report += f"  Precision@{k}: {metrics[f'precision@{k}']:.4f}\n"
                report += f"  Recall@{k}: {metrics[f'recall@{k}']:.4f}\n"
                report += f"  F1@{k}: {metrics[f'f1@{k}']:.4f}\n"
                report += f"  NDCG@{k}: {metrics[f'ndcg@{k}']:.4f}\n\n"
        
        # å¤šæ ·æ€§æŒ‡æ ‡
        report += "ğŸ¯ å¤šæ ·æ€§æŒ‡æ ‡:\n"
        report += f"  ç›®å½•è¦†ç›–ç‡: {metrics.get('coverage', 0):.4f}\n"
        report += f"  åˆ—è¡¨å†…å¤šæ ·æ€§: {metrics.get('diversity', 0):.4f}\n"
        report += f"  æ–°é¢–æ€§: {metrics.get('novelty', 0):.4f}\n\n"
        
        # æ€§èƒ½è¯„ä¼°
        report += "âš¡ æ€§èƒ½å»ºè®®:\n"
        
        precision_10 = metrics.get('precision@10', 0)
        if precision_10 < 0.1:
            report += "  âš ï¸  Precision@10è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–æ¨èç®—æ³•\n"
        elif precision_10 > 0.2:
            report += "  âœ… Precision@10è¡¨ç°è‰¯å¥½\n"
        
        coverage = metrics.get('coverage', 0)
        if coverage < 0.1:
            report += "  âš ï¸  ç›®å½•è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ¨èå¤šæ ·æ€§\n"
        elif coverage > 0.3:
            report += "  âœ… ç›®å½•è¦†ç›–ç‡è¡¨ç°è‰¯å¥½\n"
        
        return report

# A/Bæµ‹è¯•æ¡†æ¶
class ABTestFramework:
    """A/Bæµ‹è¯•æ¡†æ¶"""
    
    def __init__(self):
        self.experiments = {}
        self.results = defaultdict(list)
    
    def create_experiment(self, experiment_id: str, variants: List[str], 
                         traffic_split: Dict[str, float]):
        """åˆ›å»ºA/Bæµ‹è¯•å®éªŒ"""
        self.experiments[experiment_id] = {
            'variants': variants,
            'traffic_split': traffic_split,
            'created_at': pd.Timestamp.now()
        }
    
    def assign_variant(self, experiment_id: str, user_id: str) -> str:
        """ä¸ºç”¨æˆ·åˆ†é…å®éªŒå˜ä½“"""
        if experiment_id not in self.experiments:
            return 'control'
        
        # åŸºäºç”¨æˆ·IDçš„å“ˆå¸Œå€¼åˆ†é…å˜ä½“
        user_hash = hash(user_id) % 100
        cumulative_prob = 0
        
        for variant, prob in self.experiments[experiment_id]['traffic_split'].items():
            cumulative_prob += prob * 100
            if user_hash < cumulative_prob:
                return variant
        
        return 'control'
    
    def record_result(self, experiment_id: str, user_id: str, variant: str, 
                     metric_name: str, metric_value: float):
        """è®°å½•å®éªŒç»“æœ"""
        self.results[experiment_id].append({
            'user_id': user_id,
            'variant': variant,
            'metric_name': metric_name,
            'metric_value': metric_value,
            'timestamp': pd.Timestamp.now()
        })
    
    def analyze_experiment(self, experiment_id: str) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        if experiment_id not in self.results:
            return {'error': 'å®éªŒç»“æœä¸å­˜åœ¨'}
        
        results_df = pd.DataFrame(self.results[experiment_id])
        
        # æŒ‰å˜ä½“åˆ†ç»„åˆ†æ
        analysis = {}
        
        for variant in results_df['variant'].unique():
            variant_data = results_df[results_df['variant'] == variant]
            
            analysis[variant] = {
                'sample_size': len(variant_data),
                'metrics': {}
            }
            
            for metric in variant_data['metric_name'].unique():
                metric_data = variant_data[variant_data['metric_name'] == metric]['metric_value']
                
                analysis[variant]['metrics'][metric] = {
                    'mean': metric_data.mean(),
                    'std': metric_data.std(),
                    'count': len(metric_data)
                }
        
        return analysis
```

## 5.2.6 Trae AIå®è·µæŒ‡å—

### åœ¨Traeä¸­æ„å»ºæ¨èç³»ç»Ÿ

**æ­¥éª¤1: ç¯å¢ƒå‡†å¤‡**

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir recommendation_engine
cd recommendation_engine

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install fastapi uvicorn pandas numpy scikit-learn torch redis kafka-python
```

**æ­¥éª¤2: æ•°æ®å‡†å¤‡**

åœ¨Traeä¸­åˆ›å»ºæ•°æ®å¤„ç†è„šæœ¬ï¼š

```python
# data_preparation.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def generate_sample_data():
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•"""
    
    # ç”Ÿæˆç”¨æˆ·æ•°æ®
    users = pd.DataFrame({
        'user_id': range(1000),
        'age': np.random.randint(18, 65, 1000),
        'gender': np.random.choice(['M', 'F'], 1000),
        'registration_date': pd.date_range('2023-01-01', periods=1000, freq='H')
    })
    
    # ç”Ÿæˆç‰©å“æ•°æ®
    items = pd.DataFrame({
        'item_id': range(5000),
        'category': np.random.choice(['tech', 'sports', 'entertainment', 'news'], 5000),
        'publish_date': pd.date_range('2023-01-01', periods=5000, freq='30min'),
        'popularity_score': np.random.exponential(2, 5000)
    })
    
    # ç”Ÿæˆäº¤äº’æ•°æ®
    interactions = []
    for _ in range(50000):
        user_id = np.random.randint(0, 1000)
        item_id = np.random.randint(0, 5000)
        rating = np.random.choice([1, 2, 3, 4, 5], p=[0.1, 0.1, 0.2, 0.3, 0.3])
        timestamp = datetime.now() - timedelta(days=np.random.randint(0, 30))
        
        interactions.append({
            'user_id': user_id,
            'item_id': item_id,
            'rating': rating,
            'timestamp': timestamp
        })
    
    interactions_df = pd.DataFrame(interactions)
    
    return users, items, interactions_df

if __name__ == "__main__":
    users, items, interactions = generate_sample_data()
    
    # ä¿å­˜æ•°æ®
    users.to_csv('users.csv', index=False)
    items.to_csv('items.csv', index=False)
    interactions.to_csv('interactions.csv', index=False)
    
    print("ç¤ºä¾‹æ•°æ®ç”Ÿæˆå®Œæˆï¼")
```

**æ­¥éª¤3: åœ¨Traeä¸­è¿è¡Œæ¨èæœåŠ¡**

```python
# main.py - Traeä¸­çš„ä¸»å¯åŠ¨æ–‡ä»¶
import asyncio
from recommendation_api import app
import uvicorn

def main():
    """å¯åŠ¨æ¨èæœåŠ¡"""
    print("ğŸš€ å¯åŠ¨å†…å®¹æ¨èå¼•æ“...")
    
    # åœ¨Traeä¸­è¿è¡ŒFastAPIæœåŠ¡
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        reload=True,  # å¼€å‘æ¨¡å¼ä¸‹å¯ç”¨çƒ­é‡è½½
        log_level="info"
    )

if __name__ == "__main__":
    main()
```

**æ­¥éª¤4: Traeä¸­çš„æµ‹è¯•å’Œè°ƒè¯•**

```python
# test_recommendations.py
import requests
import json
from datetime import datetime

def test_recommendation_api():
    """æµ‹è¯•æ¨èAPI"""
    base_url = "http://localhost:8000"
    
    # æµ‹è¯•è·å–æ¨è
    print("ğŸ“‹ æµ‹è¯•è·å–æ¨è...")
    response = requests.post(f"{base_url}/recommendations", json={
        "user_id": "user_123",
        "scenario": "homepage",
        "num_recommendations": 10
    })
    
    if response.status_code == 200:
        recommendations = response.json()
        print(f"âœ… è·å–åˆ° {len(recommendations['recommendations'])} ä¸ªæ¨è")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {recommendations['processing_time_ms']:.2f}ms")
        
        for i, rec in enumerate(recommendations['recommendations'][:3]):
            print(f"  {i+1}. {rec['item_id']} (åˆ†æ•°: {rec['score']:.3f})")
    else:
        print(f"âŒ æ¨èè¯·æ±‚å¤±è´¥: {response.status_code}")
    
    # æµ‹è¯•ç”¨æˆ·åé¦ˆ
    print("\nğŸ“ æµ‹è¯•ç”¨æˆ·åé¦ˆ...")
    feedback_response = requests.post(f"{base_url}/feedback", json={
        "user_id": "user_123",
        "item_id": "item_456",
        "feedback_type": "like",
        "timestamp": datetime.now().isoformat()
    })
    
    if feedback_response.status_code == 200:
        print("âœ… åé¦ˆè®°å½•æˆåŠŸ")
    else:
        print(f"âŒ åé¦ˆè®°å½•å¤±è´¥: {feedback_response.status_code}")
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥
    print("\nğŸ¥ æµ‹è¯•å¥åº·æ£€æŸ¥...")
    health_response = requests.get(f"{base_url}/health")
    
    if health_response.status_code == 200:
        health_data = health_response.json()
        print(f"âœ… æœåŠ¡å¥åº·çŠ¶æ€: {health_data['status']}")
    else:
        print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {health_response.status_code}")

if __name__ == "__main__":
    test_recommendation_api()
```

**æ­¥éª¤5: Traeä¸­çš„æ€§èƒ½ç›‘æ§**

```python
# monitoring.py
import time
import psutil
import threading
from datetime import datetime

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = []
        self.monitoring = False
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        monitor_thread = threading.Thread(target=self._monitor_loop)
        monitor_thread.daemon = True
        monitor_thread.start()
        print("ğŸ“Š æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        print("ğŸ“Š æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
            cpu_percent = psutil.cpu_percent()
            memory = psutil.virtual_memory()
            
            metric = {
                'timestamp': datetime.now(),
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_used_mb': memory.used / 1024 / 1024
            }
            
            self.metrics.append(metric)
            
            # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
            if len(self.metrics) > 1000:
                self.metrics = self.metrics[-1000:]
            
            time.sleep(5)  # æ¯5ç§’æ”¶é›†ä¸€æ¬¡
    
    def get_current_metrics(self):
        """è·å–å½“å‰æŒ‡æ ‡"""
        if not self.metrics:
            return None
        
        latest = self.metrics[-1]
        
        # è®¡ç®—æœ€è¿‘10åˆ†é’Ÿçš„å¹³å‡å€¼
        recent_metrics = [
            m for m in self.metrics 
            if (datetime.now() - m['timestamp']).seconds <= 600
        ]
        
        if recent_metrics:
            avg_cpu = sum(m['cpu_percent'] for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m['memory_percent'] for m in recent_metrics) / len(recent_metrics)
        else:
            avg_cpu = latest['cpu_percent']
            avg_memory = latest['memory_percent']
        
        return {
            'current_cpu': latest['cpu_percent'],
            'current_memory': latest['memory_percent'],
            'avg_cpu_10min': avg_cpu,
            'avg_memory_10min': avg_memory,
            'total_metrics_collected': len(self.metrics)
        }

# åœ¨Traeä¸­ä½¿ç”¨ç›‘æ§å™¨
if __name__ == "__main__":
    monitor = PerformanceMonitor()
    monitor.start_monitoring()
    
    try:
        while True:
            time.sleep(30)
            metrics = monitor.get_current_metrics()
            if metrics:
                print(f"CPU: {metrics['current_cpu']:.1f}%, å†…å­˜: {metrics['current_memory']:.1f}%")
    except KeyboardInterrupt:
        monitor.stop_monitoring()
        print("ç›‘æ§å·²åœæ­¢")
```

### é¡¹ç›®æ€»ç»“

æœ¬å†…å®¹æ¨èå¼•æ“é¡¹ç›®å®ç°äº†ï¼š

**ğŸ¯ æ ¸å¿ƒåŠŸèƒ½**:
- å¤šç­–ç•¥èåˆæ¨èç®—æ³•
- å®æ—¶ç”¨æˆ·ç”»åƒæ„å»º
- ä¸ªæ€§åŒ–æ¨èæœåŠ¡
- A/Bæµ‹è¯•æ¡†æ¶
- æ€§èƒ½ç›‘æ§ç³»ç»Ÿ

**ğŸ› ï¸ æŠ€æœ¯ç‰¹ç‚¹**:
- å¼‚æ­¥å¤„ç†æ¶æ„
- å¾®æœåŠ¡è®¾è®¡æ¨¡å¼
- å®æ—¶æ•°æ®æµå¤„ç†
- æœºå™¨å­¦ä¹ æ¨¡å‹é›†æˆ
- RESTful APIæ¥å£

**ğŸ“ˆ ä¸šåŠ¡ä»·å€¼**:
- æå‡ç”¨æˆ·ä½“éªŒ
- å¢åŠ ç”¨æˆ·ç²˜æ€§
- ä¼˜åŒ–å†…å®¹åˆ†å‘
- æ”¯æŒä¸šåŠ¡å†³ç­–
- å®ç°ç²¾å‡†è¥é”€

**ğŸ”§ Traeå®è·µè¦ç‚¹**:
1. ä½¿ç”¨Traeçš„ä»£ç è¡¥å…¨åŠŸèƒ½å¿«é€Ÿå¼€å‘
2. åˆ©ç”¨Traeçš„è°ƒè¯•å·¥å…·è¿›è¡Œé—®é¢˜æ’æŸ¥
3. é€šè¿‡Traeçš„ç‰ˆæœ¬æ§åˆ¶ç®¡ç†ä»£ç å˜æ›´
4. ä½¿ç”¨Traeçš„æ€§èƒ½åˆ†æå·¥å…·ä¼˜åŒ–ç³»ç»Ÿ
5. å€ŸåŠ©Traeçš„åä½œåŠŸèƒ½è¿›è¡Œå›¢é˜Ÿå¼€å‘