# 3.1 预训练技术

## 学习目标

通过本节学习，你将能够：
- 深入理解自监督学习的核心思想和技术原理
- 掌握掩码语言模型(MLM)和自回归语言模型的设计理念
- 了解预训练数据的准备和处理流程
- 认识大模型预训练的计算资源需求和优化策略
- 在Trae中实现和体验不同的预训练方法

---

## 3.1.1 自监督学习原理

### 核心概念解析

自监督学习是现代大模型成功的基石，它巧妙地从数据本身构造监督信号，无需人工标注即可学习到丰富的语言表示。

**自监督学习的定义**：
- 利用数据内在结构和规律作为监督信号
- 通过预测任务学习数据的潜在表示
- 无需外部标注，降低数据获取成本

### 学习范式对比分析

让我们通过Python代码来可视化不同学习范式的特点：

```python
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.patches import Rectangle
import seaborn as sns

# 设置中文字体和样式
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# 创建学习范式对比图
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('机器学习范式对比分析', fontsize=16, fontweight='bold')

# 监督学习
ax1 = axes[0, 0]
ax1.set_title('监督学习', fontsize=14, fontweight='bold')
data_sizes = [100, 1000, 10000]
performance = [60, 75, 85]
ax1.plot(data_sizes, performance, 'o-', linewidth=3, markersize=8, color='#2E86AB')
ax1.set_xlabel('标注数据量')
ax1.set_ylabel('模型性能(%)')
ax1.grid(True, alpha=0.3)
ax1.text(5000, 70, '需要大量\n标注数据', fontsize=10, 
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.7))

# 无监督学习
ax2 = axes[0, 1]
ax2.set_title('无监督学习', fontsize=14, fontweight='bold')
clusters = ['聚类1', '聚类2', '聚类3', '聚类4']
cluster_sizes = [25, 35, 20, 20]
colors = ['#A23B72', '#F18F01', '#C73E1D', '#592E83']
ax2.pie(cluster_sizes, labels=clusters, colors=colors, autopct='%1.1f%%')
ax2.text(0, -1.5, '发现数据模式\n无标签要求', fontsize=10, ha='center',
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightcoral', alpha=0.7))

# 自监督学习
ax3 = axes[1, 0]
ax3.set_title('自监督学习', fontsize=14, fontweight='bold')
tasks = ['掩码预测', '下词预测', '句子排序', '对比学习']
effectiveness = [85, 90, 75, 80]
bars = ax3.bar(tasks, effectiveness, color=['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])
ax3.set_ylabel('任务有效性(%)')
ax3.set_xticklabels(tasks, rotation=45, ha='right')
for bar, val in zip(bars, effectiveness):
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
             f'{val}%', ha='center', va='bottom', fontweight='bold')

# 强化学习
ax4 = axes[1, 1]
ax4.set_title('强化学习', fontsize=14, fontweight='bold')
episodes = np.arange(0, 1000, 50)
reward = 100 * (1 - np.exp(-episodes/300)) + np.random.normal(0, 5, len(episodes))
ax4.plot(episodes, reward, color='#E17055', linewidth=2)
ax4.fill_between(episodes, reward, alpha=0.3, color='#E17055')
ax4.set_xlabel('训练轮数')
ax4.set_ylabel('累积奖励')
ax4.text(500, 50, '通过试错\n学习策略', fontsize=10,
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightsalmon', alpha=0.7))

plt.tight_layout()
plt.show()

# 自监督学习优势分析
print("\n=== 自监督学习的核心优势 ===")
advantages = {
    "数据利用效率": "可以利用海量无标注文本数据",
    "通用表示学习": "学习到的特征可以迁移到多个下游任务",
    "成本效益": "无需昂贵的人工标注过程",
    "规模化能力": "数据量越大，模型性能越好",
    "知识获取": "从文本中学习世界知识和常识"
}

for advantage, description in advantages.items():
    print(f"• {advantage}: {description}")
```

### 常见自监督任务类型

让我们深入分析各种自监督学习任务：

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 自监督任务特性分析
task_data = {
    '任务类型': ['语言模型', '掩码预测', '句子排序', '对比学习', '替换检测', '下句预测'],
    '计算复杂度': [3, 4, 2, 5, 3, 2],
    '学习效果': [5, 4, 3, 4, 3, 2],
    '适用场景': [5, 4, 3, 4, 3, 3],
    '实现难度': [2, 3, 2, 4, 2, 2]
}

df = pd.DataFrame(task_data)

# 创建雷达图
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# 任务特性热力图
sns.heatmap(df.set_index('任务类型').T, annot=True, cmap='YlOrRd', 
            ax=ax1, cbar_kws={'label': '评分(1-5)'})
ax1.set_title('自监督学习任务特性对比', fontsize=14, fontweight='bold')
ax1.set_xlabel('任务类型')
ax1.set_ylabel('评估维度')

# 任务流行度趋势
years = list(range(2018, 2025))
task_popularity = {
    '语言模型': [20, 40, 60, 80, 90, 95, 98],
    '掩码预测': [10, 80, 85, 75, 70, 65, 60],
    '对比学习': [5, 10, 30, 50, 70, 80, 85],
    '句子排序': [15, 25, 20, 15, 10, 8, 5]
}

for task, popularity in task_popularity.items():
    ax2.plot(years, popularity, marker='o', linewidth=2, label=task)

ax2.set_title('自监督学习任务发展趋势', fontsize=14, fontweight='bold')
ax2.set_xlabel('年份')
ax2.set_ylabel('流行度(%)')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 详细任务分析
print("\n=== 自监督学习任务详细分析 ===")
task_details = {
    "语言模型(LM)": {
        "原理": "基于前文预测下一个词",
        "优势": "自然的生成能力，适合对话和创作",
        "代表模型": "GPT系列",
        "适用场景": "文本生成、对话系统"
    },
    "掩码语言模型(MLM)": {
        "原理": "预测被遮挡的词汇",
        "优势": "双向上下文，更好的理解能力",
        "代表模型": "BERT系列",
        "适用场景": "文本分类、信息抽取"
    },
    "对比学习": {
        "原理": "学习相似和不相似样本的区别",
        "优势": "学习高质量表示，泛化能力强",
        "代表模型": "SimCLR、CLIP",
        "适用场景": "多模态学习、零样本分类"
    }
}

for task, details in task_details.items():
    print(f"\n【{task}】")
    for key, value in details.items():
        print(f"  {key}: {value}")
```

### 生活化类比理解

为了更好地理解自监督学习，让我们用生活中的例子来类比：

```python
# 生活化类比可视化
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('自监督学习的生活化类比', fontsize=16, fontweight='bold')

# 类比1：学习阅读
ax1 = axes[0, 0]
ax1.set_title('学习阅读：通过上下文猜词', fontsize=12, fontweight='bold')
sentence = "我喜欢吃___果"
context_words = ['苹', '橙', '葡萄', '芒']
probs = [0.4, 0.3, 0.2, 0.1]
bars = ax1.bar(context_words, probs, color=['red', 'orange', 'purple', 'yellow'])
ax1.set_ylabel('预测概率')
ax1.set_title(f'句子: "{sentence}"', fontsize=10)
for bar, prob in zip(bars, probs):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{prob:.1f}', ha='center', va='bottom')

# 类比2：拼图游戏
ax2 = axes[0, 1]
ax2.set_title('拼图游戏：根据已有片段推断缺失', fontsize=12, fontweight='bold')
# 创建拼图网格
puzzle = np.ones((4, 4))
puzzle[1:3, 1:3] = 0  # 缺失的部分
im = ax2.imshow(puzzle, cmap='RdYlBu', alpha=0.8)
ax2.set_xticks([])
ax2.set_yticks([])
ax2.text(1.5, 1.5, '?', fontsize=24, ha='center', va='center', fontweight='bold')
ax2.text(2, -0.5, '根据周围片段\n推断中心内容', ha='center', fontsize=10,
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.7))

# 类比3：语言习得
ax3 = axes[1, 0]
ax3.set_title('儿童语言习得：模仿和预测', fontsize=12, fontweight='bold')
age_months = [12, 18, 24, 30, 36, 42]
vocabulary = [10, 50, 200, 500, 1000, 2000]
ax3.plot(age_months, vocabulary, 'o-', linewidth=3, markersize=8, color='green')
ax3.set_xlabel('年龄(月)')
ax3.set_ylabel('词汇量')
ax3.fill_between(age_months, vocabulary, alpha=0.3, color='green')
ax3.text(30, 800, '通过听和模仿\n自然学习语言', fontsize=10,
         bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgreen', alpha=0.7))

# 类比4：专业技能学习
ax4 = axes[1, 1]
ax4.set_title('专业技能：从基础到专精', fontsize=12, fontweight='bold')
skills = ['基础\n语法', '词汇\n积累', '语义\n理解', '推理\n能力', '创作\n能力']
levels = [1, 2, 3, 4, 5]
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
bars = ax4.bar(skills, levels, color=colors)
ax4.set_ylabel('掌握程度')
for i, (bar, level) in enumerate(zip(bars, levels)):
    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
             f'L{level}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

print("\n=== 自监督学习的生活化理解 ===")
analogies = {
    "学习阅读": "就像我们学习阅读时，遇到不认识的字会根据上下文猜测含义",
    "拼图游戏": "类似拼图时，根据已有的片段推断缺失部分的样子",
    "语言习得": "如同儿童通过大量听说练习自然习得语言规律",
    "技能积累": "像学习任何技能一样，从基础逐步建立复杂能力"
}

for analogy, description in analogies.items():
    print(f"• {analogy}: {description}")
```

---

## 3.1.2 掩码语言模型(MLM)

### MLM的核心思想

掩码语言模型是BERT等双向模型的核心预训练方法，通过随机遮挡输入中的部分token，让模型学会利用双向上下文进行预测。

### MLM的掩码策略详解

让我们通过代码深入理解MLM的掩码策略：

```python
import random
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

class MLMProcessor:
    def __init__(self, vocab_size=30000, mask_prob=0.15):
        self.vocab_size = vocab_size
        self.mask_prob = mask_prob
        self.mask_token = "[MASK]"
        self.vocab = {f"word_{i}": i for i in range(vocab_size)}
        self.vocab[self.mask_token] = vocab_size
        
    def apply_masking(self, tokens):
        """应用BERT式的掩码策略"""
        masked_tokens = tokens.copy()
        labels = [-100] * len(tokens)  # -100表示不计算损失
        
        # 选择需要掩码的位置
        num_to_mask = max(1, int(len(tokens) * self.mask_prob))
        mask_indices = random.sample(range(len(tokens)), num_to_mask)
        
        for idx in mask_indices:
            labels[idx] = tokens[idx]  # 保存原始token作为标签
            
            rand = random.random()
            if rand < 0.8:
                # 80%的概率替换为[MASK]
                masked_tokens[idx] = self.mask_token
            elif rand < 0.9:
                # 10%的概率替换为随机token
                masked_tokens[idx] = f"word_{random.randint(0, self.vocab_size-1)}"
            # 10%的概率保持不变
            
        return masked_tokens, labels
    
    def visualize_masking_strategy(self):
        """可视化掩码策略"""
        strategies = ['替换为[MASK]', '替换为随机词', '保持不变']
        probabilities = [0.8, 0.1, 0.1]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        # 掩码策略饼图
        ax1.pie(probabilities, labels=strategies, colors=colors, autopct='%1.1f%%',
                startangle=90, textprops={'fontsize': 12})
        ax1.set_title('MLM掩码策略分布', fontsize=14, fontweight='bold')
        
        # 掩码效果示例
        original_text = "我 喜欢 在 春天 的 时候 去 公园 散步"
        tokens = original_text.split()
        masked_tokens, labels = self.apply_masking(tokens)
        
        # 创建对比显示
        y_pos = [1, 0]
        ax2.set_ylim(-0.5, 1.5)
        ax2.set_xlim(-0.5, len(tokens) - 0.5)
        
        # 显示原始文本
        for i, token in enumerate(tokens):
            ax2.text(i, 1, token, ha='center', va='center', fontsize=10,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.7))
        
        # 显示掩码后文本
        for i, (token, label) in enumerate(zip(masked_tokens, labels)):
            color = 'lightcoral' if label != -100 else 'lightgray'
            ax2.text(i, 0, token, ha='center', va='center', fontsize=10,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor=color, alpha=0.7))
        
        ax2.text(-0.3, 1, '原始:', ha='right', va='center', fontweight='bold')
        ax2.text(-0.3, 0, '掩码:', ha='right', va='center', fontweight='bold')
        ax2.set_title('MLM掩码示例', fontsize=14, fontweight='bold')
        ax2.set_xticks([])
        ax2.set_yticks([])
        ax2.spines['top'].set_visible(False)
        ax2.spines['right'].set_visible(False)
        ax2.spines['bottom'].set_visible(False)
        ax2.spines['left'].set_visible(False)
        
        plt.tight_layout()
        plt.show()
        
        return masked_tokens, labels

# 实例化并演示
mlm_processor = MLMProcessor()
masked_tokens, labels = mlm_processor.visualize_masking_strategy()

print("\n=== MLM掩码策略分析 ===")
print(f"掩码后的tokens: {masked_tokens}")
print(f"对应的标签: {labels}")
print("\n策略解释:")
print("• 80%替换为[MASK]: 让模型学会处理特殊标记")
print("• 10%替换为随机词: 增强模型的鲁棒性")
print("• 10%保持不变: 减少预训练和微调的分布差异")
```

### 双向上下文的优势分析

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import seaborn as sns

class AttentionVisualizer:
    def __init__(self):
        self.seq_len = 8
        
    def create_attention_masks(self):
        """创建不同类型的注意力掩码"""
        # 双向注意力（BERT式）
        bidirectional_mask = torch.ones(self.seq_len, self.seq_len)
        
        # 单向注意力（GPT式）
        unidirectional_mask = torch.tril(torch.ones(self.seq_len, self.seq_len))
        
        return bidirectional_mask, unidirectional_mask
    
    def visualize_attention_patterns(self):
        """可视化不同的注意力模式"""
        bi_mask, uni_mask = self.create_attention_masks()
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        # 双向注意力
        sns.heatmap(bi_mask.numpy(), annot=True, cmap='Blues', 
                   ax=axes[0], cbar=False, square=True)
        axes[0].set_title('双向注意力 (BERT/MLM)', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Key位置')
        axes[0].set_ylabel('Query位置')
        
        # 单向注意力
        sns.heatmap(uni_mask.numpy(), annot=True, cmap='Reds', 
                   ax=axes[1], cbar=False, square=True)
        axes[1].set_title('单向注意力 (GPT/自回归)', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Key位置')
        axes[1].set_ylabel('Query位置')
        
        # 性能对比
        tasks = ['文本分类', '命名实体识别', '问答系统', '文本生成', '机器翻译']
        bert_scores = [92, 89, 85, 70, 82]
        gpt_scores = [88, 82, 78, 95, 88]
        
        x = np.arange(len(tasks))
        width = 0.35
        
        bars1 = axes[2].bar(x - width/2, bert_scores, width, label='BERT(双向)', color='#4ECDC4')
        bars2 = axes[2].bar(x + width/2, gpt_scores, width, label='GPT(单向)', color='#FF6B6B')
        
        axes[2].set_xlabel('任务类型')
        axes[2].set_ylabel('性能分数')
        axes[2].set_title('双向vs单向模型性能对比', fontsize=14, fontweight='bold')
        axes[2].set_xticks(x)
        axes[2].set_xticklabels(tasks, rotation=45, ha='right')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
        
        # 添加数值标签
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                           f'{height}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.show()

# 可视化注意力模式
visualizer = AttentionVisualizer()
visualizer.visualize_attention_patterns()

print("\n=== 双向上下文的优势分析 ===")
advantages = {
    "完整语义理解": "可以同时利用左右两侧的上下文信息",
    "更好的歧义消解": "通过全局信息解决词汇和句法歧义",
    "丰富的特征表示": "学习到更全面的语言表示",
    "任务适应性强": "在理解类任务上表现优异"
}

for advantage, description in advantages.items():
    print(f"• {advantage}: {description}")

limitations = {
    "生成能力受限": "无法直接用于自回归文本生成",
    "预训练效率": "相比自回归模型训练效率较低",
    "推理复杂度": "需要处理完整序列，无法流式生成"
}

print("\n=== MLM的局限性 ===")
for limitation, description in limitations.items():
    print(f"• {limitation}: {description}")
```

### MLM实际应用示例

```python
class SimpleBERTMLM:
    def __init__(self, vocab_size=1000, hidden_size=256):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.mask_token_id = vocab_size - 1
        
    def forward_example(self, input_ids, attention_mask):
        """模拟BERT MLM的前向传播"""
        batch_size, seq_len = input_ids.shape
        
        # 模拟嵌入层
        embeddings = torch.randn(batch_size, seq_len, self.hidden_size)
        
        # 模拟Transformer编码器
        hidden_states = embeddings
        for layer in range(12):  # 12层Transformer
            # 简化的自注意力计算
            attention_scores = torch.matmul(hidden_states, hidden_states.transpose(-1, -2))
            attention_probs = torch.softmax(attention_scores, dim=-1)
            hidden_states = torch.matmul(attention_probs, hidden_states)
        
        # MLM预测头
        prediction_scores = torch.matmul(hidden_states, 
                                       torch.randn(self.hidden_size, self.vocab_size))
        
        return prediction_scores
    
    def compute_mlm_loss(self, prediction_scores, labels):
        """计算MLM损失"""
        # 只对被掩码的位置计算损失
        mask = (labels != -100)
        if mask.sum() == 0:
            return torch.tensor(0.0)
        
        # 交叉熵损失
        loss_fct = nn.CrossEntropyLoss()
        masked_lm_loss = loss_fct(prediction_scores[mask], labels[mask])
        
        return masked_lm_loss
    
    def predict_masked_tokens(self, text_with_masks):
        """预测被掩码的词汇"""
        # 这里是一个简化的预测示例
        predictions = []
        for token in text_with_masks:
            if token == "[MASK]":
                # 模拟预测过程
                candidates = ["苹果", "香蕉", "橙子", "葡萄"]
                probabilities = [0.4, 0.3, 0.2, 0.1]
                predicted = np.random.choice(candidates, p=probabilities)
                predictions.append(predicted)
            else:
                predictions.append(token)
        return predictions

# 演示MLM预测
bert_mlm = SimpleBERTMLM()

# 示例文本
original_text = ["我", "喜欢", "吃", "苹果", "和", "香蕉"]
masked_text = ["我", "喜欢", "吃", "[MASK]", "和", "[MASK]"]

print("\n=== MLM预测示例 ===")
print(f"原始文本: {' '.join(original_text)}")
print(f"掩码文本: {' '.join(masked_text)}")

predicted_text = bert_mlm.predict_masked_tokens(masked_text)
print(f"预测结果: {' '.join(predicted_text)}")

# 可视化预测概率
fig, ax = plt.subplots(1, 1, figsize=(10, 6))

mask_positions = [i for i, token in enumerate(masked_text) if token == "[MASK]"]
candidates = ["苹果", "香蕉", "橙子", "葡萄"]
probs_pos1 = [0.4, 0.3, 0.2, 0.1]
probs_pos2 = [0.2, 0.5, 0.2, 0.1]

x = np.arange(len(candidates))
width = 0.35

bars1 = ax.bar(x - width/2, probs_pos1, width, label='位置3预测', color='#4ECDC4')
bars2 = ax.bar(x + width/2, probs_pos2, width, label='位置5预测', color='#FF6B6B')

ax.set_xlabel('候选词汇')
ax.set_ylabel('预测概率')
ax.set_title('MLM掩码位置预测概率分布', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(candidates)
ax.legend()
ax.grid(True, alpha=0.3)

# 添加概率标签
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
               f'{height:.1f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()
```

---

## 3.1.3 自回归语言模型

### 自回归的基本概念

自回归语言模型是GPT系列模型的核心，它基于历史信息预测未来，具有天然的生成能力。

### 因果注意力机制详解

```python
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

class CausalAttention:
    def __init__(self, seq_len=8):
        self.seq_len = seq_len
        
    def create_causal_mask(self):
        """创建因果注意力掩码"""
        mask = torch.tril(torch.ones(self.seq_len, self.seq_len))
        # 将上三角部分设为负无穷，softmax后变为0
        causal_mask = mask.masked_fill(mask == 0, float('-inf'))
        causal_mask = causal_mask.masked_fill(mask == 1, 0.0)
        return mask, causal_mask
    
    def visualize_causal_attention(self):
        """可视化因果注意力机制"""
        mask, causal_mask = self.create_causal_mask()
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('自回归语言模型的因果注意力机制', fontsize=16, fontweight='bold')
        
        # 因果掩码可视化
        im1 = axes[0, 0].imshow(mask.numpy(), cmap='Blues', aspect='auto')
        axes[0, 0].set_title('因果注意力掩码', fontsize=12, fontweight='bold')
        axes[0, 0].set_xlabel('Key位置')
        axes[0, 0].set_ylabel('Query位置')
        
        # 添加网格和数值
        for i in range(self.seq_len):
            for j in range(self.seq_len):
                text = axes[0, 0].text(j, i, int(mask[i, j].item()),
                                     ha="center", va="center", color="white" if mask[i, j] == 1 else "black")
        
        # 注意力权重示例
        # 模拟一个位置的注意力权重
        position = 4
        attention_weights = torch.zeros(self.seq_len)
        attention_weights[:position+1] = torch.softmax(torch.randn(position+1), dim=0)
        
        bars = axes[0, 1].bar(range(self.seq_len), attention_weights.numpy(), 
                             color=['#4ECDC4' if i <= position else '#CCCCCC' for i in range(self.seq_len)])
        axes[0, 1].set_title(f'位置{position}的注意力权重分布', fontsize=12, fontweight='bold')
        axes[0, 1].set_xlabel('位置')
        axes[0, 1].set_ylabel('注意力权重')
        axes[0, 1].axvline(x=position, color='red', linestyle='--', alpha=0.7, label='当前位置')
        axes[0, 1].legend()
        
        # 序列生成过程
        sequence = ["我", "喜欢", "吃", "苹果", "[生成]", "", "", ""]
        colors = ['lightblue'] * 4 + ['lightcoral'] + ['white'] * 3
        
        for i, (word, color) in enumerate(zip(sequence, colors)):
            rect = plt.Rectangle((i-0.4, 0.4), 0.8, 0.2, facecolor=color, edgecolor='black')
            axes[1, 0].add_patch(rect)
            if word:
                axes[1, 0].text(i, 0.5, word, ha='center', va='center', fontweight='bold')
        
        axes[1, 0].set_xlim(-0.5, self.seq_len-0.5)
        axes[1, 0].set_ylim(0.3, 0.7)
        axes[1, 0].set_title('自回归生成过程', fontsize=12, fontweight='bold')
        axes[1, 0].set_xlabel('位置')
        axes[1, 0].set_yticks([])
        
        # 添加箭头表示生成方向
        for i in range(3):
            axes[1, 0].annotate('', xy=(i+1, 0.5), xytext=(i, 0.5),
                              arrowprops=dict(arrowstyle='->', color='red', lw=2))
        
        # 生成概率分布
        vocab = ["苹果", "香蕉", "橙子", "葡萄", "西瓜"]
        probs = [0.4, 0.25, 0.15, 0.12, 0.08]
        
        bars = axes[1, 1].bar(vocab, probs, color='lightgreen')
        axes[1, 1].set_title('下一个词的预测概率', fontsize=12, fontweight='bold')
        axes[1, 1].set_ylabel('概率')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        # 添加概率标签
        for bar, prob in zip(bars, probs):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                           f'{prob:.2f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.show()

# 演示因果注意力
causal_attn = CausalAttention()
causal_attn.visualize_causal_attention()

print("\n=== 因果注意力机制特点 ===")
features = {
    "单向信息流": "只能看到当前位置之前的信息",
    "保证自回归性质": "确保生成过程的因果性",
    "支持并行训练": "训练时可以并行计算所有位置",
    "流式推理": "推理时可以逐步生成，无需重新计算"
}

for feature, description in features.items():
    print(f"• {feature}: {description}")
```

### 自回归训练目标分析

```python
class AutoregressiveLM:
    def __init__(self, vocab_size=1000):
        self.vocab_size = vocab_size
        
    def compute_language_modeling_loss(self, logits, labels):
        """计算语言模型损失"""
        # 将预测和标签对齐：预测位置i对应标签位置i+1
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        
        # 计算交叉熵损失
        loss_fct = torch.nn.CrossEntropyLoss()
        loss = loss_fct(shift_logits.view(-1, self.vocab_size), shift_labels.view(-1))
        
        return loss
    
    def calculate_perplexity(self, loss):
        """计算困惑度"""
        return torch.exp(loss)
    
    def visualize_training_objective(self):
        """可视化训练目标"""
        # 示例序列
        sequence = ["我", "喜欢", "吃", "苹果"]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('自回归语言模型训练目标', fontsize=16, fontweight='bold')
        
        # 训练数据对齐
        input_seq = sequence[:-1]
        target_seq = sequence[1:]
        
        # 可视化输入输出对齐
        for i, (inp, tgt) in enumerate(zip(input_seq, target_seq)):
            # 输入
            rect_inp = plt.Rectangle((i-0.3, 1.4), 0.6, 0.2, facecolor='lightblue', edgecolor='black')
            axes[0, 0].add_patch(rect_inp)
            axes[0, 0].text(i, 1.5, inp, ha='center', va='center', fontweight='bold')
            
            # 目标
            rect_tgt = plt.Rectangle((i-0.3, 0.4), 0.6, 0.2, facecolor='lightcoral', edgecolor='black')
            axes[0, 0].add_patch(rect_tgt)
            axes[0, 0].text(i, 0.5, tgt, ha='center', va='center', fontweight='bold')
            
            # 箭头
            axes[0, 0].annotate('', xy=(i, 0.6), xytext=(i, 1.4),
                              arrowprops=dict(arrowstyle='->', color='green', lw=2))
        
        axes[0, 0].set_xlim(-0.5, len(input_seq)-0.5)
        axes[0, 0].set_ylim(0.2, 1.8)
        axes[0, 0].set_title('输入-目标对齐', fontsize=12, fontweight='bold')
        axes[0, 0].text(-0.4, 1.5, '输入:', ha='right', va='center', fontweight='bold')
        axes[0, 0].text(-0.4, 0.5, '目标:', ha='right', va='center', fontweight='bold')
        axes[0, 0].set_xticks([])
        axes[0, 0].set_yticks([])
        
        # 损失计算过程
        positions = list(range(len(input_seq)))
        losses = [2.3, 1.8, 1.5, 1.2]  # 示例损失值
        
        bars = axes[0, 1].bar(positions, losses, color='orange', alpha=0.7)
        axes[0, 1].set_title('各位置的损失值', fontsize=12, fontweight='bold')
        axes[0, 1].set_xlabel('位置')
        axes[0, 1].set_ylabel('交叉熵损失')
        axes[0, 1].set_xticks(positions)
        axes[0, 1].set_xticklabels([f'{inp}→{tgt}' for inp, tgt in zip(input_seq, target_seq)], rotation=45)
        
        # 添加损失值标签
        for bar, loss in zip(bars, losses):
            axes[0, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                           f'{loss:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 训练过程中的困惑度变化
        epochs = list(range(1, 21))
        perplexity = [100 * np.exp(-0.1 * epoch) + 10 + np.random.normal(0, 2) for epoch in epochs]
        perplexity = [max(10, p) for p in perplexity]  # 确保困惑度不小于10
        
        axes[1, 0].plot(epochs, perplexity, 'o-', linewidth=2, markersize=6, color='purple')
        axes[1, 0].set_title('训练过程中的困惑度变化', fontsize=12, fontweight='bold')
        axes[1, 0].set_xlabel('训练轮数')
        axes[1, 0].set_ylabel('困惑度')
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].fill_between(epochs, perplexity, alpha=0.3, color='purple')
        
        # 不同模型规模的性能对比
        model_sizes = ['125M', '350M', '760M', '1.3B', '2.7B']
        perplexities = [45, 35, 28, 22, 18]
        
        bars = axes[1, 1].bar(model_sizes, perplexities, color='teal')
        axes[1, 1].set_title('模型规模与困惑度关系', fontsize=12, fontweight='bold')
        axes[1, 1].set_xlabel('模型参数量')
        axes[1, 1].set_ylabel('困惑度')
        
        # 添加困惑度标签
        for bar, ppl in zip(bars, perplexities):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,
                           f'{ppl}', ha='center', va='bottom', fontsize=9, fontweight='bold')
        
        plt.tight_layout()
        plt.show()

# 演示自回归训练
ar_lm = AutoregressiveLM()
ar_lm.visualize_training_objective()

print("\n=== 自回归语言模型数学表示 ===")
print("联合概率分解:")
print("P(x₁, x₂, ..., xₙ) = ∏ᵢ₌₁ⁿ P(xᵢ|x₁, ..., xᵢ₋₁)")
print("\n训练目标:")
print("最大化: log P(x₁, x₂, ..., xₙ) = Σᵢ₌₁ⁿ log P(xᵢ|x₁, ..., xᵢ₋₁)")
print("\n损失函数:")
print("L = -Σᵢ₌₁ⁿ log P(xᵢ|x₁, ..., xᵢ₋₁)")
```

### 生成策略对比

```python
class TextGeneration:
    def __init__(self):
        self.vocab = ["我", "喜欢", "吃", "苹果", "香蕉", "橙子", "很", "甜", "的", "。"]
        
    def greedy_search(self, logits):
        """贪心搜索"""
        return torch.argmax(logits, dim=-1)
    
    def top_k_sampling(self, logits, k=3, temperature=1.0):
        """Top-k采样"""
        # 应用温度
        logits = logits / temperature
        
        # 获取top-k
        top_k_logits, top_k_indices = torch.topk(logits, k)
        
        # 计算概率
        probs = F.softmax(top_k_logits, dim=-1)
        
        # 采样
        sampled_index = torch.multinomial(probs, 1)
        return top_k_indices[sampled_index]
    
    def nucleus_sampling(self, logits, p=0.9, temperature=1.0):
        """Nucleus (top-p) 采样"""
        # 应用温度
        logits = logits / temperature
        
        # 排序
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        
        # 计算累积概率
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # 找到累积概率超过p的位置
        sorted_indices_to_remove = cumulative_probs > p
        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
        sorted_indices_to_remove[0] = 0
        
        # 移除低概率token
        sorted_logits[sorted_indices_to_remove] = float('-inf')
        
        # 采样
        probs = F.softmax(sorted_logits, dim=-1)
        sampled_index = torch.multinomial(probs, 1)
        return sorted_indices[sampled_index]
    
    def visualize_generation_strategies(self):
        """可视化不同生成策略"""
        # 模拟logits
        torch.manual_seed(42)
        logits = torch.randn(len(self.vocab))
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('文本生成策略对比', fontsize=16, fontweight='bold')
        
        # 原始概率分布
        probs = F.softmax(logits, dim=-1)
        bars1 = axes[0, 0].bar(self.vocab, probs.numpy(), color='lightblue')
        axes[0, 0].set_title('原始概率分布', fontsize=12, fontweight='bold')
        axes[0, 0].set_ylabel('概率')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 标记贪心选择
        max_idx = torch.argmax(probs)
        bars1[max_idx].set_color('red')
        axes[0, 0].text(max_idx, probs[max_idx] + 0.01, '贪心选择', 
                        ha='center', va='bottom', fontweight='bold', color='red')
        
        # Top-k采样 (k=3)
        k = 3
        top_k_probs, top_k_indices = torch.topk(probs, k)
        top_k_normalized = F.softmax(top_k_probs, dim=-1)
        
        vocab_subset = [self.vocab[i] for i in top_k_indices]
        bars2 = axes[0, 1].bar(vocab_subset, top_k_normalized.numpy(), color='lightgreen')
        axes[0, 1].set_title(f'Top-{k} 采样', fontsize=12, fontweight='bold')
        axes[0, 1].set_ylabel('重新归一化概率')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # Nucleus采样 (p=0.8)
        p = 0.8
        sorted_probs, sorted_indices = torch.sort(probs, descending=True)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        nucleus_mask = cumulative_probs <= p
        nucleus_mask[0] = True  # 至少保留一个token
        
        nucleus_probs = sorted_probs[nucleus_mask]
        nucleus_indices = sorted_indices[nucleus_mask]
        nucleus_normalized = nucleus_probs / nucleus_probs.sum()
        
        vocab_nucleus = [self.vocab[i] for i in nucleus_indices]
        bars3 = axes[1, 0].bar(vocab_nucleus, nucleus_normalized.numpy(), color='lightyellow')
        axes[1, 0].set_title(f'Nucleus采样 (p={p})', fontsize=12, fontweight='bold')
        axes[1, 0].set_ylabel('重新归一化概率')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 温度对比
        temperatures = [0.5, 1.0, 1.5, 2.0]
        temp_colors = ['blue', 'green', 'orange', 'red']
        
        for temp, color in zip(temperatures, temp_colors):
            temp_probs = F.softmax(logits / temp, dim=-1)
            axes[1, 1].plot(range(len(self.vocab)), temp_probs.numpy(), 
                          'o-', label=f'T={temp}', color=color, linewidth=2)
        
        axes[1, 1].set_title('温度参数对概率分布的影响', fontsize=12, fontweight='bold')
        axes[1, 1].set_xlabel('词汇索引')
        axes[1, 1].set_ylabel('概率')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return logits

# 演示生成策略
text_gen = TextGeneration()
logits = text_gen.visualize_generation_strategies()

print("\n=== 文本生成策略对比 ===")
strategies = {
    "贪心搜索": {
        "特点": "总是选择概率最高的词",
        "优势": "确定性输出，计算简单",
        "劣势": "容易产生重复，缺乏多样性"
    },
    "Top-k采样": {
        "特点": "从概率最高的k个词中随机采样",
        "优势": "平衡质量和多样性",
        "劣势": "k值需要调优"
    },
    "Nucleus采样": {
        "特点": "从累积概率达到p的词集中采样",
        "优势": "自适应词汇集大小",
        "劣势": "p值需要调优"
    },
    "温度采样": {
        "特点": "通过温度参数调节概率分布",
        "优势": "灵活控制随机性",
        "劣势": "温度值需要精心设置"
    }
}

for strategy, details in strategies.items():
    print(f"\n【{strategy}】")
    for key, value in details.items():
        print(f"  {key}: {value}")
```

---

## 3.1.4 预训练数据的准备和处理

### 数据来源和收集策略

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from collections import Counter
import re

class PretrainingDataProcessor:
    def __init__(self):
        self.data_sources = {
            "Common Crawl": {"size_tb": 800, "quality": 3, "diversity": 5, "cost": 1},
            "Wikipedia": {"size_tb": 0.02, "quality": 5, "diversity": 4, "cost": 1},
            "BookCorpus": {"size_tb": 0.8, "quality": 5, "diversity": 3, "cost": 2},
            "OpenWebText": {"size_tb": 38, "quality": 4, "diversity": 4, "cost": 1},
            "GitHub": {"size_tb": 1.2, "quality": 4, "diversity": 2, "cost": 1},
            "News Articles": {"size_tb": 16, "quality": 4, "diversity": 3, "cost": 2}
        }
    
    def visualize_data_sources(self):
        """可视化数据来源特征"""
        df = pd.DataFrame(self.data_sources).T
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('预训练数据来源分析', fontsize=16, fontweight='bold')
        
        # 数据规模对比
        sources = list(self.data_sources.keys())
        sizes = [self.data_sources[src]["size_tb"] for src in sources]
        colors = plt.cm.Set3(np.linspace(0, 1, len(sources)))
        
        bars1 = axes[0, 0].bar(sources, sizes, color=colors)
        axes[0, 0].set_title('数据源规模对比 (TB)', fontsize=12, fontweight='bold')
        axes[0, 0].set_ylabel('数据量 (TB)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].set_yscale('log')
        
        # 添加数值标签
        for bar, size in zip(bars1, sizes):
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.1,
                           f'{size}TB', ha='center', va='bottom', fontsize=9)
        
        # 质量-多样性散点图
        quality = [self.data_sources[src]["quality"] for src in sources]
        diversity = [self.data_sources[src]["diversity"] for src in sources]
        
        scatter = axes[0, 1].scatter(quality, diversity, s=[s*10 for s in sizes], 
                                   c=colors, alpha=0.7, edgecolors='black')
        axes[0, 1].set_title('数据质量 vs 多样性', fontsize=12, fontweight='bold')
        axes[0, 1].set_xlabel('数据质量 (1-5)')
        axes[0, 1].set_ylabel('数据多样性 (1-5)')
        
        # 添加数据源标签
        for i, src in enumerate(sources):
            axes[0, 1].annotate(src, (quality[i], diversity[i]), 
                              xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        # 数据处理流程
        process_steps = ['原始数据', '去重', '过滤', '清洗', '格式化', '编码']
        data_retention = [100, 85, 70, 60, 58, 57]
        
        axes[1, 0].plot(process_steps, data_retention, 'o-', linewidth=3, 
                       markersize=8, color='#FF6B6B')
        axes[1, 0].fill_between(process_steps, data_retention, alpha=0.3, color='#FF6B6B')
        axes[1, 0].set_title('数据处理流程中的数据保留率', fontsize=12, fontweight='bold')
        axes[1, 0].set_ylabel('数据保留率 (%)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # 质量控制指标
        quality_metrics = ['长度过滤', '语言检测', '重复检测', '内容过滤', '格式检查']
        filter_rates = [15, 8, 12, 5, 2]
        
        bars2 = axes[1, 1].bar(quality_metrics, filter_rates, color='lightcoral')
        axes[1, 1].set_title('各质量控制步骤的过滤率', fontsize=12, fontweight='bold')
        axes[1, 1].set_ylabel('过滤率 (%)')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        # 添加过滤率标签
        for bar, rate in zip(bars2, filter_rates):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.2,
                           f'{rate}%', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.show()
    
    def demonstrate_data_cleaning(self):
        """演示数据清洗过程"""
        # 模拟原始文本数据
        raw_texts = [
            "这是一个很好的例子。",
            "This is an English sentence.",
            "这是重复的内容。",
            "这是重复的内容。",  # 重复
            "a",  # 太短
            "" * 1000 + "这是一个超长的文本" + "a" * 1000,  # 太长
            "这里有很多!!!特殊符号@#$%",
            "正常的中文文本，应该被保留。"
        ]
        
        print("\n=== 数据清洗演示 ===")
        print(f"原始数据量: {len(raw_texts)}")
        
        # 步骤1: 长度过滤
        length_filtered = [text for text in raw_texts if 5 <= len(text) <= 500]
        print(f"长度过滤后: {len(length_filtered)}")
        
        # 步骤2: 语言检测（简化版）
        def is_chinese(text):
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            return chinese_chars / len(text) > 0.5 if text else False
        
        language_filtered = [text for text in length_filtered if is_chinese(text)]
        print(f"语言过滤后: {len(language_filtered)}")
        
        # 步骤3: 去重
        deduplicated = list(set(language_filtered))
        print(f"去重后: {len(deduplicated)}")
        
        # 步骤4: 内容过滤（移除特殊符号过多的文本）
        def has_too_many_symbols(text):
            symbols = len(re.findall(r'[!@#$%^&*()_+\-=\[\]{};:\'"\\|,.<>\?]', text))
            return symbols / len(text) > 0.2 if text else False
        
        content_filtered = [text for text in deduplicated if not has_too_many_symbols(text)]
        print(f"内容过滤后: {len(content_filtered)}")
        
        print("\n最终保留的文本:")
        for i, text in enumerate(content_filtered, 1):
            print(f"{i}. {text}")
        
        return content_filtered

# 演示数据处理
data_processor = PretrainingDataProcessor()
data_processor.visualize_data_sources()
cleaned_data = data_processor.demonstrate_data_cleaning()

print("\n=== 数据来源详细分析 ===")
data_source_details = {
    "Common Crawl": {
        "描述": "网页爬取数据，规模最大但质量参差不齐",
        "优势": "数据量巨大，覆盖面广",
        "挑战": "噪声多，需要大量清洗",
        "使用建议": "需要严格的质量控制流程"
    },
    "Wikipedia": {
        "描述": "高质量的百科全书内容",
        "优势": "质量高，结构化好",
        "挑战": "数据量相对较小",
        "使用建议": "作为高质量数据的重要补充"
    },
    "BookCorpus": {
        "描述": "书籍文本，语言质量高",
        "优势": "语言规范，逻辑性强",
        "挑战": "版权问题，获取困难",
        "使用建议": "注意版权合规性"
    }
}

for source, details in data_source_details.items():
    print(f"\n【{source}】")
    for key, value in details.items():
        print(f"  {key}: {value}")
```

### 数据预处理技术详解

```python
class DataPreprocessor:
    def __init__(self):
        self.tokenizer_vocab = {"[PAD]": 0, "[UNK]": 1, "[CLS]": 2, "[SEP]": 3, "[MASK]": 4}
        self.max_seq_length = 512
    
    def tokenization_demo(self):
        """演示分词过程"""
        text = "我喜欢吃苹果和香蕉。"
        
        # 字符级分词
        char_tokens = list(text)
        
        # 词级分词（简化版）
        word_tokens = ["我", "喜欢", "吃", "苹果", "和", "香蕉", "。"]
        
        # 子词分词（BPE风格，简化版）
        subword_tokens = ["我", "喜", "##欢", "吃", "苹", "##果", "和", "香", "##蕉", "。"]
        
        fig, axes = plt.subplots(3, 1, figsize=(14, 10))
        fig.suptitle('不同分词策略对比', fontsize=16, fontweight='bold')
        
        # 字符级分词
        axes[0].text(0.5, 0.5, ' | '.join(char_tokens), ha='center', va='center', 
                    fontsize=14, transform=axes[0].transAxes,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue', alpha=0.7))
        axes[0].set_title('字符级分词', fontsize=12, fontweight='bold')
        axes[0].set_xlim(0, 1)
        axes[0].set_ylim(0, 1)
        axes[0].axis('off')
        
        # 词级分词
        axes[1].text(0.5, 0.5, ' | '.join(word_tokens), ha='center', va='center', 
                    fontsize=14, transform=axes[1].transAxes,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgreen', alpha=0.7))
        axes[1].set_title('词级分词', fontsize=12, fontweight='bold')
        axes[1].set_xlim(0, 1)
        axes[1].set_ylim(0, 1)
        axes[1].axis('off')
        
        # 子词分词
        axes[2].text(0.5, 0.5, ' | '.join(subword_tokens), ha='center', va='center', 
                    fontsize=14, transform=axes[2].transAxes,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightyellow', alpha=0.7))
        axes[2].set_title('子词分词 (BPE)', fontsize=12, fontweight='bold')
        axes[2].set_xlim(0, 1)
        axes[2].set_ylim(0, 1)
        axes[2].axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return char_tokens, word_tokens, subword_tokens
    
    def encoding_demo(self):
        """演示编码过程"""
        tokens = ["[CLS]", "我", "喜欢", "吃", "苹果", "[SEP]"]
        
        # 模拟词汇表
        vocab = {"[CLS]": 2, "我": 100, "喜欢": 200, "吃": 300, "苹果": 400, "[SEP]": 3, "[PAD]": 0}
        
        # 转换为ID
        input_ids = [vocab.get(token, 1) for token in tokens]  # 1是[UNK]
        
        # 创建注意力掩码
        attention_mask = [1] * len(tokens)
        
        # 填充到最大长度
        while len(input_ids) < 10:  # 简化的最大长度
            input_ids.append(0)  # [PAD]
            attention_mask.append(0)
            tokens.append("[PAD]")
        
        # 可视化编码过程
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 8))
        fig.suptitle('文本编码过程', fontsize=16, fontweight='bold')
        
        # Token序列
        for i, token in enumerate(tokens):
            color = 'lightblue' if token != '[PAD]' else 'lightgray'
            rect = plt.Rectangle((i-0.4, 0.4), 0.8, 0.2, facecolor=color, edgecolor='black')
            ax1.add_patch(rect)
            ax1.text(i, 0.5, token, ha='center', va='center', fontsize=10, fontweight='bold')
        
        ax1.set_xlim(-0.5, len(tokens)-0.5)
        ax1.set_ylim(0.3, 0.7)
        ax1.set_title('Token序列', fontsize=12, fontweight='bold')
        ax1.set_xticks([])
        ax1.set_yticks([])
        
        # Input IDs
        for i, id_val in enumerate(input_ids):
            color = 'lightgreen' if id_val != 0 else 'lightgray'
            rect = plt.Rectangle((i-0.4, 0.4), 0.8, 0.2, facecolor=color, edgecolor='black')
            ax2.add_patch(rect)
            ax2.text(i, 0.5, str(id_val), ha='center', va='center', fontsize=10, fontweight='bold')
        
        ax2.set_xlim(-0.5, len(input_ids)-0.5)
        ax2.set_ylim(0.3, 0.7)
        ax2.set_title('Input IDs', fontsize=12, fontweight='bold')
        ax2.set_xticks([])
        ax2.set_yticks([])
        
        # Attention Mask
        for i, mask_val in enumerate(attention_mask):
            color = 'lightcoral' if mask_val == 1 else 'lightgray'
            rect = plt.Rectangle((i-0.4, 0.4), 0.8, 0.2, facecolor=color, edgecolor='black')
            ax3.add_patch(rect)
            ax3.text(i, 0.5, str(mask_val), ha='center', va='center', fontsize=10, fontweight='bold')
        
        ax3.set_xlim(-0.5, len(attention_mask)-0.5)
        ax3.set_ylim(0.3, 0.7)
        ax3.set_title('Attention Mask', fontsize=12, fontweight='bold')
        ax3.set_xticks([])
        ax3.set_yticks([])
        
        plt.tight_layout()
        plt.show()
        
        return input_ids, attention_mask
    
    def batch_processing_demo(self):
        """演示批处理过程"""
        # 模拟不同长度的句子
        sentences = [
            "我喜欢吃苹果。",
            "今天天气很好。",
            "这是一个比较长的句子，包含更多的信息内容。"
        ]
        
        # 分词后的长度
        token_lengths = [6, 6, 15]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        fig.suptitle('批处理中的序列长度处理', fontsize=16, fontweight='bold')
        
        # 原始长度分布
        bars1 = ax1.bar(range(len(sentences)), token_lengths, 
                        color=['lightblue', 'lightgreen', 'lightcoral'])
        ax1.set_title('原始序列长度', fontsize=12, fontweight='bold')
        ax1.set_xlabel('句子索引')
        ax1.set_ylabel('Token数量')
        ax1.set_xticks(range(len(sentences)))
        
        # 添加长度标签
        for bar, length in zip(bars1, token_lengths):
            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.2,
                    str(length), ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        # 填充后的长度（统一为最大长度）
        max_length = max(token_lengths)
        padded_lengths = [max_length] * len(sentences)
        
        bars2 = ax2.bar(range(len(sentences)), padded_lengths, 
                        color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)
        
        # 显示有效部分和填充部分
        for i, (original, padded) in enumerate(zip(token_lengths, padded_lengths)):
            # 有效部分
            ax2.bar(i, original, color=['lightblue', 'lightgreen', 'lightcoral'][i], alpha=1.0)
            # 填充部分
            if padded > original:
                ax2.bar(i, padded - original, bottom=original, color='gray', alpha=0.5)
        
        ax2.set_title('填充后序列长度', fontsize=12, fontweight='bold')
        ax2.set_xlabel('句子索引')
        ax2.set_ylabel('Token数量')
        ax2.set_xticks(range(len(sentences)))
        ax2.axhline(y=max_length, color='red', linestyle='--', alpha=0.7, label='最大长度')
        ax2.legend()
        
        plt.tight_layout()
        plt.show()
        
        print("\n=== 批处理策略分析 ===")
        print(f"原始长度: {token_lengths}")
        print(f"最大长度: {max_length}")
        print(f"填充后长度: {padded_lengths}")
        print(f"计算效率: {sum(token_lengths) / (len(sentences) * max_length) * 100:.1f}%")

# 演示数据预处理
preprocessor = DataPreprocessor()
char_tokens, word_tokens, subword_tokens = preprocessor.tokenization_demo()
input_ids, attention_mask = preprocessor.encoding_demo()
preprocessor.batch_processing_demo()

print("\n=== 分词策略对比 ===")
tokenization_comparison = {
    "字符级分词": {
        "优势": "词汇表小，无未知词问题",
        "劣势": "序列长，语义信息分散",
        "适用场景": "中文等无明显词边界的语言"
    },
    "词级分词": {
        "优势": "语义完整，序列较短",
        "劣势": "词汇表大，未知词问题严重",
        "适用场景": "词汇相对固定的领域"
    },
    "子词分词": {
        "优势": "平衡词汇表大小和语义完整性",
        "劣势": "需要预训练分词器",
        "适用场景": "大多数现代NLP任务"
    }
}

for method, details in tokenization_comparison.items():
    print(f"\n【{method}】")
    for key, value in details.items():
        print(f"  {key}: {value}")

---

## 3.1.5 预训练的计算资源需求

### 计算资源需求分析

```python
class ComputeResourceAnalyzer:
    def __init__(self):
        self.model_configs = {
            "GPT-2 Small": {"params": 117, "layers": 12, "hidden": 768, "heads": 12},
            "GPT-2 Medium": {"params": 345, "layers": 24, "hidden": 1024, "heads": 16},
            "GPT-2 Large": {"params": 762, "layers": 36, "hidden": 1280, "heads": 20},
            "GPT-2 XL": {"params": 1542, "layers": 48, "hidden": 1600, "heads": 25},
            "GPT-3": {"params": 175000, "layers": 96, "hidden": 12288, "heads": 96}
        }
    
    def calculate_training_flops(self, params_millions, tokens_billions, forward_passes=3):
        """计算训练所需的FLOPs"""
        # 简化的FLOPs计算：每个token大约需要6*参数量的FLOPs
        flops_per_token = 6 * params_millions * 1e6
        total_tokens = tokens_billions * 1e9
        total_flops = flops_per_token * total_tokens * forward_passes
        return total_flops
    
    def estimate_training_time(self, total_flops, gpu_flops_per_second):
        """估算训练时间"""
        training_seconds = total_flops / gpu_flops_per_second
        training_days = training_seconds / (24 * 3600)
        return training_days
    
    def visualize_resource_requirements(self):
        """可视化资源需求"""
        models = list(self.model_configs.keys())
        params = [self.model_configs[model]["params"] for model in models]
        
        # 计算训练成本（简化估算）
        training_costs = []
        training_times = []
        
        for param_count in params:
            if param_count < 1000:  # 小于1B参数
                cost = param_count * 100  # 每M参数100美元
                time_days = param_count * 0.01  # 每M参数0.01天
            else:  # 大模型
                cost = param_count * 50  # 规模效应，单位成本降低
                time_days = param_count * 0.005
            
            training_costs.append(cost)
            training_times.append(time_days)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('大模型训练资源需求分析', fontsize=16, fontweight='bold')
        
        # 参数规模对比
        bars1 = axes[0, 0].bar(range(len(models)), params, color='lightblue')
        axes[0, 0].set_title('模型参数规模', fontsize=12, fontweight='bold')
        axes[0, 0].set_ylabel('参数量 (百万)')
        axes[0, 0].set_yscale('log')
        axes[0, 0].set_xticks(range(len(models)))
        axes[0, 0].set_xticklabels(models, rotation=45, ha='right')
        
        # 添加参数量标签
        for bar, param in zip(bars1, params):
            if param < 1000:
                label = f'{param}M'
            else:
                label = f'{param/1000:.1f}B'
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.1,
                           label, ha='center', va='bottom', fontsize=9)
        
        # 训练成本估算
        bars2 = axes[0, 1].bar(range(len(models)), training_costs, color='lightcoral')
        axes[0, 1].set_title('训练成本估算', fontsize=12, fontweight='bold')
        axes[0, 1].set_ylabel('成本 (千美元)')
        axes[0, 1].set_yscale('log')
        axes[0, 1].set_xticks(range(len(models)))
        axes[0, 1].set_xticklabels(models, rotation=45, ha='right')
        
        # 训练时间估算
        bars3 = axes[1, 0].bar(range(len(models)), training_times, color='lightgreen')
        axes[1, 0].set_title('训练时间估算', fontsize=12, fontweight='bold')
        axes[1, 0].set_ylabel('时间 (天)')
        axes[1, 0].set_yscale('log')
        axes[1, 0].set_xticks(range(len(models)))
        axes[1, 0].set_xticklabels(models, rotation=45, ha='right')
        
        # 硬件需求对比
        hardware_types = ['V100', 'A100', 'H100']
        memory_gb = [32, 80, 80]
        flops_tflops = [125, 312, 989]
        
        x = np.arange(len(hardware_types))
        width = 0.35
        
        ax4_twin = axes[1, 1].twinx()
        bars4 = axes[1, 1].bar(x - width/2, memory_gb, width, label='显存 (GB)', color='orange')
        bars5 = ax4_twin.bar(x + width/2, flops_tflops, width, label='算力 (TFLOPS)', color='purple')
        
        axes[1, 1].set_title('GPU硬件对比', fontsize=12, fontweight='bold')
        axes[1, 1].set_xlabel('GPU型号')
        axes[1, 1].set_ylabel('显存 (GB)', color='orange')
        ax4_twin.set_ylabel('算力 (TFLOPS)', color='purple')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(hardware_types)
        
        # 添加图例
        lines1, labels1 = axes[1, 1].get_legend_handles_labels()
        lines2, labels2 = ax4_twin.get_legend_handles_labels()
        axes[1, 1].legend(lines1 + lines2, labels1 + labels2, loc='upper left')
        
        plt.tight_layout()
        plt.show()
    
    def analyze_scaling_laws(self):
        """分析规模化定律"""
        # 模拟数据：参数量与性能的关系
        param_counts = np.logspace(2, 5, 20)  # 100M到100B参数
        
        # 根据scaling law: Loss ∝ N^(-α), α ≈ 0.076
        alpha = 0.076
        base_loss = 4.0
        losses = base_loss * (param_counts / param_counts[0]) ** (-alpha)
        
        # 训练成本与参数量的关系（超线性增长）
        costs = (param_counts / 1000) ** 1.2 * 100  # 成本随参数量超线性增长
        
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))
        fig.suptitle('大模型规模化定律分析', fontsize=16, fontweight='bold')
        
        # 参数量与性能关系
        ax1.loglog(param_counts, losses, 'o-', linewidth=2, markersize=6, color='blue')
        ax1.set_xlabel('参数量 (百万)')
        ax1.set_ylabel('损失值')
        ax1.set_title('规模化定律：参数量 vs 性能', fontsize=12, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # 添加拟合线
        ax1.plot(param_counts, base_loss * (param_counts / param_counts[0]) ** (-alpha), 
                '--', color='red', alpha=0.7, label=f'拟合线: L ∝ N^{-alpha:.3f}')
        ax1.legend()
        
        # 参数量与成本关系
        ax2.loglog(param_counts, costs, 's-', linewidth=2, markersize=6, color='red')
        ax2.set_xlabel('参数量 (百万)')
        ax2.set_ylabel('训练成本 (千美元)')
        ax2.set_title('参数量 vs 训练成本', fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # 成本效益分析
        efficiency = 1 / (losses * costs)  # 简化的效益指标
        ax3.semilogx(param_counts, efficiency, '^-', linewidth=2, markersize=6, color='green')
        ax3.set_xlabel('参数量 (百万)')
        ax3.set_ylabel('成本效益 (1/损失·成本)')
        ax3.set_title('成本效益分析', fontsize=12, fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 找到最优点
        optimal_idx = np.argmax(efficiency)
        ax3.scatter(param_counts[optimal_idx], efficiency[optimal_idx], 
                   s=100, color='red', zorder=5)
        ax3.annotate(f'最优点\n{param_counts[optimal_idx]:.0f}M参数', 
                    xy=(param_counts[optimal_idx], efficiency[optimal_idx]),
                    xytext=(param_counts[optimal_idx]*2, efficiency[optimal_idx]*0.8),
                    arrowprops=dict(arrowstyle='->', color='red'))
        
        plt.tight_layout()
        plt.show()
        
        return param_counts, losses, costs

# 演示资源需求分析
resource_analyzer = ComputeResourceAnalyzer()
resource_analyzer.visualize_resource_requirements()
param_counts, losses, costs = resource_analyzer.analyze_scaling_laws()

print("\n=== 计算资源需求总结 ===")
resource_summary = {
    "GPU集群": "大模型训练需要多GPU并行计算",
    "内存需求": "模型参数、梯度、优化器状态需要大量内存",
    "存储需求": "训练数据和检查点需要高速存储",
    "网络带宽": "多机训练需要高带宽网络通信",
    "训练时间": "从几天到几个月不等",
    "电力消耗": "大模型训练耗电量巨大"
}

for aspect, description in resource_summary.items():
    print(f"• {aspect}: {description}")

print("\n=== 优化策略 ===")
optimization_strategies = {
    "混合精度训练": "使用FP16减少内存使用和加速计算",
    "梯度累积": "在小批次上累积梯度，模拟大批次训练",
    "数据并行": "在多个GPU上复制模型，并行处理不同数据",
    "模型并行": "将大模型分割到多个GPU上",
    "流水线并行": "将模型层分布到不同设备，流水线执行",
    "检查点机制": "定期保存训练状态，支持断点续训"
}

for strategy, description in optimization_strategies.items():
    print(f"• {strategy}: {description}")
```

---

## 本节总结

通过本节的学习，我们深入了解了预训练技术的核心要素：

### 关键要点回顾

1. **自监督学习**是大模型成功的基础，通过巧妙的任务设计从无标注数据中学习
2. **MLM和自回归**代表了两种主要的预训练范式，各有优势和适用场景
3. **数据质量**比数据数量更重要，需要严格的清洗和处理流程
4. **计算资源**是制约因素，需要合理的优化策略和资源规划

### Trae实践要点

- 使用Trae实现不同的预训练方法
- 体验数据预处理的完整流程
- 监控训练过程中的资源使用
- 对比不同策略的效果差异

### 深度思考题

1. 为什么自监督学习能够从无标注数据中学到有用的表示？
2. MLM和自回归预训练的本质区别是什么？各自适合什么任务？
3. 如何在有限的计算资源下高效地进行预训练？
4. 预训练数据的质量如何影响最终模型的性能？

下一节我们将学习如何在预训练模型的基础上进行微调，实现特定任务的适配。