# 3.5 检索增强生成 (Retrieval-Augmented Generation, RAG)

> "RAG技术将大语言模型的生成能力与外部知识库的检索能力相结合，为AI系统注入了实时、准确的知识更新能力。" —— RAG技术专家

---

## 3.5.1 RAG架构原理与核心概念

### RAG的核心思想

RAG通过将检索系统与生成模型相结合，使AI能够访问和利用外部知识库中的信息，从而生成更准确、更具时效性的回答。

```python
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib.patches import FancyBboxPatch, Rectangle, Circle, Arrow
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class RAGArchitectureAnalyzer:
    def __init__(self):
        self.rag_components = {
            '知识库': '存储结构化和非结构化的外部知识',
            '向量化': '将文本转换为高维向量表示',
            '检索器': '根据查询找到相关的知识片段',
            '生成器': '基于检索结果生成最终回答',
            '融合模块': '整合检索信息和生成逻辑'
        }
        
    def demonstrate_rag_architecture(self):
        """演示RAG架构图"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))
        fig.suptitle('RAG架构原理与工作流程', fontsize=16, fontweight='bold')
        
        # RAG架构图
        ax1.set_xlim(0, 10)
        ax1.set_ylim(0, 10)
        ax1.set_title('RAG系统架构', fontsize=14, fontweight='bold')
        
        # 绘制组件
        components = [
            {'name': '用户查询', 'pos': (1, 8), 'size': (1.5, 0.8), 'color': 'lightblue'},
            {'name': '查询编码', 'pos': (1, 6.5), 'size': (1.5, 0.8), 'color': 'lightgreen'},
            {'name': '知识库', 'pos': (5, 8), 'size': (2, 1.5), 'color': 'lightyellow'},
            {'name': '向量检索', 'pos': (5, 6), 'size': (2, 0.8), 'color': 'lightcoral'},
            {'name': '相关文档', 'pos': (5, 4.5), 'size': (2, 0.8), 'color': 'lightgray'},
            {'name': '上下文融合', 'pos': (1, 3), 'size': (1.5, 0.8), 'color': 'plum'},
            {'name': '生成模型', 'pos': (1, 1.5), 'size': (1.5, 0.8), 'color': 'orange'},
            {'name': '最终回答', 'pos': (5, 1.5), 'size': (1.5, 0.8), 'color': 'lightsteelblue'}
        ]
        
        for comp in components:
            rect = FancyBboxPatch(
                comp['pos'], comp['size'][0], comp['size'][1],
                boxstyle="round,pad=0.1", 
                facecolor=comp['color'], 
                edgecolor='black', 
                linewidth=2
            )
            ax1.add_patch(rect)
            ax1.text(
                comp['pos'][0] + comp['size'][0]/2, 
                comp['pos'][1] + comp['size'][1]/2,
                comp['name'], 
                ha='center', va='center', 
                fontsize=10, fontweight='bold'
            )
        
        # 绘制箭头连接
        arrows = [
            ((1.75, 7.7), (1.75, 7.3)),  # 用户查询 -> 查询编码
            ((2.5, 6.9), (5, 6.4)),       # 查询编码 -> 向量检索
            ((6, 7.5), (6, 6.8)),         # 知识库 -> 向量检索
            ((6, 5.8), (6, 5.3)),         # 向量检索 -> 相关文档
            ((5, 4.9), (2.5, 3.4)),       # 相关文档 -> 上下文融合
            ((1.75, 2.8), (1.75, 2.3)),   # 上下文融合 -> 生成模型
            ((2.5, 1.9), (5, 1.9))        # 生成模型 -> 最终回答
        ]
        
        for start, end in arrows:
            ax1.annotate('', xy=end, xytext=start,
                        arrowprops=dict(arrowstyle='->', lw=2, color='red'))
        
        ax1.axis('off')
        
        # RAG vs 传统LLM对比
        comparison_metrics = ['准确性', '时效性', '可解释性', '知识覆盖', '计算成本']
        traditional_llm = [0.7, 0.5, 0.6, 0.8, 0.9]
        rag_system = [0.9, 0.95, 0.85, 0.95, 0.7]
        
        x = np.arange(len(comparison_metrics))
        width = 0.35
        
        bars1 = ax2.bar(x - width/2, traditional_llm, width, 
                       label='传统LLM', color='lightcoral', alpha=0.8)
        bars2 = ax2.bar(x + width/2, rag_system, width, 
                       label='RAG系统', color='lightgreen', alpha=0.8)
        
        ax2.set_xlabel('评估指标')
        ax2.set_ylabel('评分')
        ax2.set_title('RAG vs 传统LLM性能对比', fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels(comparison_metrics, rotation=45, ha='right')
        ax2.legend()
        ax2.set_ylim(0, 1)
        
        # 添加数值标签
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{height:.2f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.show()
    
    def analyze_rag_workflow(self):
        """分析RAG工作流程"""
        workflow_steps = {
            '1. 查询处理': {
                '描述': '对用户查询进行预处理和理解',
                '技术': '查询扩展、意图识别、关键词提取',
                '时间': 0.1,
                '重要性': 0.8
            },
            '2. 向量检索': {
                '描述': '在向量空间中搜索相关文档',
                '技术': '语义相似度计算、近似最近邻搜索',
                '时间': 0.3,
                '重要性': 0.95
            },
            '3. 文档排序': {
                '描述': '对检索结果进行相关性排序',
                '技术': '重排序算法、相关性评分',
                '时间': 0.2,
                '重要性': 0.85
            },
            '4. 上下文构建': {
                '描述': '将检索文档整合为生成上下文',
                '技术': '文档截断、信息融合、去重',
                '时间': 0.15,
                '重要性': 0.9
            },
            '5. 答案生成': {
                '描述': '基于上下文生成最终回答',
                '技术': '条件生成、一致性检查',
                '时间': 0.8,
                '重要性': 0.92
            },
            '6. 后处理': {
                '描述': '对生成结果进行优化和验证',
                '技术': '事实检查、格式化、质量评估',
                '时间': 0.1,
                '重要性': 0.75
            }
        }
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('RAG工作流程分析', fontsize=16, fontweight='bold')
        
        # 时间分布
        steps = list(workflow_steps.keys())
        times = [workflow_steps[step]['时间'] for step in steps]
        
        wedges, texts, autotexts = ax1.pie(times, labels=steps, autopct='%1.1f%%',
                                          startangle=90, colors=plt.cm.Set3(np.linspace(0, 1, len(steps))))
        ax1.set_title('各步骤时间分布', fontweight='bold')
        
        # 重要性评分
        importance = [workflow_steps[step]['重要性'] for step in steps]
        
        bars = ax2.barh(steps, importance, color=plt.cm.viridis(np.array(importance)))
        ax2.set_xlabel('重要性评分')
        ax2.set_title('各步骤重要性评估', fontweight='bold')
        ax2.set_xlim(0, 1)
        
        for bar, imp in zip(bars, importance):
            width = bar.get_width()
            ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2,
                    f'{imp:.2f}', ha='left', va='center', fontweight='bold')
        
        # 时间-重要性散点图
        colors = plt.cm.plasma(np.array(importance))
        scatter = ax3.scatter(times, importance, s=200, c=colors, alpha=0.7, edgecolors='black')
        
        for i, step in enumerate(steps):
            ax3.annotate(step.split('.')[1].strip(), (times[i], importance[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax3.set_xlabel('执行时间(秒)')
        ax3.set_ylabel('重要性评分')
        ax3.set_title('时间-重要性关系分析', fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 优化潜力分析
        optimization_potential = []
        for step in steps:
            time_cost = workflow_steps[step]['时间']
            importance = workflow_steps[step]['重要性']
            # 高时间成本、低重要性的步骤有更大优化潜力
            potential = time_cost * (1 - importance)
            optimization_potential.append(potential)
        
        bars = ax4.bar(range(len(steps)), optimization_potential, 
                      color=plt.cm.coolwarm(np.array(optimization_potential)/max(optimization_potential)))
        ax4.set_xlabel('工作流程步骤')
        ax4.set_ylabel('优化潜力')
        ax4.set_title('各步骤优化潜力分析', fontweight='bold')
        ax4.set_xticks(range(len(steps)))
        ax4.set_xticklabels([s.split('.')[0] for s in steps])
        
        for bar, pot in zip(bars, optimization_potential):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{pot:.3f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.show()
    
    def demonstrate_retrieval_methods(self):
        """演示不同检索方法"""
        retrieval_methods = {
            '关键词检索': {
                '原理': '基于词汇匹配的传统检索',
                '优势': '简单快速、可解释性强',
                '劣势': '无法理解语义、召回率低',
                '准确率': 0.65,
                '召回率': 0.70,
                '速度': 0.95
            },
            '向量检索': {
                '原理': '基于语义向量的相似度检索',
                '优势': '语义理解强、召回率高',
                '劣势': '计算复杂、需要向量化',
                '准确率': 0.85,
                '召回率': 0.88,
                '速度': 0.75
            },
            '混合检索': {
                '原理': '结合关键词和向量检索',
                '优势': '兼顾精确匹配和语义理解',
                '劣势': '系统复杂度高',
                '准确率': 0.90,
                '召回率': 0.92,
                '速度': 0.70
            },
            '重排序检索': {
                '原理': '多阶段检索和重新排序',
                '优势': '检索质量最高',
                '劣势': '计算成本最高',
                '准确率': 0.95,
                '召回率': 0.90,
                '速度': 0.60
            }
        }
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle('检索方法对比分析', fontsize=16, fontweight='bold')
        
        # 性能雷达图
        methods = list(retrieval_methods.keys())
        metrics = ['准确率', '召回率', '速度']
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # 闭合图形
        
        ax1 = plt.subplot(121, projection='polar')
        
        colors = ['red', 'blue', 'green', 'orange']
        for i, method in enumerate(methods):
            values = [retrieval_methods[method][metric] for metric in metrics]
            values += values[:1]  # 闭合图形
            
            ax1.plot(angles, values, 'o-', linewidth=2, 
                    label=method, color=colors[i])
            ax1.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax1.set_xticks(angles[:-1])
        ax1.set_xticklabels(metrics)
        ax1.set_ylim(0, 1)
        ax1.set_title('检索方法性能雷达图', fontweight='bold', pad=20)
        ax1.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
        ax1.grid(True)
        
        # 方法选择决策树
        ax2.set_xlim(0, 10)
        ax2.set_ylim(0, 10)
        ax2.set_title('检索方法选择指南', fontweight='bold')
        
        # 决策节点
        decisions = [
            {'text': '性能要求高？', 'pos': (5, 8), 'size': (2, 1)},
            {'text': '计算资源充足？', 'pos': (2, 6), 'size': (1.8, 0.8)},
            {'text': '需要可解释性？', 'pos': (8, 6), 'size': (1.8, 0.8)},
            {'text': '重排序检索', 'pos': (1, 4), 'size': (1.5, 0.8)},
            {'text': '混合检索', 'pos': (3, 4), 'size': (1.5, 0.8)},
            {'text': '向量检索', 'pos': (7, 4), 'size': (1.5, 0.8)},
            {'text': '关键词检索', 'pos': (9, 4), 'size': (1.5, 0.8)}
        ]
        
        for i, decision in enumerate(decisions):
            if i < 3:  # 决策节点
                shape = Circle((decision['pos'][0], decision['pos'][1]), 0.8, 
                              facecolor='lightblue', edgecolor='black')
            else:  # 结果节点
                shape = Rectangle((decision['pos'][0]-0.75, decision['pos'][1]-0.4), 
                                1.5, 0.8, facecolor='lightgreen', edgecolor='black')
            
            ax2.add_patch(shape)
            ax2.text(decision['pos'][0], decision['pos'][1], decision['text'],
                    ha='center', va='center', fontsize=9, fontweight='bold')
        
        # 连接线
        connections = [
            ((5, 7.2), (2, 6.8), '是'),
            ((5, 7.2), (8, 6.8), '否'),
            ((2, 5.2), (1, 4.8), '是'),
            ((2, 5.2), (3, 4.8), '否'),
            ((8, 5.2), (7, 4.8), '否'),
            ((8, 5.2), (9, 4.8), '是')
        ]
        
        for start, end, label in connections:
            ax2.annotate('', xy=end, xytext=start,
                        arrowprops=dict(arrowstyle='->', lw=1.5, color='red'))
            # 添加标签
            mid_x, mid_y = (start[0] + end[0])/2, (start[1] + end[1])/2
            ax2.text(mid_x, mid_y, label, ha='center', va='center', 
                    fontsize=8, bbox=dict(boxstyle="round,pad=0.2", facecolor='white'))
        
        ax2.axis('off')
        
        plt.tight_layout()
        plt.show()

# 演示RAG架构分析
rag_analyzer = RAGArchitectureAnalyzer()
rag_analyzer.demonstrate_rag_architecture()
rag_analyzer.analyze_rag_workflow()
rag_analyzer.demonstrate_retrieval_methods()

print("\n=== RAG核心概念 ===")
core_concepts = {
    "RAG定义": "结合检索和生成的混合AI架构",
    "核心优势": "实时知识更新、提高准确性、增强可解释性",
    "关键组件": "知识库、检索器、生成器、融合模块",
    "应用场景": "问答系统、知识助手、文档分析、客服机器人",
    "技术挑战": "检索质量、上下文长度、计算效率、一致性保证"
}

for concept, description in core_concepts.items():
    print(f"• {concept}: {description}")
```

---

## 3.5.2 向量数据库与嵌入技术

### 向量数据库的核心作用

向量数据库是RAG系统的核心基础设施，负责高效存储和检索高维向量表示的文档。

```python
class VectorDatabaseAnalyzer:
    def __init__(self):
        self.embedding_models = {
            'Word2Vec': {'维度': 300, '训练数据': '通用文本', '性能': 0.7},
            'GloVe': {'维度': 300, '训练数据': '通用文本', '性能': 0.72},
            'FastText': {'维度': 300, '训练数据': '通用文本', '性能': 0.75},
            'BERT': {'维度': 768, '训练数据': '大规模预训练', '性能': 0.85},
            'Sentence-BERT': {'维度': 768, '训练数据': '句子对', '性能': 0.88},
            'OpenAI Ada': {'维度': 1536, '训练数据': '多模态', '性能': 0.92}
        }
        
    def demonstrate_embedding_evolution(self):
        """演示嵌入技术的演进"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('向量嵌入技术演进分析', fontsize=16, fontweight='bold')
        
        models = list(self.embedding_models.keys())
        dimensions = [self.embedding_models[m]['维度'] for m in models]
        performances = [self.embedding_models[m]['性能'] for m in models]
        
        # 维度演进
        colors = plt.cm.viridis(np.linspace(0, 1, len(models)))
        bars1 = ax1.bar(models, dimensions, color=colors)
        ax1.set_ylabel('向量维度')
        ax1.set_title('嵌入模型维度演进', fontweight='bold')
        ax1.tick_params(axis='x', rotation=45)
        
        for bar, dim in zip(bars1, dimensions):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 10,
                    f'{dim}', ha='center', va='bottom', fontweight='bold')
        
        # 性能提升
        ax2.plot(models, performances, 'ro-', linewidth=3, markersize=8)
        ax2.set_ylabel('性能评分')
        ax2.set_title('嵌入模型性能演进', fontweight='bold')
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0.6, 1.0)
        
        for i, (model, perf) in enumerate(zip(models, performances)):
            ax2.annotate(f'{perf:.2f}', (i, perf), 
                        xytext=(0, 10), textcoords='offset points',
                        ha='center', fontweight='bold')
        
        # 维度vs性能散点图
        scatter = ax3.scatter(dimensions, performances, s=200, 
                            c=range(len(models)), cmap='plasma', alpha=0.7)
        
        for i, model in enumerate(models):
            ax3.annotate(model, (dimensions[i], performances[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax3.set_xlabel('向量维度')
        ax3.set_ylabel('性能评分')
        ax3.set_title('维度-性能关系分析', fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 技术特点雷达图
        features = ['语义理解', '计算效率', '存储效率', '训练复杂度', '适用性']
        model_features = {
            'Word2Vec': [0.6, 0.8, 0.9, 0.7, 0.8],
            'BERT': [0.9, 0.4, 0.3, 0.3, 0.9],
            'Sentence-BERT': [0.95, 0.6, 0.4, 0.4, 0.95],
            'OpenAI Ada': [0.98, 0.5, 0.2, 0.2, 0.98]
        }
        
        angles = np.linspace(0, 2 * np.pi, len(features), endpoint=False).tolist()
        angles += angles[:1]
        
        ax4 = plt.subplot(224, projection='polar')
        
        colors = ['red', 'blue', 'green', 'orange']
        for i, (model, values) in enumerate(model_features.items()):
            values += values[:1]
            ax4.plot(angles, values, 'o-', linewidth=2, 
                    label=model, color=colors[i])
            ax4.fill(angles, values, alpha=0.1, color=colors[i])
        
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(features)
        ax4.set_ylim(0, 1)
        ax4.set_title('嵌入模型特征对比', fontweight='bold', pad=20)
        ax4.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
        ax4.grid(True)
        
        plt.tight_layout()
        plt.show()
    
    def analyze_vector_databases(self):
        """分析向量数据库特性"""
        vector_dbs = {
            'Faiss': {
                '类型': 'Facebook开源',
                '性能': 0.95,
                '易用性': 0.7,
                '扩展性': 0.8,
                '功能': 0.85,
                '特点': '高性能、多种索引算法'
            },
            'Pinecone': {
                '类型': '云服务',
                '性能': 0.9,
                '易用性': 0.95,
                '扩展性': 0.95,
                '功能': 0.9,
                '特点': '托管服务、自动扩展'
            },
            'Weaviate': {
                '类型': '开源图数据库',
                '性能': 0.85,
                '易用性': 0.8,
                '扩展性': 0.85,
                '功能': 0.95,
                '特点': '图结构、多模态支持'
            },
            'Milvus': {
                '类型': '开源分布式',
                '性能': 0.9,
                '易用性': 0.75,
                '扩展性': 0.9,
                '功能': 0.88,
                '特点': '分布式、高并发'
            },
            'Chroma': {
                '类型': '轻量级开源',
                '性能': 0.8,
                '易用性': 0.9,
                '扩展性': 0.7,
                '功能': 0.8,
                '特点': '简单易用、快速部署'
            }
        }
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('向量数据库对比分析', fontsize=16, fontweight='bold')
        
        # 综合评分对比
        dbs = list(vector_dbs.keys())
        metrics = ['性能', '易用性', '扩展性', '功能']
        
        # 计算综合评分
        overall_scores = []
        for db in dbs:
            score = sum(vector_dbs[db][metric] for metric in metrics) / len(metrics)
            overall_scores.append(score)
        
        bars = ax1.bar(dbs, overall_scores, 
                      color=plt.cm.coolwarm(np.array(overall_scores)))
        ax1.set_ylabel('综合评分')
        ax1.set_title('向量数据库综合评分', fontweight='bold')
        ax1.tick_params(axis='x', rotation=45)
        ax1.set_ylim(0, 1)
        
        for bar, score in zip(bars, overall_scores):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # 性能矩阵热力图
        performance_matrix = np.array([
            [vector_dbs[db][metric] for metric in metrics] for db in dbs
        ])
        
        im = ax2.imshow(performance_matrix, cmap='RdYlGn', aspect='auto')
        ax2.set_xticks(range(len(metrics)))
        ax2.set_xticklabels(metrics)
        ax2.set_yticks(range(len(dbs)))
        ax2.set_yticklabels(dbs)
        ax2.set_title('性能指标热力图', fontweight='bold')
        
        # 添加数值标签
        for i in range(len(dbs)):
            for j in range(len(metrics)):
                text = ax2.text(j, i, f'{performance_matrix[i, j]:.2f}',
                               ha="center", va="center", color="black", fontweight='bold')
        
        # 颜色条
        cbar = plt.colorbar(im, ax=ax2, shrink=0.8)
        cbar.set_label('评分', rotation=270, labelpad=15)
        
        # 使用场景分布
        use_cases = {
            '原型开发': ['Chroma', 'Faiss'],
            '生产环境': ['Pinecone', 'Milvus'],
            '企业级': ['Weaviate', 'Milvus'],
            '研究实验': ['Faiss', 'Chroma'],
            '云原生': ['Pinecone']
        }
        
        # 统计每个数据库的使用场景数量
        db_usage_count = {db: 0 for db in dbs}
        for scenarios in use_cases.values():
            for db in scenarios:
                if db in db_usage_count:
                    db_usage_count[db] += 1
        
        usage_dbs = list(db_usage_count.keys())
        usage_counts = list(db_usage_count.values())
        
        wedges, texts, autotexts = ax3.pie(usage_counts, labels=usage_dbs, 
                                          autopct='%1.1f%%', startangle=90,
                                          colors=plt.cm.Set3(np.linspace(0, 1, len(usage_dbs))))
        ax3.set_title('使用场景适用性分布', fontweight='bold')
        
        # 选择决策矩阵
        decision_factors = {
            '数据规模': {'小': 'Chroma', '中': 'Faiss', '大': 'Milvus/Pinecone'},
            '技术团队': {'初级': 'Pinecone', '中级': 'Chroma', '高级': 'Faiss/Milvus'},
            '预算': {'低': 'Faiss/Chroma', '中': 'Milvus', '高': 'Pinecone'},
            '维护': {'托管': 'Pinecone', '自维护': 'Faiss/Milvus/Chroma'}
        }
        
        ax4.axis('off')
        ax4.set_title('选择决策指南', fontweight='bold')
        
        y_pos = 0.9
        for factor, options in decision_factors.items():
            ax4.text(0.1, y_pos, f"{factor}:", fontweight='bold', fontsize=12)
            y_pos -= 0.1
            for level, recommendation in options.items():
                ax4.text(0.15, y_pos, f"• {level}: {recommendation}", fontsize=10)
                y_pos -= 0.08
            y_pos -= 0.05
        
        plt.tight_layout()
        plt.show()
    
    def demonstrate_similarity_search(self):
        """演示相似度搜索算法"""
        # 模拟向量数据
        np.random.seed(42)
        n_docs = 1000
        n_dims = 128
        
        # 生成文档向量
        doc_vectors = np.random.randn(n_docs, n_dims)
        doc_vectors = doc_vectors / np.linalg.norm(doc_vectors, axis=1, keepdims=True)
        
        # 查询向量
        query_vector = np.random.randn(1, n_dims)
        query_vector = query_vector / np.linalg.norm(query_vector)
        
        # 计算相似度
        similarities = cosine_similarity(query_vector, doc_vectors)[0]
        
        # 不同搜索算法的性能模拟
        search_algorithms = {
            '线性搜索': {'时间复杂度': 'O(n)', '准确率': 1.0, '速度': 0.3},
            'LSH': {'时间复杂度': 'O(log n)', '准确率': 0.85, '速度': 0.8},
            'IVF': {'时间复杂度': 'O(√n)', '准确率': 0.92, '速度': 0.7},
            'HNSW': {'时间复杂度': 'O(log n)', '准确率': 0.95, '速度': 0.9},
            'Annoy': {'时间复杂度': 'O(log n)', '准确率': 0.88, '速度': 0.85}
        }
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('相似度搜索算法分析', fontsize=16, fontweight='bold')
        
        # 相似度分布
        ax1.hist(similarities, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.axvline(np.mean(similarities), color='red', linestyle='--', 
                   linewidth=2, label=f'平均值: {np.mean(similarities):.3f}')
        ax1.axvline(np.percentile(similarities, 95), color='green', linestyle='--', 
                   linewidth=2, label=f'95%分位: {np.percentile(similarities, 95):.3f}')
        ax1.set_xlabel('余弦相似度')
        ax1.set_ylabel('文档数量')
        ax1.set_title('文档相似度分布', fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 算法性能对比
        algorithms = list(search_algorithms.keys())
        accuracies = [search_algorithms[alg]['准确率'] for alg in algorithms]
        speeds = [search_algorithms[alg]['速度'] for alg in algorithms]
        
        scatter = ax2.scatter(speeds, accuracies, s=200, 
                            c=range(len(algorithms)), cmap='viridis', alpha=0.7)
        
        for i, alg in enumerate(algorithms):
            ax2.annotate(alg, (speeds[i], accuracies[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax2.set_xlabel('搜索速度')
        ax2.set_ylabel('准确率')
        ax2.set_title('算法性能权衡分析', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0.8, 1.0)
        
        # Top-K检索结果
        top_k = 10
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        top_similarities = similarities[top_indices]
        
        bars = ax3.bar(range(top_k), top_similarities, 
                      color=plt.cm.plasma(top_similarities))
        ax3.set_xlabel('排名')
        ax3.set_ylabel('相似度')
        ax3.set_title(f'Top-{top_k} 检索结果', fontweight='bold')
        ax3.set_xticks(range(top_k))
        ax3.set_xticklabels([f'#{i+1}' for i in range(top_k)])
        
        for bar, sim in zip(bars, top_similarities):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                    f'{sim:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 搜索效率vs数据规模
        data_sizes = [1000, 10000, 100000, 1000000, 10000000]
        
        # 模拟不同算法的搜索时间（相对值）
        linear_times = [size/1000 for size in data_sizes]
        hnsw_times = [np.log2(size/1000) for size in data_sizes]
        ivf_times = [np.sqrt(size/1000) for size in data_sizes]
        
        ax4.loglog(data_sizes, linear_times, 'r-o', label='线性搜索', linewidth=2)
        ax4.loglog(data_sizes, hnsw_times, 'b-s', label='HNSW', linewidth=2)
        ax4.loglog(data_sizes, ivf_times, 'g-^', label='IVF', linewidth=2)
        
        ax4.set_xlabel('数据规模')
        ax4.set_ylabel('搜索时间(相对值)')
        ax4.set_title('搜索效率vs数据规模', fontweight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# 演示向量数据库分析
vdb_analyzer = VectorDatabaseAnalyzer()
vdb_analyzer.demonstrate_embedding_evolution()
vdb_analyzer.analyze_vector_databases()
vdb_analyzer.demonstrate_similarity_search()

print("\n=== 向量数据库核心要点 ===")
vector_db_concepts = {
    "嵌入技术": "将文本转换为高维向量的核心技术",
    "相似度计算": "余弦相似度、欧氏距离、点积等度量方法",
    "索引算法": "HNSW、IVF、LSH等高效检索算法",
    "存储优化": "向量压缩、分片存储、缓存策略",
    "性能调优": "索引参数、批处理、并行计算"
}

for concept, description in vector_db_concepts.items():
    print(f"• {concept}: {description}")
```

---

## 本节小结

通过本节的学习，我们深入了解了RAG技术的核心原理和实现方法：

### 🎯 核心要点回顾

1. **RAG架构**: 理解检索-生成混合架构的设计原理
2. **工作流程**: 掌握从查询到答案生成的完整流程
3. **检索方法**: 学习不同检索策略的特点和适用场景
4. **向量数据库**: 了解嵌入技术和向量存储的核心技术
5. **相似度搜索**: 掌握高效的向量检索算法

### 🛠️ Trae实践要点

- **架构设计**: 根据应用需求选择合适的RAG架构
- **数据准备**: 重视知识库的构建和向量化质量
- **检索优化**: 平衡检索精度和效率
- **系统集成**: 确保各组件的协调工作

### 🤔 深度思考题

1. 如何评估RAG系统的检索质量？
2. 向量数据库的选择标准是什么？
3. 如何处理检索结果与生成内容的一致性？

---

## 3.5.3 检索策略优化与高级技术

### 多阶段检索策略

现代RAG系统采用多阶段检索来提高检索质量和效率。

```python
class AdvancedRetrievalAnalyzer:
    def __init__(self):
        self.retrieval_stages = {
            '粗检索': {'召回数量': 1000, '精度': 0.7, '速度': 0.95},
            '精检索': {'召回数量': 100, '精度': 0.85, '速度': 0.7},
            '重排序': {'召回数量': 10, '精度': 0.95, '速度': 0.5}
        }
        
    def demonstrate_multi_stage_retrieval(self):
        """演示多阶段检索流程"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('多阶段检索策略分析', fontsize=16, fontweight='bold')
        
        # 检索漏斗图
        stages = list(self.retrieval_stages.keys())
        recalls = [self.retrieval_stages[stage]['召回数量'] for stage in stages]
        precisions = [self.retrieval_stages[stage]['精度'] for stage in stages]
        
        # 漏斗图
        y_positions = np.arange(len(stages))
        colors = ['lightcoral', 'lightblue', 'lightgreen']
        
        for i, (stage, recall, color) in enumerate(zip(stages, recalls, colors)):
            width = recall / max(recalls) * 0.8  # 归一化宽度
            rect = Rectangle((0.5 - width/2, i - 0.3), width, 0.6, 
                           facecolor=color, edgecolor='black', alpha=0.7)
            ax1.add_patch(rect)
            ax1.text(0.5, i, f'{stage}\n{recall}个文档', 
                    ha='center', va='center', fontweight='bold')
        
        ax1.set_xlim(0, 1)
        ax1.set_ylim(-0.5, len(stages) - 0.5)
        ax1.set_yticks(y_positions)
        ax1.set_yticklabels(stages)
        ax1.set_title('检索漏斗流程', fontweight='bold')
        ax1.axis('off')
        
        # 精度提升曲线
        ax2.plot(stages, precisions, 'ro-', linewidth=3, markersize=10)
        ax2.set_ylabel('检索精度')
        ax2.set_title('各阶段精度提升', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0.6, 1.0)
        
        for i, (stage, precision) in enumerate(zip(stages, precisions)):
            ax2.annotate(f'{precision:.2f}', (i, precision), 
                        xytext=(0, 10), textcoords='offset points',
                        ha='center', fontweight='bold')
        
        # 成本效益分析
        speeds = [self.retrieval_stages[stage]['速度'] for stage in stages]
        
        # 计算累积成本（1-速度）和累积收益（精度提升）
        cumulative_cost = np.cumsum([1-s for s in speeds])
        cumulative_benefit = np.array(precisions)
        
        ax3.plot(cumulative_cost, cumulative_benefit, 'bs-', linewidth=2, markersize=8)
        
        for i, stage in enumerate(stages):
            ax3.annotate(stage, (cumulative_cost[i], cumulative_benefit[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax3.set_xlabel('累积计算成本')
        ax3.set_ylabel('检索精度')
        ax3.set_title('成本-效益权衡分析', fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 检索策略对比
        strategies = {
            '单阶段检索': {'精度': 0.75, '速度': 0.8, '复杂度': 0.3},
            '两阶段检索': {'精度': 0.88, '速度': 0.65, '复杂度': 0.6},
            '三阶段检索': {'精度': 0.95, '速度': 0.5, '复杂度': 0.9}
        }
        
        strategy_names = list(strategies.keys())
        metrics = ['精度', '速度', '复杂度']
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]
        
        ax4 = plt.subplot(224, projection='polar')
        
        colors = ['red', 'blue', 'green']
        for i, (strategy, values_dict) in enumerate(strategies.items()):
            values = [values_dict[metric] for metric in metrics]
            values += values[:1]
            
            ax4.plot(angles, values, 'o-', linewidth=2, 
                    label=strategy, color=colors[i])
            ax4.fill(angles, values, alpha=0.1, color=colors[i])
        
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(metrics)
        ax4.set_ylim(0, 1)
        ax4.set_title('检索策略综合对比', fontweight='bold', pad=20)
        ax4.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
        ax4.grid(True)
        
        plt.tight_layout()
        plt.show()
    
    def analyze_query_expansion(self):
        """分析查询扩展技术"""
        expansion_methods = {
            '同义词扩展': {
                '原理': '基于词典的同义词替换',
                '效果': 0.15,
                '复杂度': 0.2,
                '适用场景': '通用查询'
            },
            '词嵌入扩展': {
                '原理': '基于词向量的语义相似词',
                '效果': 0.25,
                '复杂度': 0.5,
                '适用场景': '语义查询'
            },
            '伪相关反馈': {
                '原理': '利用初始检索结果扩展查询',
                '效果': 0.35,
                '复杂度': 0.7,
                '适用场景': '迭代检索'
            },
            'LLM查询重写': {
                '原理': '使用语言模型重写和扩展查询',
                '效果': 0.45,
                '复杂度': 0.9,
                '适用场景': '复杂查询'
            }
        }
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('查询扩展技术分析', fontsize=16, fontweight='bold')
        
        methods = list(expansion_methods.keys())
        effects = [expansion_methods[m]['效果'] for m in methods]
        complexities = [expansion_methods[m]['复杂度'] for m in methods]
        
        # 效果对比
        bars = ax1.bar(methods, effects, color=plt.cm.viridis(np.array(effects)))
        ax1.set_ylabel('性能提升')
        ax1.set_title('查询扩展效果对比', fontweight='bold')
        ax1.tick_params(axis='x', rotation=45)
        
        for bar, effect in zip(bars, effects):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{effect:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # 效果vs复杂度散点图
        scatter = ax2.scatter(complexities, effects, s=200, 
                            c=range(len(methods)), cmap='plasma', alpha=0.7)
        
        for i, method in enumerate(methods):
            ax2.annotate(method, (complexities[i], effects[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax2.set_xlabel('实现复杂度')
        ax2.set_ylabel('性能提升')
        ax2.set_title('效果-复杂度权衡', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # 查询扩展示例
        query_examples = {
            '原始查询': 'Python机器学习',
            '同义词扩展': 'Python机器学习 ML 人工智能 算法',
            '语义扩展': 'Python机器学习 scikit-learn pandas numpy 数据科学',
            'LLM重写': 'Python编程语言在机器学习和人工智能领域的应用，包括相关库和框架'
        }
        
        ax3.axis('off')
        ax3.set_title('查询扩展示例', fontweight='bold')
        
        y_pos = 0.9
        for query_type, query_text in query_examples.items():
            ax3.text(0.05, y_pos, f"{query_type}:", fontweight='bold', fontsize=11)
            y_pos -= 0.1
            # 文本换行处理
            wrapped_text = '\n'.join([query_text[i:i+50] for i in range(0, len(query_text), 50)])
            ax3.text(0.1, y_pos, wrapped_text, fontsize=10, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightgray'))
            y_pos -= 0.15
        
        # 扩展策略选择指南
        selection_guide = {
            '查询长度': {'短查询': 'LLM重写', '中等查询': '语义扩展', '长查询': '伪相关反馈'},
            '领域特性': {'通用': '同义词扩展', '专业': '词嵌入扩展', '复杂': 'LLM重写'},
            '性能要求': {'实时': '同义词扩展', '准确': 'LLM重写', '平衡': '语义扩展'}
        }
        
        ax4.axis('off')
        ax4.set_title('扩展策略选择指南', fontweight='bold')
        
        y_pos = 0.9
        for factor, recommendations in selection_guide.items():
            ax4.text(0.05, y_pos, f"{factor}:", fontweight='bold', fontsize=11)
            y_pos -= 0.1
            for condition, method in recommendations.items():
                ax4.text(0.1, y_pos, f"• {condition}: {method}", fontsize=10)
                y_pos -= 0.08
            y_pos -= 0.05
        
        plt.tight_layout()
        plt.show()
    
    def demonstrate_hybrid_search(self):
        """演示混合搜索策略"""
        # 模拟不同搜索方法的结果
        np.random.seed(42)
        n_docs = 100
        
        # 关键词搜索结果（精确匹配高，语义理解低）
        keyword_scores = np.random.beta(2, 5, n_docs)  # 偏向低分
        keyword_scores[:10] = np.random.beta(5, 2, 10)  # 前10个高分
        
        # 向量搜索结果（语义理解高，精确匹配中等）
        vector_scores = np.random.beta(3, 3, n_docs)  # 均匀分布
        
        # 混合搜索结果
        alpha = 0.6  # 向量搜索权重
        hybrid_scores = alpha * vector_scores + (1 - alpha) * keyword_scores
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('混合搜索策略分析', fontsize=16, fontweight='bold')
        
        # 分数分布对比
        bins = np.linspace(0, 1, 20)
        
        ax1.hist(keyword_scores, bins=bins, alpha=0.5, label='关键词搜索', 
                color='red', density=True)
        ax1.hist(vector_scores, bins=bins, alpha=0.5, label='向量搜索', 
                color='blue', density=True)
        ax1.hist(hybrid_scores, bins=bins, alpha=0.5, label='混合搜索', 
                color='green', density=True)
        
        ax1.set_xlabel('相关性分数')
        ax1.set_ylabel('密度')
        ax1.set_title('不同搜索方法分数分布', fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Top-K结果对比
        k = 10
        top_k_keyword = np.argsort(keyword_scores)[-k:][::-1]
        top_k_vector = np.argsort(vector_scores)[-k:][::-1]
        top_k_hybrid = np.argsort(hybrid_scores)[-k:][::-1]
        
        x = np.arange(k)
        width = 0.25
        
        bars1 = ax2.bar(x - width, keyword_scores[top_k_keyword], width, 
                       label='关键词', color='red', alpha=0.7)
        bars2 = ax2.bar(x, vector_scores[top_k_vector], width, 
                       label='向量', color='blue', alpha=0.7)
        bars3 = ax2.bar(x + width, hybrid_scores[top_k_hybrid], width, 
                       label='混合', color='green', alpha=0.7)
        
        ax2.set_xlabel('排名')
        ax2.set_ylabel('相关性分数')
        ax2.set_title(f'Top-{k} 结果质量对比', fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels([f'#{i+1}' for i in range(k)])
        ax2.legend()
        
        # 权重调优分析
        alphas = np.linspace(0, 1, 21)
        precision_scores = []
        recall_scores = []
        
        # 模拟真实相关文档（前20个）
        relevant_docs = set(range(20))
        
        for alpha in alphas:
            hybrid = alpha * vector_scores + (1 - alpha) * keyword_scores
            top_10 = set(np.argsort(hybrid)[-10:])
            
            precision = len(top_10 & relevant_docs) / len(top_10)
            recall = len(top_10 & relevant_docs) / len(relevant_docs)
            
            precision_scores.append(precision)
            recall_scores.append(recall)
        
        ax3.plot(alphas, precision_scores, 'r-o', label='精确率', linewidth=2)
        ax3.plot(alphas, recall_scores, 'b-s', label='召回率', linewidth=2)
        
        # 找到最优权重
        f1_scores = [2 * p * r / (p + r) if (p + r) > 0 else 0 
                    for p, r in zip(precision_scores, recall_scores)]
        best_alpha_idx = np.argmax(f1_scores)
        best_alpha = alphas[best_alpha_idx]
        
        ax3.axvline(best_alpha, color='green', linestyle='--', 
                   label=f'最优权重: {best_alpha:.2f}')
        
        ax3.set_xlabel('向量搜索权重 (α)')
        ax3.set_ylabel('评估指标')
        ax3.set_title('混合搜索权重调优', fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 搜索策略适用场景
        scenarios = {
            '精确匹配': {'关键词': 0.9, '向量': 0.6, '混合': 0.85},
            '语义理解': {'关键词': 0.4, '向量': 0.95, '混合': 0.8},
            '多语言': {'关键词': 0.3, '向量': 0.8, '混合': 0.7},
            '长文档': {'关键词': 0.6, '向量': 0.85, '混合': 0.9},
            '实时性': {'关键词': 0.95, '向量': 0.7, '混合': 0.75}
        }
        
        scenario_names = list(scenarios.keys())
        search_methods = ['关键词', '向量', '混合']
        
        # 创建热力图数据
        heatmap_data = np.array([
            [scenarios[scenario][method] for method in search_methods]
            for scenario in scenario_names
        ])
        
        im = ax4.imshow(heatmap_data, cmap='RdYlGn', aspect='auto')
        ax4.set_xticks(range(len(search_methods)))
        ax4.set_xticklabels(search_methods)
        ax4.set_yticks(range(len(scenario_names)))
        ax4.set_yticklabels(scenario_names)
        ax4.set_title('不同场景适用性热力图', fontweight='bold')
        
        # 添加数值标签
        for i in range(len(scenario_names)):
            for j in range(len(search_methods)):
                text = ax4.text(j, i, f'{heatmap_data[i, j]:.2f}',
                               ha="center", va="center", color="black", fontweight='bold')
        
        # 颜色条
        cbar = plt.colorbar(im, ax=ax4, shrink=0.8)
        cbar.set_label('适用性评分', rotation=270, labelpad=15)
        
        plt.tight_layout()
        plt.show()

# 演示高级检索技术
advanced_retrieval = AdvancedRetrievalAnalyzer()
advanced_retrieval.demonstrate_multi_stage_retrieval()
advanced_retrieval.analyze_query_expansion()
advanced_retrieval.demonstrate_hybrid_search()

print("\n=== 高级检索策略要点 ===")
advanced_concepts = {
    "多阶段检索": "粗检索→精检索→重排序的渐进式策略",
    "查询扩展": "通过同义词、语义词、LLM重写扩展查询",
    "混合搜索": "结合关键词和向量搜索的优势",
    "权重调优": "根据应用场景调整不同搜索方法的权重",
    "性能优化": "平衡检索质量和计算效率"
}

for concept, description in advanced_concepts.items():
    print(f"• {concept}: {description}")
```

---

## 3.5.4 RAG系统评估与优化

### 评估指标体系

RAG系统的评估需要从多个维度进行综合考量。

```python
class RAGEvaluationAnalyzer:
    def __init__(self):
        self.evaluation_metrics = {
            '检索质量': {
                'Hit Rate': '检索结果中包含相关文档的比例',
                'MRR': '平均倒数排名，衡量相关文档的排名质量',
                'NDCG': '归一化折扣累积增益，考虑排名和相关性',
                'Precision@K': '前K个结果中相关文档的比例'
            },
            '生成质量': {
                'BLEU': '与参考答案的n-gram重叠度',
                'ROUGE': '召回导向的文本相似度',
                'BERTScore': '基于BERT的语义相似度',
                'Faithfulness': '生成内容对检索文档的忠实度'
            },
            '系统性能': {
                'Latency': '端到端响应时间',
                'Throughput': '单位时间处理的查询数量',
                'Memory Usage': '系统内存占用',
                'Cost': '计算和存储成本'
            }
        }
    
    def demonstrate_evaluation_framework(self):
        """演示RAG评估框架"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('RAG系统评估框架', fontsize=16, fontweight='bold')
        
        # 评估维度权重分布
        dimensions = list(self.evaluation_metrics.keys())
        weights = [0.4, 0.4, 0.2]  # 检索质量、生成质量、系统性能的权重
        
        wedges, texts, autotexts = ax1.pie(weights, labels=dimensions, autopct='%1.1f%%',
                                          startangle=90, colors=['lightcoral', 'lightblue', 'lightgreen'])
        ax1.set_title('评估维度权重分布', fontweight='bold')
        
        # 不同RAG系统的性能对比
        rag_systems = {
            'Basic RAG': {'检索质量': 0.7, '生成质量': 0.75, '系统性能': 0.9},
            'Advanced RAG': {'检索质量': 0.85, '生成质量': 0.82, '系统性能': 0.75},
            'Optimized RAG': {'检索质量': 0.9, '生成质量': 0.88, '系统性能': 0.8}
        }
        
        systems = list(rag_systems.keys())
        metrics = list(rag_systems['Basic RAG'].keys())
        
        x = np.arange(len(metrics))
        width = 0.25
        
        colors = ['red', 'blue', 'green']
        for i, (system, color) in enumerate(zip(systems, colors)):
            values = [rag_systems[system][metric] for metric in metrics]
            bars = ax2.bar(x + i * width, values, width, label=system, 
                          color=color, alpha=0.7)
            
            # 添加数值标签
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{value:.2f}', ha='center', va='bottom', fontsize=9)
        
        ax2.set_xlabel('评估维度')
        ax2.set_ylabel('评分')
        ax2.set_title('不同RAG系统性能对比', fontweight='bold')
        ax2.set_xticks(x + width)
        ax2.set_xticklabels(metrics)
        ax2.legend()
        ax2.set_ylim(0, 1)
        
        # 评估指标相关性分析
        np.random.seed(42)
        n_experiments = 50
        
        # 模拟不同指标的评分
        hit_rate = np.random.beta(3, 2, n_experiments)
        bleu_score = hit_rate * 0.8 + np.random.normal(0, 0.1, n_experiments)
        bleu_score = np.clip(bleu_score, 0, 1)
        
        latency = 1 - hit_rate * 0.6 + np.random.normal(0, 0.1, n_experiments)
        latency = np.clip(latency, 0.2, 1)
        
        # 散点图矩阵
        metrics_data = {
            'Hit Rate': hit_rate,
            'BLEU Score': bleu_score,
            'Latency': latency
        }
        
        # Hit Rate vs BLEU Score
        ax3.scatter(hit_rate, bleu_score, alpha=0.6, color='blue')
        
        # 计算相关系数
        correlation = np.corrcoef(hit_rate, bleu_score)[0, 1]
        ax3.text(0.05, 0.95, f'相关系数: {correlation:.3f}', 
                transform=ax3.transAxes, fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='white'))
        
        ax3.set_xlabel('Hit Rate')
        ax3.set_ylabel('BLEU Score')
        ax3.set_title('检索质量 vs 生成质量', fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 性能优化路径
        optimization_stages = {
            '基线系统': {'性能': 0.6, '成本': 0.3, '复杂度': 0.2},
            '检索优化': {'性能': 0.75, '成本': 0.5, '复杂度': 0.4},
            '生成优化': {'性能': 0.85, '成本': 0.7, '复杂度': 0.6},
            '端到端优化': {'性能': 0.92, '成本': 0.9, '复杂度': 0.8}
        }
        
        stages = list(optimization_stages.keys())
        performance = [optimization_stages[stage]['性能'] for stage in stages]
        cost = [optimization_stages[stage]['成本'] for stage in stages]
        complexity = [optimization_stages[stage]['复杂度'] for stage in stages]
        
        # 优化路径图
        ax4.plot(cost, performance, 'ro-', linewidth=3, markersize=8, label='性能提升路径')
        
        # 添加复杂度信息（气泡大小）
        for i, (stage, perf, c, comp) in enumerate(zip(stages, performance, cost, complexity)):
            ax4.scatter(c, perf, s=comp*500, alpha=0.3, color='blue')
            ax4.annotate(stage, (c, perf), xytext=(5, 5), 
                        textcoords='offset points', fontsize=9)
        
        ax4.set_xlabel('实施成本')
        ax4.set_ylabel('系统性能')
        ax4.set_title('RAG优化路径分析', fontweight='bold')
        ax4.grid(True, alpha=0.3)
        ax4.legend()
        
        plt.tight_layout()
        plt.show()
    
    def analyze_ablation_study(self):
        """分析消融实验结果"""
        # 模拟消融实验数据
        components = {
            '完整系统': 0.85,
            '移除查询扩展': 0.78,
            '移除重排序': 0.75,
            '移除多阶段检索': 0.70,
            '仅关键词检索': 0.60,
            '仅向量检索': 0.72
        }
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('RAG系统消融实验分析', fontsize=16, fontweight='bold')
        
        # 组件贡献度分析
        comp_names = list(components.keys())
        comp_scores = list(components.values())
        
        # 计算各组件的贡献度
        baseline_score = components['完整系统']
        contributions = {}
        for comp in comp_names[1:]:
            if '移除' in comp:
                contribution = baseline_score - components[comp]
                component_name = comp.replace('移除', '')
                contributions[component_name] = contribution
        
        contrib_names = list(contributions.keys())
        contrib_values = list(contributions.values())
        
        bars = ax1.bar(contrib_names, contrib_values, 
                      color=plt.cm.viridis(np.array(contrib_values)/max(contrib_values)))
        ax1.set_ylabel('性能贡献度')
        ax1.set_title('各组件性能贡献分析', fontweight='bold')
        ax1.tick_params(axis='x', rotation=45)
        
        for bar, value in zip(bars, contrib_values):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 系统配置对比
        bars = ax2.barh(comp_names, comp_scores, 
                       color=plt.cm.plasma(np.array(comp_scores)))
        ax2.set_xlabel('系统性能')
        ax2.set_title('不同配置性能对比', fontweight='bold')
        
        for bar, score in zip(bars, comp_scores):
            width = bar.get_width()
            ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2,
                    f'{score:.2f}', ha='left', va='center', fontweight='bold')
        
        # 性能vs复杂度权衡
        complexity_scores = {
            '完整系统': 1.0,
            '移除查询扩展': 0.8,
            '移除重排序': 0.7,
            '移除多阶段检索': 0.5,
            '仅关键词检索': 0.2,
            '仅向量检索': 0.4
        }
        
        complexity_values = [complexity_scores[comp] for comp in comp_names]
        
        scatter = ax3.scatter(complexity_values, comp_scores, s=200, 
                            c=range(len(comp_names)), cmap='viridis', alpha=0.7)
        
        for i, comp in enumerate(comp_names):
            ax3.annotate(comp, (complexity_values[i], comp_scores[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        ax3.set_xlabel('系统复杂度')
        ax3.set_ylabel('系统性能')
        ax3.set_title('性能-复杂度权衡分析', fontweight='bold')
        ax3.grid(True, alpha=0.3)
        
        # 优化建议决策树
        ax4.set_xlim(0, 10)
        ax4.set_ylim(0, 10)
        ax4.set_title('系统优化决策指南', fontweight='bold')
        
        # 决策节点和建议
        decisions = [
            {'text': '性能要求', 'pos': (5, 8.5), 'type': 'decision'},
            {'text': '高性能需求', 'pos': (2, 7), 'type': 'decision'},
            {'text': '中等性能需求', 'pos': (8, 7), 'type': 'decision'},
            {'text': '完整系统\n(0.85)', 'pos': (1, 5.5), 'type': 'result'},
            {'text': '移除查询扩展\n(0.78)', 'pos': (3, 5.5), 'type': 'result'},
            {'text': '移除重排序\n(0.75)', 'pos': (7, 5.5), 'type': 'result'},
            {'text': '仅向量检索\n(0.72)', 'pos': (9, 5.5), 'type': 'result'}
        ]
        
        for decision in decisions:
            if decision['type'] == 'decision':
                shape = Circle((decision['pos'][0], decision['pos'][1]), 0.8, 
                              facecolor='lightblue', edgecolor='black')
            else:
                shape = Rectangle((decision['pos'][0]-0.8, decision['pos'][1]-0.6), 
                                1.6, 1.2, facecolor='lightgreen', edgecolor='black')
            
            ax4.add_patch(shape)
            ax4.text(decision['pos'][0], decision['pos'][1], decision['text'],
                    ha='center', va='center', fontsize=9, fontweight='bold')
        
        # 连接线
        connections = [
            ((5, 7.7), (2, 7.8)),
            ((5, 7.7), (8, 7.8)),
            ((2, 6.2), (1, 6.1)),
            ((2, 6.2), (3, 6.1)),
            ((8, 6.2), (7, 6.1)),
            ((8, 6.2), (9, 6.1))
        ]
        
        for start, end in connections:
            ax4.annotate('', xy=end, xytext=start,
                        arrowprops=dict(arrowstyle='->', lw=1.5, color='red'))
        
        ax4.axis('off')
        
        plt.tight_layout()
        plt.show()
    
    def demonstrate_online_evaluation(self):
        """演示在线评估方法"""
        # 模拟在线评估数据
        np.random.seed(42)
        days = 30
        
        # 生成时间序列数据
        base_performance = 0.8
        performance_trend = base_performance + 0.1 * np.sin(np.linspace(0, 4*np.pi, days)) + np.random.normal(0, 0.02, days)
        user_satisfaction = performance_trend + np.random.normal(0, 0.05, days)
        query_volume = 1000 + 200 * np.sin(np.linspace(0, 2*np.pi, days)) + np.random.normal(0, 50, days)
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('RAG系统在线评估监控', fontsize=16, fontweight='bold')
        
        # 性能趋势监控
        days_range = range(1, days + 1)
        
        ax1.plot(days_range, performance_trend, 'b-o', linewidth=2, 
                markersize=4, label='系统性能')
        ax1.plot(days_range, user_satisfaction, 'r-s', linewidth=2, 
                markersize=4, label='用户满意度')
        
        # 添加趋势线
        z1 = np.polyfit(days_range, performance_trend, 1)
        p1 = np.poly1d(z1)
        ax1.plot(days_range, p1(days_range), "b--", alpha=0.8, label='性能趋势')
        
        z2 = np.polyfit(days_range, user_satisfaction, 1)
        p2 = np.poly1d(z2)
        ax1.plot(days_range, p2(days_range), "r--", alpha=0.8, label='满意度趋势')
        
        ax1.set_xlabel('天数')
        ax1.set_ylabel('评分')
        ax1.set_title('系统性能趋势监控', fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 查询量与性能关系
        ax2.scatter(query_volume, performance_trend, alpha=0.6, color='blue')
        
        # 拟合回归线
        z = np.polyfit(query_volume, performance_trend, 1)
        p = np.poly1d(z)
        ax2.plot(sorted(query_volume), p(sorted(query_volume)), "r--", alpha=0.8)
        
        correlation = np.corrcoef(query_volume, performance_trend)[0, 1]
        ax2.text(0.05, 0.95, f'相关系数: {correlation:.3f}', 
                transform=ax2.transAxes, fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='white'))
        
        ax2.set_xlabel('日查询量')
        ax2.set_ylabel('系统性能')
        ax2.set_title('负载与性能关系分析', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # A/B测试结果
        ab_test_data = {
            '版本A (基线)': {'转化率': 0.12, '满意度': 0.78, '延迟': 0.8},
            '版本B (优化)': {'转化率': 0.15, '满意度': 0.85, '延迟': 0.6}
        }
        
        versions = list(ab_test_data.keys())
        metrics = ['转化率', '满意度', '延迟']
        
        x = np.arange(len(metrics))
        width = 0.35
        
        version_a_values = [ab_test_data['版本A (基线)'][metric] for metric in metrics]
        version_b_values = [ab_test_data['版本B (优化)'][metric] for metric in metrics]
        
        bars1 = ax3.bar(x - width/2, version_a_values, width, 
                       label='版本A (基线)', color='lightcoral', alpha=0.8)
        bars2 = ax3.bar(x + width/2, version_b_values, width, 
                       label='版本B (优化)', color='lightgreen', alpha=0.8)
        
        ax3.set_xlabel('评估指标')
        ax3.set_ylabel('评分')
        ax3.set_title('A/B测试结果对比', fontweight='bold')
        ax3.set_xticks(x)
        ax3.set_xticklabels(metrics)
        ax3.legend()
        
        # 添加数值标签和提升百分比
        for i, (bar_a, bar_b, val_a, val_b) in enumerate(zip(bars1, bars2, version_a_values, version_b_values)):
            height_a = bar_a.get_height()
            height_b = bar_b.get_height()
            
            ax3.text(bar_a.get_x() + bar_a.get_width()/2., height_a + 0.01,
                    f'{val_a:.2f}', ha='center', va='bottom', fontweight='bold')
            ax3.text(bar_b.get_x() + bar_b.get_width()/2., height_b + 0.01,
                    f'{val_b:.2f}', ha='center', va='bottom', fontweight='bold')
            
            # 计算提升百分比（延迟是越低越好）
            if metrics[i] == '延迟':
                improvement = (val_a - val_b) / val_a * 100
            else:
                improvement = (val_b - val_a) / val_a * 100
            
            ax3.text(i, max(height_a, height_b) + 0.05, f'+{improvement:.1f}%', 
                    ha='center', va='bottom', fontweight='bold', color='green')
        
        # 实时监控仪表板
        current_metrics = {
            '系统可用性': 0.995,
            '平均响应时间': 0.85,  # 归一化值
            '错误率': 0.02,
            '用户满意度': 0.88
        }
        
        # 创建仪表板样式的可视化
        metric_names = list(current_metrics.keys())
        metric_values = list(current_metrics.values())
        
        # 为错误率反转颜色（越低越好）
        colors = []
        for i, (name, value) in enumerate(zip(metric_names, metric_values)):
            if name == '错误率':
                color_value = 1 - value  # 反转
            else:
                color_value = value
            
            if color_value >= 0.8:
                colors.append('green')
            elif color_value >= 0.6:
                colors.append('orange')
            else:
                colors.append('red')
        
        bars = ax4.bar(metric_names, metric_values, color=colors, alpha=0.7)
        ax4.set_ylabel('指标值')
        ax4.set_title('实时系统监控仪表板', fontweight='bold')
        ax4.tick_params(axis='x', rotation=45)
        ax4.set_ylim(0, 1)
        
        # 添加阈值线
        ax4.axhline(y=0.8, color='orange', linestyle='--', alpha=0.7, label='警告阈值')
        ax4.axhline(y=0.6, color='red', linestyle='--', alpha=0.7, label='危险阈值')
        
        for bar, value in zip(bars, metric_values):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax4.legend()
        
        plt.tight_layout()
        plt.show()

# 演示RAG评估分析
rag_evaluation = RAGEvaluationAnalyzer()
rag_evaluation.demonstrate_evaluation_framework()
rag_evaluation.analyze_ablation_study()
rag_evaluation.demonstrate_online_evaluation()

print("\n=== RAG评估优化要点 ===")
evaluation_concepts = {
    "多维评估": "检索质量、生成质量、系统性能的综合评估",
    "消融实验": "通过移除组件分析各部分的贡献度",
    "在线监控": "实时跟踪系统性能和用户满意度",
    "A/B测试": "对比不同版本的实际效果",
    "持续优化": "基于评估结果进行迭代改进"
}

for concept, description in evaluation_concepts.items():
    print(f"• {concept}: {description}")
```

---

## 本节小结

通过本节的学习，我们全面掌握了RAG技术的核心原理和实践方法：

### 🎯 核心要点回顾

1. **RAG架构**: 深入理解检索-生成混合架构的设计原理和工作流程
2. **向量数据库**: 掌握嵌入技术和高效向量存储检索方法
3. **检索策略**: 学习多阶段检索、查询扩展、混合搜索等高级技术
4. **系统评估**: 建立多维度评估体系和持续优化机制

### 🛠️ Trae实践要点

- **架构选择**: 根据应用场景选择合适的RAG架构和组件
- **数据质量**: 重视知识库构建和向量化的质量控制
- **检索优化**: 平衡检索精度、召回率和计算效率
- **评估监控**: 建立完善的评估指标和在线监控体系
- **迭代改进**: 基于评估结果持续优化系统性能

### 🤔 深度思考题

1. 如何设计RAG系统来处理多模态信息（文本、图像、音频）？
2. 在大规模部署中，如何平衡RAG系统的准确性和实时性？
3. 如何评估和提高RAG系统生成内容的事实准确性？
4. 面对知识更新频繁的领域，如何设计动态知识库更新机制？

### 📚 延伸学习

- **多模态RAG**: 探索图文、音频等多模态信息的检索增强
- **实时RAG**: 研究流式处理和增量更新技术
- **个性化RAG**: 学习用户偏好建模和个性化检索
- **联邦RAG**: 了解分布式知识库的协同检索方法

---

**下一节预告**: 我们将学习第三章的总结，回顾大语言模型训练与优化的完整技术体系。