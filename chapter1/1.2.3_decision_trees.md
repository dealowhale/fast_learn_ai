# 1.2.3 å†³ç­–æ ‘

## å­¦ä¹ ç›®æ ‡
ç†è§£å†³ç­–æ ‘çš„æ„å»ºåŸç†ï¼ŒæŒæ¡ä¿¡æ¯å¢ç›Šå’ŒåŸºå°¼ä¸çº¯åº¦ç­‰åˆ†è£‚å‡†åˆ™ï¼Œå­¦ä¼šå¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜ã€‚

## å¼•è¨€ï¼šåƒäººä¸€æ ·æ€è€ƒ

æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªåŒ»ç”Ÿï¼Œéœ€è¦è¯Šæ–­æ‚£è€…æ˜¯å¦æ‚£æœ‰æŸç§ç–¾ç—…ï¼š

```
æ‚£è€…å¹´é¾„ > 50å²ï¼Ÿ
â”œâ”€ æ˜¯ â†’ ä½“æ¸© > 38Â°Cï¼Ÿ
â”‚   â”œâ”€ æ˜¯ â†’ ç™½ç»†èƒè®¡æ•° > 10000ï¼Ÿ
â”‚   â”‚   â”œâ”€ æ˜¯ â†’ è¯Šæ–­ï¼šæ„ŸæŸ“ (90%æ¦‚ç‡)
â”‚   â”‚   â””â”€ å¦ â†’ è¯Šæ–­ï¼šæ™®é€šæ„Ÿå†’ (70%æ¦‚ç‡)
â”‚   â””â”€ å¦ â†’ è¯Šæ–­ï¼šå¥åº· (95%æ¦‚ç‡)
â””â”€ å¦ â†’ å’³å—½æŒç»­ > 7å¤©ï¼Ÿ
    â”œâ”€ æ˜¯ â†’ è¯Šæ–­ï¼šæ”¯æ°”ç®¡ç‚ (80%æ¦‚ç‡)
    â””â”€ å¦ â†’ è¯Šæ–­ï¼šå¥åº· (98%æ¦‚ç‡)
```

è¿™ç§**æ ‘çŠ¶çš„å†³ç­–è¿‡ç¨‹**å°±æ˜¯å†³ç­–æ ‘ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼

## ä»€ä¹ˆæ˜¯å†³ç­–æ ‘ï¼Ÿ

**å†³ç­–æ ‘ (Decision Tree)** æ˜¯ä¸€ç§åŸºäºæ ‘ç»“æ„çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡ä¸€ç³»åˆ—if-elseè§„åˆ™æ¥è¿›è¡Œé¢„æµ‹ã€‚

### æ ¸å¿ƒç»„æˆ

1. **æ ¹èŠ‚ç‚¹ (Root Node)**ï¼šæ•´ä¸ªæ ‘çš„èµ·ç‚¹
2. **å†…éƒ¨èŠ‚ç‚¹ (Internal Node)**ï¼šè¡¨ç¤ºç‰¹å¾æµ‹è¯•
3. **å¶èŠ‚ç‚¹ (Leaf Node)**ï¼šè¡¨ç¤ºé¢„æµ‹ç»“æœ
4. **åˆ†æ”¯ (Branch)**ï¼šè¡¨ç¤ºæµ‹è¯•ç»“æœ

### å†³ç­–æ ‘çš„ä¼˜åŠ¿

- **ç›´è§‚æ˜“æ‡‚**ï¼šå†³ç­–è¿‡ç¨‹æ¸…æ™°å¯è§
- **æ— éœ€æ•°æ®é¢„å¤„ç†**ï¼šå¯å¤„ç†æ•°å€¼å’Œç±»åˆ«ç‰¹å¾
- **è‡ªåŠ¨ç‰¹å¾é€‰æ‹©**ï¼šé‡è¦ç‰¹å¾ä¼šå‡ºç°åœ¨æ ‘çš„ä¸Šå±‚
- **å¤„ç†éçº¿æ€§å…³ç³»**ï¼šé€šè¿‡åˆ†æ®µçº¿æ€§é€¼è¿‘
- **å¤„ç†ç¼ºå¤±å€¼**ï¼šæœ‰ä¸“é—¨çš„å¤„ç†æœºåˆ¶

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.tree import plot_tree, export_text
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
from sklearn.datasets import make_classification, make_regression
import seaborn as sns
import pandas as pd

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ç”Ÿæˆç®€å•çš„äºŒåˆ†ç±»æ•°æ®ç”¨äºæ¼”ç¤º
np.random.seed(42)
n_samples = 200

# ç‰¹å¾ï¼šå¹´é¾„ã€æ”¶å…¥
age = np.random.uniform(18, 80, n_samples)
income = np.random.uniform(20, 200, n_samples)

# æ ‡ç­¾ï¼šæ˜¯å¦è´­ä¹°äº§å“ï¼ˆåŸºäºç®€å•è§„åˆ™ï¼‰
# è§„åˆ™ï¼šå¹´é¾„>40ä¸”æ”¶å…¥>100ï¼Œæˆ–è€…å¹´é¾„<30ä¸”æ”¶å…¥>150
buy_product = ((age > 40) & (income > 100)) | ((age < 30) & (income > 150))
y_simple = buy_product.astype(int)

X_simple = np.column_stack([age, income])
feature_names = ['å¹´é¾„', 'æ”¶å…¥']

print(f"ç®€å•å†³ç­–æ ‘æ•°æ®:")
print(f"æ ·æœ¬æ•°é‡: {len(y_simple)}")
print(f"è´­ä¹°ç‡: {y_simple.mean():.3f}")
print(f"ç‰¹å¾: {feature_names}")
```

## å†³ç­–æ ‘çš„æ„å»ºè¿‡ç¨‹

### 1. åˆ†è£‚å‡†åˆ™

å†³ç­–æ ‘é€šè¿‡é€‰æ‹©æœ€ä½³åˆ†è£‚ç‚¹æ¥æ„å»ºï¼Œå¸¸ç”¨çš„åˆ†è£‚å‡†åˆ™æœ‰ï¼š

#### ä¿¡æ¯å¢ç›Š (Information Gain)

åŸºäº**ä¿¡æ¯ç†µ (Entropy)** çš„æ¦‚å¿µï¼š

**ç†µçš„å®šä¹‰**ï¼š
```
H(S) = -Î£áµ¢ páµ¢ logâ‚‚(páµ¢)
```

**ä¿¡æ¯å¢ç›Š**ï¼š
```
IG(S, A) = H(S) - Î£áµ¥ (|Sáµ¥|/|S|) Ã— H(Sáµ¥)
```

#### åŸºå°¼ä¸çº¯åº¦ (Gini Impurity)

**åŸºå°¼ä¸çº¯åº¦**ï¼š
```
Gini(S) = 1 - Î£áµ¢ páµ¢Â²
```

**åŸºå°¼å¢ç›Š**ï¼š
```
Gini_Gain(S, A) = Gini(S) - Î£áµ¥ (|Sáµ¥|/|S|) Ã— Gini(Sáµ¥)
```

```python
def calculate_entropy(y):
    """è®¡ç®—ç†µ"""
    if len(y) == 0:
        return 0
    
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    
    # é¿å…log(0)
    probabilities = probabilities[probabilities > 0]
    entropy = -np.sum(probabilities * np.log2(probabilities))
    
    return entropy

def calculate_gini(y):
    """è®¡ç®—åŸºå°¼ä¸çº¯åº¦"""
    if len(y) == 0:
        return 0
    
    _, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    gini = 1 - np.sum(probabilities ** 2)
    
    return gini

def calculate_information_gain(y, y_left, y_right):
    """è®¡ç®—ä¿¡æ¯å¢ç›Š"""
    n = len(y)
    n_left, n_right = len(y_left), len(y_right)
    
    if n_left == 0 or n_right == 0:
        return 0
    
    # åŸå§‹ç†µ
    entropy_parent = calculate_entropy(y)
    
    # åˆ†è£‚åçš„åŠ æƒç†µ
    entropy_children = (n_left/n) * calculate_entropy(y_left) + (n_right/n) * calculate_entropy(y_right)
    
    # ä¿¡æ¯å¢ç›Š
    information_gain = entropy_parent - entropy_children
    
    return information_gain

def calculate_gini_gain(y, y_left, y_right):
    """è®¡ç®—åŸºå°¼å¢ç›Š"""
    n = len(y)
    n_left, n_right = len(y_left), len(y_right)
    
    if n_left == 0 or n_right == 0:
        return 0
    
    # åŸå§‹åŸºå°¼ä¸çº¯åº¦
    gini_parent = calculate_gini(y)
    
    # åˆ†è£‚åçš„åŠ æƒåŸºå°¼ä¸çº¯åº¦
    gini_children = (n_left/n) * calculate_gini(y_left) + (n_right/n) * calculate_gini(y_right)
    
    # åŸºå°¼å¢ç›Š
    gini_gain = gini_parent - gini_children
    
    return gini_gain

# æ¼”ç¤ºä¸åŒåˆ†è£‚å‡†åˆ™
def demonstrate_splitting_criteria():
    """æ¼”ç¤ºä¸åŒåˆ†è£‚å‡†åˆ™çš„æ•ˆæœ"""
    
    # åˆ›å»ºä¸åŒçº¯åº¦çš„æ•°æ®é›†
    datasets = {
        'çº¯å‡€æ•°æ®': np.array([0, 0, 0, 0, 0]),
        'å®Œå…¨æ··åˆ': np.array([0, 0, 1, 1, 1]),
        'è½»å¾®ä¸å¹³è¡¡': np.array([0, 0, 0, 1, 1]),
        'ä¸¥é‡ä¸å¹³è¡¡': np.array([0, 0, 0, 0, 1])
    }
    
    print("ä¸åŒæ•°æ®é›†çš„ä¸çº¯åº¦æŒ‡æ ‡:")
    print("-" * 50)
    print(f"{'æ•°æ®é›†':<12} {'ç†µ':<8} {'åŸºå°¼ä¸çº¯åº¦':<12}")
    print("-" * 50)
    
    for name, data in datasets.items():
        entropy = calculate_entropy(data)
        gini = calculate_gini(data)
        print(f"{name:<12} {entropy:<8.3f} {gini:<12.3f}")
    
    # å¯è§†åŒ–ä¸çº¯åº¦æŒ‡æ ‡
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # ç†µå’ŒåŸºå°¼ä¸çº¯åº¦éšæ¦‚ç‡å˜åŒ–
    p = np.linspace(0.001, 0.999, 1000)
    entropy_values = -p * np.log2(p) - (1-p) * np.log2(1-p)
    gini_values = 2 * p * (1-p)
    
    axes[0].plot(p, entropy_values, 'b-', linewidth=2, label='ç†µ')
    axes[0].plot(p, gini_values, 'r-', linewidth=2, label='åŸºå°¼ä¸çº¯åº¦')
    axes[0].set_xlabel('æ­£ç±»æ¦‚ç‡ p')
    axes[0].set_ylabel('ä¸çº¯åº¦')
    axes[0].set_title('ä¸çº¯åº¦æŒ‡æ ‡å¯¹æ¯”')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ä¿¡æ¯å¢ç›Šç¤ºä¾‹
    # å‡è®¾ä¸€ä¸ªåˆ†è£‚ï¼šæ€»ä½“ [0,0,1,1] åˆ†æˆ [0,0] å’Œ [1,1]
    y_total = np.array([0, 0, 1, 1])
    y_left = np.array([0, 0])
    y_right = np.array([1, 1])
    
    ig = calculate_information_gain(y_total, y_left, y_right)
    gg = calculate_gini_gain(y_total, y_left, y_right)
    
    # å¯è§†åŒ–åˆ†è£‚æ•ˆæœ
    categories = ['åˆ†è£‚å‰', 'å·¦å­æ ‘', 'å³å­æ ‘']
    entropy_vals = [calculate_entropy(y_total), calculate_entropy(y_left), calculate_entropy(y_right)]
    gini_vals = [calculate_gini(y_total), calculate_gini(y_left), calculate_gini(y_right)]
    
    x = np.arange(len(categories))
    width = 0.35
    
    axes[1].bar(x - width/2, entropy_vals, width, label='ç†µ', alpha=0.8)
    axes[1].bar(x + width/2, gini_vals, width, label='åŸºå°¼ä¸çº¯åº¦', alpha=0.8)
    axes[1].set_xlabel('èŠ‚ç‚¹')
    axes[1].set_ylabel('ä¸çº¯åº¦å€¼')
    axes[1].set_title(f'å®Œç¾åˆ†è£‚ç¤ºä¾‹\nä¿¡æ¯å¢ç›Š={ig:.3f}, åŸºå°¼å¢ç›Š={gg:.3f}')
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(categories)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

demonstrate_splitting_criteria()
```

### 2. æœ€ä½³åˆ†è£‚ç‚¹é€‰æ‹©

```python
class SimpleDecisionTree:
    """ç®€å•å†³ç­–æ ‘å®ç°ï¼ˆä»…æ”¯æŒæ•°å€¼ç‰¹å¾çš„äºŒåˆ†ç±»ï¼‰"""
    
    def __init__(self, max_depth=5, min_samples_split=2, criterion='gini'):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.criterion = criterion
        self.tree = None
        
    def _calculate_impurity(self, y):
        """è®¡ç®—ä¸çº¯åº¦"""
        if self.criterion == 'gini':
            return calculate_gini(y)
        elif self.criterion == 'entropy':
            return calculate_entropy(y)
        else:
            raise ValueError("Criterion must be 'gini' or 'entropy'")
    
    def _calculate_gain(self, y, y_left, y_right):
        """è®¡ç®—å¢ç›Š"""
        if self.criterion == 'gini':
            return calculate_gini_gain(y, y_left, y_right)
        elif self.criterion == 'entropy':
            return calculate_information_gain(y, y_left, y_right)
    
    def _find_best_split(self, X, y):
        """å¯»æ‰¾æœ€ä½³åˆ†è£‚ç‚¹"""
        best_gain = -1
        best_feature = None
        best_threshold = None
        
        n_features = X.shape[1]
        
        for feature_idx in range(n_features):
            # è·å–è¯¥ç‰¹å¾çš„æ‰€æœ‰å”¯ä¸€å€¼ä½œä¸ºå€™é€‰é˜ˆå€¼
            thresholds = np.unique(X[:, feature_idx])
            
            for threshold in thresholds:
                # åˆ†è£‚æ•°æ®
                left_mask = X[:, feature_idx] <= threshold
                right_mask = ~left_mask
                
                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
                    continue
                
                y_left = y[left_mask]
                y_right = y[right_mask]
                
                # è®¡ç®—å¢ç›Š
                gain = self._calculate_gain(y, y_left, y_right)
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_gain
    
    def _build_tree(self, X, y, depth=0):
        """é€’å½’æ„å»ºå†³ç­–æ ‘"""
        # åœæ­¢æ¡ä»¶
        if (depth >= self.max_depth or 
            len(y) < self.min_samples_split or 
            len(np.unique(y)) == 1):
            
            # åˆ›å»ºå¶èŠ‚ç‚¹
            leaf_value = np.bincount(y).argmax()  # å¤šæ•°ç±»
            return {
                'type': 'leaf',
                'value': leaf_value,
                'samples': len(y),
                'impurity': self._calculate_impurity(y)
            }
        
        # å¯»æ‰¾æœ€ä½³åˆ†è£‚
        best_feature, best_threshold, best_gain = self._find_best_split(X, y)
        
        if best_feature is None:
            # æ— æ³•æ‰¾åˆ°æœ‰æ•ˆåˆ†è£‚ï¼Œåˆ›å»ºå¶èŠ‚ç‚¹
            leaf_value = np.bincount(y).argmax()
            return {
                'type': 'leaf',
                'value': leaf_value,
                'samples': len(y),
                'impurity': self._calculate_impurity(y)
            }
        
        # åˆ†è£‚æ•°æ®
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        # é€’å½’æ„å»ºå­æ ‘
        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return {
            'type': 'split',
            'feature': best_feature,
            'threshold': best_threshold,
            'gain': best_gain,
            'samples': len(y),
            'impurity': self._calculate_impurity(y),
            'left': left_subtree,
            'right': right_subtree
        }
    
    def fit(self, X, y):
        """è®­ç»ƒå†³ç­–æ ‘"""
        self.tree = self._build_tree(X, y)
        return self
    
    def _predict_sample(self, x, tree):
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        if tree['type'] == 'leaf':
            return tree['value']
        
        if x[tree['feature']] <= tree['threshold']:
            return self._predict_sample(x, tree['left'])
        else:
            return self._predict_sample(x, tree['right'])
    
    def predict(self, X):
        """é¢„æµ‹"""
        return np.array([self._predict_sample(x, self.tree) for x in X])
    
    def print_tree(self, tree=None, depth=0, prefix="Root"):
        """æ‰“å°å†³ç­–æ ‘ç»“æ„"""
        if tree is None:
            tree = self.tree
        
        indent = "  " * depth
        
        if tree['type'] == 'leaf':
            print(f"{indent}{prefix}: é¢„æµ‹={tree['value']}, æ ·æœ¬æ•°={tree['samples']}, ä¸çº¯åº¦={tree['impurity']:.3f}")
        else:
            print(f"{indent}{prefix}: ç‰¹å¾{tree['feature']} <= {tree['threshold']:.3f}")
            print(f"{indent}  (æ ·æœ¬æ•°={tree['samples']}, ä¸çº¯åº¦={tree['impurity']:.3f}, å¢ç›Š={tree['gain']:.3f})")
            
            self.print_tree(tree['left'], depth + 1, "â”œâ”€ True")
            self.print_tree(tree['right'], depth + 1, "â””â”€ False")

# è®­ç»ƒè‡ªå®ç°çš„å†³ç­–æ ‘
X_train, X_test, y_train, y_test = train_test_split(
    X_simple, y_simple, test_size=0.3, random_state=42
)

# æ¯”è¾ƒä¸åŒåˆ†è£‚å‡†åˆ™
criterions = ['gini', 'entropy']
for criterion in criterions:
    print(f"\n=== ä½¿ç”¨ {criterion.upper()} å‡†åˆ™ ===")
    
    # è‡ªå®ç°å†³ç­–æ ‘
    tree_custom = SimpleDecisionTree(max_depth=3, criterion=criterion)
    tree_custom.fit(X_train, y_train)
    
    y_pred_custom = tree_custom.predict(X_test)
    accuracy_custom = accuracy_score(y_test, y_pred_custom)
    
    print(f"è‡ªå®ç°å†³ç­–æ ‘å‡†ç¡®ç‡: {accuracy_custom:.4f}")
    print("\nå†³ç­–æ ‘ç»“æ„:")
    tree_custom.print_tree()
```

### 3. å†³ç­–è¾¹ç•Œå¯è§†åŒ–

```python
def plot_decision_boundary_tree(X, y, model, title, feature_names):
    """ç»˜åˆ¶å†³ç­–æ ‘çš„å†³ç­–è¾¹ç•Œ"""
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºç½‘æ ¼
    h = 0.5
    x_min, x_max = X[:, 0].min() - 5, X[:, 0].max() + 5
    y_min, y_max = X[:, 1].min() - 10, X[:, 1].max() + 10
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # é¢„æµ‹ç½‘æ ¼ç‚¹
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    Z = model.predict(grid_points)
    Z = Z.reshape(xx.shape)
    
    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
    
    # ç»˜åˆ¶æ•°æ®ç‚¹
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
    plt.colorbar(scatter, label='ç±»åˆ«')
    
    plt.xlabel(feature_names[0])
    plt.ylabel(feature_names[1])
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()

# ä½¿ç”¨sklearnå†³ç­–æ ‘è¿›è¡Œå¯¹æ¯”
fig, axes = plt.subplots(1, 2, figsize=(20, 8))

for i, criterion in enumerate(['gini', 'entropy']):
    # sklearnå†³ç­–æ ‘
    tree_sklearn = DecisionTreeClassifier(
        criterion=criterion, max_depth=3, random_state=42
    )
    tree_sklearn.fit(X_train, y_train)
    
    y_pred_sklearn = tree_sklearn.predict(X_test)
    accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)
    
    print(f"\nSklearn {criterion} å†³ç­–æ ‘å‡†ç¡®ç‡: {accuracy_sklearn:.4f}")
    
    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    h = 0.5
    x_min, x_max = X_simple[:, 0].min() - 5, X_simple[:, 0].max() + 5
    y_min, y_max = X_simple[:, 1].min() - 10, X_simple[:, 1].max() + 10
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = tree_sklearn.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    axes[i].contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
    scatter = axes[i].scatter(X_simple[:, 0], X_simple[:, 1], c=y_simple, 
                             cmap='RdYlBu', edgecolors='black')
    
    axes[i].set_xlabel('å¹´é¾„')
    axes[i].set_ylabel('æ”¶å…¥')
    axes[i].set_title(f'å†³ç­–æ ‘å†³ç­–è¾¹ç•Œ ({criterion.upper()})\nå‡†ç¡®ç‡: {accuracy_sklearn:.3f}')
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## å†³ç­–æ ‘å¯è§†åŒ–

```python
# å¯è§†åŒ–å†³ç­–æ ‘ç»“æ„
def visualize_decision_tree():
    """å¯è§†åŒ–å†³ç­–æ ‘ç»“æ„"""
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å†³ç­–æ ‘
    tree_viz = DecisionTreeClassifier(
        criterion='gini', max_depth=3, min_samples_split=10, random_state=42
    )
    tree_viz.fit(X_train, y_train)
    
    # æ–¹æ³•1: ä½¿ç”¨plot_tree
    plt.figure(figsize=(20, 12))
    plot_tree(tree_viz, 
              feature_names=feature_names,
              class_names=['ä¸è´­ä¹°', 'è´­ä¹°'],
              filled=True,
              rounded=True,
              fontsize=12)
    plt.title('å†³ç­–æ ‘å¯è§†åŒ–', fontsize=16)
    plt.show()
    
    # æ–¹æ³•2: æ–‡æœ¬å½¢å¼
    print("\nå†³ç­–æ ‘æ–‡æœ¬è¡¨ç¤º:")
    tree_rules = export_text(tree_viz, 
                            feature_names=feature_names,
                            class_names=['ä¸è´­ä¹°', 'è´­ä¹°'])
    print(tree_rules)
    
    return tree_viz

tree_viz = visualize_decision_tree()
```

## å†³ç­–æ ‘çš„è¿‡æ‹Ÿåˆé—®é¢˜

### 1. è¿‡æ‹Ÿåˆç°è±¡

```python
def demonstrate_overfitting():
    """æ¼”ç¤ºå†³ç­–æ ‘çš„è¿‡æ‹Ÿåˆç°è±¡"""
    
    # ç”Ÿæˆæ›´å¤æ‚çš„æ•°æ®
    X_complex, y_complex = make_classification(
        n_samples=300, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=1, random_state=42
    )
    
    X_train_complex, X_test_complex, y_train_complex, y_test_complex = train_test_split(
        X_complex, y_complex, test_size=0.3, random_state=42
    )
    
    # ä¸åŒæ·±åº¦çš„å†³ç­–æ ‘
    max_depths = [1, 3, 5, 10, None]
    
    train_accuracies = []
    test_accuracies = []
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()
    
    for i, max_depth in enumerate(max_depths):
        # è®­ç»ƒå†³ç­–æ ‘
        tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
        tree.fit(X_train_complex, y_train_complex)
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = tree.score(X_train_complex, y_train_complex)
        test_acc = tree.score(X_test_complex, y_test_complex)
        
        train_accuracies.append(train_acc)
        test_accuracies.append(test_acc)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        if i < 5:  # åªç»˜åˆ¶å‰5ä¸ª
            h = 0.02
            x_min, x_max = X_complex[:, 0].min() - 1, X_complex[:, 0].max() + 1
            y_min, y_max = X_complex[:, 1].min() - 1, X_complex[:, 1].max() + 1
            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                 np.arange(y_min, y_max, h))
            
            Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)
            
            axes[i].contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
            axes[i].scatter(X_complex[:, 0], X_complex[:, 1], c=y_complex, 
                           cmap='RdYlBu', edgecolors='black')
            
            depth_str = str(max_depth) if max_depth is not None else 'æ— é™åˆ¶'
            axes[i].set_title(f'æœ€å¤§æ·±åº¦: {depth_str}\nè®­ç»ƒ: {train_acc:.3f}, æµ‹è¯•: {test_acc:.3f}')
            axes[i].grid(True, alpha=0.3)
    
    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    axes[5].plot(range(len(max_depths)), train_accuracies, 'o-', 
                label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2, markersize=8)
    axes[5].plot(range(len(max_depths)), test_accuracies, 's-', 
                label='æµ‹è¯•å‡†ç¡®ç‡', linewidth=2, markersize=8)
    axes[5].set_xlabel('æ¨¡å‹å¤æ‚åº¦')
    axes[5].set_ylabel('å‡†ç¡®ç‡')
    axes[5].set_title('è¿‡æ‹Ÿåˆç°è±¡')
    axes[5].set_xticks(range(len(max_depths)))
    axes[5].set_xticklabels([str(d) if d is not None else 'âˆ' for d in max_depths])
    axes[5].legend()
    axes[5].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print("\nè¿‡æ‹Ÿåˆåˆ†æ:")
    print("-" * 50)
    print(f"{'æœ€å¤§æ·±åº¦':<10} {'è®­ç»ƒå‡†ç¡®ç‡':<12} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'å·®å¼‚':<10}")
    print("-" * 50)
    
    for i, max_depth in enumerate(max_depths):
        depth_str = str(max_depth) if max_depth is not None else 'æ— é™åˆ¶'
        diff = train_accuracies[i] - test_accuracies[i]
        print(f"{depth_str:<10} {train_accuracies[i]:<12.4f} {test_accuracies[i]:<12.4f} {diff:<10.4f}")

demonstrate_overfitting()
```

### 2. å‰ªææŠ€æœ¯

#### é¢„å‰ªæ (Pre-pruning)

```python
def demonstrate_pre_pruning():
    """æ¼”ç¤ºé¢„å‰ªææŠ€æœ¯"""
    
    # ç”Ÿæˆæ•°æ®
    X_prune, y_prune = make_classification(
        n_samples=500, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=2, random_state=42
    )
    
    X_train_prune, X_test_prune, y_train_prune, y_test_prune = train_test_split(
        X_prune, y_prune, test_size=0.3, random_state=42
    )
    
    # ä¸åŒçš„é¢„å‰ªæå‚æ•°
    pruning_params = [
        {'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1},  # æ— å‰ªæ
        {'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1},     # æ·±åº¦é™åˆ¶
        {'max_depth': None, 'min_samples_split': 20, 'min_samples_leaf': 1}, # åˆ†è£‚æ ·æœ¬é™åˆ¶
        {'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 10}, # å¶èŠ‚ç‚¹æ ·æœ¬é™åˆ¶
        {'max_depth': 5, 'min_samples_split': 20, 'min_samples_leaf': 10},   # ç»¼åˆé™åˆ¶
    ]
    
    param_names = ['æ— å‰ªæ', 'æ·±åº¦é™åˆ¶', 'åˆ†è£‚é™åˆ¶', 'å¶èŠ‚ç‚¹é™åˆ¶', 'ç»¼åˆé™åˆ¶']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()
    
    results = []
    
    for i, (params, name) in enumerate(zip(pruning_params, param_names)):
        # è®­ç»ƒå†³ç­–æ ‘
        tree = DecisionTreeClassifier(random_state=42, **params)
        tree.fit(X_train_prune, y_train_prune)
        
        # è¯„ä¼°æ€§èƒ½
        train_acc = tree.score(X_train_prune, y_train_prune)
        test_acc = tree.score(X_test_prune, y_test_prune)
        n_nodes = tree.tree_.node_count
        max_depth_actual = tree.tree_.max_depth
        
        results.append({
            'name': name,
            'train_acc': train_acc,
            'test_acc': test_acc,
            'n_nodes': n_nodes,
            'max_depth': max_depth_actual
        })
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        if i < 5:
            h = 0.02
            x_min, x_max = X_prune[:, 0].min() - 1, X_prune[:, 0].max() + 1
            y_min, y_max = X_prune[:, 1].min() - 1, X_prune[:, 1].max() + 1
            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                 np.arange(y_min, y_max, h))
            
            Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)
            
            axes[i].contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
            axes[i].scatter(X_prune[:, 0], X_prune[:, 1], c=y_prune, 
                           cmap='RdYlBu', edgecolors='black')
            
            axes[i].set_title(f'{name}\nèŠ‚ç‚¹æ•°: {n_nodes}, æ·±åº¦: {max_depth_actual}\næµ‹è¯•å‡†ç¡®ç‡: {test_acc:.3f}')
            axes[i].grid(True, alpha=0.3)
    
    # ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”
    names = [r['name'] for r in results]
    train_accs = [r['train_acc'] for r in results]
    test_accs = [r['test_acc'] for r in results]
    n_nodes_list = [r['n_nodes'] for r in results]
    
    x = np.arange(len(names))
    width = 0.35
    
    axes[5].bar(x - width/2, train_accs, width, label='è®­ç»ƒå‡†ç¡®ç‡', alpha=0.8)
    axes[5].bar(x + width/2, test_accs, width, label='æµ‹è¯•å‡†ç¡®ç‡', alpha=0.8)
    axes[5].set_xlabel('å‰ªææ–¹æ³•')
    axes[5].set_ylabel('å‡†ç¡®ç‡')
    axes[5].set_title('ä¸åŒå‰ªææ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”')
    axes[5].set_xticks(x)
    axes[5].set_xticklabels(names, rotation=45)
    axes[5].legend()
    axes[5].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\né¢„å‰ªææ•ˆæœå¯¹æ¯”:")
    print("-" * 80)
    print(f"{'æ–¹æ³•':<12} {'è®­ç»ƒå‡†ç¡®ç‡':<10} {'æµ‹è¯•å‡†ç¡®ç‡':<10} {'èŠ‚ç‚¹æ•°':<8} {'æœ€å¤§æ·±åº¦':<8} {'è¿‡æ‹Ÿåˆç¨‹åº¦':<10}")
    print("-" * 80)
    
    for result in results:
        overfitting = result['train_acc'] - result['test_acc']
        print(f"{result['name']:<12} {result['train_acc']:<10.4f} {result['test_acc']:<10.4f} "
              f"{result['n_nodes']:<8} {result['max_depth']:<8} {overfitting:<10.4f}")

demonstrate_pre_pruning()
```

#### åå‰ªæ (Post-pruning)

```python
def demonstrate_post_pruning():
    """æ¼”ç¤ºåå‰ªææŠ€æœ¯ï¼ˆæˆæœ¬å¤æ‚åº¦å‰ªæï¼‰"""
    
    # ç”Ÿæˆæ•°æ®
    X_post, y_post = make_classification(
        n_samples=400, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=2, random_state=42
    )
    
    X_train_post, X_test_post, y_train_post, y_test_post = train_test_split(
        X_post, y_post, test_size=0.3, random_state=42
    )
    
    # é¦–å…ˆè®­ç»ƒä¸€ä¸ªå®Œæ•´çš„æ ‘
    tree_full = DecisionTreeClassifier(random_state=42)
    tree_full.fit(X_train_post, y_train_post)
    
    # è·å–æˆæœ¬å¤æ‚åº¦å‰ªæè·¯å¾„
    path = tree_full.cost_complexity_pruning_path(X_train_post, y_train_post)
    ccp_alphas, impurities = path.ccp_alphas, path.impurities
    
    # è®­ç»ƒä¸åŒalphaå€¼çš„å‰ªææ ‘
    trees = []
    train_scores = []
    test_scores = []
    
    for ccp_alpha in ccp_alphas:
        tree = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
        tree.fit(X_train_post, y_train_post)
        trees.append(tree)
        train_scores.append(tree.score(X_train_post, y_train_post))
        test_scores.append(tree.score(X_test_post, y_test_post))
    
    # å¯è§†åŒ–å‰ªææ•ˆæœ
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. å‡†ç¡®ç‡ vs alpha
    axes[0, 0].plot(ccp_alphas, train_scores, marker='o', label='è®­ç»ƒå‡†ç¡®ç‡', drawstyle="steps-post")
    axes[0, 0].plot(ccp_alphas, test_scores, marker='s', label='æµ‹è¯•å‡†ç¡®ç‡', drawstyle="steps-post")
    axes[0, 0].set_xlabel('Alpha (å¤æ‚åº¦å‚æ•°)')
    axes[0, 0].set_ylabel('å‡†ç¡®ç‡')
    axes[0, 0].set_title('æˆæœ¬å¤æ‚åº¦å‰ªææ•ˆæœ')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. èŠ‚ç‚¹æ•° vs alpha
    node_counts = [tree.tree_.node_count for tree in trees]
    axes[0, 1].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
    axes[0, 1].set_xlabel('Alpha (å¤æ‚åº¦å‚æ•°)')
    axes[0, 1].set_ylabel('èŠ‚ç‚¹æ•°')
    axes[0, 1].set_title('æ ‘çš„å¤æ‚åº¦å˜åŒ–')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. ä¸çº¯åº¦ vs alpha
    axes[1, 0].plot(ccp_alphas, impurities, marker='o', drawstyle="steps-post")
    axes[1, 0].set_xlabel('Alpha (å¤æ‚åº¦å‚æ•°)')
    axes[1, 0].set_ylabel('å¶èŠ‚ç‚¹ä¸çº¯åº¦')
    axes[1, 0].set_title('ä¸çº¯åº¦å˜åŒ–')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. é€‰æ‹©æœ€ä½³alpha
    best_alpha_idx = np.argmax(test_scores)
    best_alpha = ccp_alphas[best_alpha_idx]
    best_test_score = test_scores[best_alpha_idx]
    
    axes[1, 1].plot(ccp_alphas, test_scores, marker='s', drawstyle="steps-post")
    axes[1, 1].axvline(x=best_alpha, color='red', linestyle='--', 
                      label=f'æœ€ä½³Alpha = {best_alpha:.6f}')
    axes[1, 1].axhline(y=best_test_score, color='red', linestyle='--', alpha=0.5)
    axes[1, 1].set_xlabel('Alpha (å¤æ‚åº¦å‚æ•°)')
    axes[1, 1].set_ylabel('æµ‹è¯•å‡†ç¡®ç‡')
    axes[1, 1].set_title('æœ€ä½³Alphaé€‰æ‹©')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ¯”è¾ƒå‰ªæå‰åçš„å†³ç­–è¾¹ç•Œ
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # åŸå§‹æ ‘ã€æœ€ä½³å‰ªææ ‘ã€è¿‡åº¦å‰ªææ ‘
    trees_to_compare = [
        (trees[0], 'åŸå§‹æ ‘ (Î±=0)'),
        (trees[best_alpha_idx], f'æœ€ä½³å‰ªææ ‘ (Î±={best_alpha:.6f})'),
        (trees[-1], f'è¿‡åº¦å‰ªææ ‘ (Î±={ccp_alphas[-1]:.6f})')
    ]
    
    for i, (tree, title) in enumerate(trees_to_compare):
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        h = 0.02
        x_min, x_max = X_post[:, 0].min() - 1, X_post[:, 0].max() + 1
        y_min, y_max = X_post[:, 1].min() - 1, X_post[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        
        Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        axes[i].contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
        axes[i].scatter(X_post[:, 0], X_post[:, 1], c=y_post, 
                       cmap='RdYlBu', edgecolors='black')
        
        test_acc = tree.score(X_test_post, y_test_post)
        node_count = tree.tree_.node_count
        axes[i].set_title(f'{title}\nèŠ‚ç‚¹æ•°: {node_count}, æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.3f}')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nåå‰ªæç»“æœ:")
    print(f"æœ€ä½³Alpha: {best_alpha:.6f}")
    print(f"æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_score:.4f}")
    print(f"åŸå§‹æ ‘èŠ‚ç‚¹æ•°: {trees[0].tree_.node_count}")
    print(f"æœ€ä½³å‰ªææ ‘èŠ‚ç‚¹æ•°: {trees[best_alpha_idx].tree_.node_count}")
    print(f"èŠ‚ç‚¹æ•°å‡å°‘: {trees[0].tree_.node_count - trees[best_alpha_idx].tree_.node_count}")

demonstrate_post_pruning()
```

## å†³ç­–æ ‘ç”¨äºå›å½’

```python
def demonstrate_regression_tree():
    """æ¼”ç¤ºå›å½’å†³ç­–æ ‘"""
    
    # ç”Ÿæˆå›å½’æ•°æ®
    np.random.seed(42)
    X_reg = np.random.uniform(0, 10, (200, 1))
    y_reg = 2 * np.sin(X_reg.ravel()) + np.random.normal(0, 0.5, 200)
    
    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
        X_reg, y_reg, test_size=0.3, random_state=42
    )
    
    # ä¸åŒæ·±åº¦çš„å›å½’æ ‘
    max_depths = [1, 3, 5, 10]
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    for i, max_depth in enumerate(max_depths):
        # è®­ç»ƒå›å½’æ ‘
        tree_reg = DecisionTreeRegressor(max_depth=max_depth, random_state=42)
        tree_reg.fit(X_train_reg, y_train_reg)
        
        # é¢„æµ‹
        X_plot = np.linspace(0, 10, 300).reshape(-1, 1)
        y_pred_plot = tree_reg.predict(X_plot)
        y_pred_test = tree_reg.predict(X_test_reg)
        
        # è®¡ç®—æ€§èƒ½
        mse = mean_squared_error(y_test_reg, y_pred_test)
        r2 = tree_reg.score(X_test_reg, y_test_reg)
        
        # ç»˜å›¾
        axes[i].scatter(X_train_reg, y_train_reg, alpha=0.6, label='è®­ç»ƒæ•°æ®')
        axes[i].scatter(X_test_reg, y_test_reg, alpha=0.6, color='red', label='æµ‹è¯•æ•°æ®')
        axes[i].plot(X_plot, y_pred_plot, color='green', linewidth=2, label='å†³ç­–æ ‘é¢„æµ‹')
        
        # çœŸå®å‡½æ•°
        y_true_plot = 2 * np.sin(X_plot.ravel())
        axes[i].plot(X_plot, y_true_plot, color='black', linestyle='--', 
                    linewidth=2, label='çœŸå®å‡½æ•°')
        
        axes[i].set_xlabel('X')
        axes[i].set_ylabel('y')
        axes[i].set_title(f'æœ€å¤§æ·±åº¦: {max_depth}\nMSE: {mse:.3f}, RÂ²: {r2:.3f}')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ç‰¹å¾é‡è¦æ€§ï¼ˆå¯¹äºå¤šç‰¹å¾å›å½’ï¼‰
    # ç”Ÿæˆå¤šç‰¹å¾å›å½’æ•°æ®
    X_multi_reg, y_multi_reg = make_regression(
        n_samples=300, n_features=5, noise=0.1, random_state=42
    )
    
    feature_names_reg = [f'ç‰¹å¾_{i+1}' for i in range(5)]
    
    X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(
        X_multi_reg, y_multi_reg, test_size=0.3, random_state=42
    )
    
    # è®­ç»ƒå›å½’æ ‘
    tree_multi_reg = DecisionTreeRegressor(max_depth=5, random_state=42)
    tree_multi_reg.fit(X_train_multi, y_train_multi)
    
    # ç‰¹å¾é‡è¦æ€§
    importances = tree_multi_reg.feature_importances_
    
    plt.figure(figsize=(10, 6))
    plt.barh(feature_names_reg, importances)
    plt.xlabel('ç‰¹å¾é‡è¦æ€§')
    plt.title('å›å½’å†³ç­–æ ‘ç‰¹å¾é‡è¦æ€§')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    print(f"\nå¤šç‰¹å¾å›å½’å†³ç­–æ ‘æ€§èƒ½:")
    print(f"è®­ç»ƒ RÂ²: {tree_multi_reg.score(X_train_multi, y_train_multi):.4f}")
    print(f"æµ‹è¯• RÂ²: {tree_multi_reg.score(X_test_multi, y_test_multi):.4f}")
    
    print(f"\nç‰¹å¾é‡è¦æ€§æ’åº:")
    for i, (name, importance) in enumerate(zip(feature_names_reg, importances)):
        print(f"{name}: {importance:.4f}")

demonstrate_regression_tree()
```

## å†³ç­–æ ‘çš„ä¼˜ç¼ºç‚¹æ€»ç»“

### ä¼˜ç‚¹

1. **ç›´è§‚æ˜“æ‡‚**ï¼šå†³ç­–è¿‡ç¨‹æ¸…æ™°ï¼Œæ˜“äºè§£é‡Š
2. **æ— éœ€æ•°æ®é¢„å¤„ç†**ï¼šå¯ç›´æ¥å¤„ç†æ•°å€¼å’Œç±»åˆ«ç‰¹å¾
3. **è‡ªåŠ¨ç‰¹å¾é€‰æ‹©**ï¼šé‡è¦ç‰¹å¾è‡ªç„¶å‡ºç°åœ¨æ ‘çš„ä¸Šå±‚
4. **å¤„ç†éçº¿æ€§å…³ç³»**ï¼šé€šè¿‡åˆ†æ®µçº¿æ€§é€¼è¿‘å¤æ‚å…³ç³»
5. **å¤„ç†ç¼ºå¤±å€¼**ï¼šæœ‰å†…ç½®çš„ç¼ºå¤±å€¼å¤„ç†æœºåˆ¶
6. **è®¡ç®—æ•ˆç‡é«˜**ï¼šè®­ç»ƒå’Œé¢„æµ‹éƒ½ç›¸å¯¹å¿«é€Ÿ
7. **æ— å‚æ•°å‡è®¾**ï¼šä¸éœ€è¦å¯¹æ•°æ®åˆ†å¸ƒåšå‡è®¾

### ç¼ºç‚¹

1. **å®¹æ˜“è¿‡æ‹Ÿåˆ**ï¼šç‰¹åˆ«æ˜¯æ·±åº¦è¾ƒå¤§æ—¶
2. **ä¸ç¨³å®š**ï¼šæ•°æ®çš„å°å˜åŒ–å¯èƒ½å¯¼è‡´å®Œå…¨ä¸åŒçš„æ ‘
3. **åå‘å¤šå€¼ç‰¹å¾**ï¼šå€¾å‘äºé€‰æ‹©å–å€¼è¾ƒå¤šçš„ç‰¹å¾
4. **éš¾ä»¥å¤„ç†çº¿æ€§å…³ç³»**ï¼šå¯¹äºç®€å•çº¿æ€§å…³ç³»æ•ˆç‡ä¸é«˜
5. **é¢„æµ‹èƒ½åŠ›æœ‰é™**ï¼šåªèƒ½é¢„æµ‹è®­ç»ƒé›†ä¸­å‡ºç°è¿‡çš„å€¼ï¼ˆå›å½’ï¼‰

```python
# æ¼”ç¤ºå†³ç­–æ ‘çš„ä¸ç¨³å®šæ€§
def demonstrate_instability():
    """æ¼”ç¤ºå†³ç­–æ ‘çš„ä¸ç¨³å®šæ€§"""
    
    # ç”Ÿæˆæ•°æ®
    X_unstable, y_unstable = make_classification(
        n_samples=200, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=1, random_state=42
    )
    
    # è®­ç»ƒå¤šä¸ªå†³ç­–æ ‘ï¼ˆä½¿ç”¨ä¸åŒçš„éšæœºç§å­ï¼‰
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.ravel()
    
    accuracies = []
    
    for i in range(6):
        # éšæœºé‡‡æ ·è®­ç»ƒæ•°æ®
        indices = np.random.RandomState(i).choice(len(X_unstable), 
                                                 size=int(0.8 * len(X_unstable)), 
                                                 replace=False)
        X_sample = X_unstable[indices]
        y_sample = y_unstable[indices]
        
        # è®­ç»ƒå†³ç­–æ ‘
        tree = DecisionTreeClassifier(max_depth=5, random_state=i)
        tree.fit(X_sample, y_sample)
        
        # åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¯„ä¼°
        accuracy = tree.score(X_unstable, y_unstable)
        accuracies.append(accuracy)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        h = 0.02
        x_min, x_max = X_unstable[:, 0].min() - 1, X_unstable[:, 0].max() + 1
        y_min, y_max = X_unstable[:, 1].min() - 1, X_unstable[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        
        Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        axes[i].contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
        axes[i].scatter(X_unstable[:, 0], X_unstable[:, 1], c=y_unstable, 
                       cmap='RdYlBu', edgecolors='black')
        axes[i].scatter(X_sample[:, 0], X_sample[:, 1], 
                       facecolors='none', edgecolors='red', s=100, linewidth=2)
        
        axes[i].set_title(f'æ ‘ {i+1} (å‡†ç¡®ç‡: {accuracy:.3f})')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nå†³ç­–æ ‘ä¸ç¨³å®šæ€§åˆ†æ:")
    print(f"å‡†ç¡®ç‡èŒƒå›´: {min(accuracies):.3f} - {max(accuracies):.3f}")
    print(f"å‡†ç¡®ç‡æ ‡å‡†å·®: {np.std(accuracies):.4f}")
    print(f"å¹³å‡å‡†ç¡®ç‡: {np.mean(accuracies):.4f}")

demonstrate_instability()
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### åŒ»ç–—è¯Šæ–­å†³ç­–æ ‘

```python
class MedicalDiagnosisTree:
    """åŒ»ç–—è¯Šæ–­å†³ç­–æ ‘ç³»ç»Ÿ"""
    
    def __init__(self):
        self.model = None
        self.feature_names = None
        self.class_names = None
        
    def generate_medical_data(self, n_samples=1000):
        """ç”Ÿæˆæ¨¡æ‹ŸåŒ»ç–—æ•°æ®"""
        np.random.seed(42)
        
        # ç‰¹å¾ï¼šå¹´é¾„ã€ä½“æ¸©ã€ç™½ç»†èƒè®¡æ•°ã€è¡€å‹ã€å¿ƒç‡
        age = np.random.uniform(18, 80, n_samples)
        temperature = np.random.normal(37, 1.5, n_samples)  # ä½“æ¸©
        wbc_count = np.random.lognormal(9, 0.5, n_samples)  # ç™½ç»†èƒè®¡æ•°
        blood_pressure = np.random.normal(120, 20, n_samples)  # æ”¶ç¼©å‹
        heart_rate = np.random.normal(75, 15, n_samples)  # å¿ƒç‡
        
        # è¯Šæ–­è§„åˆ™ï¼ˆç®€åŒ–ï¼‰
        # 0: å¥åº·, 1: æ„Ÿå†’, 2: æ„ŸæŸ“, 3: é«˜è¡€å‹
        diagnosis = np.zeros(n_samples, dtype=int)
        
        for i in range(n_samples):
            if blood_pressure[i] > 140:
                diagnosis[i] = 3  # é«˜è¡€å‹
            elif temperature[i] > 38.5 and wbc_count[i] > 12000:
                diagnosis[i] = 2  # æ„ŸæŸ“
            elif temperature[i] > 37.5:
                diagnosis[i] = 1  # æ„Ÿå†’
            else:
                diagnosis[i] = 0  # å¥åº·
        
        # æ·»åŠ ä¸€äº›å™ªå£°
        noise_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)
        diagnosis[noise_indices] = np.random.randint(0, 4, len(noise_indices))
        
        X = np.column_stack([age, temperature, wbc_count, blood_pressure, heart_rate])
        self.feature_names = ['å¹´é¾„', 'ä½“æ¸©', 'ç™½ç»†èƒè®¡æ•°', 'æ”¶ç¼©å‹', 'å¿ƒç‡']
        self.class_names = ['å¥åº·', 'æ„Ÿå†’', 'æ„ŸæŸ“', 'é«˜è¡€å‹']
        
        return X, diagnosis
    
    def train(self, X, y):
        """è®­ç»ƒè¯Šæ–­æ¨¡å‹"""
        # ä½¿ç”¨é€‚å½“çš„å‰ªæå‚æ•°
        self.model = DecisionTreeClassifier(
            criterion='gini',
            max_depth=6,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42
        )
        
        self.model.fit(X, y)
        return self
    
    def diagnose(self, age, temperature, wbc_count, blood_pressure, heart_rate):
        """è¯Šæ–­å•ä¸ªæ‚£è€…"""
        features = np.array([[age, temperature, wbc_count, blood_pressure, heart_rate]])
        
        # é¢„æµ‹æ¦‚ç‡
        probabilities = self.model.predict_proba(features)[0]
        predicted_class = self.model.predict(features)[0]
        
        print(f"\n=== æ‚£è€…è¯Šæ–­æŠ¥å‘Š ===")
        print(f"æ‚£è€…ä¿¡æ¯:")
        print(f"  å¹´é¾„: {age} å²")
        print(f"  ä½“æ¸©: {temperature:.1f}Â°C")
        print(f"  ç™½ç»†èƒè®¡æ•°: {wbc_count:.0f}/Î¼L")
        print(f"  æ”¶ç¼©å‹: {blood_pressure:.0f} mmHg")
        print(f"  å¿ƒç‡: {heart_rate:.0f} bpm")
        
        print(f"\nè¯Šæ–­ç»“æœ:")
        print(f"  ä¸»è¦è¯Šæ–­: {self.class_names[predicted_class]}")
        print(f"  ç½®ä¿¡åº¦: {probabilities[predicted_class]:.3f}")
        
        print(f"\nå„ç–¾ç—…æ¦‚ç‡:")
        for i, (class_name, prob) in enumerate(zip(self.class_names, probabilities)):
            print(f"  {class_name}: {prob:.3f}")
        
        # ç»™å‡ºå»ºè®®
        if predicted_class == 0:
            print(f"\nå»ºè®®: æ‚£è€…å¥åº·çŠ¶å†µè‰¯å¥½ï¼Œå»ºè®®å®šæœŸä½“æ£€")
        elif predicted_class == 1:
            print(f"\nå»ºè®®: ç–‘ä¼¼æ„Ÿå†’ï¼Œå»ºè®®å¤šä¼‘æ¯ï¼Œå¤šå–æ°´ï¼Œå¿…è¦æ—¶æœç”¨æ„Ÿå†’è¯")
        elif predicted_class == 2:
            print(f"\nå»ºè®®: ç–‘ä¼¼æ„ŸæŸ“ï¼Œå»ºè®®ç«‹å³å°±åŒ»ï¼Œå¯èƒ½éœ€è¦æŠ—ç”Ÿç´ æ²»ç–—")
        elif predicted_class == 3:
            print(f"\nå»ºè®®: è¡€å‹åé«˜ï¼Œå»ºè®®æ§åˆ¶é¥®é£Ÿï¼Œé€‚é‡è¿åŠ¨ï¼Œå¿…è¦æ—¶æœç”¨é™å‹è¯")
        
        return predicted_class, probabilities
    
    def visualize_tree(self):
        """å¯è§†åŒ–å†³ç­–æ ‘"""
        plt.figure(figsize=(25, 15))
        plot_tree(self.model, 
                  feature_names=self.feature_names,
                  class_names=self.class_names,
                  filled=True,
                  rounded=True,
                  fontsize=10)
        plt.title('åŒ»ç–—è¯Šæ–­å†³ç­–æ ‘', fontsize=16)
        plt.show()
    
    def analyze_feature_importance(self):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        importances = self.model.feature_importances_
        
        # æ’åº
        indices = np.argsort(importances)[::-1]
        
        plt.figure(figsize=(10, 6))
        plt.bar(range(len(importances)), importances[indices])
        plt.xticks(range(len(importances)), 
                  [self.feature_names[i] for i in indices], rotation=45)
        plt.xlabel('ç‰¹å¾')
        plt.ylabel('é‡è¦æ€§')
        plt.title('åŒ»ç–—è¯Šæ–­ç‰¹å¾é‡è¦æ€§')
        plt.tight_layout()
        plt.show()
        
        print("\nç‰¹å¾é‡è¦æ€§æ’åº:")
        for i, idx in enumerate(indices):
            print(f"{i+1}. {self.feature_names[idx]}: {importances[idx]:.4f}")

# æ¼”ç¤ºåŒ»ç–—è¯Šæ–­ç³»ç»Ÿ
diagnosis_system = MedicalDiagnosisTree()
X_medical, y_medical = diagnosis_system.generate_medical_data()

# è®­ç»ƒæ¨¡å‹
X_train_med, X_test_med, y_train_med, y_test_med = train_test_split(
    X_medical, y_medical, test_size=0.3, random_state=42
)

diagnosis_system.train(X_train_med, y_train_med)

# è¯„ä¼°æ€§èƒ½
y_pred_med = diagnosis_system.model.predict(X_test_med)
accuracy_med = accuracy_score(y_test_med, y_pred_med)

print(f"\nåŒ»ç–—è¯Šæ–­ç³»ç»Ÿæ€§èƒ½:")
print(f"å‡†ç¡®ç‡: {accuracy_med:.4f}")
print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test_med, y_pred_med, 
                          target_names=diagnosis_system.class_names))

# åˆ†æç‰¹å¾é‡è¦æ€§
diagnosis_system.analyze_feature_importance()

# è¯Šæ–­å‡ ä¸ªç¤ºä¾‹æ‚£è€…
print("\n=== ç¤ºä¾‹è¯Šæ–­ ===")

# æ‚£è€…1ï¼šå¥åº·
diagnosis_system.diagnose(35, 36.8, 7000, 115, 72)

# æ‚£è€…2ï¼šæ„Ÿå†’
diagnosis_system.diagnose(28, 38.2, 9000, 120, 85)

# æ‚£è€…3ï¼šæ„ŸæŸ“
diagnosis_system.diagnose(45, 39.1, 15000, 125, 95)

# æ‚£è€…4ï¼šé«˜è¡€å‹
diagnosis_system.diagnose(60, 37.0, 8000, 155, 78)
```

## Traeå®è·µç¯èŠ‚

### ä½¿ç”¨Traeæ„å»ºå†³ç­–æ ‘

```python
# Traeå†³ç­–æ ‘å®ç°ç¤ºä¾‹
class TraeDecisionTree:
    """ä½¿ç”¨Traeé£æ ¼çš„å†³ç­–æ ‘å®ç°"""
    
    def __init__(self, max_depth=5, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None
        
    def trae_fit(self, X, y):
        """Traeé£æ ¼çš„è®­ç»ƒæ–¹æ³•"""
        print("ğŸŒ³ å¼€å§‹æ„å»ºå†³ç­–æ ‘...")
        
        # æ•°æ®é¢„å¤„ç†
        X = np.array(X)
        y = np.array(y)
        
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        print(f"ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # æ„å»ºæ ‘
        self.tree = self._trae_build_tree(X, y, 0)
        
        print("âœ… å†³ç­–æ ‘æ„å»ºå®Œæˆ!")
        return self
    
    def _trae_build_tree(self, X, y, depth):
        """Traeé£æ ¼çš„æ ‘æ„å»º"""
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))
        
        # åœæ­¢æ¡ä»¶
        if (depth >= self.max_depth or 
            n_samples < self.min_samples_split or 
            n_classes == 1):
            
            leaf_value = np.bincount(y).argmax()
            return {'type': 'leaf', 'value': leaf_value, 'samples': n_samples}
        
        # å¯»æ‰¾æœ€ä½³åˆ†è£‚
        best_feature, best_threshold = self._trae_find_best_split(X, y)
        
        if best_feature is None:
            leaf_value = np.bincount(y).argmax()
            return {'type': 'leaf', 'value': leaf_value, 'samples': n_samples}
        
        # åˆ†è£‚æ•°æ®
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = ~left_mask
        
        left_subtree = self._trae_build_tree(X[left_mask], y[left_mask], depth + 1)
        right_subtree = self._trae_build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return {
            'type': 'split',
            'feature': best_feature,
            'threshold': best_threshold,
            'left': left_subtree,
            'right': right_subtree,
            'samples': n_samples
        }
    
    def _trae_find_best_split(self, X, y):
        """Traeé£æ ¼çš„æœ€ä½³åˆ†è£‚æŸ¥æ‰¾"""
        best_gain = -1
        best_feature = None
        best_threshold = None
        
        for feature_idx in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_idx])
            
            for threshold in thresholds:
                left_mask = X[:, feature_idx] <= threshold
                right_mask = ~left_mask
                
                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
                    continue
                
                gain = calculate_gini_gain(y, y[left_mask], y[right_mask])
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold
    
    def trae_predict(self, X):
        """Traeé£æ ¼çš„é¢„æµ‹"""
        print(f"ğŸ”® é¢„æµ‹ {len(X)} ä¸ªæ ·æœ¬...")
        predictions = []
        
        for x in X:
            pred = self._trae_predict_sample(x, self.tree)
            predictions.append(pred)
        
        print("âœ… é¢„æµ‹å®Œæˆ!")
        return np.array(predictions)
    
    def _trae_predict_sample(self, x, tree):
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        if tree['type'] == 'leaf':
            return tree['value']
        
        if x[tree['feature']] <= tree['threshold']:
            return self._trae_predict_sample(x, tree['left'])
        else:
            return self._trae_predict_sample(x, tree['right'])
    
    def trae_visualize(self, feature_names=None):
        """Traeé£æ ¼çš„å¯è§†åŒ–"""
        print("ğŸ¨ ç”Ÿæˆå†³ç­–æ ‘å¯è§†åŒ–...")
        self._trae_print_tree(self.tree, feature_names, 0)
    
    def _trae_print_tree(self, tree, feature_names, depth):
        """æ‰“å°æ ‘ç»“æ„"""
        indent = "  " * depth
        
        if tree['type'] == 'leaf':
            print(f"{indent}ğŸƒ é¢„æµ‹: {tree['value']} (æ ·æœ¬æ•°: {tree['samples']})")
        else:
            feature_name = f"ç‰¹å¾{tree['feature']}" if feature_names is None else feature_names[tree['feature']]
            print(f"{indent}ğŸŒ¿ {feature_name} <= {tree['threshold']:.3f} (æ ·æœ¬æ•°: {tree['samples']})")
            
            print(f"{indent}â”œâ”€ True:")
            self._trae_print_tree(tree['left'], feature_names, depth + 1)
            
            print(f"{indent}â””â”€ False:")
            self._trae_print_tree(tree['right'], feature_names, depth + 1)

# ä½¿ç”¨Traeå†³ç­–æ ‘
print("\n=== Traeå†³ç­–æ ‘æ¼”ç¤º ===")
trae_tree = TraeDecisionTree(max_depth=4)
trae_tree.trae_fit(X_train, y_train)

# é¢„æµ‹
y_pred_trae = trae_tree.trae_predict(X_test)
accuracy_trae = accuracy_score(y_test, y_pred_trae)
print(f"\nğŸ¯ Traeå†³ç­–æ ‘å‡†ç¡®ç‡: {accuracy_trae:.4f}")

# å¯è§†åŒ–
print("\nğŸŒ³ Traeå†³ç­–æ ‘ç»“æ„:")
trae_tree.trae_visualize(feature_names)
```

## æ€è€ƒé¢˜

1. **åˆ†è£‚å‡†åˆ™æ¯”è¾ƒ**ï¼šåœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©ä¿¡æ¯å¢ç›Šè€Œä¸æ˜¯åŸºå°¼ä¸çº¯åº¦ï¼Ÿ

2. **å‰ªæç­–ç•¥**ï¼šé¢„å‰ªæå’Œåå‰ªæå„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿåœ¨å®é™…åº”ç”¨ä¸­å¦‚ä½•é€‰æ‹©ï¼Ÿ

3. **ç‰¹å¾é€‰æ‹©åå‘**ï¼šä¸ºä»€ä¹ˆå†³ç­–æ ‘ä¼šåå‘é€‰æ‹©å–å€¼è¾ƒå¤šçš„ç‰¹å¾ï¼Ÿå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ

4. **ç¼ºå¤±å€¼å¤„ç†**ï¼šå†³ç­–æ ‘å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ï¼Ÿè¿™ç§æ–¹æ³•æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

5. **å†³ç­–è¾¹ç•Œ**ï¼šè§‚å¯Ÿå†³ç­–æ ‘çš„å†³ç­–è¾¹ç•Œï¼Œå®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿé€‚åˆå¤„ç†ä»€ä¹ˆç±»å‹çš„æ•°æ®ï¼Ÿ

## æœ¬èŠ‚å°ç»“

å†³ç­–æ ‘æ˜¯ä¸€ç§ç›´è§‚ä¸”å¼ºå¤§çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### æ ¸å¿ƒæ¦‚å¿µ
- **æ ‘ç»“æ„**ï¼šæ ¹èŠ‚ç‚¹ã€å†…éƒ¨èŠ‚ç‚¹ã€å¶èŠ‚ç‚¹
- **åˆ†è£‚å‡†åˆ™**ï¼šä¿¡æ¯å¢ç›Šã€åŸºå°¼ä¸çº¯åº¦
- **æ„å»ºè¿‡ç¨‹**ï¼šé€’å½’åˆ†è£‚ã€è´ªå¿ƒé€‰æ‹©

### å…³é”®æŠ€æœ¯
- **è¿‡æ‹Ÿåˆæ§åˆ¶**ï¼šé¢„å‰ªæã€åå‰ªæ
- **å‚æ•°è°ƒä¼˜**ï¼šæœ€å¤§æ·±åº¦ã€æœ€å°åˆ†è£‚æ ·æœ¬æ•°
- **æ€§èƒ½è¯„ä¼°**ï¼šå‡†ç¡®ç‡ã€ç‰¹å¾é‡è¦æ€§

### å®é™…åº”ç”¨
- **åŒ»ç–—è¯Šæ–­**ï¼šç—‡çŠ¶åˆ°ç–¾ç—…çš„æ˜ å°„
- **é‡‘èé£æ§**ï¼šä¿¡è´·å®¡æ‰¹å†³ç­–
- **æ¨èç³»ç»Ÿ**ï¼šç”¨æˆ·åå¥½åˆ†æ

### ä¸‹ä¸€æ­¥å­¦ä¹ 
- **é›†æˆæ–¹æ³•**ï¼šéšæœºæ£®æ—ã€æ¢¯åº¦æå‡
- **é«˜çº§æŠ€æœ¯**ï¼šç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è§£é‡Š
- **å®æˆ˜é¡¹ç›®**ï¼šç«¯åˆ°ç«¯å†³ç­–æ ‘åº”ç”¨

å†³ç­–æ ‘ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç†è§£æœºå™¨å­¦ä¹ çš„ç»ä½³èµ·ç‚¹ï¼Œå®ƒçš„å¯è§£é‡Šæ€§ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ·±å…¥ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œä¸ºåç»­å­¦ä¹ æ›´å¤æ‚çš„ç®—æ³•å¥ å®šäº†åšå®åŸºç¡€ã€‚