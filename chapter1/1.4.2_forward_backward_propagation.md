# 1.4.2 å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­

## 1. å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰

### 1.1 å‰å‘ä¼ æ’­æ¦‚è¿°

å‰å‘ä¼ æ’­æ˜¯ç¥ç»ç½‘ç»œä¸­ä¿¡æ¯ä»è¾“å…¥å±‚æµå‘è¾“å‡ºå±‚çš„è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€å±‚çš„ç¥ç»å…ƒæ¥æ”¶å‰ä¸€å±‚çš„è¾“å‡ºï¼Œç»è¿‡çº¿æ€§å˜æ¢å’Œæ¿€æ´»å‡½æ•°å¤„ç†åï¼Œå°†ç»“æœä¼ é€’ç»™ä¸‹ä¸€å±‚ã€‚

```mermaid
graph LR
    subgraph "å‰å‘ä¼ æ’­æµç¨‹"
        A[è¾“å…¥å±‚ X] --> B[éšè—å±‚1]
        B --> C[éšè—å±‚2]
        C --> D[è¾“å‡ºå±‚ Y]
    end
    
    subgraph "è®¡ç®—è¿‡ç¨‹"
        E["ZÂ¹ = XÂ·WÂ¹ + bÂ¹"] --> F["AÂ¹ = f(ZÂ¹)"]
        F --> G["ZÂ² = AÂ¹Â·WÂ² + bÂ²"] 
        G --> H["AÂ² = f(ZÂ²)"]
        H --> I["Å¶ = AÂ²"]
    end
```

### 1.2 æ•°å­¦è¡¨è¾¾å¼

å¯¹äºä¸€ä¸ªLå±‚çš„ç¥ç»ç½‘ç»œï¼Œå‰å‘ä¼ æ’­çš„æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š

**ç¬¬lå±‚çš„è®¡ç®—ï¼š**
$$Z^{[l]} = A^{[l-1]} \cdot W^{[l]} + b^{[l]}$$
$$A^{[l]} = g^{[l]}(Z^{[l]})$$

å…¶ä¸­ï¼š
- $A^{[l]}$ï¼šç¬¬lå±‚çš„æ¿€æ´»å€¼
- $W^{[l]}$ï¼šç¬¬lå±‚çš„æƒé‡çŸ©é˜µ
- $b^{[l]}$ï¼šç¬¬lå±‚çš„åç½®å‘é‡
- $g^{[l]}$ï¼šç¬¬lå±‚çš„æ¿€æ´»å‡½æ•°
- $A^{[0]} = X$ï¼šè¾“å…¥æ•°æ®

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import seaborn as sns

class ForwardPropagationDemo:
    """å‰å‘ä¼ æ’­æ¼”ç¤ºç±»"""
    
    def __init__(self, layer_sizes, activations=None):
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)
        
        # é»˜è®¤æ¿€æ´»å‡½æ•°
        if activations is None:
            activations = ['relu'] * (self.num_layers - 2) + ['sigmoid']
        self.activations = activations
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weights = []
        self.biases = []
        
        np.random.seed(42)
        for i in range(self.num_layers - 1):
            # Heåˆå§‹åŒ–ï¼ˆé€‚ç”¨äºReLUï¼‰
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros((1, layer_sizes[i+1]))
            
            self.weights.append(w)
            self.biases.append(b)
        
        # å­˜å‚¨å‰å‘ä¼ æ’­è¿‡ç¨‹
        self.forward_cache = {}
    
    def activation_function(self, z, activation_type):
        """æ¿€æ´»å‡½æ•°"""
        if activation_type == 'sigmoid':
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif activation_type == 'relu':
            return np.maximum(0, z)
        elif activation_type == 'tanh':
            return np.tanh(z)
        elif activation_type == 'softmax':
            exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
            return exp_z / np.sum(exp_z, axis=1, keepdims=True)
        else:
            return z  # çº¿æ€§æ¿€æ´»
    
    def forward_propagation_step_by_step(self, X, verbose=True):
        """é€æ­¥å±•ç¤ºå‰å‘ä¼ æ’­è¿‡ç¨‹"""
        if verbose:
            print(f"\n{'='*80}")
            print(f"ğŸš€ å‰å‘ä¼ æ’­é€æ­¥æ¼”ç¤º")
            print(f"{'='*80}")
            print(f"\nğŸ“Š è¾“å…¥æ•°æ®å½¢çŠ¶: {X.shape}")
            print(f"   ç½‘ç»œç»“æ„: {' -> '.join(map(str, self.layer_sizes))}")
        
        # åˆå§‹åŒ–
        current_activation = X
        self.forward_cache = {'A0': X}
        
        if verbose:
            print(f"\nğŸ”¸ è¾“å…¥å±‚ (Aâ°):")
            print(f"   å½¢çŠ¶: {current_activation.shape}")
            print(f"   æ•°å€¼: {current_activation.flatten()[:5]}..." if current_activation.size > 5 else f"   æ•°å€¼: {current_activation.flatten()}")
        
        # é€å±‚å‰å‘ä¼ æ’­
        for layer in range(self.num_layers - 1):
            if verbose:
                print(f"\n{'â”€'*60}")
                print(f"ğŸ”¸ ç¬¬ {layer + 1} å±‚è®¡ç®—:")
            
            # çº¿æ€§å˜æ¢: Z = AÂ·W + b
            z = np.dot(current_activation, self.weights[layer]) + self.biases[layer]
            
            if verbose:
                print(f"   æƒé‡ W{layer+1} å½¢çŠ¶: {self.weights[layer].shape}")
                print(f"   åç½® b{layer+1} å½¢çŠ¶: {self.biases[layer].shape}")
                print(f"   çº¿æ€§è¾“å‡º Z{layer+1} å½¢çŠ¶: {z.shape}")
                print(f"   Z{layer+1} èŒƒå›´: [{z.min():.4f}, {z.max():.4f}]")
            
            # æ¿€æ´»å‡½æ•°: A = g(Z)
            activation_type = self.activations[layer]
            current_activation = self.activation_function(z, activation_type)
            
            if verbose:
                print(f"   æ¿€æ´»å‡½æ•°: {activation_type}")
                print(f"   æ¿€æ´»è¾“å‡º A{layer+1} å½¢çŠ¶: {current_activation.shape}")
                print(f"   A{layer+1} èŒƒå›´: [{current_activation.min():.4f}, {current_activation.max():.4f}]")
            
            # ç¼“å­˜ç»“æœ
            self.forward_cache[f'Z{layer+1}'] = z
            self.forward_cache[f'A{layer+1}'] = current_activation
        
        if verbose:
            print(f"\nâœ… å‰å‘ä¼ æ’­å®Œæˆ!")
            print(f"   æœ€ç»ˆè¾“å‡ºå½¢çŠ¶: {current_activation.shape}")
            print(f"   è¾“å‡ºå€¼: {current_activation.flatten()}")
        
        return current_activation
    
    def visualize_forward_flow(self, X):
        """å¯è§†åŒ–å‰å‘ä¼ æ’­æ•°æ®æµ"""
        # æ‰§è¡Œå‰å‘ä¼ æ’­
        output = self.forward_propagation_step_by_step(X, verbose=False)
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(2, self.num_layers, figsize=(4*self.num_layers, 10))
        if self.num_layers == 1:
            axes = axes.reshape(2, 1)
        
        # ä¸Šæ’ï¼šæ¿€æ´»å€¼çƒ­å›¾
        for layer in range(self.num_layers):
            activation_key = f'A{layer}'
            activation_data = self.forward_cache[activation_key]
            
            # å¦‚æœæ˜¯1Dæ•°æ®ï¼Œè½¬æ¢ä¸º2Dç”¨äºæ˜¾ç¤º
            if len(activation_data.shape) == 1:
                activation_data = activation_data.reshape(-1, 1)
            elif activation_data.shape[0] == 1:
                activation_data = activation_data.T
            
            im1 = axes[0, layer].imshow(activation_data, cmap='viridis', aspect='auto')
            axes[0, layer].set_title(f'A{layer} (æ¿€æ´»å€¼)', fontweight='bold')
            axes[0, layer].set_xlabel('ç¥ç»å…ƒ')
            axes[0, layer].set_ylabel('æ ·æœ¬' if layer == 0 else 'æ¿€æ´»å€¼')
            plt.colorbar(im1, ax=axes[0, layer])
        
        # ä¸‹æ’ï¼šæƒé‡çƒ­å›¾
        for layer in range(self.num_layers - 1):
            im2 = axes[1, layer].imshow(self.weights[layer], cmap='RdBu', aspect='auto')
            axes[1, layer].set_title(f'W{layer+1} (æƒé‡)', fontweight='bold')
            axes[1, layer].set_xlabel(f'ç¬¬{layer+2}å±‚ç¥ç»å…ƒ')
            axes[1, layer].set_ylabel(f'ç¬¬{layer+1}å±‚ç¥ç»å…ƒ')
            plt.colorbar(im2, ax=axes[1, layer])
        
        # æœ€åä¸€åˆ—æ˜¾ç¤ºè¾“å‡ºåˆ†å¸ƒ
        if self.num_layers > 1:
            axes[1, -1].hist(output.flatten(), bins=20, alpha=0.7, color='green')
            axes[1, -1].set_title('è¾“å‡ºåˆ†å¸ƒ', fontweight='bold')
            axes[1, -1].set_xlabel('è¾“å‡ºå€¼')
            axes[1, -1].set_ylabel('é¢‘æ¬¡')
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def analyze_activation_statistics(self, X):
        """åˆ†æå„å±‚æ¿€æ´»å€¼ç»Ÿè®¡ä¿¡æ¯"""
        output = self.forward_propagation_step_by_step(X, verbose=False)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š å„å±‚æ¿€æ´»å€¼ç»Ÿè®¡åˆ†æ")
        print(f"{'='*80}")
        
        statistics = {}
        
        for layer in range(self.num_layers):
            activation_key = f'A{layer}'
            activation_data = self.forward_cache[activation_key]
            
            stats = {
                'mean': np.mean(activation_data),
                'std': np.std(activation_data),
                'min': np.min(activation_data),
                'max': np.max(activation_data),
                'zeros_ratio': np.mean(activation_data == 0),  # å¯¹ReLUæœ‰æ„ä¹‰
                'shape': activation_data.shape
            }
            
            statistics[f'Layer_{layer}'] = stats
            
            layer_name = 'è¾“å…¥å±‚' if layer == 0 else f'éšè—å±‚{layer}' if layer < self.num_layers-1 else 'è¾“å‡ºå±‚'
            print(f"\nğŸ”¸ {layer_name} (A{layer}):")
            print(f"   å½¢çŠ¶: {stats['shape']}")
            print(f"   å‡å€¼: {stats['mean']:.4f}")
            print(f"   æ ‡å‡†å·®: {stats['std']:.4f}")
            print(f"   èŒƒå›´: [{stats['min']:.4f}, {stats['max']:.4f}]")
            if layer > 0 and self.activations[layer-1] == 'relu':
                print(f"   é›¶å€¼æ¯”ä¾‹: {stats['zeros_ratio']:.2%} (ReLUæ­»ç¥ç»å…ƒ)")
        
        return statistics

# å‰å‘ä¼ æ’­æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ§  å‰å‘ä¼ æ’­è¯¦ç»†æ¼”ç¤º")
print("=" * 80)

# åˆ›å»ºæ¼”ç¤ºç½‘ç»œ
forward_demo = ForwardPropagationDemo(
    layer_sizes=[3, 5, 4, 2],
    activations=['relu', 'relu', 'sigmoid']
)

# åˆ›å»ºæµ‹è¯•æ•°æ®
test_input = np.array([[0.5, -0.3, 0.8]])
print(f"\nğŸ“ æµ‹è¯•è¾“å…¥: {test_input}")

# é€æ­¥æ¼”ç¤ºå‰å‘ä¼ æ’­
output = forward_demo.forward_propagation_step_by_step(test_input)

# å¯è§†åŒ–æ•°æ®æµ
forward_demo.visualize_forward_flow(test_input)

# åˆ†ææ¿€æ´»å€¼ç»Ÿè®¡
stats = forward_demo.analyze_activation_statistics(test_input)
```

## 2. åå‘ä¼ æ’­ï¼ˆBackward Propagationï¼‰

### 2.1 åå‘ä¼ æ’­æ¦‚è¿°

åå‘ä¼ æ’­æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œå®ƒé€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å‡½æ•°å¯¹ç½‘ç»œå‚æ•°çš„æ¢¯åº¦ï¼Œç„¶åä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°ã€‚

```mermaid
graph RL
    subgraph "åå‘ä¼ æ’­æµç¨‹"
        A[æŸå¤±å‡½æ•° L] --> B[è¾“å‡ºå±‚æ¢¯åº¦]
        B --> C[éšè—å±‚2æ¢¯åº¦]
        C --> D[éšè—å±‚1æ¢¯åº¦]
        D --> E[å‚æ•°æ›´æ–°]
    end
    
    subgraph "æ¢¯åº¦è®¡ç®—"
        F["âˆ‚L/âˆ‚WÂ³, âˆ‚L/âˆ‚bÂ³"] --> G["âˆ‚L/âˆ‚AÂ²"]
        G --> H["âˆ‚L/âˆ‚WÂ², âˆ‚L/âˆ‚bÂ²"]
        H --> I["âˆ‚L/âˆ‚AÂ¹"]
        I --> J["âˆ‚L/âˆ‚WÂ¹, âˆ‚L/âˆ‚bÂ¹"]
    end
```

### 2.2 æ•°å­¦æ¨å¯¼

**é“¾å¼æ³•åˆ™ï¼š**
$$\frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial Z^{[l]}} \cdot \frac{\partial Z^{[l]}}{\partial W^{[l]}}$$

**å…·ä½“è®¡ç®—å…¬å¼ï¼š**

1. **è¾“å‡ºå±‚æ¢¯åº¦ï¼š**
   $$dZ^{[L]} = A^{[L]} - Y$$

2. **éšè—å±‚æ¢¯åº¦ï¼š**
   $$dZ^{[l]} = (W^{[l+1]})^T \cdot dZ^{[l+1]} \odot g'^{[l]}(Z^{[l]})$$

3. **æƒé‡å’Œåç½®æ¢¯åº¦ï¼š**
   $$dW^{[l]} = \frac{1}{m} dZ^{[l]} \cdot (A^{[l-1]})^T$$
   $$db^{[l]} = \frac{1}{m} \sum_{i=1}^{m} dZ^{[l](i)}$$

```python
class BackwardPropagationDemo:
    """åå‘ä¼ æ’­æ¼”ç¤ºç±»"""
    
    def __init__(self, forward_demo):
        self.forward_demo = forward_demo
        self.layer_sizes = forward_demo.layer_sizes
        self.num_layers = forward_demo.num_layers
        self.activations = forward_demo.activations
        self.weights = forward_demo.weights
        self.biases = forward_demo.biases
        
        # å­˜å‚¨åå‘ä¼ æ’­è¿‡ç¨‹
        self.backward_cache = {}
    
    def activation_derivative(self, z, activation_type):
        """æ¿€æ´»å‡½æ•°å¯¼æ•°"""
        if activation_type == 'sigmoid':
            s = 1 / (1 + np.exp(-np.clip(z, -500, 500)))
            return s * (1 - s)
        elif activation_type == 'relu':
            return (z > 0).astype(float)
        elif activation_type == 'tanh':
            return 1 - np.tanh(z) ** 2
        else:
            return np.ones_like(z)  # çº¿æ€§æ¿€æ´»å¯¼æ•°ä¸º1
    
    def compute_loss(self, y_pred, y_true, loss_type='mse'):
        """è®¡ç®—æŸå¤±å‡½æ•°"""
        if loss_type == 'mse':
            return np.mean((y_pred - y_true) ** 2)
        elif loss_type == 'cross_entropy':
            return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))
        else:
            return np.mean(np.abs(y_pred - y_true))
    
    def compute_loss_derivative(self, y_pred, y_true, loss_type='mse'):
        """è®¡ç®—æŸå¤±å‡½æ•°å¯¼æ•°"""
        m = y_true.shape[0]
        
        if loss_type == 'mse':
            return 2 * (y_pred - y_true) / m
        elif loss_type == 'cross_entropy':
            return (y_pred - y_true) / m
        else:
            return np.sign(y_pred - y_true) / m
    
    def backward_propagation_step_by_step(self, X, y_true, loss_type='mse', verbose=True):
        """é€æ­¥å±•ç¤ºåå‘ä¼ æ’­è¿‡ç¨‹"""
        if verbose:
            print(f"\n{'='*80}")
            print(f"â¬…ï¸ åå‘ä¼ æ’­é€æ­¥æ¼”ç¤º")
            print(f"{'='*80}")
        
        # é¦–å…ˆè¿›è¡Œå‰å‘ä¼ æ’­
        y_pred = self.forward_demo.forward_propagation_step_by_step(X, verbose=False)
        forward_cache = self.forward_demo.forward_cache
        
        # è®¡ç®—æŸå¤±
        loss = self.compute_loss(y_pred, y_true, loss_type)
        
        if verbose:
            print(f"\nğŸ“Š æŸå¤±è®¡ç®—:")
            print(f"   é¢„æµ‹å€¼: {y_pred.flatten()}")
            print(f"   çœŸå®å€¼: {y_true.flatten()}")
            print(f"   æŸå¤±å‡½æ•°: {loss_type}")
            print(f"   æŸå¤±å€¼: {loss:.6f}")
        
        # åˆå§‹åŒ–æ¢¯åº¦ç¼“å­˜
        self.backward_cache = {}
        
        # è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦
        if loss_type == 'cross_entropy' and self.activations[-1] == 'softmax':
            # å¯¹äºsoftmax + äº¤å‰ç†µï¼Œæ¢¯åº¦ç®€åŒ–
            dZ = y_pred - y_true
        else:
            # ä¸€èˆ¬æƒ…å†µ
            dL_dA = self.compute_loss_derivative(y_pred, y_true, loss_type)
            last_layer_idx = self.num_layers - 2
            dA_dZ = self.activation_derivative(
                forward_cache[f'Z{self.num_layers-1}'], 
                self.activations[last_layer_idx]
            )
            dZ = dL_dA * dA_dZ
        
        if verbose:
            print(f"\nğŸ”¸ è¾“å‡ºå±‚æ¢¯åº¦ (dZ{self.num_layers-1}):")
            print(f"   å½¢çŠ¶: {dZ.shape}")
            print(f"   èŒƒå›´: [{dZ.min():.6f}, {dZ.max():.6f}]")
            print(f"   å‡å€¼: {dZ.mean():.6f}")
        
        self.backward_cache[f'dZ{self.num_layers-1}'] = dZ
        
        # ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­
        current_dZ = dZ
        
        for layer in range(self.num_layers - 2, -1, -1):
            if verbose:
                print(f"\n{'â”€'*60}")
                print(f"ğŸ”¸ ç¬¬ {layer + 1} å±‚åå‘ä¼ æ’­:")
            
            # è®¡ç®—æƒé‡æ¢¯åº¦: dW = (1/m) * A_prev^T * dZ
            A_prev = forward_cache[f'A{layer}']
            dW = np.dot(A_prev.T, current_dZ) / X.shape[0]
            
            # è®¡ç®—åç½®æ¢¯åº¦: db = (1/m) * sum(dZ)
            db = np.mean(current_dZ, axis=0, keepdims=True)
            
            if verbose:
                print(f"   æƒé‡æ¢¯åº¦ dW{layer+1}:")
                print(f"     å½¢çŠ¶: {dW.shape}")
                print(f"     èŒƒå›´: [{dW.min():.6f}, {dW.max():.6f}]")
                print(f"     L2èŒƒæ•°: {np.linalg.norm(dW):.6f}")
                
                print(f"   åç½®æ¢¯åº¦ db{layer+1}:")
                print(f"     å½¢çŠ¶: {db.shape}")
                print(f"     å€¼: {db.flatten()}")
            
            # å­˜å‚¨æ¢¯åº¦
            self.backward_cache[f'dW{layer+1}'] = dW
            self.backward_cache[f'db{layer+1}'] = db
            
            # è®¡ç®—å‰ä¸€å±‚çš„æ¿€æ´»æ¢¯åº¦ï¼ˆå¦‚æœä¸æ˜¯è¾“å…¥å±‚ï¼‰
            if layer > 0:
                # dA_prev = W^T * dZ
                dA_prev = np.dot(current_dZ, self.weights[layer].T)
                
                # dZ_prev = dA_prev * g'(Z_prev)
                Z_prev = forward_cache[f'Z{layer}']
                dZ_prev = dA_prev * self.activation_derivative(Z_prev, self.activations[layer-1])
                
                if verbose:
                    print(f"   å‰å±‚æ¿€æ´»æ¢¯åº¦ dA{layer}:")
                    print(f"     å½¢çŠ¶: {dA_prev.shape}")
                    print(f"     èŒƒå›´: [{dA_prev.min():.6f}, {dA_prev.max():.6f}]")
                    
                    print(f"   å‰å±‚è¾“å…¥æ¢¯åº¦ dZ{layer}:")
                    print(f"     å½¢çŠ¶: {dZ_prev.shape}")
                    print(f"     èŒƒå›´: [{dZ_prev.min():.6f}, {dZ_prev.max():.6f}]")
                
                self.backward_cache[f'dA{layer}'] = dA_prev
                self.backward_cache[f'dZ{layer}'] = dZ_prev
                current_dZ = dZ_prev
        
        if verbose:
            print(f"\nâœ… åå‘ä¼ æ’­å®Œæˆ!")
            print(f"   è®¡ç®—äº† {len([k for k in self.backward_cache.keys() if k.startswith('dW')])} ä¸ªæƒé‡æ¢¯åº¦")
            print(f"   è®¡ç®—äº† {len([k for k in self.backward_cache.keys() if k.startswith('db')])} ä¸ªåç½®æ¢¯åº¦")
        
        return loss, self.backward_cache
    
    def visualize_gradients(self, X, y_true):
        """å¯è§†åŒ–æ¢¯åº¦"""
        loss, gradients = self.backward_propagation_step_by_step(X, y_true, verbose=False)
        
        # è®¡ç®—æƒé‡æ¢¯åº¦çš„æ•°é‡
        num_weight_layers = len([k for k in gradients.keys() if k.startswith('dW')])
        
        fig, axes = plt.subplots(2, num_weight_layers, figsize=(5*num_weight_layers, 10))
        if num_weight_layers == 1:
            axes = axes.reshape(2, 1)
        
        for i in range(num_weight_layers):
            layer_idx = i + 1
            
            # ä¸Šæ’ï¼šæƒé‡æ¢¯åº¦çƒ­å›¾
            dW = gradients[f'dW{layer_idx}']
            im1 = axes[0, i].imshow(dW, cmap='RdBu', aspect='auto')
            axes[0, i].set_title(f'æƒé‡æ¢¯åº¦ dW{layer_idx}', fontweight='bold')
            axes[0, i].set_xlabel(f'ç¬¬{layer_idx+1}å±‚ç¥ç»å…ƒ')
            axes[0, i].set_ylabel(f'ç¬¬{layer_idx}å±‚ç¥ç»å…ƒ')
            plt.colorbar(im1, ax=axes[0, i])
            
            # ä¸‹æ’ï¼šæ¢¯åº¦ç»Ÿè®¡
            gradient_flat = dW.flatten()
            axes[1, i].hist(gradient_flat, bins=20, alpha=0.7, color='blue')
            axes[1, i].axvline(gradient_flat.mean(), color='red', linestyle='--', 
                              label=f'å‡å€¼: {gradient_flat.mean():.6f}')
            axes[1, i].set_title(f'dW{layer_idx} åˆ†å¸ƒ', fontweight='bold')
            axes[1, i].set_xlabel('æ¢¯åº¦å€¼')
            axes[1, i].set_ylabel('é¢‘æ¬¡')
            axes[1, i].legend()
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def gradient_checking(self, X, y_true, epsilon=1e-7):
        """æ¢¯åº¦æ£€æŸ¥ï¼ˆæ•°å€¼æ¢¯åº¦ vs è§£ææ¢¯åº¦ï¼‰"""
        print(f"\n{'='*80}")
        print(f"ğŸ” æ¢¯åº¦æ£€æŸ¥")
        print(f"{'='*80}")
        
        # è®¡ç®—è§£ææ¢¯åº¦
        loss, analytical_gradients = self.backward_propagation_step_by_step(X, y_true, verbose=False)
        
        # æ•°å€¼æ¢¯åº¦æ£€æŸ¥
        print(f"\nğŸ“Š æ£€æŸ¥ç»“æœ:")
        
        for layer in range(self.num_layers - 1):
            layer_idx = layer + 1
            
            # æ£€æŸ¥æƒé‡æ¢¯åº¦
            dW_analytical = analytical_gradients[f'dW{layer_idx}']
            
            # éšæœºé€‰æ‹©å‡ ä¸ªæƒé‡è¿›è¡Œæ•°å€¼æ¢¯åº¦æ£€æŸ¥
            num_checks = min(5, dW_analytical.size)
            indices = np.random.choice(dW_analytical.size, num_checks, replace=False)
            
            print(f"\nğŸ”¸ ç¬¬ {layer_idx} å±‚æƒé‡æ¢¯åº¦æ£€æŸ¥:")
            
            for idx in indices:
                i, j = np.unravel_index(idx, dW_analytical.shape)
                
                # è®¡ç®—æ•°å€¼æ¢¯åº¦
                original_weight = self.weights[layer][i, j]
                
                # f(x + epsilon)
                self.weights[layer][i, j] = original_weight + epsilon
                y_pred_plus = self.forward_demo.forward_propagation_step_by_step(X, verbose=False)
                loss_plus = self.compute_loss(y_pred_plus, y_true)
                
                # f(x - epsilon)
                self.weights[layer][i, j] = original_weight - epsilon
                y_pred_minus = self.forward_demo.forward_propagation_step_by_step(X, verbose=False)
                loss_minus = self.compute_loss(y_pred_minus, y_true)
                
                # æ¢å¤åŸå§‹æƒé‡
                self.weights[layer][i, j] = original_weight
                
                # æ•°å€¼æ¢¯åº¦
                numerical_gradient = (loss_plus - loss_minus) / (2 * epsilon)
                analytical_gradient = dW_analytical[i, j]
                
                # è®¡ç®—ç›¸å¯¹è¯¯å·®
                relative_error = abs(numerical_gradient - analytical_gradient) / \
                               (abs(numerical_gradient) + abs(analytical_gradient) + 1e-8)
                
                status = "âœ…" if relative_error < 1e-5 else "âŒ"
                print(f"     W{layer_idx}[{i},{j}]: æ•°å€¼={numerical_gradient:.8f}, "
                      f"è§£æ={analytical_gradient:.8f}, è¯¯å·®={relative_error:.2e} {status}")
        
        print(f"\nğŸ’¡ æ¢¯åº¦æ£€æŸ¥è¯´æ˜:")
        print(f"   - ç›¸å¯¹è¯¯å·® < 1e-5: æ¢¯åº¦è®¡ç®—æ­£ç¡® âœ…")
        print(f"   - ç›¸å¯¹è¯¯å·® > 1e-3: å¯èƒ½å­˜åœ¨é”™è¯¯ âŒ")

# åå‘ä¼ æ’­æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ§  åå‘ä¼ æ’­è¯¦ç»†æ¼”ç¤º")
print("=" * 80)

# ä½¿ç”¨ä¹‹å‰çš„å‰å‘ä¼ æ’­æ¼”ç¤ºç½‘ç»œ
backward_demo = BackwardPropagationDemo(forward_demo)

# åˆ›å»ºæµ‹è¯•æ•°æ®å’Œæ ‡ç­¾
test_input = np.array([[0.5, -0.3, 0.8]])
test_target = np.array([[0.2, 0.8]])  # ç›®æ ‡è¾“å‡º

print(f"\nğŸ“ æµ‹è¯•æ•°æ®:")
print(f"   è¾“å…¥: {test_input}")
print(f"   ç›®æ ‡: {test_target}")

# é€æ­¥æ¼”ç¤ºåå‘ä¼ æ’­
loss, gradients = backward_demo.backward_propagation_step_by_step(test_input, test_target)

# å¯è§†åŒ–æ¢¯åº¦
backward_demo.visualize_gradients(test_input, test_target)

# æ¢¯åº¦æ£€æŸ¥
backward_demo.gradient_checking(test_input, test_target)
```

## 3. å®Œæ•´çš„è®­ç»ƒå¾ªç¯

### 3.1 æ¢¯åº¦ä¸‹é™ä¼˜åŒ–

```python
class NeuralNetworkTrainer:
    """å®Œæ•´çš„ç¥ç»ç½‘ç»œè®­ç»ƒå™¨"""
    
    def __init__(self, layer_sizes, activations=None, learning_rate=0.01, 
                 optimizer='sgd', loss_function='mse'):
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.loss_function = loss_function
        
        # æ¿€æ´»å‡½æ•°
        if activations is None:
            activations = ['relu'] * (self.num_layers - 2) + ['sigmoid']
        self.activations = activations
        
        # åˆå§‹åŒ–å‚æ•°
        self.initialize_parameters()
        
        # ä¼˜åŒ–å™¨ç›¸å…³å‚æ•°
        self.initialize_optimizer()
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'losses': [],
            'accuracies': [],
            'gradient_norms': []
        }
    
    def initialize_parameters(self):
        """åˆå§‹åŒ–ç½‘ç»œå‚æ•°"""
        self.weights = []
        self.biases = []
        
        np.random.seed(42)
        for i in range(self.num_layers - 1):
            # Xavier/Heåˆå§‹åŒ–
            if self.activations[i] == 'relu':
                # Heåˆå§‹åŒ–
                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * \
                    np.sqrt(2.0 / self.layer_sizes[i])
            else:
                # Xavieråˆå§‹åŒ–
                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * \
                    np.sqrt(1.0 / self.layer_sizes[i])
            
            b = np.zeros((1, self.layer_sizes[i+1]))
            
            self.weights.append(w)
            self.biases.append(b)
    
    def initialize_optimizer(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨å‚æ•°"""
        if self.optimizer == 'momentum':
            self.momentum = 0.9
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
        
        elif self.optimizer == 'adam':
            self.beta1 = 0.9
            self.beta2 = 0.999
            self.epsilon = 1e-8
            self.m_weights = [np.zeros_like(w) for w in self.weights]
            self.m_biases = [np.zeros_like(b) for b in self.biases]
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
            self.t = 0  # æ—¶é—´æ­¥
    
    def activation_function(self, z, activation_type):
        """æ¿€æ´»å‡½æ•°"""
        if activation_type == 'sigmoid':
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif activation_type == 'relu':
            return np.maximum(0, z)
        elif activation_type == 'tanh':
            return np.tanh(z)
        elif activation_type == 'softmax':
            exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
            return exp_z / np.sum(exp_z, axis=1, keepdims=True)
        else:
            return z
    
    def activation_derivative(self, z, activation_type):
        """æ¿€æ´»å‡½æ•°å¯¼æ•°"""
        if activation_type == 'sigmoid':
            s = self.activation_function(z, 'sigmoid')
            return s * (1 - s)
        elif activation_type == 'relu':
            return (z > 0).astype(float)
        elif activation_type == 'tanh':
            return 1 - np.tanh(z) ** 2
        else:
            return np.ones_like(z)
    
    def forward_propagation(self, X):
        """å‰å‘ä¼ æ’­"""
        self.activations_cache = [X]
        self.z_cache = []
        
        current_input = X
        
        for i in range(self.num_layers - 1):
            z = np.dot(current_input, self.weights[i]) + self.biases[i]
            self.z_cache.append(z)
            
            a = self.activation_function(z, self.activations[i])
            self.activations_cache.append(a)
            
            current_input = a
        
        return current_input
    
    def compute_loss(self, y_pred, y_true):
        """è®¡ç®—æŸå¤±"""
        if self.loss_function == 'mse':
            return np.mean((y_pred - y_true) ** 2)
        elif self.loss_function == 'cross_entropy':
            return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))
        else:
            return np.mean(np.abs(y_pred - y_true))
    
    def backward_propagation(self, X, y_true, y_pred):
        """åå‘ä¼ æ’­"""
        m = X.shape[0]
        
        # è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦
        if (self.loss_function == 'cross_entropy' and 
            self.activations[-1] == 'softmax'):
            dz = y_pred - y_true
        else:
            if self.loss_function == 'mse':
                dL_dA = 2 * (y_pred - y_true) / m
            else:
                dL_dA = np.sign(y_pred - y_true) / m
            
            dA_dZ = self.activation_derivative(self.z_cache[-1], self.activations[-1])
            dz = dL_dA * dA_dZ
        
        # å­˜å‚¨æ¢¯åº¦
        dw_list = []
        db_list = []
        
        # ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­
        for i in range(self.num_layers - 2, -1, -1):
            # è®¡ç®—æƒé‡å’Œåç½®æ¢¯åº¦
            dw = np.dot(self.activations_cache[i].T, dz) / m
            db = np.mean(dz, axis=0, keepdims=True)
            
            dw_list.insert(0, dw)
            db_list.insert(0, db)
            
            # è®¡ç®—å‰ä¸€å±‚çš„è¯¯å·®ï¼ˆå¦‚æœä¸æ˜¯è¾“å…¥å±‚ï¼‰
            if i > 0:
                dz = np.dot(dz, self.weights[i].T) * \
                     self.activation_derivative(self.z_cache[i-1], self.activations[i-1])
        
        return dw_list, db_list
    
    def update_parameters(self, dw_list, db_list):
        """æ›´æ–°å‚æ•°"""
        if self.optimizer == 'sgd':
            # æ ‡å‡†æ¢¯åº¦ä¸‹é™
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * dw_list[i]
                self.biases[i] -= self.learning_rate * db_list[i]
        
        elif self.optimizer == 'momentum':
            # åŠ¨é‡æ¢¯åº¦ä¸‹é™
            for i in range(len(self.weights)):
                self.v_weights[i] = self.momentum * self.v_weights[i] - self.learning_rate * dw_list[i]
                self.v_biases[i] = self.momentum * self.v_biases[i] - self.learning_rate * db_list[i]
                
                self.weights[i] += self.v_weights[i]
                self.biases[i] += self.v_biases[i]
        
        elif self.optimizer == 'adam':
            # Adamä¼˜åŒ–å™¨
            self.t += 1
            
            for i in range(len(self.weights)):
                # æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡
                self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * dw_list[i]
                self.m_biases[i] = self.beta1 * self.m_biases[i] + (1 - self.beta1) * db_list[i]
                
                # æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡
                self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * (dw_list[i] ** 2)
                self.v_biases[i] = self.beta2 * self.v_biases[i] + (1 - self.beta2) * (db_list[i] ** 2)
                
                # åå·®ä¿®æ­£
                m_w_corrected = self.m_weights[i] / (1 - self.beta1 ** self.t)
                m_b_corrected = self.m_biases[i] / (1 - self.beta1 ** self.t)
                v_w_corrected = self.v_weights[i] / (1 - self.beta2 ** self.t)
                v_b_corrected = self.v_biases[i] / (1 - self.beta2 ** self.t)
                
                # æ›´æ–°å‚æ•°
                self.weights[i] -= self.learning_rate * m_w_corrected / (np.sqrt(v_w_corrected) + self.epsilon)
                self.biases[i] -= self.learning_rate * m_b_corrected / (np.sqrt(v_b_corrected) + self.epsilon)
    
    def train(self, X, y, epochs=1000, batch_size=None, validation_data=None, verbose=True):
        """è®­ç»ƒç½‘ç»œ"""
        if batch_size is None:
            batch_size = X.shape[0]
        
        if verbose:
            print(f"\n{'='*80}")
            print(f"ğŸš€ å¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œ")
            print(f"{'='*80}")
            print(f"   ç½‘ç»œç»“æ„: {' -> '.join(map(str, self.layer_sizes))}")
            print(f"   ä¼˜åŒ–å™¨: {self.optimizer}")
            print(f"   å­¦ä¹ ç‡: {self.learning_rate}")
            print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"   è®­ç»ƒè½®æ•°: {epochs}")
        
        for epoch in range(epochs):
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = np.random.permutation(X.shape[0])
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            epoch_loss = 0
            num_batches = 0
            
            # æ‰¹æ¬¡è®­ç»ƒ
            for i in range(0, X.shape[0], batch_size):
                batch_X = X_shuffled[i:i+batch_size]
                batch_y = y_shuffled[i:i+batch_size]
                
                # å‰å‘ä¼ æ’­
                y_pred = self.forward_propagation(batch_X)
                
                # è®¡ç®—æŸå¤±
                batch_loss = self.compute_loss(y_pred, batch_y)
                epoch_loss += batch_loss
                num_batches += 1
                
                # åå‘ä¼ æ’­
                dw_list, db_list = self.backward_propagation(batch_X, batch_y, y_pred)
                
                # æ›´æ–°å‚æ•°
                self.update_parameters(dw_list, db_list)
            
            # è®°å½•è®­ç»ƒå†å²
            avg_loss = epoch_loss / num_batches
            self.training_history['losses'].append(avg_loss)
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            gradient_norm = np.sqrt(sum(np.sum(dw**2) for dw in dw_list))
            self.training_history['gradient_norms'].append(gradient_norm)
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¦‚æœæ˜¯åˆ†ç±»é—®é¢˜ï¼‰
            if self.loss_function == 'cross_entropy':
                train_pred = self.predict(X)
                train_accuracy = np.mean(np.argmax(train_pred, axis=1) == np.argmax(y, axis=1))
                self.training_history['accuracies'].append(train_accuracy)
            
            # æ‰“å°è¿›åº¦
            if verbose and (epoch + 1) % (epochs // 10) == 0:
                print(f"   Epoch {epoch + 1:4d}/{epochs}: Loss = {avg_loss:.6f}", end="")
                if self.loss_function == 'cross_entropy':
                    print(f", Accuracy = {train_accuracy:.4f}", end="")
                print(f", Grad Norm = {gradient_norm:.6f}")
        
        if verbose:
            print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        
        return self.training_history
    
    def predict(self, X):
        """é¢„æµ‹"""
        return self.forward_propagation(X)
    
    def visualize_training_history(self):
        """å¯è§†åŒ–è®­ç»ƒå†å²"""
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # æŸå¤±æ›²çº¿
        axes[0].plot(self.training_history['losses'], linewidth=2, color='red')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('è®­ç»ƒæŸå¤±æ›²çº¿')
        axes[0].grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.training_history['accuracies']:
            axes[1].plot(self.training_history['accuracies'], linewidth=2, color='green')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Accuracy')
            axes[1].set_title('è®­ç»ƒå‡†ç¡®ç‡æ›²çº¿')
            axes[1].grid(True, alpha=0.3)
        else:
            axes[1].text(0.5, 0.5, 'æ— å‡†ç¡®ç‡æ•°æ®\n(å›å½’ä»»åŠ¡)', 
                        ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title('å‡†ç¡®ç‡')
        
        # æ¢¯åº¦èŒƒæ•°
        axes[2].plot(self.training_history['gradient_norms'], linewidth=2, color='blue')
        axes[2].set_xlabel('Epoch')
        axes[2].set_ylabel('Gradient Norm')
        axes[2].set_title('æ¢¯åº¦èŒƒæ•°æ›²çº¿')
        axes[2].grid(True, alpha=0.3)
        axes[2].set_yscale('log')
        
        plt.tight_layout()
        plt.show()
        
        return fig

# å®Œæ•´è®­ç»ƒæ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ§  å®Œæ•´ç¥ç»ç½‘ç»œè®­ç»ƒæ¼”ç¤º")
print("=" * 80)

# åˆ›å»ºæ›´å¤æ‚çš„æ•°æ®é›†
np.random.seed(42)
n_samples = 1000
X = np.random.randn(n_samples, 2)
# åˆ›å»ºèºæ—‹å½¢æ•°æ®
t = np.linspace(0, 4*np.pi, n_samples//2)
spiral1_x = t * np.cos(t) * 0.1
spiral1_y = t * np.sin(t) * 0.1
spiral2_x = t * np.cos(t + np.pi) * 0.1
spiral2_y = t * np.sin(t + np.pi) * 0.1

X = np.vstack([
    np.column_stack([spiral1_x, spiral1_y]),
    np.column_stack([spiral2_x, spiral2_y])
])
y = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])
y_onehot = np.eye(2)[y.astype(int)]

print(f"\nğŸ“Š èºæ—‹æ•°æ®é›†:")
print(f"   æ ·æœ¬æ•°é‡: {X.shape[0]}")
print(f"   ç‰¹å¾ç»´åº¦: {X.shape[1]}")
print(f"   ç±»åˆ«æ•°é‡: {len(np.unique(y))}")

# å¯è§†åŒ–æ•°æ®é›†
fig, ax = plt.subplots(figsize=(8, 8))
scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', alpha=0.7)
ax.set_xlabel('ç‰¹å¾1')
ax.set_ylabel('ç‰¹å¾2')
ax.set_title('èºæ—‹å½¢æ•°æ®é›†')
plt.colorbar(scatter)
plt.show()

# æ¯”è¾ƒä¸åŒä¼˜åŒ–å™¨
optimizers = ['sgd', 'momentum', 'adam']
training_histories = {}

for optimizer in optimizers:
    print(f"\nğŸ”§ ä½¿ç”¨ {optimizer.upper()} ä¼˜åŒ–å™¨è®­ç»ƒ...")
    
    trainer = NeuralNetworkTrainer(
        layer_sizes=[2, 16, 16, 2],
        activations=['relu', 'relu', 'softmax'],
        learning_rate=0.01,
        optimizer=optimizer,
        loss_function='cross_entropy'
    )
    
    history = trainer.train(X, y_onehot, epochs=500, batch_size=32, verbose=False)
    training_histories[optimizer] = history
    
    # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
    final_pred = trainer.predict(X)
    final_accuracy = np.mean(np.argmax(final_pred, axis=1) == y)
    print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.4f}")

# æ¯”è¾ƒä¼˜åŒ–å™¨æ€§èƒ½
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

for optimizer, history in training_histories.items():
    axes[0].plot(history['losses'], label=optimizer.upper(), linewidth=2)
    axes[1].plot(history['accuracies'], label=optimizer.upper(), linewidth=2)

axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('ä¸åŒä¼˜åŒ–å™¨çš„æŸå¤±å¯¹æ¯”')
axes[0].legend()
axes[0].grid(True, alpha=0.3)
axes[0].set_yscale('log')

axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].set_title('ä¸åŒä¼˜åŒ–å™¨çš„å‡†ç¡®ç‡å¯¹æ¯”')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nâœ… å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­æ¼”ç¤ºå®Œæˆ!")
```

## 4. æ€è€ƒé¢˜

1. **æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**: ä¸ºä»€ä¹ˆæ·±å±‚ç½‘ç»œå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±ï¼Ÿå¦‚ä½•é€šè¿‡æ¿€æ´»å‡½æ•°é€‰æ‹©å’Œæƒé‡åˆå§‹åŒ–æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Ÿ

2. **æ•°å€¼ç¨³å®šæ€§**: åœ¨å®ç°Softmaxå‡½æ•°æ—¶ï¼Œä¸ºä»€ä¹ˆè¦å‡å»è¾“å…¥çš„æœ€å¤§å€¼ï¼Ÿè¿™å¦‚ä½•æé«˜æ•°å€¼ç¨³å®šæ€§ï¼Ÿ

3. **ä¼˜åŒ–å™¨æ¯”è¾ƒ**: SGDã€Momentumå’ŒAdamä¼˜åŒ–å™¨å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿåœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©å“ªç§ä¼˜åŒ–å™¨ï¼Ÿ

4. **æ‰¹æ¬¡å¤§å°å½±å“**: æ‰¹æ¬¡å¤§å°å¯¹è®­ç»ƒè¿‡ç¨‹æœ‰ä»€ä¹ˆå½±å“ï¼Ÿå¤§æ‰¹æ¬¡å’Œå°æ‰¹æ¬¡å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ

5. **æ¢¯åº¦æ£€æŸ¥**: æ¢¯åº¦æ£€æŸ¥çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿåœ¨å®é™…é¡¹ç›®ä¸­åº”è¯¥å¦‚ä½•ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥æ¥è°ƒè¯•ç½‘ç»œï¼Ÿ

## 5. å°ç»“

### 5.1 æ ¸å¿ƒæ¦‚å¿µ

- **å‰å‘ä¼ æ’­**: æ•°æ®ä»è¾“å…¥å±‚æµå‘è¾“å‡ºå±‚çš„è®¡ç®—è¿‡ç¨‹
- **åå‘ä¼ æ’­**: é€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°çš„è¿‡ç¨‹
- **æ¿€æ´»å‡½æ•°**: ä¸ºç½‘ç»œå¼•å…¥éçº¿æ€§çš„å…³é”®ç»„ä»¶
- **æŸå¤±å‡½æ•°**: è¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼å·®å¼‚çš„æŒ‡æ ‡
- **ä¼˜åŒ–å™¨**: æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°çš„ç®—æ³•

### 5.2 å…³é”®æŠ€æœ¯

- **é“¾å¼æ³•åˆ™**: åå‘ä¼ æ’­çš„æ•°å­¦åŸºç¡€
- **æ¢¯åº¦ä¸‹é™**: å‚æ•°ä¼˜åŒ–çš„åŸºæœ¬æ–¹æ³•
- **æƒé‡åˆå§‹åŒ–**: å½±å“è®­ç»ƒç¨³å®šæ€§çš„é‡è¦å› ç´ 
- **æ‰¹æ¬¡è®­ç»ƒ**: æé«˜è®­ç»ƒæ•ˆç‡çš„ç­–ç•¥
- **æ¢¯åº¦æ£€æŸ¥**: éªŒè¯æ¢¯åº¦è®¡ç®—æ­£ç¡®æ€§çš„æ–¹æ³•

### 5.3 å®è·µè¦ç‚¹

- é€‰æ‹©åˆé€‚çš„æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°
- æ­£ç¡®åˆå§‹åŒ–ç½‘ç»œå‚æ•°
- ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦èŒƒæ•°
- ä½¿ç”¨åˆé€‚çš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡
- å®šæœŸè¿›è¡Œæ¢¯åº¦æ£€æŸ¥ä»¥ç¡®ä¿å®ç°æ­£ç¡®

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œä½ å·²ç»æ·±å…¥ç†è§£äº†ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•â€”â€”å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚è¿™ä¸ºä½ è¿›ä¸€æ­¥å­¦ä¹ æ·±åº¦å­¦ä¹ çš„é«˜çº§æŠ€æœ¯å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚