# 1.3.1 K-meansèšç±»ç®—æ³•

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£èšç±»åˆ†æçš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨åœºæ™¯
- æŒæ¡K-meansç®—æ³•çš„æ ¸å¿ƒæ€æƒ³å’Œå®ç°æ­¥éª¤
- å­¦ä¼šä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜èšç±»æ•°K
- å®ç°å®Œæ•´çš„å®¢æˆ·åˆ†ç¾¤åˆ†æé¡¹ç›®
- äº†è§£K-meansç®—æ³•çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯

## 1. èšç±»åˆ†æåŸºç¡€

### 1.1 ä»€ä¹ˆæ˜¯èšç±»

èšç±»æ˜¯æ— ç›‘ç£å­¦ä¹ çš„ä¸€ç§é‡è¦æ–¹æ³•ï¼Œå…¶ç›®æ ‡æ˜¯å°†æ•°æ®é›†ä¸­çš„æ ·æœ¬åˆ†æˆè‹¥å¹²ä¸ªç»„ï¼ˆç°‡ï¼‰ï¼Œä½¿å¾—ï¼š
- **ç°‡å†…ç›¸ä¼¼æ€§é«˜**ï¼šåŒä¸€ç°‡å†…çš„æ•°æ®ç‚¹å°½å¯èƒ½ç›¸ä¼¼
- **ç°‡é—´ç›¸ä¼¼æ€§ä½**ï¼šä¸åŒç°‡ä¹‹é—´çš„æ•°æ®ç‚¹å°½å¯èƒ½ä¸åŒ

### 1.2 èšç±»çš„åº”ç”¨åœºæ™¯

```python
# å¸¸è§èšç±»åº”ç”¨åœºæ™¯
applications = {
    "å®¢æˆ·åˆ†ç¾¤": "æ ¹æ®è´­ä¹°è¡Œä¸ºå°†å®¢æˆ·åˆ†ä¸ºä¸åŒç¾¤ä½“",
    "å¸‚åœºç»†åˆ†": "è¯†åˆ«ä¸åŒçš„ç›®æ ‡å¸‚åœºå’Œç”¨æˆ·ç¾¤ä½“",
    "å›¾åƒåˆ†å‰²": "å°†å›¾åƒåˆ†å‰²æˆä¸åŒçš„åŒºåŸŸæˆ–å¯¹è±¡",
    "åŸºå› åˆ†æ": "æ ¹æ®åŸºå› è¡¨è¾¾æ¨¡å¼å¯¹åŸºå› è¿›è¡Œåˆ†ç±»",
    "ç¤¾äº¤ç½‘ç»œ": "å‘ç°ç¤¾äº¤ç½‘ç»œä¸­çš„ç¤¾åŒºç»“æ„",
    "å¼‚å¸¸æ£€æµ‹": "è¯†åˆ«ä¸æ­£å¸¸æ¨¡å¼ä¸ç¬¦çš„å¼‚å¸¸æ•°æ®"
}
```

## 2. K-meansç®—æ³•åŸç†

### 2.1 ç®—æ³•æ ¸å¿ƒæ€æƒ³

K-meansç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**å°†æ•°æ®åˆ†æˆKä¸ªç°‡ï¼Œä½¿å¾—æ¯ä¸ªæ•°æ®ç‚¹åˆ°å…¶æ‰€å±ç°‡ä¸­å¿ƒçš„è·ç¦»å¹³æ–¹å’Œæœ€å°**ã€‚

### 2.2 ç®—æ³•æ­¥éª¤

```mermaid
flowchart TD
    A[å¼€å§‹] --> B[è®¾å®šèšç±»æ•°K]
    B --> C[éšæœºåˆå§‹åŒ–Kä¸ªèšç±»ä¸­å¿ƒ]
    C --> D[è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹åˆ°å„ä¸­å¿ƒçš„è·ç¦»]
    D --> E[å°†æ•°æ®ç‚¹åˆ†é…ç»™æœ€è¿‘çš„èšç±»ä¸­å¿ƒ]
    E --> F[é‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„ä¸­å¿ƒç‚¹]
    F --> G{ä¸­å¿ƒç‚¹æ˜¯å¦æ”¶æ•›?}
    G -->|å¦| D
    G -->|æ˜¯| H[è¾“å‡ºèšç±»ç»“æœ]
    H --> I[ç»“æŸ]
```

### 2.3 æ•°å­¦è¡¨è¾¾

**ç›®æ ‡å‡½æ•°**ï¼ˆæœ€å°åŒ–ç°‡å†…å¹³æ–¹å’Œï¼‰ï¼š
$$J = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$

å…¶ä¸­ï¼š
- $k$ æ˜¯èšç±»æ•°
- $C_i$ æ˜¯ç¬¬iä¸ªç°‡
- $\mu_i$ æ˜¯ç¬¬iä¸ªç°‡çš„ä¸­å¿ƒç‚¹
- $||x - \mu_i||^2$ æ˜¯æ¬§å‡ é‡Œå¾—è·ç¦»çš„å¹³æ–¹

**ä¸­å¿ƒç‚¹æ›´æ–°å…¬å¼**ï¼š
$$\mu_i = \frac{1}{|C_i|} \sum_{x \in C_i} x$$

## 3. K-meansç®—æ³•å®ç°

### 3.1 åŸºç¡€å®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import seaborn as sns
from matplotlib.colors import ListedColormap

class SimpleKMeans:
    """ç®€å•çš„K-meansèšç±»å®ç°"""
    
    def __init__(self, k=3, max_iters=100, random_state=42):
        self.k = k
        self.max_iters = max_iters
        self.random_state = random_state
        
    def fit(self, X):
        """è®­ç»ƒK-meansæ¨¡å‹"""
        np.random.seed(self.random_state)
        
        # åˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        n_samples, n_features = X.shape
        self.centroids = X[np.random.choice(n_samples, self.k, replace=False)]
        
        # å­˜å‚¨è®­ç»ƒå†å²
        self.history = {'centroids': [self.centroids.copy()], 'inertia': []}
        
        for iteration in range(self.max_iters):
            # åˆ†é…æ•°æ®ç‚¹åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
            distances = self._calculate_distances(X)
            self.labels_ = np.argmin(distances, axis=1)
            
            # æ›´æ–°èšç±»ä¸­å¿ƒ
            new_centroids = np.zeros((self.k, n_features))
            for i in range(self.k):
                if np.sum(self.labels_ == i) > 0:
                    new_centroids[i] = X[self.labels_ == i].mean(axis=0)
                else:
                    new_centroids[i] = self.centroids[i]
            
            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(self.centroids, new_centroids):
                print(f"ç®—æ³•åœ¨ç¬¬{iteration+1}æ¬¡è¿­ä»£åæ”¶æ•›")
                break
                
            self.centroids = new_centroids
            self.history['centroids'].append(self.centroids.copy())
            
            # è®¡ç®—ç°‡å†…å¹³æ–¹å’Œ
            inertia = self._calculate_inertia(X)
            self.history['inertia'].append(inertia)
        
        self.inertia_ = self._calculate_inertia(X)
        return self
    
    def _calculate_distances(self, X):
        """è®¡ç®—æ•°æ®ç‚¹åˆ°å„èšç±»ä¸­å¿ƒçš„è·ç¦»"""
        distances = np.zeros((X.shape[0], self.k))
        for i, centroid in enumerate(self.centroids):
            distances[:, i] = np.sqrt(np.sum((X - centroid) ** 2, axis=1))
        return distances
    
    def _calculate_inertia(self, X):
        """è®¡ç®—ç°‡å†…å¹³æ–¹å’Œ"""
        inertia = 0
        for i in range(self.k):
            cluster_points = X[self.labels_ == i]
            if len(cluster_points) > 0:
                inertia += np.sum((cluster_points - self.centroids[i]) ** 2)
        return inertia
    
    def predict(self, X):
        """é¢„æµ‹æ–°æ•°æ®ç‚¹çš„èšç±»æ ‡ç­¾"""
        distances = self._calculate_distances(X)
        return np.argmin(distances, axis=1)
    
    def fit_predict(self, X):
        """è®­ç»ƒå¹¶é¢„æµ‹"""
        return self.fit(X).labels_

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, 
                       random_state=42, center_box=(-10.0, 10.0))

print("æ•°æ®é›†ä¿¡æ¯ï¼š")
print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
print(f"çœŸå®èšç±»æ•°: {len(np.unique(y_true))}")
```

### 3.2 ç®—æ³•è®­ç»ƒå’Œå¯è§†åŒ–

```python
# è®­ç»ƒK-meansæ¨¡å‹
kmeans = SimpleKMeans(k=4, max_iters=100)
kmeans.fit(X)

print(f"\nè®­ç»ƒç»“æœï¼š")
print(f"æœ€ç»ˆç°‡å†…å¹³æ–¹å’Œ: {kmeans.inertia_:.2f}")
print(f"èšç±»ä¸­å¿ƒ:\n{kmeans.centroids}")

# å¯è§†åŒ–èšç±»ç»“æœ
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# åŸå§‹æ•°æ®
axes[0, 0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.7)
axes[0, 0].set_title('åŸå§‹æ•°æ®ï¼ˆçœŸå®æ ‡ç­¾ï¼‰', fontsize=14)
axes[0, 0].set_xlabel('ç‰¹å¾1')
axes[0, 0].set_ylabel('ç‰¹å¾2')
axes[0, 0].grid(True, alpha=0.3)

# K-meansèšç±»ç»“æœ
axes[0, 1].scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.7)
axes[0, 1].scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], 
                   c='red', marker='x', s=200, linewidths=3, label='èšç±»ä¸­å¿ƒ')
axes[0, 1].set_title('K-meansèšç±»ç»“æœ', fontsize=14)
axes[0, 1].set_xlabel('ç‰¹å¾1')
axes[0, 1].set_ylabel('ç‰¹å¾2')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç°‡å†…å¹³æ–¹å’Œå˜åŒ–
if kmeans.history['inertia']:
    axes[1, 0].plot(range(1, len(kmeans.history['inertia']) + 1), 
                    kmeans.history['inertia'], 'bo-')
    axes[1, 0].set_title('è®­ç»ƒè¿‡ç¨‹ä¸­ç°‡å†…å¹³æ–¹å’Œçš„å˜åŒ–', fontsize=14)
    axes[1, 0].set_xlabel('è¿­ä»£æ¬¡æ•°')
    axes[1, 0].set_ylabel('ç°‡å†…å¹³æ–¹å’Œ')
    axes[1, 0].grid(True, alpha=0.3)

# èšç±»ä¸­å¿ƒçš„ç§»åŠ¨è½¨è¿¹
colors = ['red', 'blue', 'green', 'orange']
for i in range(kmeans.k):
    centroid_history = [centroids[i] for centroids in kmeans.history['centroids']]
    centroid_history = np.array(centroid_history)
    axes[1, 1].plot(centroid_history[:, 0], centroid_history[:, 1], 
                     'o-', color=colors[i], label=f'ä¸­å¿ƒ{i+1}', linewidth=2, markersize=6)
    axes[1, 1].scatter(centroid_history[-1, 0], centroid_history[-1, 1], 
                       color=colors[i], s=200, marker='*', edgecolor='black')

axes[1, 1].scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', alpha=0.3)
axes[1, 1].set_title('èšç±»ä¸­å¿ƒç§»åŠ¨è½¨è¿¹', fontsize=14)
axes[1, 1].set_xlabel('ç‰¹å¾1')
axes[1, 1].set_ylabel('ç‰¹å¾2')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## 4. ç¡®å®šæœ€ä¼˜èšç±»æ•°K

### 4.1 è‚˜éƒ¨æ³•åˆ™ï¼ˆElbow Methodï¼‰

```python
def elbow_method(X, max_k=10):
    """ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜Kå€¼"""
    inertias = []
    k_range = range(1, max_k + 1)
    
    for k in k_range:
        kmeans = SimpleKMeans(k=k, random_state=42)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
    
    return k_range, inertias

# è®¡ç®—ä¸åŒKå€¼çš„ç°‡å†…å¹³æ–¹å’Œ
k_range, inertias = elbow_method(X, max_k=10)

# å¯è§†åŒ–è‚˜éƒ¨æ³•åˆ™
plt.figure(figsize=(12, 5))

# è‚˜éƒ¨æ³•åˆ™å›¾
plt.subplot(1, 2, 1)
plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)
plt.xlabel('èšç±»æ•° K', fontsize=12)
plt.ylabel('ç°‡å†…å¹³æ–¹å’Œ (WCSS)', fontsize=12)
plt.title('è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜Kå€¼', fontsize=14)
plt.grid(True, alpha=0.3)

# æ ‡æ³¨å¯èƒ½çš„æœ€ä¼˜Kå€¼
optimal_k = 4  # æ ¹æ®è‚˜éƒ¨æ³•åˆ™åˆ¤æ–­
plt.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7, label=f'æœ€ä¼˜K={optimal_k}')
plt.legend()

# è®¡ç®—ç›¸é‚»Kå€¼çš„æ”¹å–„ç¨‹åº¦
improvements = []
for i in range(1, len(inertias)):
    improvement = inertias[i-1] - inertias[i]
    improvements.append(improvement)

plt.subplot(1, 2, 2)
plt.plot(k_range[1:], improvements, 'ro-', linewidth=2, markersize=8)
plt.xlabel('èšç±»æ•° K', fontsize=12)
plt.ylabel('WCSSæ”¹å–„ç¨‹åº¦', fontsize=12)
plt.title('ç›¸é‚»Kå€¼çš„æ”¹å–„ç¨‹åº¦', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("ä¸åŒKå€¼çš„ç°‡å†…å¹³æ–¹å’Œï¼š")
for k, inertia in zip(k_range, inertias):
    print(f"K={k}: WCSS={inertia:.2f}")
```

### 4.2 è½®å»“ç³»æ•°æ³•

```python
from sklearn.metrics import silhouette_score, silhouette_samples

def silhouette_analysis(X, max_k=10):
    """ä½¿ç”¨è½®å»“ç³»æ•°åˆ†ææœ€ä¼˜Kå€¼"""
    silhouette_scores = []
    k_range = range(2, max_k + 1)  # è½®å»“ç³»æ•°è‡³å°‘éœ€è¦2ä¸ªç°‡
    
    for k in k_range:
        kmeans = SimpleKMeans(k=k, random_state=42)
        labels = kmeans.fit_predict(X)
        score = silhouette_score(X, labels)
        silhouette_scores.append(score)
    
    return k_range, silhouette_scores

# è®¡ç®—è½®å»“ç³»æ•°
k_range_sil, silhouette_scores = silhouette_analysis(X, max_k=10)

# å¯è§†åŒ–è½®å»“ç³»æ•°
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(k_range_sil, silhouette_scores, 'go-', linewidth=2, markersize=8)
plt.xlabel('èšç±»æ•° K', fontsize=12)
plt.ylabel('å¹³å‡è½®å»“ç³»æ•°', fontsize=12)
plt.title('è½®å»“ç³»æ•°æ³•ç¡®å®šæœ€ä¼˜Kå€¼', fontsize=14)
plt.grid(True, alpha=0.3)

# æ ‡æ³¨æœ€ä¼˜Kå€¼
best_k = k_range_sil[np.argmax(silhouette_scores)]
plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, label=f'æœ€ä¼˜K={best_k}')
plt.legend()

# è¯¦ç»†çš„è½®å»“åˆ†æï¼ˆä»¥K=4ä¸ºä¾‹ï¼‰
kmeans_optimal = SimpleKMeans(k=4, random_state=42)
labels_optimal = kmeans_optimal.fit_predict(X)
silhouette_avg = silhouette_score(X, labels_optimal)
sample_silhouette_values = silhouette_samples(X, labels_optimal)

plt.subplot(1, 2, 2)
y_lower = 10
colors = ['red', 'blue', 'green', 'orange']

for i in range(4):
    cluster_silhouette_values = sample_silhouette_values[labels_optimal == i]
    cluster_silhouette_values.sort()
    
    size_cluster_i = cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i
    
    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_values,
                      facecolor=colors[i], edgecolor=colors[i], alpha=0.7)
    
    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

plt.axvline(x=silhouette_avg, color="red", linestyle="--", 
            label=f'å¹³å‡è½®å»“ç³»æ•° = {silhouette_avg:.3f}')
plt.xlabel('è½®å»“ç³»æ•°å€¼', fontsize=12)
plt.ylabel('ç°‡æ ‡ç­¾', fontsize=12)
plt.title('å„ç°‡çš„è½®å»“ç³»æ•°åˆ†å¸ƒ (K=4)', fontsize=14)
plt.legend()

plt.tight_layout()
plt.show()

print("\nè½®å»“ç³»æ•°åˆ†æç»“æœï¼š")
for k, score in zip(k_range_sil, silhouette_scores):
    print(f"K={k}: è½®å»“ç³»æ•°={score:.3f}")
print(f"\næ¨èçš„æœ€ä¼˜Kå€¼: {best_k}")
```

## 5. å®é™…åº”ç”¨æ¡ˆä¾‹ï¼šå®¢æˆ·åˆ†ç¾¤åˆ†æ

### 5.1 æ•°æ®å‡†å¤‡

```python
# ç”Ÿæˆå®¢æˆ·æ•°æ®
np.random.seed(42)
n_customers = 1000

# å®¢æˆ·ç‰¹å¾ï¼šå¹´é¾„ã€å¹´æ”¶å…¥ã€æ¶ˆè´¹é¢‘ç‡ã€å¹³å‡è®¢å•é‡‘é¢
customer_data = {
    'age': np.random.normal(35, 12, n_customers),
    'annual_income': np.random.normal(50000, 20000, n_customers),
    'purchase_frequency': np.random.poisson(12, n_customers),
    'avg_order_value': np.random.normal(150, 50, n_customers)
}

# åˆ›å»ºDataFrame
import pandas as pd
customer_df = pd.DataFrame(customer_data)

# æ•°æ®æ¸…æ´—
customer_df['age'] = np.clip(customer_df['age'], 18, 80)
customer_df['annual_income'] = np.clip(customer_df['annual_income'], 20000, 150000)
customer_df['purchase_frequency'] = np.clip(customer_df['purchase_frequency'], 1, 50)
customer_df['avg_order_value'] = np.clip(customer_df['avg_order_value'], 20, 500)

print("å®¢æˆ·æ•°æ®æ¦‚è§ˆï¼š")
print(customer_df.describe())

# æ•°æ®æ ‡å‡†åŒ–
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(customer_df)

print("\næ ‡å‡†åŒ–åçš„æ•°æ®å½¢çŠ¶:", X_scaled.shape)
```

### 5.2 å®¢æˆ·åˆ†ç¾¤å®ç°

```python
class CustomerSegmentation:
    """å®¢æˆ·åˆ†ç¾¤åˆ†æç³»ç»Ÿ"""
    
    def __init__(self, n_clusters=4):
        self.n_clusters = n_clusters
        self.kmeans = None
        self.scaler = StandardScaler()
        self.feature_names = None
        
    def fit(self, data, feature_names=None):
        """è®­ç»ƒå®¢æˆ·åˆ†ç¾¤æ¨¡å‹"""
        self.feature_names = feature_names or [f'feature_{i}' for i in range(data.shape[1])]
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(data)
        
        # K-meansèšç±»
        self.kmeans = SimpleKMeans(k=self.n_clusters, random_state=42)
        self.labels_ = self.kmeans.fit_predict(X_scaled)
        
        return self
    
    def predict(self, data):
        """é¢„æµ‹æ–°å®¢æˆ·çš„åˆ†ç¾¤"""
        X_scaled = self.scaler.transform(data)
        return self.kmeans.predict(X_scaled)
    
    def analyze_segments(self, data):
        """åˆ†æå„å®¢æˆ·ç¾¤ä½“çš„ç‰¹å¾"""
        df = pd.DataFrame(data, columns=self.feature_names)
        df['segment'] = self.labels_
        
        segment_analysis = {}
        
        for segment in range(self.n_clusters):
            segment_data = df[df['segment'] == segment]
            
            analysis = {
                'size': len(segment_data),
                'percentage': len(segment_data) / len(df) * 100,
                'characteristics': {}
            }
            
            for feature in self.feature_names:
                analysis['characteristics'][feature] = {
                    'mean': segment_data[feature].mean(),
                    'std': segment_data[feature].std(),
                    'min': segment_data[feature].min(),
                    'max': segment_data[feature].max()
                }
            
            segment_analysis[f'segment_{segment}'] = analysis
        
        return segment_analysis
    
    def visualize_segments(self, data):
        """å¯è§†åŒ–å®¢æˆ·åˆ†ç¾¤ç»“æœ"""
        df = pd.DataFrame(data, columns=self.feature_names)
        df['segment'] = self.labels_
        
        # åˆ›å»ºå­å›¾
        n_features = len(self.feature_names)
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.ravel()
        
        colors = ['red', 'blue', 'green', 'orange', 'purple']
        
        # ç‰¹å¾å¯¹æ¯”å›¾
        for i in range(min(4, n_features)):
            for segment in range(self.n_clusters):
                segment_data = df[df['segment'] == segment]
                axes[i].scatter(segment_data.index, segment_data[self.feature_names[i]], 
                               c=colors[segment], alpha=0.6, label=f'ç¾¤ä½“{segment}', s=30)
            
            axes[i].set_title(f'{self.feature_names[i]} åˆ†å¸ƒ', fontsize=12)
            axes[i].set_xlabel('å®¢æˆ·ID')
            axes[i].set_ylabel(self.feature_names[i])
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # ç®±çº¿å›¾å¯¹æ¯”
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.ravel()
        
        for i, feature in enumerate(self.feature_names[:4]):
            df.boxplot(column=feature, by='segment', ax=axes[i])
            axes[i].set_title(f'{feature} æŒ‰ç¾¤ä½“åˆ†å¸ƒ')
            axes[i].set_xlabel('å®¢æˆ·ç¾¤ä½“')
            axes[i].set_ylabel(feature)
        
        plt.suptitle('å„å®¢æˆ·ç¾¤ä½“ç‰¹å¾å¯¹æ¯”', fontsize=16)
        plt.tight_layout()
        plt.show()

# æ‰§è¡Œå®¢æˆ·åˆ†ç¾¤
segmentation = CustomerSegmentation(n_clusters=4)
segmentation.fit(customer_df.values, customer_df.columns.tolist())

# åˆ†æç»“æœ
segment_analysis = segmentation.analyze_segments(customer_df.values)

print("å®¢æˆ·åˆ†ç¾¤åˆ†æç»“æœï¼š")
for segment_name, analysis in segment_analysis.items():
    print(f"\n{segment_name.upper()}:")
    print(f"  å®¢æˆ·æ•°é‡: {analysis['size']} ({analysis['percentage']:.1f}%)")
    print("  ç‰¹å¾å‡å€¼:")
    for feature, stats in analysis['characteristics'].items():
        print(f"    {feature}: {stats['mean']:.2f} (Â±{stats['std']:.2f})")
```

### 5.3 åˆ†ç¾¤ç»“æœå¯è§†åŒ–

```python
# å¯è§†åŒ–åˆ†ç¾¤ç»“æœ
segmentation.visualize_segments(customer_df.values)

# åˆ›å»ºå®¢æˆ·ç¾¤ä½“æ ‡ç­¾
segment_labels = {
    0: "é«˜ä»·å€¼å®¢æˆ·",
    1: "æ½œåŠ›å®¢æˆ·", 
    2: "æ™®é€šå®¢æˆ·",
    3: "æµå¤±é£é™©å®¢æˆ·"
}

# è¯¦ç»†çš„åˆ†ç¾¤ç‰¹å¾åˆ†æ
df_with_segments = customer_df.copy()
df_with_segments['segment'] = segmentation.labels_
df_with_segments['segment_name'] = df_with_segments['segment'].map(segment_labels)

# å„ç¾¤ä½“çš„ä¸šåŠ¡ç‰¹å¾
print("\nå„å®¢æˆ·ç¾¤ä½“çš„ä¸šåŠ¡ç‰¹å¾ï¼š")
for segment_id, segment_name in segment_labels.items():
    segment_data = df_with_segments[df_with_segments['segment'] == segment_id]
    
    print(f"\n{segment_name} (ç¾¤ä½“{segment_id}):")
    print(f"  å®¢æˆ·æ•°é‡: {len(segment_data)}")
    print(f"  å¹³å‡å¹´é¾„: {segment_data['age'].mean():.1f}å²")
    print(f"  å¹³å‡å¹´æ”¶å…¥: Â¥{segment_data['annual_income'].mean():,.0f}")
    print(f"  å¹³å‡è´­ä¹°é¢‘ç‡: {segment_data['purchase_frequency'].mean():.1f}æ¬¡/å¹´")
    print(f"  å¹³å‡è®¢å•é‡‘é¢: Â¥{segment_data['avg_order_value'].mean():.0f}")
    
    # è®¡ç®—å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼ (CLV)
    clv = segment_data['purchase_frequency'] * segment_data['avg_order_value']
    print(f"  å¹³å‡å¹´åº¦ä»·å€¼: Â¥{clv.mean():,.0f}")

# è¥é”€å»ºè®®
marketing_strategies = {
    "é«˜ä»·å€¼å®¢æˆ·": "VIPæœåŠ¡ã€ä¸ªæ€§åŒ–æ¨èã€å¿ è¯šåº¦å¥–åŠ±",
    "æ½œåŠ›å®¢æˆ·": "ä¿ƒé”€æ´»åŠ¨ã€äº§å“æ¨èã€æå‡è´­ä¹°é¢‘ç‡",
    "æ™®é€šå®¢æˆ·": "æ ‡å‡†æœåŠ¡ã€å®šæœŸæ²Ÿé€šã€äº¤å‰é”€å”®",
    "æµå¤±é£é™©å®¢æˆ·": "æŒ½å›æ´»åŠ¨ã€ç‰¹åˆ«ä¼˜æƒ ã€é‡æ–°æ¿€æ´»"
}

print("\nè¥é”€ç­–ç•¥å»ºè®®ï¼š")
for segment_name, strategy in marketing_strategies.items():
    print(f"{segment_name}: {strategy}")
```

## 6. ç®—æ³•ä¼˜åŒ–å’Œå˜ä½“

### 6.1 K-means++åˆå§‹åŒ–

```python
class KMeansPlusPlus:
    """ä½¿ç”¨K-means++åˆå§‹åŒ–çš„æ”¹è¿›ç‰ˆæœ¬"""
    
    def __init__(self, k=3, max_iters=100, random_state=42):
        self.k = k
        self.max_iters = max_iters
        self.random_state = random_state
        
    def _kmeans_plus_plus_init(self, X):
        """K-means++åˆå§‹åŒ–æ–¹æ³•"""
        np.random.seed(self.random_state)
        n_samples, n_features = X.shape
        centroids = np.zeros((self.k, n_features))
        
        # éšæœºé€‰æ‹©ç¬¬ä¸€ä¸ªä¸­å¿ƒç‚¹
        centroids[0] = X[np.random.randint(n_samples)]
        
        # é€‰æ‹©å‰©ä½™çš„ä¸­å¿ƒç‚¹
        for i in range(1, self.k):
            # è®¡ç®—æ¯ä¸ªç‚¹åˆ°æœ€è¿‘ä¸­å¿ƒçš„è·ç¦»å¹³æ–¹
            distances = np.array([min([np.sum((x - c)**2) for c in centroids[:i]]) for x in X])
            
            # æŒ‰è·ç¦»å¹³æ–¹çš„æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªä¸­å¿ƒ
            probabilities = distances / distances.sum()
            cumulative_probabilities = probabilities.cumsum()
            r = np.random.rand()
            
            for j, p in enumerate(cumulative_probabilities):
                if r < p:
                    centroids[i] = X[j]
                    break
        
        return centroids
    
    def fit(self, X):
        """ä½¿ç”¨K-means++åˆå§‹åŒ–è®­ç»ƒæ¨¡å‹"""
        # ä½¿ç”¨K-means++åˆå§‹åŒ–
        self.centroids = self._kmeans_plus_plus_init(X)
        
        # å…¶ä½™æ­¥éª¤ä¸æ ‡å‡†K-meansç›¸åŒ
        for iteration in range(self.max_iters):
            # åˆ†é…æ•°æ®ç‚¹
            distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
            self.labels_ = np.argmin(distances, axis=0)
            
            # æ›´æ–°ä¸­å¿ƒç‚¹
            new_centroids = np.array([X[self.labels_ == i].mean(axis=0) 
                                    for i in range(self.k)])
            
            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(self.centroids, new_centroids):
                break
                
            self.centroids = new_centroids
        
        return self
    
    def predict(self, X):
        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)

# æ¯”è¾ƒä¸åŒåˆå§‹åŒ–æ–¹æ³•
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# ç”Ÿæˆæµ‹è¯•æ•°æ®
X_test, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)

# æ ‡å‡†K-meansï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
kmeans_standard = SimpleKMeans(k=3, random_state=42)
kmeans_standard.fit(X_test)

axes[0].scatter(X_test[:, 0], X_test[:, 1], c=kmeans_standard.labels_, cmap='viridis', alpha=0.7)
axes[0].scatter(kmeans_standard.centroids[:, 0], kmeans_standard.centroids[:, 1], 
               c='red', marker='x', s=200, linewidths=3)
axes[0].set_title(f'æ ‡å‡†K-means\nç°‡å†…å¹³æ–¹å’Œ: {kmeans_standard.inertia_:.2f}', fontsize=12)
axes[0].grid(True, alpha=0.3)

# K-means++åˆå§‹åŒ–
kmeans_plus = KMeansPlusPlus(k=3, random_state=42)
kmeans_plus.fit(X_test)

axes[1].scatter(X_test[:, 0], X_test[:, 1], c=kmeans_plus.labels_, cmap='viridis', alpha=0.7)
axes[1].scatter(kmeans_plus.centroids[:, 0], kmeans_plus.centroids[:, 1], 
               c='red', marker='x', s=200, linewidths=3)
axes[1].set_title('K-means++åˆå§‹åŒ–', fontsize=12)
axes[1].grid(True, alpha=0.3)

# sklearnå®ç°å¯¹æ¯”
from sklearn.cluster import KMeans as SklearnKMeans
sklearn_kmeans = SklearnKMeans(n_clusters=3, init='k-means++', random_state=42)
sklearn_labels = sklearn_kmeans.fit_predict(X_test)

axes[2].scatter(X_test[:, 0], X_test[:, 1], c=sklearn_labels, cmap='viridis', alpha=0.7)
axes[2].scatter(sklearn_kmeans.cluster_centers_[:, 0], sklearn_kmeans.cluster_centers_[:, 1], 
               c='red', marker='x', s=200, linewidths=3)
axes[2].set_title(f'Sklearn K-means\nç°‡å†…å¹³æ–¹å’Œ: {sklearn_kmeans.inertia_:.2f}', fontsize=12)
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 6.2 Mini-batch K-means

```python
class MiniBatchKMeans:
    """Mini-batch K-meanså®ç°ï¼ˆé€‚ç”¨äºå¤§æ•°æ®é›†ï¼‰"""
    
    def __init__(self, k=3, batch_size=100, max_iters=100, random_state=42):
        self.k = k
        self.batch_size = batch_size
        self.max_iters = max_iters
        self.random_state = random_state
        
    def fit(self, X):
        """ä½¿ç”¨mini-batchæ–¹æ³•è®­ç»ƒ"""
        np.random.seed(self.random_state)
        n_samples, n_features = X.shape
        
        # åˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        self.centroids = X[np.random.choice(n_samples, self.k, replace=False)]
        
        # è®°å½•æ¯ä¸ªä¸­å¿ƒç‚¹çš„æ›´æ–°æ¬¡æ•°
        center_counts = np.zeros(self.k)
        
        for iteration in range(self.max_iters):
            # éšæœºé€‰æ‹©ä¸€ä¸ªmini-batch
            batch_indices = np.random.choice(n_samples, 
                                           min(self.batch_size, n_samples), 
                                           replace=False)
            batch = X[batch_indices]
            
            # ä¸ºbatchä¸­çš„æ¯ä¸ªç‚¹åˆ†é…æœ€è¿‘çš„ä¸­å¿ƒ
            distances = np.sqrt(((batch - self.centroids[:, np.newaxis])**2).sum(axis=2))
            labels = np.argmin(distances, axis=0)
            
            # æ›´æ–°èšç±»ä¸­å¿ƒï¼ˆä½¿ç”¨ç§»åŠ¨å¹³å‡ï¼‰
            for i in range(self.k):
                mask = labels == i
                if np.sum(mask) > 0:
                    center_counts[i] += np.sum(mask)
                    eta = 1.0 / center_counts[i]  # å­¦ä¹ ç‡
                    self.centroids[i] = (1 - eta) * self.centroids[i] + eta * batch[mask].mean(axis=0)
        
        # æœ€ç»ˆåˆ†é…æ‰€æœ‰æ•°æ®ç‚¹
        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
        self.labels_ = np.argmin(distances, axis=0)
        
        return self
    
    def predict(self, X):
        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
print("ç®—æ³•æ€§èƒ½å¯¹æ¯”ï¼š")

# ç”Ÿæˆå¤§æ•°æ®é›†
large_X, _ = make_blobs(n_samples=5000, centers=5, cluster_std=2.0, random_state=42)

import time

# æ ‡å‡†K-means
start_time = time.time()
standard_kmeans = SimpleKMeans(k=5)
standard_kmeans.fit(large_X)
standard_time = time.time() - start_time

# Mini-batch K-means
start_time = time.time()
minibatch_kmeans = MiniBatchKMeans(k=5, batch_size=200)
minibatch_kmeans.fit(large_X)
minibatch_time = time.time() - start_time

print(f"æ ‡å‡†K-meansè®­ç»ƒæ—¶é—´: {standard_time:.3f}ç§’")
print(f"Mini-batch K-meansè®­ç»ƒæ—¶é—´: {minibatch_time:.3f}ç§’")
print(f"é€Ÿåº¦æå‡: {standard_time/minibatch_time:.1f}å€")
```

## 7. Traeé£æ ¼å®ç°

```python
class TraeKMeans:
    """Traeé£æ ¼çš„K-meansèšç±»å®ç°"""
    
    def __init__(self, n_clusters=3, init='k-means++', max_iter=300, 
                 tol=1e-4, random_state=None, verbose=True):
        self.n_clusters = n_clusters
        self.init = init
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state
        self.verbose = verbose
        
        # Traeç‰¹è‰²ï¼šè®­ç»ƒå†å²è®°å½•
        self.training_history = {
            'inertia': [],
            'centroids_movement': [],
            'iteration_times': []
        }
    
    def trae_fit(self, X, feature_names=None):
        """Traeé£æ ¼çš„è®­ç»ƒæ–¹æ³•"""
        if self.verbose:
            print("ğŸš€ Trae K-means èšç±»è®­ç»ƒå¼€å§‹")
            print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
            print(f"ğŸ¯ ç›®æ ‡èšç±»æ•°: {self.n_clusters}")
        
        self.feature_names_ = feature_names or [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]
        
        # åˆå§‹åŒ–
        np.random.seed(self.random_state)
        if self.init == 'k-means++':
            self.cluster_centers_ = self._trae_kmeans_plus_plus_init(X)
        else:
            self.cluster_centers_ = X[np.random.choice(X.shape[0], self.n_clusters, replace=False)]
        
        prev_inertia = float('inf')
        
        for iteration in range(self.max_iter):
            start_time = time.time()
            
            # åˆ†é…æ­¥éª¤
            distances = self._trae_calculate_distances(X)
            self.labels_ = np.argmin(distances, axis=1)
            
            # æ›´æ–°æ­¥éª¤
            new_centers = np.zeros_like(self.cluster_centers_)
            for k in range(self.n_clusters):
                mask = self.labels_ == k
                if np.sum(mask) > 0:
                    new_centers[k] = X[mask].mean(axis=0)
                else:
                    new_centers[k] = self.cluster_centers_[k]
            
            # è®¡ç®—ä¸­å¿ƒç‚¹ç§»åŠ¨è·ç¦»
            movement = np.sqrt(np.sum((new_centers - self.cluster_centers_) ** 2))
            self.cluster_centers_ = new_centers
            
            # è®¡ç®—ç°‡å†…å¹³æ–¹å’Œ
            inertia = self._trae_calculate_inertia(X)
            
            # è®°å½•è®­ç»ƒå†å²
            iteration_time = time.time() - start_time
            self.training_history['inertia'].append(inertia)
            self.training_history['centroids_movement'].append(movement)
            self.training_history['iteration_times'].append(iteration_time)
            
            if self.verbose and (iteration + 1) % 10 == 0:
                print(f"ğŸ“ˆ è¿­ä»£ {iteration + 1}: ç°‡å†…å¹³æ–¹å’Œ={inertia:.2f}, ä¸­å¿ƒç§»åŠ¨={movement:.4f}")
            
            # æ”¶æ•›æ£€æŸ¥
            if abs(prev_inertia - inertia) < self.tol:
                if self.verbose:
                    print(f"âœ… ç®—æ³•åœ¨ç¬¬ {iteration + 1} æ¬¡è¿­ä»£åæ”¶æ•›")
                break
            
            prev_inertia = inertia
        
        self.inertia_ = inertia
        self.n_iter_ = iteration + 1
        
        if self.verbose:
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ç»ˆç°‡å†…å¹³æ–¹å’Œ: {self.inertia_:.2f}")
            print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {sum(self.training_history['iteration_times']):.3f}ç§’")
        
        return self
    
    def _trae_kmeans_plus_plus_init(self, X):
        """Traeå®ç°çš„K-means++åˆå§‹åŒ–"""
        if self.verbose:
            print("ğŸ² ä½¿ç”¨K-means++æ™ºèƒ½åˆå§‹åŒ–...")
        
        n_samples, n_features = X.shape
        centers = np.zeros((self.n_clusters, n_features))
        
        # ç¬¬ä¸€ä¸ªä¸­å¿ƒéšæœºé€‰æ‹©
        centers[0] = X[np.random.randint(n_samples)]
        
        for i in range(1, self.n_clusters):
            # è®¡ç®—åˆ°æœ€è¿‘ä¸­å¿ƒçš„è·ç¦»å¹³æ–¹
            distances = np.array([min([np.sum((x - c)**2) for c in centers[:i]]) for x in X])
            
            # æŒ‰æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªä¸­å¿ƒ
            probabilities = distances / distances.sum()
            cumulative_probs = probabilities.cumsum()
            r = np.random.rand()
            
            for j, p in enumerate(cumulative_probs):
                if r < p:
                    centers[i] = X[j]
                    break
        
        return centers
    
    def _trae_calculate_distances(self, X):
        """è®¡ç®—è·ç¦»çŸ©é˜µ"""
        distances = np.zeros((X.shape[0], self.n_clusters))
        for i, center in enumerate(self.cluster_centers_):
            distances[:, i] = np.sqrt(np.sum((X - center) ** 2, axis=1))
        return distances
    
    def _trae_calculate_inertia(self, X):
        """è®¡ç®—ç°‡å†…å¹³æ–¹å’Œ"""
        inertia = 0
        for k in range(self.n_clusters):
            mask = self.labels_ == k
            if np.sum(mask) > 0:
                inertia += np.sum((X[mask] - self.cluster_centers_[k]) ** 2)
        return inertia
    
    def trae_predict(self, X):
        """Traeé£æ ¼çš„é¢„æµ‹æ–¹æ³•"""
        distances = self._trae_calculate_distances(X)
        predictions = np.argmin(distances, axis=1)
        
        if self.verbose:
            print(f"ğŸ”® é¢„æµ‹å®Œæˆ: {X.shape[0]} ä¸ªæ ·æœ¬")
            for k in range(self.n_clusters):
                count = np.sum(predictions == k)
                print(f"   ç°‡ {k}: {count} ä¸ªæ ·æœ¬ ({count/len(predictions)*100:.1f}%)")
        
        return predictions
    
    def trae_visualize_training(self):
        """å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"""
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        # ç°‡å†…å¹³æ–¹å’Œå˜åŒ–
        axes[0].plot(self.training_history['inertia'], 'b-o', linewidth=2, markersize=6)
        axes[0].set_title('ğŸ“ˆ ç°‡å†…å¹³æ–¹å’Œå˜åŒ–', fontsize=14)
        axes[0].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0].set_ylabel('ç°‡å†…å¹³æ–¹å’Œ')
        axes[0].grid(True, alpha=0.3)
        
        # ä¸­å¿ƒç‚¹ç§»åŠ¨è·ç¦»
        axes[1].plot(self.training_history['centroids_movement'], 'r-o', linewidth=2, markersize=6)
        axes[1].set_title('ğŸ¯ èšç±»ä¸­å¿ƒç§»åŠ¨è·ç¦»', fontsize=14)
        axes[1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[1].set_ylabel('ç§»åŠ¨è·ç¦»')
        axes[1].grid(True, alpha=0.3)
        
        # æ¯æ¬¡è¿­ä»£æ—¶é—´
        axes[2].bar(range(len(self.training_history['iteration_times'])), 
                   self.training_history['iteration_times'], alpha=0.7, color='green')
        axes[2].set_title('â±ï¸ æ¯æ¬¡è¿­ä»£è€—æ—¶', fontsize=14)
        axes[2].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[2].set_ylabel('æ—¶é—´ (ç§’)')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def trae_cluster_analysis(self, X, detailed=True):
        """Traeé£æ ¼çš„èšç±»åˆ†æ"""
        print("ğŸ“Š Trae èšç±»åˆ†ææŠ¥å‘Š")
        print("=" * 50)
        
        analysis_results = {}
        
        for k in range(self.n_clusters):
            mask = self.labels_ == k
            cluster_data = X[mask]
            
            if len(cluster_data) == 0:
                continue
            
            print(f"\nğŸ·ï¸  ç°‡ {k} åˆ†æ:")
            print(f"   æ ·æœ¬æ•°é‡: {len(cluster_data)} ({len(cluster_data)/len(X)*100:.1f}%)")
            
            cluster_analysis = {
                'size': len(cluster_data),
                'percentage': len(cluster_data)/len(X)*100,
                'center': self.cluster_centers_[k],
                'features': {}
            }
            
            if detailed:
                print(f"   èšç±»ä¸­å¿ƒ: [{', '.join([f'{x:.2f}' for x in self.cluster_centers_[k]])}]")
                
                for i, feature_name in enumerate(self.feature_names_):
                    feature_values = cluster_data[:, i]
                    feature_stats = {
                        'mean': np.mean(feature_values),
                        'std': np.std(feature_values),
                        'min': np.min(feature_values),
                        'max': np.max(feature_values)
                    }
                    
                    cluster_analysis['features'][feature_name] = feature_stats
                    
                    print(f"   {feature_name}: å‡å€¼={feature_stats['mean']:.2f}, "
                          f"æ ‡å‡†å·®={feature_stats['std']:.2f}")
            
            analysis_results[f'cluster_{k}'] = cluster_analysis
        
        print(f"\nğŸ“ˆ æ•´ä½“æŒ‡æ ‡:")
        print(f"   ç°‡å†…å¹³æ–¹å’Œ: {self.inertia_:.2f}")
        print(f"   è®­ç»ƒè¿­ä»£æ¬¡æ•°: {self.n_iter_}")
        print(f"   å¹³å‡æ¯æ¬¡è¿­ä»£æ—¶é—´: {np.mean(self.training_history['iteration_times']):.4f}ç§’")
        
        return analysis_results

# Trae K-means ä½¿ç”¨ç¤ºä¾‹
print("ğŸ¯ Trae K-means èšç±»æ¼”ç¤º")
print("=" * 40)

# ä½¿ç”¨ä¹‹å‰çš„å®¢æˆ·æ•°æ®
trae_kmeans = TraeKMeans(n_clusters=4, init='k-means++', random_state=42, verbose=True)
trae_kmeans.trae_fit(customer_df.values, customer_df.columns.tolist())

# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
trae_kmeans.trae_visualize_training()

# è¯¦ç»†åˆ†æ
analysis = trae_kmeans.trae_cluster_analysis(customer_df.values, detailed=True)

# é¢„æµ‹æ–°å®¢æˆ·
new_customers = np.array([
    [25, 30000, 5, 80],   # å¹´è½»ä½æ”¶å…¥å®¢æˆ·
    [45, 80000, 20, 200], # ä¸­å¹´é«˜ä»·å€¼å®¢æˆ·
    [35, 50000, 12, 150]  # æ™®é€šå®¢æˆ·
])

print("\nğŸ”® æ–°å®¢æˆ·åˆ†ç¾¤é¢„æµ‹:")
predictions = trae_kmeans.trae_predict(new_customers)
for i, (customer, pred) in enumerate(zip(new_customers, predictions)):
    print(f"å®¢æˆ·{i+1} {customer} -> ç°‡ {pred}")
```

## 8. æ€è€ƒé¢˜

1. **Kå€¼é€‰æ‹©**: é™¤äº†è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°ï¼Œè¿˜æœ‰å“ªäº›æ–¹æ³•å¯ä»¥ç¡®å®šæœ€ä¼˜çš„Kå€¼ï¼Ÿå„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ

2. **åˆå§‹åŒ–æ•æ„Ÿæ€§**: ä¸ºä»€ä¹ˆK-meansç®—æ³•å¯¹åˆå§‹åŒ–æ•æ„Ÿï¼ŸK-means++æ˜¯å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼Ÿ

3. **è·ç¦»åº¦é‡**: K-meansé»˜è®¤ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œåœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥è€ƒè™‘ä½¿ç”¨å…¶ä»–è·ç¦»åº¦é‡ï¼ˆå¦‚æ›¼å“ˆé¡¿è·ç¦»ã€ä½™å¼¦è·ç¦»ï¼‰ï¼Ÿ

4. **æ•°æ®é¢„å¤„ç†**: ä¸ºä»€ä¹ˆåœ¨K-meansèšç±»å‰é€šå¸¸éœ€è¦å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Ÿä¸æ ‡å‡†åŒ–ä¼šäº§ç”Ÿä»€ä¹ˆé—®é¢˜ï¼Ÿ

5. **ç®—æ³•å±€é™æ€§**: K-meansç®—æ³•åœ¨å¤„ç†ä»€ä¹ˆç±»å‹çš„æ•°æ®åˆ†å¸ƒæ—¶æ•ˆæœä¸ä½³ï¼Ÿåº”è¯¥å¦‚ä½•æ”¹è¿›ï¼Ÿ

## 9. å°ç»“

### 9.1 K-meansæ ¸å¿ƒä¼˜åŠ¿

- **ç®€å•æ˜“æ‡‚**: ç®—æ³•æ€æƒ³ç›´è§‚ï¼Œæ˜“äºç†è§£å’Œå®ç°
- **è®¡ç®—é«˜æ•ˆ**: æ—¶é—´å¤æ‚åº¦O(nkt)ï¼Œé€‚åˆå¤§æ•°æ®é›†
- **æ”¶æ•›ä¿è¯**: ç®—æ³•ä¿è¯æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£
- **å¹¿æ³›åº”ç”¨**: åœ¨å®¢æˆ·åˆ†ç¾¤ã€å›¾åƒåˆ†å‰²ç­‰é¢†åŸŸåº”ç”¨å¹¿æ³›

### 9.2 å…³é”®æŠ€æœ¯è¦ç‚¹

- **èšç±»æ•°Kçš„é€‰æ‹©**: è‚˜éƒ¨æ³•åˆ™ã€è½®å»“ç³»æ•°æ³•
- **åˆå§‹åŒ–ç­–ç•¥**: K-means++æ™ºèƒ½åˆå§‹åŒ–
- **æ”¶æ•›åˆ¤æ–­**: ä¸­å¿ƒç‚¹ç§»åŠ¨è·ç¦»æˆ–ç›®æ ‡å‡½æ•°å˜åŒ–
- **æ€§èƒ½ä¼˜åŒ–**: Mini-batchæ–¹æ³•å¤„ç†å¤§æ•°æ®é›†

### 9.3 å®é™…åº”ç”¨åœºæ™¯

- **å®¢æˆ·åˆ†ç¾¤**: æ ¹æ®è¡Œä¸ºç‰¹å¾è¿›è¡Œç²¾å‡†è¥é”€
- **å¸‚åœºç»†åˆ†**: è¯†åˆ«ä¸åŒçš„ç›®æ ‡å¸‚åœº
- **å›¾åƒå¤„ç†**: å›¾åƒåˆ†å‰²å’Œé¢œè‰²é‡åŒ–
- **æ•°æ®å‹ç¼©**: å‘é‡é‡åŒ–å’Œç‰¹å¾é™ç»´

### 9.4 ç®—æ³•å±€é™æ€§

- **éœ€è¦é¢„è®¾Kå€¼**: èšç±»æ•°éœ€è¦äº‹å…ˆç¡®å®š
- **å¯¹åˆå§‹åŒ–æ•æ„Ÿ**: ä¸åŒåˆå§‹åŒ–å¯èƒ½å¾—åˆ°ä¸åŒç»“æœ
- **å‡è®¾çƒå½¢ç°‡**: å¯¹éçƒå½¢åˆ†å¸ƒæ•ˆæœä¸ä½³
- **å¯¹å™ªå£°æ•æ„Ÿ**: å¼‚å¸¸å€¼ä¼šå½±å“èšç±»ä¸­å¿ƒ

### 9.5 ä½¿ç”¨å»ºè®®

- **æ•°æ®é¢„å¤„ç†**: æ ‡å‡†åŒ–æ•°æ®ï¼Œå¤„ç†å¼‚å¸¸å€¼
- **Kå€¼é€‰æ‹©**: ç»“åˆä¸šåŠ¡éœ€æ±‚å’Œç»Ÿè®¡æ–¹æ³•
- **å¤šæ¬¡è¿è¡Œ**: ä½¿ç”¨ä¸åŒåˆå§‹åŒ–å¤šæ¬¡è¿è¡Œå–æœ€ä½³ç»“æœ
- **ç»“æœéªŒè¯**: ç»“åˆä¸šåŠ¡çŸ¥è¯†éªŒè¯èšç±»åˆç†æ€§

### 9.6 ä¸‹ä¸€æ­¥å­¦ä¹ 

- **å±‚æ¬¡èšç±»**: ä¸éœ€è¦é¢„è®¾èšç±»æ•°çš„æ–¹æ³•
- **å¯†åº¦èšç±»**: DBSCANç­‰å¤„ç†ä»»æ„å½¢çŠ¶ç°‡çš„ç®—æ³•
- **è°±èšç±»**: å¤„ç†éå‡¸å½¢çŠ¶æ•°æ®çš„é«˜çº§æ–¹æ³•
- **æ·±åº¦èšç±»**: ç»“åˆç¥ç»ç½‘ç»œçš„ç°ä»£èšç±»æ–¹æ³•

---

**æœ¬èŠ‚é‡ç‚¹**: K-meansæ˜¯æœ€åŸºç¡€ä¹Ÿæ˜¯æœ€é‡è¦çš„èšç±»ç®—æ³•ï¼ŒæŒæ¡å…¶åŸç†å’Œå®ç°å¯¹ç†è§£å…¶ä»–èšç±»æ–¹æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¦æ³¨æ„æ•°æ®é¢„å¤„ç†ã€å‚æ•°é€‰æ‹©å’Œç»“æœè§£é‡Šçš„é‡è¦æ€§ã€‚