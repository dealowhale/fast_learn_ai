# 1.4.1 ç¥ç»ç½‘ç»œåŸºæœ¬æ¦‚å¿µ

## 1. ç¥ç»ç½‘ç»œæ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œ

ç¥ç»ç½‘ç»œï¼ˆNeural Networkï¼‰æ˜¯ä¸€ç§å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿå¤§è„‘ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥å’Œä¿¡æ¯ä¼ é€’æ–¹å¼æ¥å¤„ç†ä¿¡æ¯ã€‚å®ƒç”±å¤§é‡ç›¸äº’è¿æ¥çš„ç®€å•å¤„ç†å•å…ƒï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„éçº¿æ€§æ˜ å°„å…³ç³»ã€‚

```mermaid
graph LR
    subgraph "ç”Ÿç‰©ç¥ç»å…ƒ"
        A[æ ‘çª] --> B[ç»†èƒä½“]
        B --> C[è½´çª]
        C --> D[çªè§¦]
    end
    
    subgraph "äººå·¥ç¥ç»å…ƒ"
        E[è¾“å…¥] --> F[åŠ æƒæ±‚å’Œ]
        F --> G[æ¿€æ´»å‡½æ•°]
        G --> H[è¾“å‡º]
    end
    
    A -.å¯¹åº”.-> E
    B -.å¯¹åº”.-> F
    C -.å¯¹åº”.-> G
    D -.å¯¹åº”.-> H
```

### 1.2 ç¥ç»ç½‘ç»œçš„å‘å±•å†ç¨‹

```python
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

# ç¥ç»ç½‘ç»œå‘å±•æ—¶é—´çº¿
class NeuralNetworkHistory:
    def __init__(self):
        self.milestones = [
            (1943, "McCulloch-Pittsç¥ç»å…ƒ", "é¦–ä¸ªæ•°å­¦ç¥ç»å…ƒæ¨¡å‹"),
            (1957, "æ„ŸçŸ¥æœº", "Rosenblattæå‡ºæ„ŸçŸ¥æœºç®—æ³•"),
            (1969, "æ„ŸçŸ¥æœºå±€é™æ€§", "MinskyæŒ‡å‡ºçº¿æ€§ä¸å¯åˆ†é—®é¢˜"),
            (1986, "åå‘ä¼ æ’­", "Rumelhartç­‰äººå®Œå–„BPç®—æ³•"),
            (1989, "é€šç”¨é€¼è¿‘å®šç†", "è¯æ˜ç¥ç»ç½‘ç»œçš„ç†è®ºåŸºç¡€"),
            (2006, "æ·±åº¦å­¦ä¹ å¤å…´", "Hintonæå‡ºæ·±åº¦ä¿¡å¿µç½‘ç»œ"),
            (2012, "ImageNetçªç ´", "AlexNetåœ¨å›¾åƒè¯†åˆ«ä¸Šçš„æˆåŠŸ"),
            (2017, "Transformer", "æ³¨æ„åŠ›æœºåˆ¶é©å‘½æ€§çªç ´"),
            (2020, "GPT-3", "å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é‡Œç¨‹ç¢‘")
        ]
    
    def visualize_timeline(self):
        """å¯è§†åŒ–ç¥ç»ç½‘ç»œå‘å±•æ—¶é—´çº¿"""
        fig, ax = plt.subplots(figsize=(15, 8))
        
        years = [milestone[0] for milestone in self.milestones]
        names = [milestone[1] for milestone in self.milestones]
        descriptions = [milestone[2] for milestone in self.milestones]
        
        # ç»˜åˆ¶æ—¶é—´çº¿
        ax.plot(years, [1]*len(years), 'o-', linewidth=3, markersize=10, color='#2E86AB')
        
        # æ·»åŠ é‡Œç¨‹ç¢‘æ ‡ç­¾
        for i, (year, name, desc) in enumerate(self.milestones):
            # äº¤æ›¿æ˜¾ç¤ºåœ¨æ—¶é—´çº¿ä¸Šä¸‹
            y_pos = 1.3 if i % 2 == 0 else 0.7
            
            ax.annotate(f'{year}\n{name}', 
                       xy=(year, 1), xytext=(year, y_pos),
                       ha='center', va='center',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7),
                       arrowprops=dict(arrowstyle='->', color='gray', lw=1))
            
            # æ·»åŠ æè¿°
            ax.text(year, y_pos-0.2 if i % 2 == 0 else y_pos+0.2, desc, 
                   ha='center', va='center', fontsize=8, style='italic')
        
        ax.set_ylim(0, 2)
        ax.set_xlabel('å¹´ä»½', fontsize=12)
        ax.set_title('ç¥ç»ç½‘ç»œå‘å±•å†ç¨‹', fontsize=16, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_yticks([])
        
        plt.tight_layout()
        plt.show()
        
        return fig

# åˆ›å»ºå¹¶æ˜¾ç¤ºå‘å±•å†ç¨‹
history = NeuralNetworkHistory()
history.visualize_timeline()
```

## 2. äººå·¥ç¥ç»å…ƒæ¨¡å‹

### 2.1 åŸºæœ¬ç¥ç»å…ƒç»“æ„

äººå·¥ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œå®ƒæ¥æ”¶å¤šä¸ªè¾“å…¥ä¿¡å·ï¼Œç»è¿‡åŠ æƒæ±‚å’Œå’Œæ¿€æ´»å‡½æ•°å¤„ç†åäº§ç”Ÿè¾“å‡ºã€‚

**æ•°å­¦è¡¨è¾¾å¼ï¼š**

$$y = f(\sum_{i=1}^{n} w_i x_i + b)$$

å…¶ä¸­ï¼š
- $x_i$ï¼šç¬¬iä¸ªè¾“å…¥
- $w_i$ï¼šç¬¬iä¸ªè¾“å…¥å¯¹åº”çš„æƒé‡
- $b$ï¼šåç½®é¡¹
- $f$ï¼šæ¿€æ´»å‡½æ•°
- $y$ï¼šè¾“å‡º

```python
class SimpleNeuron:
    """ç®€å•ç¥ç»å…ƒå®ç°"""
    
    def __init__(self, num_inputs, activation='sigmoid'):
        # éšæœºåˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weights = np.random.randn(num_inputs) * 0.1
        self.bias = np.random.randn() * 0.1
        self.activation = activation
    
    def sigmoid(self, x):
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def relu(self, x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    def tanh(self, x):
        """Tanhæ¿€æ´»å‡½æ•°"""
        return np.tanh(x)
    
    def forward(self, inputs):
        """å‰å‘ä¼ æ’­"""
        # åŠ æƒæ±‚å’Œ
        weighted_sum = np.dot(inputs, self.weights) + self.bias
        
        # åº”ç”¨æ¿€æ´»å‡½æ•°
        if self.activation == 'sigmoid':
            output = self.sigmoid(weighted_sum)
        elif self.activation == 'relu':
            output = self.relu(weighted_sum)
        elif self.activation == 'tanh':
            output = self.tanh(weighted_sum)
        else:
            output = weighted_sum  # çº¿æ€§æ¿€æ´»
        
        return output
    
    def visualize_neuron(self):
        """å¯è§†åŒ–ç¥ç»å…ƒç»“æ„"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å·¦å›¾ï¼šç¥ç»å…ƒç»“æ„
        ax1.set_xlim(-2, 4)
        ax1.set_ylim(-2, 3)
        
        # ç»˜åˆ¶è¾“å…¥
        num_inputs = len(self.weights)
        input_y = np.linspace(-1, 2, num_inputs)
        
        for i, y in enumerate(input_y):
            # è¾“å…¥èŠ‚ç‚¹
            circle = plt.Circle((-1, y), 0.2, color='lightblue', ec='black')
            ax1.add_patch(circle)
            ax1.text(-1, y, f'x{i+1}', ha='center', va='center', fontweight='bold')
            
            # è¿æ¥çº¿å’Œæƒé‡
            ax1.arrow(-0.8, y, 1.3, 0.5-y, head_width=0.1, head_length=0.1, 
                     fc='red', ec='red', alpha=0.7)
            ax1.text(-0.2, y+0.2, f'w{i+1}={self.weights[i]:.2f}', 
                    fontsize=8, color='red')
        
        # ç¥ç»å…ƒä¸»ä½“
        circle = plt.Circle((1, 0.5), 0.4, color='yellow', ec='black', linewidth=2)
        ax1.add_patch(circle)
        ax1.text(1, 0.5, 'Î£', ha='center', va='center', fontsize=20, fontweight='bold')
        
        # åç½®
        ax1.text(1, -0.8, f'b={self.bias:.2f}', ha='center', va='center', 
                bbox=dict(boxstyle='round', facecolor='lightgreen'))
        
        # æ¿€æ´»å‡½æ•°
        rect = plt.Rectangle((2, 0.2), 0.6, 0.6, facecolor='orange', ec='black')
        ax1.add_patch(rect)
        ax1.text(2.3, 0.5, self.activation, ha='center', va='center', fontweight='bold')
        
        # è¾“å‡º
        ax1.arrow(2.6, 0.5, 0.8, 0, head_width=0.1, head_length=0.1, 
                 fc='green', ec='green')
        ax1.text(3.5, 0.5, 'y', ha='center', va='center', fontsize=16, 
                fontweight='bold', color='green')
        
        ax1.set_title('ç¥ç»å…ƒç»“æ„å›¾', fontsize=14, fontweight='bold')
        ax1.axis('off')
        
        # å³å›¾ï¼šæ¿€æ´»å‡½æ•°
        x = np.linspace(-5, 5, 100)
        if self.activation == 'sigmoid':
            y = self.sigmoid(x)
        elif self.activation == 'relu':
            y = self.relu(x)
        elif self.activation == 'tanh':
            y = self.tanh(x)
        else:
            y = x
        
        ax2.plot(x, y, linewidth=3, color='blue')
        ax2.grid(True, alpha=0.3)
        ax2.set_xlabel('è¾“å…¥ (x)', fontsize=12)
        ax2.set_ylabel('è¾“å‡º f(x)', fontsize=12)
        ax2.set_title(f'{self.activation.upper()} æ¿€æ´»å‡½æ•°', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.show()
        
        return fig

# æ¼”ç¤ºä¸åŒæ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒ
print("=" * 60)
print("ğŸ§  ç¥ç»å…ƒæ¨¡å‹æ¼”ç¤º")
print("=" * 60)

# åˆ›å»ºä¸åŒæ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒ
activations = ['sigmoid', 'relu', 'tanh']
for activation in activations:
    print(f"\nğŸ“Š {activation.upper()} ç¥ç»å…ƒ:")
    neuron = SimpleNeuron(num_inputs=3, activation=activation)
    
    # æµ‹è¯•è¾“å…¥
    test_input = np.array([0.5, -0.3, 0.8])
    output = neuron.forward(test_input)
    
    print(f"   æƒé‡: {neuron.weights}")
    print(f"   åç½®: {neuron.bias:.4f}")
    print(f"   è¾“å…¥: {test_input}")
    print(f"   è¾“å‡º: {output:.4f}")
    
    # å¯è§†åŒ–ç¬¬ä¸€ä¸ªç¥ç»å…ƒ
    if activation == 'sigmoid':
        neuron.visualize_neuron()
```

### 2.2 æ¿€æ´»å‡½æ•°è¯¦è§£

æ¿€æ´»å‡½æ•°ä¸ºç¥ç»ç½‘ç»œå¼•å…¥éçº¿æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ¨¡å¼ã€‚

```python
class ActivationFunctions:
    """æ¿€æ´»å‡½æ•°é›†åˆ"""
    
    @staticmethod
    def sigmoid(x):
        """Sigmoidå‡½æ•°: f(x) = 1/(1+e^(-x))"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    @staticmethod
    def sigmoid_derivative(x):
        """Sigmoidå¯¼æ•°"""
        s = ActivationFunctions.sigmoid(x)
        return s * (1 - s)
    
    @staticmethod
    def relu(x):
        """ReLUå‡½æ•°: f(x) = max(0, x)"""
        return np.maximum(0, x)
    
    @staticmethod
    def relu_derivative(x):
        """ReLUå¯¼æ•°"""
        return (x > 0).astype(float)
    
    @staticmethod
    def tanh(x):
        """Tanhå‡½æ•°: f(x) = tanh(x)"""
        return np.tanh(x)
    
    @staticmethod
    def tanh_derivative(x):
        """Tanhå¯¼æ•°"""
        return 1 - np.tanh(x) ** 2
    
    @staticmethod
    def leaky_relu(x, alpha=0.01):
        """Leaky ReLUå‡½æ•°"""
        return np.where(x > 0, x, alpha * x)
    
    @staticmethod
    def leaky_relu_derivative(x, alpha=0.01):
        """Leaky ReLUå¯¼æ•°"""
        return np.where(x > 0, 1, alpha)
    
    @staticmethod
    def softmax(x):
        """Softmaxå‡½æ•°ï¼ˆç”¨äºå¤šåˆ†ç±»ï¼‰"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def compare_activations(self):
        """æ¯”è¾ƒä¸åŒæ¿€æ´»å‡½æ•°"""
        x = np.linspace(-5, 5, 100)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        functions = [
            ('Sigmoid', self.sigmoid, self.sigmoid_derivative),
            ('ReLU', self.relu, self.relu_derivative),
            ('Tanh', self.tanh, self.tanh_derivative),
            ('Leaky ReLU', self.leaky_relu, self.leaky_relu_derivative),
        ]
        
        for i, (name, func, deriv) in enumerate(functions):
            if i < len(axes):
                y = func(x)
                dy = deriv(x)
                
                axes[i].plot(x, y, label=f'{name}', linewidth=3, color='blue')
                axes[i].plot(x, dy, label=f'{name} å¯¼æ•°', linewidth=2, 
                           color='red', linestyle='--')
                
                axes[i].grid(True, alpha=0.3)
                axes[i].legend()
                axes[i].set_title(f'{name} æ¿€æ´»å‡½æ•°', fontweight='bold')
                axes[i].set_xlabel('x')
                axes[i].set_ylabel('f(x)')
        
        # Softmaxç‰¹æ®Šå¤„ç†ï¼ˆå¤šç»´è¾“å…¥ï¼‰
        x_soft = np.array([[1, 2, 3], [1, 5, 1], [3, 1, 2]])
        y_soft = self.softmax(x_soft)
        
        axes[4].bar(range(len(x_soft)), x_soft.flatten(), alpha=0.7, label='è¾“å…¥')
        axes[4].bar(range(len(y_soft)), y_soft.flatten(), alpha=0.7, label='Softmaxè¾“å‡º')
        axes[4].set_title('Softmax ç¤ºä¾‹', fontweight='bold')
        axes[4].legend()
        
        # éšè—æœ€åä¸€ä¸ªå­å›¾
        axes[5].axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def activation_properties(self):
        """æ¿€æ´»å‡½æ•°ç‰¹æ€§åˆ†æ"""
        properties = {
            'Sigmoid': {
                'èŒƒå›´': '(0, 1)',
                'ä¼˜ç‚¹': 'å¹³æ»‘ã€å¯å¾®ã€è¾“å‡ºæœ‰ç•Œ',
                'ç¼ºç‚¹': 'æ¢¯åº¦æ¶ˆå¤±ã€è®¡ç®—å¤æ‚',
                'é€‚ç”¨': 'äºŒåˆ†ç±»è¾“å‡ºå±‚'
            },
            'ReLU': {
                'èŒƒå›´': '[0, +âˆ)',
                'ä¼˜ç‚¹': 'è®¡ç®—ç®€å•ã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤±',
                'ç¼ºç‚¹': 'ç¥ç»å…ƒæ­»äº¡ã€è¾“å‡ºæ— ä¸Šç•Œ',
                'é€‚ç”¨': 'éšè—å±‚ï¼ˆæœ€å¸¸ç”¨ï¼‰'
            },
            'Tanh': {
                'èŒƒå›´': '(-1, 1)',
                'ä¼˜ç‚¹': 'é›¶ä¸­å¿ƒã€å¹³æ»‘å¯å¾®',
                'ç¼ºç‚¹': 'æ¢¯åº¦æ¶ˆå¤±',
                'é€‚ç”¨': 'éšè—å±‚ï¼ˆRNNä¸­å¸¸ç”¨ï¼‰'
            },
            'Leaky ReLU': {
                'èŒƒå›´': '(-âˆ, +âˆ)',
                'ä¼˜ç‚¹': 'é¿å…ç¥ç»å…ƒæ­»äº¡',
                'ç¼ºç‚¹': 'éœ€è¦è°ƒæ•´Î±å‚æ•°',
                'é€‚ç”¨': 'æ·±å±‚ç½‘ç»œéšè—å±‚'
            },
            'Softmax': {
                'èŒƒå›´': '(0, 1)ï¼Œå’Œä¸º1',
                'ä¼˜ç‚¹': 'æ¦‚ç‡è§£é‡Šã€å¤šåˆ†ç±»',
                'ç¼ºç‚¹': 'è®¡ç®—å¤æ‚',
                'é€‚ç”¨': 'å¤šåˆ†ç±»è¾“å‡ºå±‚'
            }
        }
        
        print("\n" + "=" * 80)
        print("ğŸ“Š æ¿€æ´»å‡½æ•°ç‰¹æ€§å¯¹æ¯”")
        print("=" * 80)
        
        for name, props in properties.items():
            print(f"\nğŸ”¸ {name}:")
            for key, value in props.items():
                print(f"   {key}: {value}")
        
        return properties

# æ¿€æ´»å‡½æ•°æ¼”ç¤º
activation_demo = ActivationFunctions()
activation_demo.compare_activations()
properties = activation_demo.activation_properties()
```

## 3. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

### 3.1 ç½‘ç»œç»“æ„

å¤šå±‚æ„ŸçŸ¥æœºæ˜¯æœ€åŸºæœ¬çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”±è¾“å…¥å±‚ã€ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆã€‚

```python
class MultiLayerPerceptron:
    """å¤šå±‚æ„ŸçŸ¥æœºå®ç°"""
    
    def __init__(self, layer_sizes, activations=None, learning_rate=0.01):
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)
        self.learning_rate = learning_rate
        
        # é»˜è®¤æ¿€æ´»å‡½æ•°
        if activations is None:
            activations = ['relu'] * (self.num_layers - 2) + ['sigmoid']
        self.activations = activations
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weights = []
        self.biases = []
        
        for i in range(self.num_layers - 1):
            # Xavieråˆå§‹åŒ–
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros((1, layer_sizes[i+1]))
            
            self.weights.append(w)
            self.biases.append(b)
        
        # å­˜å‚¨å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ
        self.activations_cache = []
        self.z_cache = []
    
    def activate(self, z, activation_type):
        """æ¿€æ´»å‡½æ•°"""
        if activation_type == 'sigmoid':
            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
        elif activation_type == 'relu':
            return np.maximum(0, z)
        elif activation_type == 'tanh':
            return np.tanh(z)
        elif activation_type == 'softmax':
            exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
            return exp_z / np.sum(exp_z, axis=1, keepdims=True)
        else:
            return z  # çº¿æ€§æ¿€æ´»
    
    def activate_derivative(self, z, activation_type):
        """æ¿€æ´»å‡½æ•°å¯¼æ•°"""
        if activation_type == 'sigmoid':
            s = self.activate(z, 'sigmoid')
            return s * (1 - s)
        elif activation_type == 'relu':
            return (z > 0).astype(float)
        elif activation_type == 'tanh':
            return 1 - np.tanh(z) ** 2
        else:
            return np.ones_like(z)  # çº¿æ€§æ¿€æ´»å¯¼æ•°ä¸º1
    
    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.activations_cache = [X]
        self.z_cache = []
        
        current_input = X
        
        for i in range(self.num_layers - 1):
            # çº¿æ€§å˜æ¢
            z = np.dot(current_input, self.weights[i]) + self.biases[i]
            self.z_cache.append(z)
            
            # æ¿€æ´»å‡½æ•°
            a = self.activate(z, self.activations[i])
            self.activations_cache.append(a)
            
            current_input = a
        
        return current_input
    
    def backward(self, X, y, output):
        """åå‘ä¼ æ’­"""
        m = X.shape[0]  # æ ·æœ¬æ•°é‡
        
        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        if self.activations[-1] == 'softmax':
            # å¯¹äºsoftmax + äº¤å‰ç†µï¼Œæ¢¯åº¦ç®€åŒ–ä¸º (y_pred - y_true)
            dz = output - y
        else:
            # å¯¹äºå…¶ä»–æ¿€æ´»å‡½æ•°
            dz = (output - y) * self.activate_derivative(self.z_cache[-1], self.activations[-1])
        
        # å­˜å‚¨æ¢¯åº¦
        dw_list = []
        db_list = []
        
        # ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­
        for i in range(self.num_layers - 2, -1, -1):
            # è®¡ç®—æƒé‡å’Œåç½®æ¢¯åº¦
            dw = np.dot(self.activations_cache[i].T, dz) / m
            db = np.mean(dz, axis=0, keepdims=True)
            
            dw_list.insert(0, dw)
            db_list.insert(0, db)
            
            # è®¡ç®—å‰ä¸€å±‚çš„è¯¯å·®ï¼ˆå¦‚æœä¸æ˜¯è¾“å…¥å±‚ï¼‰
            if i > 0:
                dz = np.dot(dz, self.weights[i].T) * self.activate_derivative(
                    self.z_cache[i-1], self.activations[i-1])
        
        return dw_list, db_list
    
    def update_parameters(self, dw_list, db_list):
        """æ›´æ–°å‚æ•°"""
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * dw_list[i]
            self.biases[i] -= self.learning_rate * db_list[i]
    
    def train(self, X, y, epochs=1000, verbose=True):
        """è®­ç»ƒç½‘ç»œ"""
        losses = []
        
        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            output = self.forward(X)
            
            # è®¡ç®—æŸå¤±
            if self.activations[-1] == 'softmax':
                # äº¤å‰ç†µæŸå¤±
                loss = -np.mean(np.sum(y * np.log(output + 1e-8), axis=1))
            else:
                # å‡æ–¹è¯¯å·®æŸå¤±
                loss = np.mean((output - y) ** 2)
            
            losses.append(loss)
            
            # åå‘ä¼ æ’­
            dw_list, db_list = self.backward(X, y, output)
            
            # æ›´æ–°å‚æ•°
            self.update_parameters(dw_list, db_list)
            
            # æ‰“å°è¿›åº¦
            if verbose and (epoch + 1) % (epochs // 10) == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}")
        
        return losses
    
    def predict(self, X):
        """é¢„æµ‹"""
        return self.forward(X)
    
    def visualize_network(self):
        """å¯è§†åŒ–ç½‘ç»œç»“æ„"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # è®¡ç®—å±‚çš„ä½ç½®
        max_neurons = max(self.layer_sizes)
        layer_positions = np.linspace(0, 10, self.num_layers)
        
        # ç»˜åˆ¶æ¯ä¸€å±‚
        for layer_idx, num_neurons in enumerate(self.layer_sizes):
            x = layer_positions[layer_idx]
            
            # è®¡ç®—ç¥ç»å…ƒçš„yåæ ‡
            if num_neurons == 1:
                y_positions = [max_neurons / 2]
            else:
                y_positions = np.linspace(0, max_neurons, num_neurons)
            
            # ç»˜åˆ¶ç¥ç»å…ƒ
            for neuron_idx, y in enumerate(y_positions):
                if layer_idx == 0:
                    color = 'lightblue'  # è¾“å…¥å±‚
                    label = f'x{neuron_idx+1}'
                elif layer_idx == self.num_layers - 1:
                    color = 'lightgreen'  # è¾“å‡ºå±‚
                    label = f'y{neuron_idx+1}'
                else:
                    color = 'lightyellow'  # éšè—å±‚
                    label = f'h{neuron_idx+1}'
                
                circle = plt.Circle((x, y), 0.3, color=color, ec='black')
                ax.add_patch(circle)
                ax.text(x, y, label, ha='center', va='center', fontweight='bold')
                
                # ç»˜åˆ¶è¿æ¥çº¿ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
                if layer_idx < self.num_layers - 1:
                    next_layer_size = self.layer_sizes[layer_idx + 1]
                    next_x = layer_positions[layer_idx + 1]
                    
                    if next_layer_size == 1:
                        next_y_positions = [max_neurons / 2]
                    else:
                        next_y_positions = np.linspace(0, max_neurons, next_layer_size)
                    
                    for next_y in next_y_positions:
                        ax.plot([x + 0.3, next_x - 0.3], [y, next_y], 
                               'k-', alpha=0.3, linewidth=1)
        
        # æ·»åŠ å±‚æ ‡ç­¾
        layer_labels = ['è¾“å…¥å±‚'] + [f'éšè—å±‚{i}' for i in range(1, self.num_layers-1)] + ['è¾“å‡ºå±‚']
        for i, (x, label) in enumerate(zip(layer_positions, layer_labels)):
            ax.text(x, -1, label, ha='center', va='center', 
                   fontweight='bold', fontsize=12)
            ax.text(x, -1.5, f'({self.layer_sizes[i]})', ha='center', va='center', 
                   fontsize=10, style='italic')
        
        ax.set_xlim(-1, 11)
        ax.set_ylim(-2, max_neurons + 1)
        ax.set_aspect('equal')
        ax.axis('off')
        ax.set_title('å¤šå±‚æ„ŸçŸ¥æœºç½‘ç»œç»“æ„', fontsize=16, fontweight='bold')
        
        plt.tight_layout()
        plt.show()
        
        return fig

# MLPæ¼”ç¤º
print("\n" + "=" * 60)
print("ğŸ§  å¤šå±‚æ„ŸçŸ¥æœºæ¼”ç¤º")
print("=" * 60)

# åˆ›å»ºç®€å•çš„åˆ†ç±»æ•°æ®é›†
np.random.seed(42)
X = np.random.randn(200, 2)
y = ((X[:, 0] ** 2 + X[:, 1] ** 2) > 1).astype(int)
y_onehot = np.eye(2)[y]  # è½¬æ¢ä¸ºone-hotç¼–ç 

print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
print(f"   æ ·æœ¬æ•°é‡: {X.shape[0]}")
print(f"   ç‰¹å¾ç»´åº¦: {X.shape[1]}")
print(f"   ç±»åˆ«æ•°é‡: {len(np.unique(y))}")

# åˆ›å»ºMLP
mlp = MultiLayerPerceptron(
    layer_sizes=[2, 8, 4, 2],  # 2è¾“å…¥ -> 8éšè— -> 4éšè— -> 2è¾“å‡º
    activations=['relu', 'relu', 'softmax'],
    learning_rate=0.1
)

# å¯è§†åŒ–ç½‘ç»œç»“æ„
mlp.visualize_network()

# è®­ç»ƒç½‘ç»œ
print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
losses = mlp.train(X, y_onehot, epochs=1000, verbose=True)

# ç»˜åˆ¶æŸå¤±æ›²çº¿
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# æŸå¤±æ›²çº¿
ax1.plot(losses, linewidth=2, color='red')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.set_title('è®­ç»ƒæŸå¤±æ›²çº¿')
ax1.grid(True, alpha=0.3)

# å†³ç­–è¾¹ç•Œå¯è§†åŒ–
h = 0.02
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

mesh_points = np.c_[xx.ravel(), yy.ravel()]
predictions = mlp.predict(mesh_points)
Z = np.argmax(predictions, axis=1)
Z = Z.reshape(xx.shape)

ax2.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
scatter = ax2.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')
ax2.set_xlabel('ç‰¹å¾1')
ax2.set_ylabel('ç‰¹å¾2')
ax2.set_title('MLPå†³ç­–è¾¹ç•Œ')

plt.tight_layout()
plt.show()

# è®¡ç®—å‡†ç¡®ç‡
train_pred = mlp.predict(X)
train_accuracy = np.mean(np.argmax(train_pred, axis=1) == y)
print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
print(f"   æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}")
print(f"   è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
```