# 1.3.4 å…³è”è§„åˆ™æŒ–æ˜ (Association Rule Mining)

## 1. å…³è”è§„åˆ™æŒ–æ˜æ¦‚è¿°

å…³è”è§„åˆ™æŒ–æ˜æ˜¯æ•°æ®æŒ–æ˜ä¸­çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œæ—¨åœ¨å‘ç°æ•°æ®é›†ä¸­é¡¹ç›®ä¹‹é—´çš„æœ‰è¶£å…³ç³»ã€‚å®ƒæœ€åˆåº”ç”¨äºå¸‚åœºç¯®å­åˆ†æï¼Œç”¨äºå‘ç°é¡¾å®¢è´­ä¹°è¡Œä¸ºä¸­çš„æ¨¡å¼ï¼Œå¦‚"è´­ä¹°é¢åŒ…çš„é¡¾å®¢å¾€å¾€ä¹Ÿä¼šè´­ä¹°ç‰›å¥¶"ã€‚

### 1.1 åŸºæœ¬æ¦‚å¿µ

```mermaid
graph TD
    A[äº‹åŠ¡æ•°æ®åº“] --> B[é¢‘ç¹é¡¹é›†æŒ–æ˜]
    B --> C[å…³è”è§„åˆ™ç”Ÿæˆ]
    C --> D[è§„åˆ™è¯„ä¼°]
    D --> E[æœ‰è¶£è§„åˆ™]
    
    F[æ”¯æŒåº¦] --> D
    G[ç½®ä¿¡åº¦] --> D
    H[æå‡åº¦] --> D
```

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- **é¡¹ç›®(Item)**: æ•°æ®ä¸­çš„åŸºæœ¬å…ƒç´ ï¼Œå¦‚å•†å“ã€åŸºå› ã€ç½‘é¡µç­‰
- **äº‹åŠ¡(Transaction)**: é¡¹ç›®çš„é›†åˆï¼Œå¦‚ä¸€æ¬¡è´­ç‰©æ¸…å•
- **é¡¹é›†(Itemset)**: é¡¹ç›®çš„å­é›†
- **é¢‘ç¹é¡¹é›†(Frequent Itemset)**: æ”¯æŒåº¦ä¸å°äºæœ€å°æ”¯æŒåº¦é˜ˆå€¼çš„é¡¹é›†
- **å…³è”è§„åˆ™(Association Rule)**: å½¢å¦‚ A â†’ B çš„è•´å«å…³ç³»

### 1.2 è¯„ä¼°æŒ‡æ ‡

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from itertools import combinations
from collections import defaultdict, Counter
import seaborn as sns

class AssociationMetrics:
    """å…³è”è§„åˆ™è¯„ä¼°æŒ‡æ ‡"""
    
    def __init__(self):
        self.metrics_info = {
            'support': 'æ”¯æŒåº¦ - é¡¹é›†åœ¨æ‰€æœ‰äº‹åŠ¡ä¸­å‡ºç°çš„é¢‘ç‡',
            'confidence': 'ç½®ä¿¡åº¦ - åœ¨å‰ä»¶å‡ºç°çš„æ¡ä»¶ä¸‹åä»¶å‡ºç°çš„æ¦‚ç‡',
            'lift': 'æå‡åº¦ - è§„åˆ™çš„ç½®ä¿¡åº¦ä¸åä»¶æ”¯æŒåº¦çš„æ¯”å€¼',
            'conviction': 'ç¡®ä¿¡åº¦ - è¡¡é‡è§„åˆ™çš„å¯é æ€§',
            'leverage': 'æ æ†å€¼ - è§‚å¯Ÿé¢‘ç‡ä¸æœŸæœ›é¢‘ç‡çš„å·®å€¼',
            'jaccard': 'Jaccardç³»æ•° - é¡¹é›†äº¤é›†ä¸å¹¶é›†çš„æ¯”å€¼'
        }
    
    def calculate_support(self, itemset, transactions):
        """è®¡ç®—æ”¯æŒåº¦"""
        count = sum(1 for transaction in transactions if itemset.issubset(set(transaction)))
        return count / len(transactions)
    
    def calculate_confidence(self, antecedent, consequent, transactions):
        """è®¡ç®—ç½®ä¿¡åº¦"""
        antecedent_support = self.calculate_support(antecedent, transactions)
        if antecedent_support == 0:
            return 0
        
        rule_support = self.calculate_support(antecedent.union(consequent), transactions)
        return rule_support / antecedent_support
    
    def calculate_lift(self, antecedent, consequent, transactions):
        """è®¡ç®—æå‡åº¦"""
        confidence = self.calculate_confidence(antecedent, consequent, transactions)
        consequent_support = self.calculate_support(consequent, transactions)
        
        if consequent_support == 0:
            return 0
        
        return confidence / consequent_support
    
    def calculate_conviction(self, antecedent, consequent, transactions):
        """è®¡ç®—ç¡®ä¿¡åº¦"""
        confidence = self.calculate_confidence(antecedent, consequent, transactions)
        if confidence == 1:
            return float('inf')
        
        consequent_support = self.calculate_support(consequent, transactions)
        return (1 - consequent_support) / (1 - confidence) if confidence != 1 else float('inf')
    
    def calculate_leverage(self, antecedent, consequent, transactions):
        """è®¡ç®—æ æ†å€¼"""
        rule_support = self.calculate_support(antecedent.union(consequent), transactions)
        antecedent_support = self.calculate_support(antecedent, transactions)
        consequent_support = self.calculate_support(consequent, transactions)
        
        expected_support = antecedent_support * consequent_support
        return rule_support - expected_support
    
    def calculate_jaccard(self, antecedent, consequent, transactions):
        """è®¡ç®—Jaccardç³»æ•°"""
        union_support = self.calculate_support(antecedent.union(consequent), transactions)
        antecedent_support = self.calculate_support(antecedent, transactions)
        consequent_support = self.calculate_support(consequent, transactions)
        
        denominator = antecedent_support + consequent_support - union_support
        return union_support / denominator if denominator > 0 else 0
    
    def evaluate_rule(self, antecedent, consequent, transactions):
        """å…¨é¢è¯„ä¼°å…³è”è§„åˆ™"""
        metrics = {
            'support': self.calculate_support(antecedent.union(consequent), transactions),
            'confidence': self.calculate_confidence(antecedent, consequent, transactions),
            'lift': self.calculate_lift(antecedent, consequent, transactions),
            'conviction': self.calculate_conviction(antecedent, consequent, transactions),
            'leverage': self.calculate_leverage(antecedent, consequent, transactions),
            'jaccard': self.calculate_jaccard(antecedent, consequent, transactions)
        }
        
        return metrics
    
    def explain_metrics(self):
        """è§£é‡Šå„ç§è¯„ä¼°æŒ‡æ ‡"""
        print("\n=== å…³è”è§„åˆ™è¯„ä¼°æŒ‡æ ‡è¯´æ˜ ===")
        for metric, description in self.metrics_info.items():
            print(f"{metric:12s}: {description}")
        
        print("\n=== æŒ‡æ ‡è§£é‡Š ===")
        print("æ”¯æŒåº¦ (Support):")
        print("  å…¬å¼: Support(Aâ†’B) = P(AâˆªB) = |AâˆªB| / |D|")
        print("  å«ä¹‰: è§„åˆ™åœ¨æ‰€æœ‰äº‹åŠ¡ä¸­å‡ºç°çš„é¢‘ç‡")
        print("  èŒƒå›´: [0, 1]ï¼Œè¶Šé«˜è¡¨ç¤ºè§„åˆ™è¶Šæ™®é")
        
        print("\nç½®ä¿¡åº¦ (Confidence):")
        print("  å…¬å¼: Confidence(Aâ†’B) = P(B|A) = Support(AâˆªB) / Support(A)")
        print("  å«ä¹‰: åœ¨Aå‡ºç°çš„æ¡ä»¶ä¸‹Bå‡ºç°çš„æ¦‚ç‡")
        print("  èŒƒå›´: [0, 1]ï¼Œè¶Šé«˜è¡¨ç¤ºè§„åˆ™è¶Šå¯é ")
        
        print("\næå‡åº¦ (Lift):")
        print("  å…¬å¼: Lift(Aâ†’B) = Confidence(Aâ†’B) / Support(B)")
        print("  å«ä¹‰: è§„åˆ™çš„ç½®ä¿¡åº¦ç›¸å¯¹äºBçš„æ”¯æŒåº¦çš„æå‡")
        print("  èŒƒå›´: [0, +âˆ)ï¼Œ>1è¡¨ç¤ºæ­£ç›¸å…³ï¼Œ<1è¡¨ç¤ºè´Ÿç›¸å…³ï¼Œ=1è¡¨ç¤ºç‹¬ç«‹")

# æ¼”ç¤ºè¯„ä¼°æŒ‡æ ‡
metrics_demo = AssociationMetrics()
metrics_demo.explain_metrics()

# ç¤ºä¾‹æ•°æ®
sample_transactions = [
    ['é¢åŒ…', 'ç‰›å¥¶'],
    ['é¢åŒ…', 'å°¿å¸ƒ', 'å•¤é…’', 'é¸¡è›‹'],
    ['ç‰›å¥¶', 'å°¿å¸ƒ', 'å•¤é…’', 'å¯ä¹'],
    ['é¢åŒ…', 'ç‰›å¥¶', 'å°¿å¸ƒ', 'å•¤é…’'],
    ['é¢åŒ…', 'ç‰›å¥¶', 'å°¿å¸ƒ', 'å¯ä¹']
]

# è®¡ç®—ç¤ºä¾‹è§„åˆ™çš„æŒ‡æ ‡
antecedent = {'é¢åŒ…'}
consequent = {'ç‰›å¥¶'}
rule_metrics = metrics_demo.evaluate_rule(antecedent, consequent, sample_transactions)

print(f"\n=== è§„åˆ™ {antecedent} â†’ {consequent} çš„è¯„ä¼°ç»“æœ ===")
for metric, value in rule_metrics.items():
    print(f"{metric:12s}: {value:.4f}")
```

## 2. Aprioriç®—æ³•

### 2.1 ç®—æ³•åŸç†

Aprioriç®—æ³•æ˜¯æœ€ç»å…¸çš„å…³è”è§„åˆ™æŒ–æ˜ç®—æ³•ï¼ŒåŸºäº"é¢‘ç¹é¡¹é›†çš„å­é›†ä¹Ÿå¿…é¡»æ˜¯é¢‘ç¹çš„"è¿™ä¸€å…ˆéªŒçŸ¥è¯†ã€‚

```python
class AprioriAlgorithm:
    """Aprioriç®—æ³•å®ç°"""
    
    def __init__(self, min_support=0.2, min_confidence=0.6):
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.frequent_itemsets = {}
        self.rules = []
    
    def get_items(self, transactions):
        """è·å–æ‰€æœ‰å•é¡¹é›†"""
        items = set()
        for transaction in transactions:
            for item in transaction:
                items.add(frozenset([item]))
        return items
    
    def get_support(self, itemset, transactions):
        """è®¡ç®—é¡¹é›†æ”¯æŒåº¦"""
        count = 0
        for transaction in transactions:
            if itemset.issubset(set(transaction)):
                count += 1
        return count / len(transactions)
    
    def filter_candidates(self, candidates, transactions):
        """è¿‡æ»¤å€™é€‰é¡¹é›†ï¼Œä¿ç•™é¢‘ç¹é¡¹é›†"""
        frequent = {}
        for candidate in candidates:
            support = self.get_support(candidate, transactions)
            if support >= self.min_support:
                frequent[candidate] = support
        return frequent
    
    def generate_candidates(self, frequent_itemsets, k):
        """ç”Ÿæˆké¡¹å€™é€‰é›†"""
        candidates = set()
        frequent_list = list(frequent_itemsets.keys())
        
        for i in range(len(frequent_list)):
            for j in range(i + 1, len(frequent_list)):
                # åˆå¹¶ä¸¤ä¸ª(k-1)é¡¹é›†
                union = frequent_list[i].union(frequent_list[j])
                if len(union) == k:
                    # æ£€æŸ¥æ‰€æœ‰(k-1)å­é›†æ˜¯å¦éƒ½æ˜¯é¢‘ç¹çš„
                    if self.has_frequent_subsets(union, frequent_itemsets):
                        candidates.add(union)
        
        return candidates
    
    def has_frequent_subsets(self, itemset, frequent_itemsets):
        """æ£€æŸ¥é¡¹é›†çš„æ‰€æœ‰å­é›†æ˜¯å¦éƒ½æ˜¯é¢‘ç¹çš„"""
        k = len(itemset)
        for item in itemset:
            subset = itemset - frozenset([item])
            if subset not in frequent_itemsets:
                return False
        return True
    
    def find_frequent_itemsets(self, transactions):
        """æŒ–æ˜é¢‘ç¹é¡¹é›†"""
        print(f"\n=== Aprioriç®—æ³•æŒ–æ˜é¢‘ç¹é¡¹é›† ===")
        print(f"æœ€å°æ”¯æŒåº¦: {self.min_support}")
        print(f"äº‹åŠ¡æ•°é‡: {len(transactions)}")
        
        # 1-é¡¹é›†
        candidates_1 = self.get_items(transactions)
        frequent_1 = self.filter_candidates(candidates_1, transactions)
        
        if not frequent_1:
            print("æ²¡æœ‰æ‰¾åˆ°é¢‘ç¹1-é¡¹é›†")
            return {}
        
        self.frequent_itemsets[1] = frequent_1
        print(f"\né¢‘ç¹1-é¡¹é›†æ•°é‡: {len(frequent_1)}")
        
        k = 2
        while True:
            # ç”Ÿæˆå€™é€‰k-é¡¹é›†
            candidates_k = self.generate_candidates(self.frequent_itemsets[k-1], k)
            
            if not candidates_k:
                break
            
            # è¿‡æ»¤å¾—åˆ°é¢‘ç¹k-é¡¹é›†
            frequent_k = self.filter_candidates(candidates_k, transactions)
            
            if not frequent_k:
                break
            
            self.frequent_itemsets[k] = frequent_k
            print(f"é¢‘ç¹{k}-é¡¹é›†æ•°é‡: {len(frequent_k)}")
            
            k += 1
        
        # ç»Ÿè®¡æ€»æ•°
        total_frequent = sum(len(itemsets) for itemsets in self.frequent_itemsets.values())
        print(f"\næ€»é¢‘ç¹é¡¹é›†æ•°é‡: {total_frequent}")
        
        return self.frequent_itemsets
    
    def generate_rules(self, transactions):
        """ç”Ÿæˆå…³è”è§„åˆ™"""
        if not self.frequent_itemsets:
            self.find_frequent_itemsets(transactions)
        
        print(f"\n=== ç”Ÿæˆå…³è”è§„åˆ™ ===")
        print(f"æœ€å°ç½®ä¿¡åº¦: {self.min_confidence}")
        
        self.rules = []
        
        # ä»2-é¡¹é›†å¼€å§‹ç”Ÿæˆè§„åˆ™
        for k in range(2, max(self.frequent_itemsets.keys()) + 1):
            for itemset, support in self.frequent_itemsets[k].items():
                # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è§„åˆ™
                for i in range(1, len(itemset)):
                    for antecedent in combinations(itemset, i):
                        antecedent = frozenset(antecedent)
                        consequent = itemset - antecedent
                        
                        # è®¡ç®—ç½®ä¿¡åº¦
                        antecedent_support = self.get_support(antecedent, transactions)
                        confidence = support / antecedent_support if antecedent_support > 0 else 0
                        
                        if confidence >= self.min_confidence:
                            # è®¡ç®—å…¶ä»–æŒ‡æ ‡
                            consequent_support = self.get_support(consequent, transactions)
                            lift = confidence / consequent_support if consequent_support > 0 else 0
                            
                            rule = {
                                'antecedent': antecedent,
                                'consequent': consequent,
                                'support': support,
                                'confidence': confidence,
                                'lift': lift
                            }
                            
                            self.rules.append(rule)
        
        print(f"ç”Ÿæˆè§„åˆ™æ•°é‡: {len(self.rules)}")
        return self.rules
    
    def print_frequent_itemsets(self, max_items=10):
        """æ‰“å°é¢‘ç¹é¡¹é›†"""
        print("\n=== é¢‘ç¹é¡¹é›†è¯¦æƒ… ===")
        
        for k, itemsets in self.frequent_itemsets.items():
            print(f"\n{k}-é¡¹é›† (å…±{len(itemsets)}ä¸ª):")
            
            # æŒ‰æ”¯æŒåº¦æ’åº
            sorted_itemsets = sorted(itemsets.items(), key=lambda x: x[1], reverse=True)
            
            for i, (itemset, support) in enumerate(sorted_itemsets[:max_items]):
                items_str = ', '.join(sorted(list(itemset)))
                print(f"  {i+1:2d}. {{{items_str}}} : {support:.4f}")
            
            if len(sorted_itemsets) > max_items:
                print(f"  ... è¿˜æœ‰ {len(sorted_itemsets) - max_items} ä¸ªé¡¹é›†")
    
    def print_rules(self, max_rules=20, sort_by='lift'):
        """æ‰“å°å…³è”è§„åˆ™"""
        if not self.rules:
            print("æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„å…³è”è§„åˆ™")
            return
        
        print(f"\n=== å…³è”è§„åˆ™ (æŒ‰{sort_by}æ’åº) ===")
        
        # æ’åºè§„åˆ™
        sorted_rules = sorted(self.rules, key=lambda x: x[sort_by], reverse=True)
        
        print(f"{'åºå·':>4} {'å‰ä»¶':>15} {'åä»¶':>15} {'æ”¯æŒåº¦':>8} {'ç½®ä¿¡åº¦':>8} {'æå‡åº¦':>8}")
        print("-" * 70)
        
        for i, rule in enumerate(sorted_rules[:max_rules]):
            antecedent_str = ', '.join(sorted(list(rule['antecedent'])))
            consequent_str = ', '.join(sorted(list(rule['consequent'])))
            
            print(f"{i+1:4d} {antecedent_str:>15} {consequent_str:>15} "
                  f"{rule['support']:8.4f} {rule['confidence']:8.4f} {rule['lift']:8.4f}")
        
        if len(sorted_rules) > max_rules:
            print(f"\n... è¿˜æœ‰ {len(sorted_rules) - max_rules} æ¡è§„åˆ™")
    
    def analyze_rules(self):
        """åˆ†æè§„åˆ™ç»Ÿè®¡ä¿¡æ¯"""
        if not self.rules:
            return
        
        print("\n=== è§„åˆ™ç»Ÿè®¡åˆ†æ ===")
        
        supports = [rule['support'] for rule in self.rules]
        confidences = [rule['confidence'] for rule in self.rules]
        lifts = [rule['lift'] for rule in self.rules]
        
        print(f"è§„åˆ™æ•°é‡: {len(self.rules)}")
        print(f"\næ”¯æŒåº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(supports):.4f}")
        print(f"  æ ‡å‡†å·®: {np.std(supports):.4f}")
        print(f"  æœ€å°å€¼: {np.min(supports):.4f}")
        print(f"  æœ€å¤§å€¼: {np.max(supports):.4f}")
        
        print(f"\nç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(confidences):.4f}")
        print(f"  æ ‡å‡†å·®: {np.std(confidences):.4f}")
        print(f"  æœ€å°å€¼: {np.min(confidences):.4f}")
        print(f"  æœ€å¤§å€¼: {np.max(confidences):.4f}")
        
        print(f"\næå‡åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {np.mean(lifts):.4f}")
        print(f"  æ ‡å‡†å·®: {np.std(lifts):.4f}")
        print(f"  æœ€å°å€¼: {np.min(lifts):.4f}")
        print(f"  æœ€å¤§å€¼: {np.max(lifts):.4f}")
        
        # æå‡åº¦åˆ†å¸ƒåˆ†æ
        lift_positive = sum(1 for lift in lifts if lift > 1)
        lift_negative = sum(1 for lift in lifts if lift < 1)
        lift_neutral = sum(1 for lift in lifts if lift == 1)
        
        print(f"\næå‡åº¦åˆ†å¸ƒ:")
        print(f"  æ­£ç›¸å…³ (lift > 1): {lift_positive} ({lift_positive/len(lifts)*100:.1f}%)")
        print(f"  è´Ÿç›¸å…³ (lift < 1): {lift_negative} ({lift_negative/len(lifts)*100:.1f}%)")
        print(f"  ç‹¬ç«‹ (lift = 1): {lift_neutral} ({lift_neutral/len(lifts)*100:.1f}%)")

# åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
def create_market_basket_data():
    """åˆ›å»ºå¸‚åœºç¯®å­æ•°æ®"""
    np.random.seed(42)
    
    # å•†å“åˆ—è¡¨
    items = ['é¢åŒ…', 'ç‰›å¥¶', 'é¸¡è›‹', 'é»„æ²¹', 'å¥¶é…ª', 'é…¸å¥¶', 'è‹¹æœ', 'é¦™è•‰', 
             'æ©™å­', 'åœŸè±†', 'æ´‹è‘±', 'èƒ¡èåœ', 'è¥¿çº¢æŸ¿', 'ç”Ÿèœ', 'é¸¡è‚‰', 
             'ç‰›è‚‰', 'é±¼', 'ç±³é¥­', 'é¢æ¡', 'å•¤é…’', 'çº¢é…’', 'å’–å•¡', 'èŒ¶']
    
    # å®šä¹‰å•†å“å…³è”æ€§ï¼ˆæŸäº›å•†å“æ›´å®¹æ˜“ä¸€èµ·è´­ä¹°ï¼‰
    associations = {
        'é¢åŒ…': ['é»„æ²¹', 'ç‰›å¥¶', 'é¸¡è›‹'],
        'ç‰›å¥¶': ['é¢åŒ…', 'é¸¡è›‹', 'å¥¶é…ª'],
        'è‹¹æœ': ['é¦™è•‰', 'æ©™å­'],
        'åœŸè±†': ['æ´‹è‘±', 'èƒ¡èåœ'],
        'é¸¡è‚‰': ['ç±³é¥­', 'åœŸè±†'],
        'å•¤é…’': ['é¸¡è‚‰', 'ç‰›è‚‰'],
        'å’–å•¡': ['ç‰›å¥¶', 'é¢åŒ…']
    }
    
    transactions = []
    
    for i in range(1000):
        transaction = []
        
        # éšæœºé€‰æ‹©1-8ä¸ªå•†å“
        n_items = np.random.randint(1, 9)
        
        # é¦–å…ˆéšæœºé€‰æ‹©ä¸€ä¸ªä¸»è¦å•†å“
        main_item = np.random.choice(items)
        transaction.append(main_item)
        
        # æ ¹æ®å…³è”æ€§æ·»åŠ ç›¸å…³å•†å“
        if main_item in associations:
            for related_item in associations[main_item]:
                if np.random.random() < 0.4:  # 40%æ¦‚ç‡æ·»åŠ ç›¸å…³å•†å“
                    transaction.append(related_item)
        
        # éšæœºæ·»åŠ å…¶ä»–å•†å“
        while len(transaction) < n_items:
            item = np.random.choice(items)
            if item not in transaction:
                transaction.append(item)
        
        transactions.append(transaction)
    
    return transactions

# æ‰§è¡ŒAprioriç®—æ³•æ¼”ç¤º
print("\n=== Aprioriç®—æ³•æ¼”ç¤º ===")

# ç”Ÿæˆæ•°æ®
transactions = create_market_basket_data()
print(f"ç”Ÿæˆäº† {len(transactions)} ä¸ªäº‹åŠ¡")
print(f"å‰5ä¸ªäº‹åŠ¡ç¤ºä¾‹:")
for i, transaction in enumerate(transactions[:5]):
    print(f"  äº‹åŠ¡{i+1}: {transaction}")

# è¿è¡ŒAprioriç®—æ³•
apriori = AprioriAlgorithm(min_support=0.05, min_confidence=0.3)
frequent_itemsets = apriori.find_frequent_itemsets(transactions)
rules = apriori.generate_rules(transactions)

# æ˜¾ç¤ºç»“æœ
apriori.print_frequent_itemsets(max_items=5)
apriori.print_rules(max_rules=15, sort_by='lift')
apriori.analyze_rules()
```

### 2.2 ç®—æ³•ä¼˜åŒ–

```python
class OptimizedApriori(AprioriAlgorithm):
    """ä¼˜åŒ–çš„Aprioriç®—æ³•"""
    
    def __init__(self, min_support=0.2, min_confidence=0.6, use_pruning=True):
        super().__init__(min_support, min_confidence)
        self.use_pruning = use_pruning
        self.transaction_matrix = None
    
    def create_transaction_matrix(self, transactions):
        """åˆ›å»ºäº‹åŠ¡çŸ©é˜µä»¥åŠ é€Ÿè®¡ç®—"""
        # è·å–æ‰€æœ‰å”¯ä¸€å•†å“
        all_items = set()
        for transaction in transactions:
            all_items.update(transaction)
        
        self.item_to_index = {item: i for i, item in enumerate(sorted(all_items))}
        self.index_to_item = {i: item for item, i in self.item_to_index.items()}
        
        # åˆ›å»ºäºŒè¿›åˆ¶çŸ©é˜µ
        n_transactions = len(transactions)
        n_items = len(all_items)
        
        self.transaction_matrix = np.zeros((n_transactions, n_items), dtype=bool)
        
        for i, transaction in enumerate(transactions):
            for item in transaction:
                j = self.item_to_index[item]
                self.transaction_matrix[i, j] = True
        
        print(f"åˆ›å»ºäº‹åŠ¡çŸ©é˜µ: {n_transactions} Ã— {n_items}")
    
    def get_support_fast(self, itemset_indices):
        """ä½¿ç”¨çŸ©é˜µå¿«é€Ÿè®¡ç®—æ”¯æŒåº¦"""
        if self.transaction_matrix is None:
            return 0
        
        # è®¡ç®—åŒ…å«æ‰€æœ‰é¡¹ç›®çš„äº‹åŠ¡æ•°
        mask = np.all(self.transaction_matrix[:, itemset_indices], axis=1)
        return np.sum(mask) / len(self.transaction_matrix)
    
    def find_frequent_itemsets_optimized(self, transactions):
        """ä¼˜åŒ–çš„é¢‘ç¹é¡¹é›†æŒ–æ˜"""
        print(f"\n=== ä¼˜åŒ–Aprioriç®—æ³• ===")
        print(f"ä½¿ç”¨å‰ªæ: {self.use_pruning}")
        
        # åˆ›å»ºäº‹åŠ¡çŸ©é˜µ
        self.create_transaction_matrix(transactions)
        
        # 1-é¡¹é›†
        frequent_1_indices = []
        for i in range(len(self.item_to_index)):
            support = self.get_support_fast([i])
            if support >= self.min_support:
                frequent_1_indices.append(i)
        
        if not frequent_1_indices:
            return {}
        
        self.frequent_itemsets = {}
        
        # è½¬æ¢ä¸ºfrozensetæ ¼å¼
        frequent_1 = {}
        for i in frequent_1_indices:
            item = self.index_to_item[i]
            itemset = frozenset([item])
            support = self.get_support_fast([i])
            frequent_1[itemset] = support
        
        self.frequent_itemsets[1] = frequent_1
        print(f"é¢‘ç¹1-é¡¹é›†æ•°é‡: {len(frequent_1)}")
        
        k = 2
        current_indices = [[i] for i in frequent_1_indices]
        
        while current_indices:
            # ç”Ÿæˆå€™é€‰k-é¡¹é›†
            candidate_indices = []
            
            for i in range(len(current_indices)):
                for j in range(i + 1, len(current_indices)):
                    # åˆå¹¶ä¸¤ä¸ª(k-1)é¡¹é›†
                    candidate = sorted(set(current_indices[i] + current_indices[j]))
                    
                    if len(candidate) == k:
                        # å‰ªæï¼šæ£€æŸ¥æ‰€æœ‰å­é›†æ˜¯å¦éƒ½æ˜¯é¢‘ç¹çš„
                        if not self.use_pruning or self.all_subsets_frequent(candidate, current_indices):
                            candidate_indices.append(candidate)
            
            if not candidate_indices:
                break
            
            # è¿‡æ»¤é¢‘ç¹é¡¹é›†
            frequent_k = {}
            next_indices = []
            
            for candidate in candidate_indices:
                support = self.get_support_fast(candidate)
                if support >= self.min_support:
                    # è½¬æ¢ä¸ºfrozenset
                    items = [self.index_to_item[i] for i in candidate]
                    itemset = frozenset(items)
                    frequent_k[itemset] = support
                    next_indices.append(candidate)
            
            if not frequent_k:
                break
            
            self.frequent_itemsets[k] = frequent_k
            print(f"é¢‘ç¹{k}-é¡¹é›†æ•°é‡: {len(frequent_k)}")
            
            current_indices = next_indices
            k += 1
        
        return self.frequent_itemsets
    
    def all_subsets_frequent(self, candidate, frequent_indices):
        """æ£€æŸ¥å€™é€‰é¡¹é›†çš„æ‰€æœ‰å­é›†æ˜¯å¦éƒ½æ˜¯é¢‘ç¹çš„"""
        k = len(candidate)
        for i in range(k):
            subset = candidate[:i] + candidate[i+1:]
            if subset not in frequent_indices:
                return False
        return True

# æ€§èƒ½å¯¹æ¯”
print("\n=== ç®—æ³•æ€§èƒ½å¯¹æ¯” ===")

import time

# æ ‡å‡†Apriori
start_time = time.time()
apriori_standard = AprioriAlgorithm(min_support=0.05, min_confidence=0.3)
frequent_standard = apriori_standard.find_frequent_itemsets(transactions)
time_standard = time.time() - start_time

# ä¼˜åŒ–Apriori
start_time = time.time()
apriori_optimized = OptimizedApriori(min_support=0.05, min_confidence=0.3)
frequent_optimized = apriori_optimized.find_frequent_itemsets_optimized(transactions)
time_optimized = time.time() - start_time

print(f"\næ€§èƒ½å¯¹æ¯”ç»“æœ:")
print(f"æ ‡å‡†Apriori: {time_standard:.3f}ç§’")
print(f"ä¼˜åŒ–Apriori: {time_optimized:.3f}ç§’")
print(f"åŠ é€Ÿæ¯”: {time_standard/time_optimized:.2f}x")

# éªŒè¯ç»“æœä¸€è‡´æ€§
total_standard = sum(len(itemsets) for itemsets in frequent_standard.values())
total_optimized = sum(len(itemsets) for itemsets in frequent_optimized.values())
print(f"\nç»“æœéªŒè¯:")
print(f"æ ‡å‡†ç®—æ³•é¢‘ç¹é¡¹é›†æ•°: {total_standard}")
print(f"ä¼˜åŒ–ç®—æ³•é¢‘ç¹é¡¹é›†æ•°: {total_optimized}")
print(f"ç»“æœä¸€è‡´: {total_standard == total_optimized}")
```

## 3. FP-Growthç®—æ³•

### 3.1 FPæ ‘æ„å»º

```python
class FPNode:
    """FPæ ‘èŠ‚ç‚¹"""
    
    def __init__(self, item=None, count=0, parent=None):
        self.item = item
        self.count = count
        self.parent = parent
        self.children = {}
        self.node_link = None  # æŒ‡å‘ä¸‹ä¸€ä¸ªç›¸åŒé¡¹ç›®çš„èŠ‚ç‚¹
    
    def increment(self, count=1):
        """å¢åŠ è®¡æ•°"""
        self.count += count
    
    def display(self, indent=0):
        """æ˜¾ç¤ºèŠ‚ç‚¹ä¿¡æ¯"""
        print('  ' * indent + f'{self.item}:{self.count}')
        for child in self.children.values():
            child.display(indent + 1)

class FPTree:
    """FPæ ‘å®ç°"""
    
    def __init__(self, min_support=0.2):
        self.min_support = min_support
        self.root = FPNode()  # æ ¹èŠ‚ç‚¹
        self.header_table = {}  # å¤´è¡¨
        self.frequent_items = {}  # é¢‘ç¹é¡¹åŠå…¶æ”¯æŒåº¦
    
    def build_tree(self, transactions):
        """æ„å»ºFPæ ‘"""
        print(f"\n=== æ„å»ºFPæ ‘ ===")
        print(f"æœ€å°æ”¯æŒåº¦: {self.min_support}")
        print(f"äº‹åŠ¡æ•°é‡: {len(transactions)}")
        
        # ç¬¬ä¸€æ¬¡æ‰«æï¼šè®¡ç®—é¡¹ç›®é¢‘ç‡
        item_counts = {}
        for transaction in transactions:
            for item in transaction:
                item_counts[item] = item_counts.get(item, 0) + 1
        
        # è¿‡æ»¤é¢‘ç¹é¡¹
        min_count = self.min_support * len(transactions)
        self.frequent_items = {item: count for item, count in item_counts.items() 
                              if count >= min_count}
        
        print(f"é¢‘ç¹é¡¹æ•°é‡: {len(self.frequent_items)}")
        
        if not self.frequent_items:
            return
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_items = sorted(self.frequent_items.items(), key=lambda x: x[1], reverse=True)
        
        # åˆå§‹åŒ–å¤´è¡¨
        for item, count in sorted_items:
            self.header_table[item] = {'count': count, 'head': None}
        
        # ç¬¬äºŒæ¬¡æ‰«æï¼šæ„å»ºFPæ ‘
        for transaction in transactions:
            # è¿‡æ»¤å¹¶æ’åºäº‹åŠ¡ä¸­çš„é¢‘ç¹é¡¹
            filtered_transaction = [item for item in transaction if item in self.frequent_items]
            filtered_transaction.sort(key=lambda x: self.frequent_items[x], reverse=True)
            
            if filtered_transaction:
                self.insert_transaction(filtered_transaction, self.root)
        
        print("FPæ ‘æ„å»ºå®Œæˆ")
    
    def insert_transaction(self, transaction, node):
        """å°†äº‹åŠ¡æ’å…¥FPæ ‘"""
        if not transaction:
            return
        
        first_item = transaction[0]
        
        # å¦‚æœå½“å‰èŠ‚ç‚¹å·²æœ‰è¯¥å­é¡¹
        if first_item in node.children:
            node.children[first_item].increment()
        else:
            # åˆ›å»ºæ–°èŠ‚ç‚¹
            new_node = FPNode(first_item, 1, node)
            node.children[first_item] = new_node
            
            # æ›´æ–°å¤´è¡¨é“¾æ¥
            if self.header_table[first_item]['head'] is None:
                self.header_table[first_item]['head'] = new_node
            else:
                # æ‰¾åˆ°é“¾è¡¨æœ«å°¾
                current = self.header_table[first_item]['head']
                while current.node_link is not None:
                    current = current.node_link
                current.node_link = new_node
        
        # é€’å½’æ’å…¥å‰©ä½™é¡¹
        self.insert_transaction(transaction[1:], node.children[first_item])
    
    def get_conditional_pattern_base(self, item):
        """è·å–æ¡ä»¶æ¨¡å¼åŸº"""
        patterns = []
        
        # éå†è¯¥é¡¹ç›®çš„æ‰€æœ‰èŠ‚ç‚¹
        node = self.header_table[item]['head']
        while node is not None:
            # è·å–ä»æ ¹åˆ°è¯¥èŠ‚ç‚¹çš„è·¯å¾„
            path = []
            current = node.parent
            while current.item is not None:  # ä¸åŒ…æ‹¬æ ¹èŠ‚ç‚¹
                path.append(current.item)
                current = current.parent
            
            if path:
                patterns.append((path[::-1], node.count))  # åè½¬è·¯å¾„ï¼Œæ·»åŠ è®¡æ•°
            
            node = node.node_link
        
        return patterns
    
    def mine_patterns(self, min_support_count=None):
        """æŒ–æ˜é¢‘ç¹æ¨¡å¼"""
        if min_support_count is None:
            min_support_count = self.min_support
        
        patterns = {}
        
        # æŒ‰é¢‘ç‡ä»å°åˆ°å¤§å¤„ç†é¡¹ç›®
        sorted_items = sorted(self.header_table.items(), key=lambda x: x[1]['count'])
        
        for item, item_info in sorted_items:
            # è¯¥é¡¹ç›®æœ¬èº«å°±æ˜¯ä¸€ä¸ªé¢‘ç¹æ¨¡å¼
            patterns[frozenset([item])] = item_info['count']
            
            # è·å–æ¡ä»¶æ¨¡å¼åŸº
            conditional_patterns = self.get_conditional_pattern_base(item)
            
            if conditional_patterns:
                # æ„å»ºæ¡ä»¶FPæ ‘
                conditional_tree = FPTree(min_support_count)
                conditional_transactions = []
                
                for pattern, count in conditional_patterns:
                    for _ in range(count):
                        conditional_transactions.append(pattern)
                
                if conditional_transactions:
                    conditional_tree.build_tree(conditional_transactions)
                    
                    # é€’å½’æŒ–æ˜æ¡ä»¶FPæ ‘
                    conditional_patterns_dict = conditional_tree.mine_patterns(min_support_count)
                    
                    # å°†å½“å‰é¡¹ç›®æ·»åŠ åˆ°æ‰€æœ‰æ¡ä»¶æ¨¡å¼ä¸­
                    for pattern, support in conditional_patterns_dict.items():
                        new_pattern = pattern.union(frozenset([item]))
                        patterns[new_pattern] = support
        
        return patterns
    
    def display_tree(self):
        """æ˜¾ç¤ºFPæ ‘ç»“æ„"""
        print("\n=== FPæ ‘ç»“æ„ ===")
        print("æ ¹èŠ‚ç‚¹:")
        self.root.display()
        
        print("\n=== å¤´è¡¨ ===")
        for item, info in self.header_table.items():
            print(f"{item}: è®¡æ•°={info['count']}")

class FPGrowthAlgorithm:
    """FP-Growthç®—æ³•å®ç°"""
    
    def __init__(self, min_support=0.2, min_confidence=0.6):
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.frequent_patterns = {}
        self.rules = []
    
    def find_frequent_patterns(self, transactions):
        """ä½¿ç”¨FP-GrowthæŒ–æ˜é¢‘ç¹æ¨¡å¼"""
        print(f"\n=== FP-Growthç®—æ³•æŒ–æ˜é¢‘ç¹æ¨¡å¼ ===")
        
        # æ„å»ºFPæ ‘
        fp_tree = FPTree(self.min_support)
        fp_tree.build_tree(transactions)
        
        # æŒ–æ˜é¢‘ç¹æ¨¡å¼
        min_support_count = self.min_support * len(transactions)
        self.frequent_patterns = fp_tree.mine_patterns(min_support_count)
        
        print(f"\né¢‘ç¹æ¨¡å¼æ•°é‡: {len(self.frequent_patterns)}")
        
        # æŒ‰é•¿åº¦åˆ†ç»„ç»Ÿè®¡
        pattern_by_length = {}
        for pattern in self.frequent_patterns:
            length = len(pattern)
            pattern_by_length[length] = pattern_by_length.get(length, 0) + 1
        
        for length, count in sorted(pattern_by_length.items()):
            print(f"  {length}-é¡¹æ¨¡å¼: {count}ä¸ª")
        
        return self.frequent_patterns
    
    def generate_rules(self, transactions):
        """ä»é¢‘ç¹æ¨¡å¼ç”Ÿæˆå…³è”è§„åˆ™"""
        if not self.frequent_patterns:
            self.find_frequent_patterns(transactions)
        
        print(f"\n=== ç”Ÿæˆå…³è”è§„åˆ™ ===")
        self.rules = []
        
        for pattern, support_count in self.frequent_patterns.items():
            if len(pattern) < 2:
                continue
            
            pattern_support = support_count / len(transactions)
            
            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è§„åˆ™
            for i in range(1, len(pattern)):
                for antecedent in combinations(pattern, i):
                    antecedent = frozenset(antecedent)
                    consequent = pattern - antecedent
                    
                    # è®¡ç®—å‰ä»¶æ”¯æŒåº¦
                    antecedent_support_count = self.frequent_patterns.get(antecedent, 0)
                    if antecedent_support_count == 0:
                        continue
                    
                    antecedent_support = antecedent_support_count / len(transactions)
                    confidence = pattern_support / antecedent_support
                    
                    if confidence >= self.min_confidence:
                        # è®¡ç®—åä»¶æ”¯æŒåº¦å’Œæå‡åº¦
                        consequent_support_count = self.frequent_patterns.get(consequent, 0)
                        consequent_support = consequent_support_count / len(transactions)
                        
                        lift = confidence / consequent_support if consequent_support > 0 else 0
                        
                        rule = {
                            'antecedent': antecedent,
                            'consequent': consequent,
                            'support': pattern_support,
                            'confidence': confidence,
                            'lift': lift
                        }
                        
                        self.rules.append(rule)
        
        print(f"ç”Ÿæˆè§„åˆ™æ•°é‡: {len(self.rules)}")
        return self.rules
    
    def print_patterns(self, max_patterns=20):
        """æ‰“å°é¢‘ç¹æ¨¡å¼"""
        print("\n=== é¢‘ç¹æ¨¡å¼ ===")
        
        # æŒ‰æ”¯æŒåº¦æ’åº
        sorted_patterns = sorted(self.frequent_patterns.items(), 
                               key=lambda x: x[1], reverse=True)
        
        print(f"{'åºå·':>4} {'æ¨¡å¼':>30} {'æ”¯æŒè®¡æ•°':>8}")
        print("-" * 50)
        
        for i, (pattern, count) in enumerate(sorted_patterns[:max_patterns]):
            pattern_str = ', '.join(sorted(list(pattern)))
            print(f"{i+1:4d} {{{pattern_str}:>28}} {count:8d}")
        
        if len(sorted_patterns) > max_patterns:
            print(f"\n... è¿˜æœ‰ {len(sorted_patterns) - max_patterns} ä¸ªæ¨¡å¼")
    
    def print_rules(self, max_rules=20, sort_by='lift'):
        """æ‰“å°å…³è”è§„åˆ™"""
        if not self.rules:
            print("æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„å…³è”è§„åˆ™")
            return
        
        print(f"\n=== å…³è”è§„åˆ™ (æŒ‰{sort_by}æ’åº) ===")
        
        # æ’åºè§„åˆ™
        sorted_rules = sorted(self.rules, key=lambda x: x[sort_by], reverse=True)
        
        print(f"{'åºå·':>4} {'å‰ä»¶':>15} {'åä»¶':>15} {'æ”¯æŒåº¦':>8} {'ç½®ä¿¡åº¦':>8} {'æå‡åº¦':>8}")
        print("-" * 70)
        
        for i, rule in enumerate(sorted_rules[:max_rules]):
            antecedent_str = ', '.join(sorted(list(rule['antecedent'])))
            consequent_str = ', '.join(sorted(list(rule['consequent'])))
            
            print(f"{i+1:4d} {antecedent_str:>15} {consequent_str:>15} "
                  f"{rule['support']:8.4f} {rule['confidence']:8.4f} {rule['lift']:8.4f}")
        
        if len(sorted_rules) > max_rules:
            print(f"\n... è¿˜æœ‰ {len(sorted_rules) - max_rules} æ¡è§„åˆ™")

# FP-Growthç®—æ³•æ¼”ç¤º
print("\n=== FP-Growthç®—æ³•æ¼”ç¤º ===")

# ä½¿ç”¨ç›¸åŒçš„äº‹åŠ¡æ•°æ®
fp_growth = FPGrowthAlgorithm(min_support=0.05, min_confidence=0.3)
frequent_patterns = fp_growth.find_frequent_patterns(transactions)
rules_fp = fp_growth.generate_rules(transactions)

# æ˜¾ç¤ºç»“æœ
fp_growth.print_patterns(max_patterns=15)
fp_growth.print_rules(max_rules=15, sort_by='lift')

# ç®—æ³•æ€§èƒ½å¯¹æ¯”
print("\n=== Apriori vs FP-Growth æ€§èƒ½å¯¹æ¯” ===")

import time

# Aprioriç®—æ³•
start_time = time.time()
apriori_comp = AprioriAlgorithm(min_support=0.05, min_confidence=0.3)
apriori_comp.find_frequent_itemsets(transactions)
apriori_comp.generate_rules(transactions)
time_apriori = time.time() - start_time

# FP-Growthç®—æ³•
start_time = time.time()
fp_growth_comp = FPGrowthAlgorithm(min_support=0.05, min_confidence=0.3)
fp_growth_comp.find_frequent_patterns(transactions)
fp_growth_comp.generate_rules(transactions)
time_fp_growth = time.time() - start_time

print(f"Aprioriç®—æ³•: {time_apriori:.3f}ç§’")
print(f"FP-Growthç®—æ³•: {time_fp_growth:.3f}ç§’")
print(f"FP-GrowthåŠ é€Ÿæ¯”: {time_apriori/time_fp_growth:.2f}x")

# ç»“æœå¯¹æ¯”
print(f"\nAprioriè§„åˆ™æ•°: {len(apriori_comp.rules)}")
print(f"FP-Growthè§„åˆ™æ•°: {len(fp_growth_comp.rules)}")
```

## 4. å®é™…åº”ç”¨æ¡ˆä¾‹

### 4.1 ç”µå•†æ¨èç³»ç»Ÿ

```python
class EcommerceRecommendation:
    """ç”µå•†æ¨èç³»ç»Ÿ"""
    
    def __init__(self, min_support=0.01, min_confidence=0.3, min_lift=1.1):
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.min_lift = min_lift
        self.rules = []
        self.product_rules = {}
    
    def load_transaction_data(self, file_path=None):
        """åŠ è½½äº¤æ˜“æ•°æ®"""
        # æ¨¡æ‹Ÿç”µå•†äº¤æ˜“æ•°æ®
        np.random.seed(42)
        
        products = {
            'ç”µå­äº§å“': ['iPhone', 'iPad', 'MacBook', 'è€³æœº', 'å……ç”µå™¨', 'ä¿æŠ¤å£³'],
            'æœè£…': ['Tæ¤', 'ç‰›ä»”è£¤', 'è¿åŠ¨é‹', 'å¸½å­', 'èƒŒåŒ…', 'æ‰‹è¡¨'],
            'å®¶å±…': ['åºŠå•', 'æ•å¤´', 'å°ç¯', 'æ”¶çº³ç›’', 'èŠ±ç“¶', 'åœ°æ¯¯'],
            'é£Ÿå“': ['å’–å•¡', 'èŒ¶å¶', 'åšæœ', 'å·§å…‹åŠ›', 'é¥¼å¹²', 'æœæ±'],
            'å›¾ä¹¦': ['å°è¯´', 'æŠ€æœ¯ä¹¦', 'å†å²ä¹¦', 'æ¼«ç”»', 'æ‚å¿—', 'å­—å…¸']
        }
        
        # å®šä¹‰äº§å“å…³è”æ€§
        associations = {
            'iPhone': ['è€³æœº', 'å……ç”µå™¨', 'ä¿æŠ¤å£³'],
            'iPad': ['ä¿æŠ¤å£³', 'å……ç”µå™¨'],
            'MacBook': ['é¼ æ ‡', 'é”®ç›˜'],
            'Tæ¤': ['ç‰›ä»”è£¤', 'è¿åŠ¨é‹'],
            'å’–å•¡': ['èŒ¶å¶', 'æ¯å­'],
            'å°è¯´': ['ä¹¦ç­¾', 'å°ç¯']
        }
        
        transactions = []
        
        for i in range(5000):
            transaction = []
            
            # é€‰æ‹©ä¸»è¦ç±»åˆ«
            main_category = np.random.choice(list(products.keys()))
            
            # ä»ä¸»è¦ç±»åˆ«é€‰æ‹©1-3ä¸ªäº§å“
            n_main = np.random.randint(1, 4)
            main_products = np.random.choice(products[main_category], 
                                           size=min(n_main, len(products[main_category])), 
                                           replace=False)
            transaction.extend(main_products)
            
            # æ ¹æ®å…³è”æ€§æ·»åŠ ç›¸å…³äº§å“
            for product in main_products:
                if product in associations:
                    for related in associations[product]:
                        if np.random.random() < 0.3:
                            transaction.append(related)
            
            # éšæœºæ·»åŠ å…¶ä»–ç±»åˆ«çš„äº§å“
            if np.random.random() < 0.4:
                other_category = np.random.choice([cat for cat in products.keys() 
                                                 if cat != main_category])
                other_product = np.random.choice(products[other_category])
                transaction.append(other_product)
            
            # å»é‡
            transaction = list(set(transaction))
            transactions.append(transaction)
        
        return transactions
    
    def analyze_purchase_patterns(self, transactions):
        """åˆ†æè´­ä¹°æ¨¡å¼"""
        print("\n=== ç”µå•†è´­ä¹°æ¨¡å¼åˆ†æ ===")
        
        # ä½¿ç”¨FP-Growthç®—æ³•
        fp_growth = FPGrowthAlgorithm(self.min_support, self.min_confidence)
        patterns = fp_growth.find_frequent_patterns(transactions)
        rules = fp_growth.generate_rules(transactions)
        
        # è¿‡æ»¤é«˜è´¨é‡è§„åˆ™
        high_quality_rules = []
        for rule in rules:
            if rule['lift'] >= self.min_lift:
                high_quality_rules.append(rule)
        
        self.rules = high_quality_rules
        print(f"é«˜è´¨é‡è§„åˆ™æ•°é‡: {len(high_quality_rules)}")
        
        return high_quality_rules
    
    def build_recommendation_engine(self):
        """æ„å»ºæ¨èå¼•æ“"""
        print("\n=== æ„å»ºæ¨èå¼•æ“ ===")
        
        # ä¸ºæ¯ä¸ªäº§å“å»ºç«‹æ¨èè§„åˆ™
        for rule in self.rules:
            for antecedent_item in rule['antecedent']:
                if antecedent_item not in self.product_rules:
                    self.product_rules[antecedent_item] = []
                
                recommendation = {
                    'recommended_items': list(rule['consequent']),
                    'confidence': rule['confidence'],
                    'lift': rule['lift'],
                    'support': rule['support']
                }
                
                self.product_rules[antecedent_item].append(recommendation)
        
        # ä¸ºæ¯ä¸ªäº§å“çš„æ¨èæŒ‰ç½®ä¿¡åº¦æ’åº
        for product in self.product_rules:
            self.product_rules[product].sort(key=lambda x: x['confidence'], reverse=True)
        
        print(f"ä¸º {len(self.product_rules)} ä¸ªäº§å“å»ºç«‹äº†æ¨èè§„åˆ™")
    
    def recommend_products(self, cart_items, max_recommendations=5):
        """ä¸ºè´­ç‰©è½¦æ¨èäº§å“"""
        recommendations = {}
        
        for item in cart_items:
            if item in self.product_rules:
                for rule in self.product_rules[item]:
                    for recommended_item in rule['recommended_items']:
                        if recommended_item not in cart_items:
                            if recommended_item not in recommendations:
                                recommendations[recommended_item] = {
                                    'score': 0,
                                    'reasons': []
                                }
                            
                            # è®¡ç®—æ¨èåˆ†æ•°ï¼ˆç½®ä¿¡åº¦ Ã— æå‡åº¦ï¼‰
                            score = rule['confidence'] * rule['lift']
                            recommendations[recommended_item]['score'] += score
                            recommendations[recommended_item]['reasons'].append({
                                'because_of': item,
                                'confidence': rule['confidence'],
                                'lift': rule['lift']
                            })
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_recommendations = sorted(recommendations.items(), 
                                      key=lambda x: x[1]['score'], reverse=True)
        
        return sorted_recommendations[:max_recommendations]
    
    def analyze_recommendation_quality(self, test_transactions):
        """åˆ†ææ¨èè´¨é‡"""
        print("\n=== æ¨èè´¨é‡åˆ†æ ===")
        
        total_recommendations = 0
        successful_recommendations = 0
        
        for transaction in test_transactions[:100]:  # æµ‹è¯•å‰100ä¸ªäº‹åŠ¡
            if len(transaction) < 2:
                continue
            
            # ä½¿ç”¨å‰ä¸€åŠå•†å“æ¨èåä¸€åŠ
            mid = len(transaction) // 2
            cart_items = transaction[:mid]
            actual_items = set(transaction[mid:])
            
            recommendations = self.recommend_products(cart_items, max_recommendations=10)
            recommended_items = set([item for item, _ in recommendations])
            
            # è®¡ç®—å‘½ä¸­ç‡
            hits = len(actual_items.intersection(recommended_items))
            total_recommendations += len(recommended_items)
            successful_recommendations += hits
        
        precision = successful_recommendations / total_recommendations if total_recommendations > 0 else 0
        
        print(f"æ¨èç²¾åº¦: {precision:.4f}")
        print(f"æ€»æ¨èæ•°: {total_recommendations}")
        print(f"æˆåŠŸæ¨èæ•°: {successful_recommendations}")
        
        return precision
    
    def demonstrate_recommendations(self):
        """æ¼”ç¤ºæ¨èç³»ç»Ÿ"""
        print("\n=== æ¨èç³»ç»Ÿæ¼”ç¤º ===")
        
        # æ¨¡æ‹Ÿå‡ ä¸ªè´­ç‰©è½¦
        test_carts = [
            ['iPhone', 'iPad'],
            ['Tæ¤', 'ç‰›ä»”è£¤'],
            ['å’–å•¡', 'èŒ¶å¶'],
            ['å°è¯´', 'æŠ€æœ¯ä¹¦'],
            ['MacBook']
        ]
        
        for i, cart in enumerate(test_carts):
            print(f"\nè´­ç‰©è½¦ {i+1}: {cart}")
            recommendations = self.recommend_products(cart, max_recommendations=3)
            
            if recommendations:
                print("æ¨èå•†å“:")
                for j, (item, info) in enumerate(recommendations):
                    print(f"  {j+1}. {item} (åˆ†æ•°: {info['score']:.3f})")
                    for reason in info['reasons'][:2]:  # æ˜¾ç¤ºå‰2ä¸ªç†ç”±
                        print(f"     å› ä¸ºè´­ä¹°äº† '{reason['because_of']}' "
                              f"(ç½®ä¿¡åº¦: {reason['confidence']:.3f}, "
                              f"æå‡åº¦: {reason['lift']:.3f})")
            else:
         print("  æš‚æ— æ¨è")
```

## 5. Traeå…³è”è§„åˆ™æŒ–æ˜ç³»ç»Ÿ

### 5.1 TraeAssociationMiningç±»å®ç°

```python
class TraeAssociationMining:
    """Traeé£æ ¼çš„å…³è”è§„åˆ™æŒ–æ˜ç³»ç»Ÿ"""
    
    def __init__(self, algorithm='fp_growth', min_support=0.1, min_confidence=0.6, 
                 min_lift=1.0, max_length=5):
        self.algorithm = algorithm
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.min_lift = min_lift
        self.max_length = max_length
        
        # Traeåˆ†æå†å²
        self.trae_history = {
            'training_log': [],
            'mining_results': {},
            'rule_analysis': {},
            'performance_metrics': {},
            'recommendations': []
        }
        
        # ç®—æ³•å®ä¾‹
        self.miner = None
        self.frequent_patterns = {}
        self.association_rules = []
    
    def trae_fit(self, transactions, transaction_names=None):
        """Traeè®­ç»ƒæ–¹æ³•"""
        import time
        start_time = time.time()
        
        print(f"\n{'='*60}")
        print(f"ğŸ” Traeå…³è”è§„åˆ™æŒ–æ˜ç³»ç»Ÿ - è®­ç»ƒé˜¶æ®µ")
        print(f"{'='*60}")
        
        # è®°å½•è®­ç»ƒå¼€å§‹
        self.trae_history['training_log'].append({
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'action': 'training_start',
            'algorithm': self.algorithm,
            'parameters': {
                'min_support': self.min_support,
                'min_confidence': self.min_confidence,
                'min_lift': self.min_lift,
                'max_length': self.max_length
            }
        })
        
        # æ•°æ®é¢„å¤„ç†å’Œåˆ†æ
        print(f"\nğŸ“Š æ•°æ®é›†åˆ†æ:")
        print(f"   äº‹åŠ¡æ•°é‡: {len(transactions)}")
        
        # ç»Ÿè®¡é¡¹ç›®ä¿¡æ¯
        all_items = set()
        transaction_lengths = []
        for transaction in transactions:
            all_items.update(transaction)
            transaction_lengths.append(len(transaction))
        
        print(f"   å”¯ä¸€é¡¹ç›®æ•°: {len(all_items)}")
        print(f"   å¹³å‡äº‹åŠ¡é•¿åº¦: {np.mean(transaction_lengths):.2f}")
        print(f"   äº‹åŠ¡é•¿åº¦èŒƒå›´: {min(transaction_lengths)} - {max(transaction_lengths)}")
        
        # é€‰æ‹©ç®—æ³•
        if self.algorithm == 'apriori':
            self.miner = AprioriAlgorithm(self.min_support, self.min_confidence)
            print(f"\nğŸ”§ ä½¿ç”¨Aprioriç®—æ³•æŒ–æ˜é¢‘ç¹é¡¹é›†...")
            self.frequent_patterns = self.miner.find_frequent_itemsets(transactions)
            self.association_rules = self.miner.generate_rules(transactions)
            
        elif self.algorithm == 'fp_growth':
            self.miner = FPGrowthAlgorithm(self.min_support, self.min_confidence)
            print(f"\nğŸ”§ ä½¿ç”¨FP-Growthç®—æ³•æŒ–æ˜é¢‘ç¹æ¨¡å¼...")
            self.frequent_patterns = self.miner.find_frequent_patterns(transactions)
            self.association_rules = self.miner.generate_rules(transactions)
        
        # è¿‡æ»¤è§„åˆ™
        filtered_rules = []
        for rule in self.association_rules:
            if (rule['lift'] >= self.min_lift and 
                len(rule['antecedent']) + len(rule['consequent']) <= self.max_length):
                filtered_rules.append(rule)
        
        self.association_rules = filtered_rules
        
        # è®­ç»ƒå®Œæˆ
        training_time = time.time() - start_time
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time:.3f}ç§’")
        print(f"   é¢‘ç¹æ¨¡å¼æ•°: {len(self.frequent_patterns) if isinstance(self.frequent_patterns, dict) else sum(len(patterns) for patterns in self.frequent_patterns.values())}")
        print(f"   å…³è”è§„åˆ™æ•°: {len(self.association_rules)}")
        
        # è®°å½•è®­ç»ƒç»“æœ
        self.trae_history['training_log'].append({
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'action': 'training_complete',
            'training_time': training_time,
            'frequent_patterns_count': len(self.frequent_patterns) if isinstance(self.frequent_patterns, dict) else sum(len(patterns) for patterns in self.frequent_patterns.values()),
            'rules_count': len(self.association_rules)
        })
        
        return self
    
    def trae_analyze_patterns(self, top_k=20):
        """Traeæ¨¡å¼åˆ†æ"""
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ Traeå…³è”è§„åˆ™åˆ†æ")
        print(f"{'='*60}")
        
        if not self.association_rules:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å…³è”è§„åˆ™ï¼Œè¯·å…ˆè¿›è¡Œè®­ç»ƒ")
            return
        
        # è§„åˆ™ç»Ÿè®¡åˆ†æ
        supports = [rule['support'] for rule in self.association_rules]
        confidences = [rule['confidence'] for rule in self.association_rules]
        lifts = [rule['lift'] for rule in self.association_rules]
        
        analysis_results = {
            'total_rules': len(self.association_rules),
            'support_stats': {
                'mean': np.mean(supports),
                'std': np.std(supports),
                'min': np.min(supports),
                'max': np.max(supports)
            },
            'confidence_stats': {
                'mean': np.mean(confidences),
                'std': np.std(confidences),
                'min': np.min(confidences),
                'max': np.max(confidences)
            },
            'lift_stats': {
                'mean': np.mean(lifts),
                'std': np.std(lifts),
                'min': np.min(lifts),
                'max': np.max(lifts)
            }
        }
        
        print(f"\nğŸ“Š è§„åˆ™ç»Ÿè®¡æ‘˜è¦:")
        print(f"   æ€»è§„åˆ™æ•°: {analysis_results['total_rules']}")
        print(f"\n   æ”¯æŒåº¦ç»Ÿè®¡:")
        print(f"     å¹³å‡å€¼: {analysis_results['support_stats']['mean']:.4f}")
        print(f"     æ ‡å‡†å·®: {analysis_results['support_stats']['std']:.4f}")
        print(f"     èŒƒå›´: [{analysis_results['support_stats']['min']:.4f}, {analysis_results['support_stats']['max']:.4f}]")
        
        print(f"\n   ç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"     å¹³å‡å€¼: {analysis_results['confidence_stats']['mean']:.4f}")
        print(f"     æ ‡å‡†å·®: {analysis_results['confidence_stats']['std']:.4f}")
        print(f"     èŒƒå›´: [{analysis_results['confidence_stats']['min']:.4f}, {analysis_results['confidence_stats']['max']:.4f}]")
        
        print(f"\n   æå‡åº¦ç»Ÿè®¡:")
        print(f"     å¹³å‡å€¼: {analysis_results['lift_stats']['mean']:.4f}")
        print(f"     æ ‡å‡†å·®: {analysis_results['lift_stats']['std']:.4f}")
        print(f"     èŒƒå›´: [{analysis_results['lift_stats']['min']:.4f}, {analysis_results['lift_stats']['max']:.4f}]")
        
        # æå‡åº¦åˆ†å¸ƒ
        positive_lift = sum(1 for lift in lifts if lift > 1)
        negative_lift = sum(1 for lift in lifts if lift < 1)
        neutral_lift = sum(1 for lift in lifts if lift == 1)
        
        print(f"\n   æå‡åº¦åˆ†å¸ƒ:")
        print(f"     æ­£ç›¸å…³ (lift > 1): {positive_lift} ({positive_lift/len(lifts)*100:.1f}%)")
        print(f"     è´Ÿç›¸å…³ (lift < 1): {negative_lift} ({negative_lift/len(lifts)*100:.1f}%)")
        print(f"     ç‹¬ç«‹ (lift = 1): {neutral_lift} ({neutral_lift/len(lifts)*100:.1f}%)")
        
        # Topè§„åˆ™åˆ†æ
        print(f"\nğŸ† Top-{top_k} å…³è”è§„åˆ™ (æŒ‰æå‡åº¦æ’åº):")
        sorted_rules = sorted(self.association_rules, key=lambda x: x['lift'], reverse=True)
        
        print(f"{'åºå·':>4} {'å‰ä»¶':>20} {'åä»¶':>20} {'æ”¯æŒåº¦':>8} {'ç½®ä¿¡åº¦':>8} {'æå‡åº¦':>8}")
        print("-" * 85)
        
        for i, rule in enumerate(sorted_rules[:top_k]):
            antecedent_str = ', '.join(sorted(list(rule['antecedent'])))
            consequent_str = ', '.join(sorted(list(rule['consequent'])))
            
            print(f"{i+1:4d} {antecedent_str:>20} {consequent_str:>20} "
                  f"{rule['support']:8.4f} {rule['confidence']:8.4f} {rule['lift']:8.4f}")
        
        # ä¿å­˜åˆ†æç»“æœ
        self.trae_history['rule_analysis'] = analysis_results
        
        return analysis_results
    
    def trae_recommend(self, items, max_recommendations=10):
        """Traeæ¨èç³»ç»Ÿ"""
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Traeæ™ºèƒ½æ¨èç³»ç»Ÿ")
        print(f"{'='*60}")
        
        if not self.association_rules:
            print("âŒ æ²¡æœ‰å…³è”è§„åˆ™ï¼Œè¯·å…ˆè¿›è¡Œè®­ç»ƒ")
            return []
        
        print(f"\nğŸ›’ è¾“å…¥é¡¹ç›®: {items}")
        
        recommendations = {}
        matching_rules = []
        
        # æŸ¥æ‰¾åŒ¹é…çš„è§„åˆ™
        for rule in self.association_rules:
            # æ£€æŸ¥è¾“å…¥é¡¹ç›®æ˜¯å¦åŒ…å«è§„åˆ™å‰ä»¶
            if rule['antecedent'].issubset(set(items)):
                matching_rules.append(rule)
                
                for recommended_item in rule['consequent']:
                    if recommended_item not in items:
                        if recommended_item not in recommendations:
                            recommendations[recommended_item] = {
                                'score': 0,
                                'supporting_rules': []
                            }
                        
                        # è®¡ç®—æ¨èåˆ†æ•°
                        score = rule['confidence'] * rule['lift'] * rule['support']
                        recommendations[recommended_item]['score'] += score
                        recommendations[recommended_item]['supporting_rules'].append({
                            'antecedent': rule['antecedent'],
                            'confidence': rule['confidence'],
                            'lift': rule['lift'],
                            'support': rule['support']
                        })
        
        print(f"\nğŸ“‹ åŒ¹é…è§„åˆ™æ•°: {len(matching_rules)}")
        
        if not recommendations:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ¨è")
            return []
        
        # æ’åºæ¨è
        sorted_recommendations = sorted(recommendations.items(), 
                                      key=lambda x: x[1]['score'], reverse=True)
        
        final_recommendations = sorted_recommendations[:max_recommendations]
        
        print(f"\nğŸ æ¨èç»“æœ (Top-{len(final_recommendations)}):")
        print(f"{'åºå·':>4} {'æ¨èé¡¹ç›®':>20} {'æ¨èåˆ†æ•°':>10} {'æ”¯æŒè§„åˆ™æ•°':>10}")
        print("-" * 50)
        
        for i, (item, info) in enumerate(final_recommendations):
            print(f"{i+1:4d} {item:>20} {info['score']:10.4f} {len(info['supporting_rules']):10d}")
        
        # æ˜¾ç¤ºæ¨èç†ç”±
        print(f"\nğŸ’¡ æ¨èç†ç”±:")
        for i, (item, info) in enumerate(final_recommendations[:5]):
            print(f"\n   {i+1}. {item}:")
            
            # æ˜¾ç¤ºæœ€å¼ºçš„æ”¯æŒè§„åˆ™
            best_rule = max(info['supporting_rules'], key=lambda x: x['confidence'])
            antecedent_str = ', '.join(sorted(list(best_rule['antecedent'])))
            
            print(f"      å› ä¸º: {antecedent_str} â†’ {item}")
            print(f"      ç½®ä¿¡åº¦: {best_rule['confidence']:.3f}")
            print(f"      æå‡åº¦: {best_rule['lift']:.3f}")
        
        # è®°å½•æ¨èå†å²
        self.trae_history['recommendations'].append({
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'input_items': items,
            'recommendations': [(item, info['score']) for item, info in final_recommendations],
            'matching_rules_count': len(matching_rules)
        })
        
        return final_recommendations
    
    def trae_export_results(self, file_path):
        """Traeç»“æœå¯¼å‡º"""
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ Traeç»“æœå¯¼å‡º")
        print(f"{'='*60}")
        
        export_data = {
            'algorithm_info': {
                'algorithm': self.algorithm,
                'parameters': {
                    'min_support': self.min_support,
                    'min_confidence': self.min_confidence,
                    'min_lift': self.min_lift,
                    'max_length': self.max_length
                }
            },
            'mining_results': {
                'frequent_patterns_count': len(self.frequent_patterns) if isinstance(self.frequent_patterns, dict) else sum(len(patterns) for patterns in self.frequent_patterns.values()),
                'association_rules_count': len(self.association_rules)
            },
            'association_rules': [],
            'analysis_history': self.trae_history
        }
        
        # å¯¼å‡ºå…³è”è§„åˆ™
        for rule in self.association_rules:
            export_data['association_rules'].append({
                'antecedent': list(rule['antecedent']),
                'consequent': list(rule['consequent']),
                'support': rule['support'],
                'confidence': rule['confidence'],
                'lift': rule['lift']
            })
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        import json
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ç»“æœå·²å¯¼å‡ºåˆ°: {file_path}")
        print(f"   åŒ…å«å†…å®¹:")
        print(f"     - ç®—æ³•é…ç½®ä¿¡æ¯")
        print(f"     - {len(self.association_rules)} æ¡å…³è”è§„åˆ™")
        print(f"     - å®Œæ•´åˆ†æå†å²")
        print(f"     - æ¨èè®°å½• ({len(self.trae_history['recommendations'])} æ¬¡)")

# Traeå…³è”è§„åˆ™æŒ–æ˜æ¼”ç¤º
print(f"\n{'='*80}")
print(f"ğŸš€ Traeå…³è”è§„åˆ™æŒ–æ˜ç³»ç»Ÿæ¼”ç¤º")
print(f"{'='*80}")

# åˆ›å»ºTraeæŒ–æ˜å™¨
trae_miner = TraeAssociationMining(
    algorithm='fp_growth',
    min_support=0.03,
    min_confidence=0.4,
    min_lift=1.2,
    max_length=4
)

# ä½¿ç”¨ç”µå•†æ•°æ®è¿›è¡Œè®­ç»ƒ
trae_miner.trae_fit(ecommerce_transactions)

# åˆ†ææŒ–æ˜ç»“æœ
analysis_results = trae_miner.trae_analyze_patterns(top_k=15)

# æ¼”ç¤ºæ¨èåŠŸèƒ½
test_items_list = [
    ['iPhone'],
    ['Tæ¤', 'ç‰›ä»”è£¤'],
    ['å’–å•¡'],
    ['å°è¯´', 'å°ç¯']
]

for test_items in test_items_list:
    recommendations = trae_miner.trae_recommend(test_items, max_recommendations=5)

# å¯¼å‡ºç»“æœ
trae_miner.trae_export_results('e:/hy/project/ks_lr/chapter1/trae_association_results.json')
```

## 6. æ€è€ƒé¢˜

1. **ç®—æ³•é€‰æ‹©**: åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©Aprioriç®—æ³•è€Œä¸æ˜¯FP-Growthç®—æ³•ï¼Ÿä¸¤ç§ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦æœ‰ä½•ä¸åŒï¼Ÿ

2. **å‚æ•°è°ƒä¼˜**: å¦‚ä½•ç¡®å®šæœ€å°æ”¯æŒåº¦ã€æœ€å°ç½®ä¿¡åº¦å’Œæœ€å°æå‡åº¦çš„åˆé€‚å€¼ï¼Ÿè¿™äº›å‚æ•°å¯¹æŒ–æ˜ç»“æœæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

3. **è¯„ä¼°æŒ‡æ ‡**: é™¤äº†æ”¯æŒåº¦ã€ç½®ä¿¡åº¦å’Œæå‡åº¦å¤–ï¼Œè¿˜æœ‰å“ªäº›æŒ‡æ ‡å¯ä»¥ç”¨æ¥è¯„ä¼°å…³è”è§„åˆ™çš„è´¨é‡ï¼Ÿå®ƒä»¬å„è‡ªçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ

4. **æ•°æ®é¢„å¤„ç†**: åœ¨è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ä¹‹å‰ï¼Œéœ€è¦å¯¹æ•°æ®è¿›è¡Œå“ªäº›é¢„å¤„ç†ï¼Ÿå¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼å’Œå™ªå£°æ•°æ®ï¼Ÿ

5. **åº”ç”¨åœºæ™¯**: å…³è”è§„åˆ™æŒ–æ˜é™¤äº†å¸‚åœºç¯®å­åˆ†æå¤–ï¼Œè¿˜å¯ä»¥åº”ç”¨åœ¨å“ªäº›é¢†åŸŸï¼Ÿæ¯ä¸ªåº”ç”¨åœºæ™¯æœ‰ä»€ä¹ˆç‰¹æ®Šçš„è€ƒè™‘å› ç´ ï¼Ÿ

## 7. å°ç»“

### 7.1 æ ¸å¿ƒä¼˜åŠ¿

- **ç›´è§‚æ˜“æ‡‚**: å…³è”è§„åˆ™ä»¥"å¦‚æœ...é‚£ä¹ˆ..."çš„å½¢å¼è¡¨è¾¾ï¼Œå®¹æ˜“ç†è§£å’Œè§£é‡Š
- **æ— ç›‘ç£å­¦ä¹ **: ä¸éœ€è¦é¢„å…ˆæ ‡è®°çš„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼
- **å®ç”¨æ€§å¼º**: åœ¨å•†ä¸šæ™ºèƒ½ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨
- **å¯æ‰©å±•æ€§**: ç®—æ³•å¯ä»¥å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†
- **å¤šæ ·åŒ–æŒ‡æ ‡**: æä¾›å¤šç§è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡è§„åˆ™è´¨é‡

### 7.2 å…³é”®æŠ€æœ¯

- **é¢‘ç¹é¡¹é›†æŒ–æ˜**: Aprioriç®—æ³•çš„å…ˆéªŒåŸç†å’ŒFP-Growthçš„æ¨¡å¼å¢é•¿ç­–ç•¥
- **è§„åˆ™ç”Ÿæˆ**: ä»é¢‘ç¹é¡¹é›†ç”Ÿæˆå…³è”è§„åˆ™çš„æ–¹æ³•
- **è´¨é‡è¯„ä¼°**: æ”¯æŒåº¦ã€ç½®ä¿¡åº¦ã€æå‡åº¦ç­‰å¤šç»´åº¦è¯„ä¼°ä½“ç³»
- **ç®—æ³•ä¼˜åŒ–**: å‰ªæç­–ç•¥ã€æ•°æ®ç»“æ„ä¼˜åŒ–ç­‰æå‡æ•ˆç‡çš„æŠ€æœ¯
- **åº”ç”¨é›†æˆ**: ä¸æ¨èç³»ç»Ÿã€ç”¨æˆ·è¡Œä¸ºåˆ†æç­‰å®é™…åº”ç”¨çš„ç»“åˆ

### 7.3 å®é™…åº”ç”¨

- **ç”µå•†æ¨è**: åŸºäºè´­ä¹°å†å²çš„å•†å“æ¨èå’Œäº¤å‰é”€å”®
- **ç½‘ç«™ä¼˜åŒ–**: ç”¨æˆ·è¡Œä¸ºåˆ†æå’Œé¡µé¢å¯¼èˆªä¼˜åŒ–
- **åº“å­˜ç®¡ç†**: å•†å“å…³è”æ€§åˆ†æå’Œåº“å­˜é…ç½®ä¼˜åŒ–
- **å¸‚åœºè¥é”€**: ä¿ƒé”€ç­–ç•¥åˆ¶å®šå’Œå®¢æˆ·ç»†åˆ†
- **ç”Ÿç‰©ä¿¡æ¯å­¦**: åŸºå› å…³è”åˆ†æå’Œè›‹ç™½è´¨ç›¸äº’ä½œç”¨ç ”ç©¶

### 7.4 å±€é™æ€§

- **è®¡ç®—å¤æ‚åº¦**: å€™é€‰é¡¹é›†æ•°é‡å¯èƒ½å‘ˆæŒ‡æ•°å¢é•¿
- **å‚æ•°æ•æ„Ÿ**: æ”¯æŒåº¦å’Œç½®ä¿¡åº¦é˜ˆå€¼çš„é€‰æ‹©å¯¹ç»“æœå½±å“å¾ˆå¤§
- **ç¨€æœ‰é¡¹ç›®**: å¯èƒ½å¿½ç•¥æ”¯æŒåº¦ä½ä½†æœ‰ä»·å€¼çš„å…³è”å…³ç³»
- **æ•°æ®è´¨é‡**: å¯¹æ•°æ®å™ªå£°å’Œå¼‚å¸¸å€¼æ¯”è¾ƒæ•æ„Ÿ
- **è§£é‡Šå±€é™**: å‘ç°çš„å…³è”ä¸ä¸€å®šä»£è¡¨å› æœå…³ç³»

### 7.5 ä½¿ç”¨å»ºè®®

1. **æ•°æ®é¢„å¤„ç†**: ç¡®ä¿æ•°æ®è´¨é‡ï¼Œå¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
2. **å‚æ•°è°ƒä¼˜**: æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯è°ƒæ•´æ”¯æŒåº¦å’Œç½®ä¿¡åº¦é˜ˆå€¼
3. **ç®—æ³•é€‰æ‹©**: æ ¹æ®æ•°æ®è§„æ¨¡å’Œæ€§èƒ½è¦æ±‚é€‰æ‹©åˆé€‚çš„ç®—æ³•
4. **ç»“æœéªŒè¯**: ç»“åˆé¢†åŸŸçŸ¥è¯†éªŒè¯æŒ–æ˜å‡ºçš„è§„åˆ™çš„åˆç†æ€§
5. **æŒç»­æ›´æ–°**: å®šæœŸæ›´æ–°æ¨¡å‹ä»¥é€‚åº”æ•°æ®åˆ†å¸ƒçš„å˜åŒ–

### 7.6 ä¸‹ä¸€æ­¥å­¦ä¹ 

- **åºåˆ—æ¨¡å¼æŒ–æ˜**: å­¦ä¹ æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„æ¨¡å¼å‘ç°
- **å›¾æŒ–æ˜**: ç ”ç©¶å›¾ç»“æ„æ•°æ®ä¸­çš„å…³è”æ¨¡å¼
- **æµæ•°æ®æŒ–æ˜**: å¤„ç†å®æ—¶æ•°æ®æµä¸­çš„å…³è”è§„åˆ™æŒ–æ˜
- **å¤šå±‚å…³è”è§„åˆ™**: å­¦ä¹ å±‚æ¬¡åŒ–æ•°æ®ä¸­çš„å…³è”å…³ç³»
- **çº¦æŸå…³è”è§„åˆ™**: ç»“åˆä¸šåŠ¡çº¦æŸçš„è§„åˆ™æŒ–æ˜æ–¹æ³•

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œä½ å·²ç»æŒæ¡äº†å…³è”è§„åˆ™æŒ–æ˜çš„æ ¸å¿ƒæ¦‚å¿µã€ä¸»è¦ç®—æ³•å’Œå®é™…åº”ç”¨ã€‚è¿™ä¸ºä½ åœ¨æ•°æ®æŒ–æ˜å’Œå•†ä¸šæ™ºèƒ½é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚

# ç”µå•†æ¨èç³»ç»Ÿæ¼”ç¤º
ecommerce_rec = EcommerceRecommendation(min_support=0.02, min_confidence=0.4, min_lift=1.2)

# åŠ è½½æ•°æ®
ecommerce_transactions = ecommerce_rec.load_transaction_data()
print(f"åŠ è½½äº† {len(ecommerce_transactions)} ä¸ªäº¤æ˜“è®°å½•")
print(f"å‰3ä¸ªäº¤æ˜“ç¤ºä¾‹:")
for i, transaction in enumerate(ecommerce_transactions[:3]):
    print(f"  äº¤æ˜“{i+1}: {transaction}")

# åˆ†æè´­ä¹°æ¨¡å¼
high_quality_rules = ecommerce_rec.analyze_purchase_patterns(ecommerce_transactions)

# æ„å»ºæ¨èå¼•æ“
ecommerce_rec.build_recommendation_engine()

# åˆ†ææ¨èè´¨é‡
test_data = ecommerce_transactions[4000:]  # ä½¿ç”¨å1000ä¸ªä½œä¸ºæµ‹è¯•
precision = ecommerce_rec.analyze_recommendation_quality(test_data)

# æ¼”ç¤ºæ¨è
ecommerce_rec.demonstrate_recommendations()
```

### 4.2 ç½‘ç«™ç”¨æˆ·è¡Œä¸ºåˆ†æ

```python
class WebUserBehaviorAnalysis:
    """ç½‘ç«™ç”¨æˆ·è¡Œä¸ºåˆ†æ"""
    
    def __init__(self, min_support=0.05, min_confidence=0.3):
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.page_sequences = []
        self.navigation_rules = []
    
    def generate_web_sessions(self, n_sessions=2000):
        """ç”Ÿæˆç½‘ç«™ä¼šè¯æ•°æ®"""
        np.random.seed(42)
        
        # ç½‘ç«™é¡µé¢
        pages = {
            'å…¥å£é¡µé¢': ['é¦–é¡µ', 'ç™»å½•é¡µ', 'æ³¨å†Œé¡µ'],
            'äº§å“é¡µé¢': ['äº§å“åˆ—è¡¨', 'äº§å“è¯¦æƒ…', 'äº§å“æ¯”è¾ƒ', 'äº§å“è¯„ä»·'],
            'è´­ç‰©é¡µé¢': ['è´­ç‰©è½¦', 'ç»“ç®—é¡µ', 'æ”¯ä»˜é¡µ', 'è®¢å•ç¡®è®¤'],
            'ç”¨æˆ·é¡µé¢': ['ä¸ªäººä¸­å¿ƒ', 'è®¢å•å†å²', 'æ”¶è—å¤¹', 'è®¾ç½®'],
            'å¸®åŠ©é¡µé¢': ['å¸®åŠ©ä¸­å¿ƒ', 'è”ç³»æˆ‘ä»¬', 'å¸¸è§é—®é¢˜', 'é€€æ¢è´§']
        }
        
        # é¡µé¢è½¬æ¢æ¦‚ç‡
        transitions = {
            'é¦–é¡µ': ['äº§å“åˆ—è¡¨', 'ç™»å½•é¡µ', 'å¸®åŠ©ä¸­å¿ƒ'],
            'ç™»å½•é¡µ': ['é¦–é¡µ', 'ä¸ªäººä¸­å¿ƒ', 'äº§å“åˆ—è¡¨'],
            'äº§å“åˆ—è¡¨': ['äº§å“è¯¦æƒ…', 'äº§å“æ¯”è¾ƒ', 'è´­ç‰©è½¦'],
            'äº§å“è¯¦æƒ…': ['è´­ç‰©è½¦', 'äº§å“è¯„ä»·', 'äº§å“æ¯”è¾ƒ'],
            'è´­ç‰©è½¦': ['ç»“ç®—é¡µ', 'äº§å“åˆ—è¡¨', 'é¦–é¡µ'],
            'ç»“ç®—é¡µ': ['æ”¯ä»˜é¡µ', 'è´­ç‰©è½¦'],
            'æ”¯ä»˜é¡µ': ['è®¢å•ç¡®è®¤', 'ç»“ç®—é¡µ'],
            'ä¸ªäººä¸­å¿ƒ': ['è®¢å•å†å²', 'æ”¶è—å¤¹', 'è®¾ç½®']
        }
        
        sessions = []
        
        for i in range(n_sessions):
            session = []
            
            # éšæœºé€‰æ‹©å…¥å£é¡µé¢
            current_page = np.random.choice(['é¦–é¡µ', 'ç™»å½•é¡µ', 'äº§å“åˆ—è¡¨'])
            session.append(current_page)
            
            # æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆè·¯å¾„
            session_length = np.random.randint(3, 12)  # 3-12ä¸ªé¡µé¢
            
            for _ in range(session_length - 1):
                if current_page in transitions:
                    # æ ¹æ®è½¬æ¢æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€é¡µ
                    next_pages = transitions[current_page]
                    weights = [0.5, 0.3, 0.2][:len(next_pages)]  # é€’å‡æƒé‡
                    current_page = np.random.choice(next_pages, p=weights)
                    session.append(current_page)
                else:
                    # éšæœºé€‰æ‹©é¡µé¢
                    all_pages = [page for category in pages.values() for page in category]
                    current_page = np.random.choice(all_pages)
                    session.append(current_page)
            
            sessions.append(session)
        
        return sessions
    
    def analyze_navigation_patterns(self, sessions):
        """åˆ†æå¯¼èˆªæ¨¡å¼"""
        print("\n=== ç½‘ç«™å¯¼èˆªæ¨¡å¼åˆ†æ ===")
        
        # ä½¿ç”¨Aprioriç®—æ³•åˆ†æé¡µé¢è®¿é—®æ¨¡å¼
        apriori = AprioriAlgorithm(self.min_support, self.min_confidence)
        frequent_itemsets = apriori.find_frequent_itemsets(sessions)
        rules = apriori.generate_rules(sessions)
        
        self.navigation_rules = rules
        
        # åˆ†æé¡µé¢è®¿é—®é¢‘ç‡
        page_counts = {}
        total_pages = 0
        
        for session in sessions:
            for page in session:
                page_counts[page] = page_counts.get(page, 0) + 1
                total_pages += 1
        
        print(f"\né¡µé¢è®¿é—®ç»Ÿè®¡:")
        sorted_pages = sorted(page_counts.items(), key=lambda x: x[1], reverse=True)
        for i, (page, count) in enumerate(sorted_pages[:10]):
            percentage = count / total_pages * 100
            print(f"  {i+1:2d}. {page:15s}: {count:4d} ({percentage:5.1f}%)")
        
        return rules
    
    def identify_user_paths(self):
        """è¯†åˆ«ç”¨æˆ·è·¯å¾„æ¨¡å¼"""
        print("\n=== ç”¨æˆ·è·¯å¾„æ¨¡å¼ ===")
        
        # åˆ†æå¸¸è§çš„é¡µé¢åºåˆ—
        path_patterns = {}
        
        for rule in self.navigation_rules:
            if rule['lift'] > 1.5:  # å¼ºå…³è”
                antecedent = list(rule['antecedent'])[0] if len(rule['antecedent']) == 1 else None
                consequent = list(rule['consequent'])[0] if len(rule['consequent']) == 1 else None
                
                if antecedent and consequent:
                    path = f"{antecedent} â†’ {consequent}"
                    path_patterns[path] = {
                        'confidence': rule['confidence'],
                        'lift': rule['lift'],
                        'support': rule['support']
                    }
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_paths = sorted(path_patterns.items(), 
                            key=lambda x: x[1]['confidence'], reverse=True)
        
        print(f"å‘ç° {len(sorted_paths)} ä¸ªå¼ºå…³è”è·¯å¾„:")
        print(f"{'åºå·':>4} {'è·¯å¾„':>30} {'ç½®ä¿¡åº¦':>8} {'æå‡åº¦':>8}")
        print("-" * 55)
        
        for i, (path, metrics) in enumerate(sorted_paths[:15]):
            print(f"{i+1:4d} {path:>30} {metrics['confidence']:8.3f} {metrics['lift']:8.3f}")
    
    def recommend_next_pages(self, current_pages, max_recommendations=5):
        """æ¨èä¸‹ä¸€ä¸ªé¡µé¢"""
        recommendations = {}
        
        for page in current_pages:
            for rule in self.navigation_rules:
                if page in rule['antecedent'] and rule['lift'] > 1.0:
                    for next_page in rule['consequent']:
                        if next_page not in current_pages:
                            if next_page not in recommendations:
                                recommendations[next_page] = 0
                            
                            # æ¨èåˆ†æ•° = ç½®ä¿¡åº¦ Ã— æå‡åº¦
                            score = rule['confidence'] * rule['lift']
                            recommendations[next_page] += score
        
        # æ’åºå¹¶è¿”å›topæ¨è
        sorted_recommendations = sorted(recommendations.items(), 
                                      key=lambda x: x[1], reverse=True)
        
        return sorted_recommendations[:max_recommendations]
    
    def analyze_conversion_funnel(self, sessions):
        """åˆ†æè½¬åŒ–æ¼æ–—"""
        print("\n=== è½¬åŒ–æ¼æ–—åˆ†æ ===")
        
        # å®šä¹‰è½¬åŒ–è·¯å¾„
        funnel_steps = ['é¦–é¡µ', 'äº§å“åˆ—è¡¨', 'äº§å“è¯¦æƒ…', 'è´­ç‰©è½¦', 'ç»“ç®—é¡µ', 'æ”¯ä»˜é¡µ', 'è®¢å•ç¡®è®¤']
        
        step_counts = {step: 0 for step in funnel_steps}
        
        for session in sessions:
            reached_steps = set()
            for page in session:
                if page in funnel_steps:
                    reached_steps.add(page)
            
            for step in reached_steps:
                step_counts[step] += 1
        
        print("è½¬åŒ–æ¼æ–—ç»Ÿè®¡:")
        total_sessions = len(sessions)
        
        for i, step in enumerate(funnel_steps):
            count = step_counts[step]
            percentage = count / total_sessions * 100
            
            if i == 0:
                conversion_rate = 100.0
            else:
                prev_count = step_counts[funnel_steps[i-1]]
                conversion_rate = count / prev_count * 100 if prev_count > 0 else 0
            
            print(f"  {step:12s}: {count:4d} ({percentage:5.1f}%) "
                  f"è½¬åŒ–ç‡: {conversion_rate:5.1f}%")
    
    def generate_optimization_suggestions(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("\n=== ç½‘ç«™ä¼˜åŒ–å»ºè®® ===")
        
        suggestions = []
        
        # åˆ†æé«˜ç½®ä¿¡åº¦ä½†ä½æ”¯æŒåº¦çš„è§„åˆ™
        for rule in self.navigation_rules:
            if rule['confidence'] > 0.7 and rule['support'] < 0.1:
                antecedent = ', '.join(rule['antecedent'])
                consequent = ', '.join(rule['consequent'])
                
                suggestion = f"åœ¨ '{antecedent}' é¡µé¢å¢åŠ åˆ° '{consequent}' çš„å¼•å¯¼é“¾æ¥"
                suggestions.append({
                    'suggestion': suggestion,
                    'confidence': rule['confidence'],
                    'potential_impact': rule['support'] * 1000  # ä¼°ç®—å½±å“ç”¨æˆ·æ•°
                })
        
        # æŒ‰æ½œåœ¨å½±å“æ’åº
        suggestions.sort(key=lambda x: x['potential_impact'], reverse=True)
        
        print("ä¼˜åŒ–å»ºè®® (æŒ‰æ½œåœ¨å½±å“æ’åº):")
        for i, suggestion in enumerate(suggestions[:10]):
            print(f"  {i+1}. {suggestion['suggestion']}")
            print(f"     ç½®ä¿¡åº¦: {suggestion['confidence']:.3f}, "
                  f"æ½œåœ¨å½±å“ç”¨æˆ·: {suggestion['potential_impact']:.0f}")

# ç½‘ç«™ç”¨æˆ·è¡Œä¸ºåˆ†ææ¼”ç¤º
web_analysis = WebUserBehaviorAnalysis(min_support=0.03, min_confidence=0.4)

# ç”Ÿæˆä¼šè¯æ•°æ®
web_sessions = web_analysis.generate_web_sessions(n_sessions=3000)
print(f"ç”Ÿæˆäº† {len(web_sessions)} ä¸ªç”¨æˆ·ä¼šè¯")
print(f"å‰3ä¸ªä¼šè¯ç¤ºä¾‹:")
for i, session in enumerate(web_sessions[:3]):
    print(f"  ä¼šè¯{i+1}: {' â†’ '.join(session)}")

# åˆ†æå¯¼èˆªæ¨¡å¼
navigation_rules = web_analysis.analyze_navigation_patterns(web_sessions)

# è¯†åˆ«ç”¨æˆ·è·¯å¾„
web_analysis.identify_user_paths()

# åˆ†æè½¬åŒ–æ¼æ–—
web_analysis.analyze_conversion_funnel(web_sessions)

# ç”Ÿæˆä¼˜åŒ–å»ºè®®
web_analysis.generate_optimization_suggestions()

# æ¼”ç¤ºé¡µé¢æ¨è
print("\n=== é¡µé¢æ¨èæ¼”ç¤º ===")
test_pages = [['é¦–é¡µ'], ['äº§å“åˆ—è¡¨', 'äº§å“è¯¦æƒ…'], ['è´­ç‰©è½¦']]

for pages in test_pages:
    print(f"\nå½“å‰é¡µé¢: {pages}")
    recommendations = web_analysis.recommend_next_pages(pages, max_recommendations=3)
    if recommendations:
        print("æ¨èä¸‹ä¸€é¡µé¢:")
        for i, (page, score) in enumerate(recommendations):
            print(f"  {i+1}. {page} (åˆ†æ•°: {score:.3f})")
    else:
        print("  æš‚æ— æ¨è")