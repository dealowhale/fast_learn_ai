# 1.3.3 ä¸»æˆåˆ†åˆ†æ (PCA)

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£ä¸»æˆåˆ†åˆ†æçš„æ•°å­¦åŸç†å’Œå‡ ä½•æ„ä¹‰
- æŒæ¡PCAçš„å®ç°æ­¥éª¤å’Œå…³é”®æŠ€æœ¯
- å­¦ä¼šä½¿ç”¨PCAè¿›è¡Œæ•°æ®é™ç»´å’Œç‰¹å¾æå–
- äº†è§£PCAåœ¨æ•°æ®å¯è§†åŒ–å’Œå™ªå£°å»é™¤ä¸­çš„åº”ç”¨
- å®ç°å®Œæ•´çš„PCAåˆ†æé¡¹ç›®

## 1. PCAåŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ä¸»æˆåˆ†åˆ†æ

ä¸»æˆåˆ†åˆ†æ(Principal Component Analysis, PCA)æ˜¯ä¸€ç§**æ— ç›‘ç£çš„çº¿æ€§é™ç»´æŠ€æœ¯**ï¼Œé€šè¿‡å¯»æ‰¾æ•°æ®ä¸­æ–¹å·®æœ€å¤§çš„æ–¹å‘ï¼ˆä¸»æˆåˆ†ï¼‰æ¥å®ç°æ•°æ®çš„ä½ç»´è¡¨ç¤ºã€‚

### 1.2 PCAçš„æ ¸å¿ƒæ€æƒ³

```mermaid
flowchart TD
    A[é«˜ç»´æ•°æ®] --> B[è®¡ç®—åæ–¹å·®çŸ©é˜µ]
    B --> C[ç‰¹å¾å€¼åˆ†è§£]
    C --> D[é€‰æ‹©ä¸»æˆåˆ†]
    D --> E[æ•°æ®æŠ•å½±]
    E --> F[ä½ç»´è¡¨ç¤º]
    
    G[ä¿æŒæœ€å¤§æ–¹å·®] --> D
    H[å»é™¤å†—ä½™ä¿¡æ¯] --> D
    I[é™ä½è®¡ç®—å¤æ‚åº¦] --> F
```

### 1.3 PCAçš„å‡ ä½•è§£é‡Š

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import make_blobs
import seaborn as sns

# ç”Ÿæˆ2Dç¤ºä¾‹æ•°æ®
np.random.seed(42)
X_2d = np.random.multivariate_normal([0, 0], [[3, 1.5], [1.5, 1]], 200)

# PCAåˆ†æ
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_2d)

# å¯è§†åŒ–PCAçš„å‡ ä½•æ„ä¹‰
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# åŸå§‹æ•°æ®
axes[0].scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7, s=50)
axes[0].set_title('åŸå§‹æ•°æ®åˆ†å¸ƒ', fontsize=14)
axes[0].set_xlabel('X1')
axes[0].set_ylabel('X2')
axes[0].grid(True, alpha=0.3)
axes[0].axis('equal')

# åŸå§‹æ•°æ® + ä¸»æˆåˆ†æ–¹å‘
axes[1].scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7, s=50)

# ç»˜åˆ¶ä¸»æˆåˆ†æ–¹å‘
mean = np.mean(X_2d, axis=0)
for i, (component, var) in enumerate(zip(pca.components_, pca.explained_variance_)):
    # ä¸»æˆåˆ†å‘é‡ï¼ˆæ”¾å¤§æ˜¾ç¤ºï¼‰
    axes[1].arrow(mean[0], mean[1], component[0]*np.sqrt(var)*2, component[1]*np.sqrt(var)*2,
                 head_width=0.2, head_length=0.3, fc=f'C{i+1}', ec=f'C{i+1}', linewidth=3,
                 label=f'PC{i+1} (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[i]:.2f})')

axes[1].set_title('ä¸»æˆåˆ†æ–¹å‘', fontsize=14)
axes[1].set_xlabel('X1')
axes[1].set_ylabel('X2')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].axis('equal')

# PCAå˜æ¢åçš„æ•°æ®
axes[2].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7, s=50)
axes[2].set_title('PCAå˜æ¢åçš„æ•°æ®', fontsize=14)
axes[2].set_xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
axes[2].set_ylabel('ç¬¬äºŒä¸»æˆåˆ†')
axes[2].grid(True, alpha=0.3)
axes[2].axis('equal')

plt.tight_layout()
plt.show()

print("PCAåˆ†æç»“æœï¼š")
print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")
print(f"ç´¯ç§¯è§£é‡Šæ–¹å·®: {np.cumsum(pca.explained_variance_ratio_)}")
print(f"ä¸»æˆåˆ†æ–¹å‘:")
for i, component in enumerate(pca.components_):
    print(f"  PC{i+1}: [{component[0]:.3f}, {component[1]:.3f}]")
```

## 2. PCAæ•°å­¦åŸç†

### 2.1 åæ–¹å·®çŸ©é˜µå’Œç‰¹å¾å€¼åˆ†è§£

**æ­¥éª¤1ï¼šæ•°æ®æ ‡å‡†åŒ–**
$$\mathbf{X}_{centered} = \mathbf{X} - \boldsymbol{\mu}$$

å…¶ä¸­ $\boldsymbol{\mu} = \frac{1}{n}\sum_{i=1}^{n} \mathbf{x}_i$ æ˜¯æ ·æœ¬å‡å€¼ã€‚

**æ­¥éª¤2ï¼šè®¡ç®—åæ–¹å·®çŸ©é˜µ**
$$\mathbf{C} = \frac{1}{n-1}\mathbf{X}_{centered}^T \mathbf{X}_{centered}$$

**æ­¥éª¤3ï¼šç‰¹å¾å€¼åˆ†è§£**
$$\mathbf{C} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^T$$

å…¶ä¸­ï¼š
- $\mathbf{V}$ æ˜¯ç‰¹å¾å‘é‡çŸ©é˜µï¼ˆä¸»æˆåˆ†æ–¹å‘ï¼‰
- $\mathbf{\Lambda}$ æ˜¯ç‰¹å¾å€¼å¯¹è§’çŸ©é˜µï¼ˆæ–¹å·®å¤§å°ï¼‰

**æ­¥éª¤4ï¼šé€‰æ‹©ä¸»æˆåˆ†**
é€‰æ‹©å‰ $k$ ä¸ªæœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ä½œä¸ºä¸»æˆåˆ†ã€‚

**æ­¥éª¤5ï¼šæ•°æ®æŠ•å½±**
$$\mathbf{Y} = \mathbf{X}_{centered} \mathbf{V}_k$$

### 2.2 PCAçš„ä¼˜åŒ–ç›®æ ‡

PCAç­‰ä»·äºæ±‚è§£ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜ï¼š

$$\max_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{C} \mathbf{w}}{\mathbf{w}^T \mathbf{w}}$$

çº¦æŸæ¡ä»¶ï¼š$||\mathbf{w}|| = 1$

è¿™ä¸ªé—®é¢˜çš„è§£å°±æ˜¯åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å‘é‡ã€‚

### 2.3 PCAå®ç°

```python
class SimplePCA:
    """ç®€å•çš„PCAå®ç°"""
    
    def __init__(self, n_components=None):
        self.n_components = n_components
        self.components_ = None
        self.explained_variance_ = None
        self.explained_variance_ratio_ = None
        self.mean_ = None
        
    def fit(self, X):
        """è®­ç»ƒPCAæ¨¡å‹"""
        # æ•°æ®ä¸­å¿ƒåŒ–
        self.mean_ = np.mean(X, axis=0)
        X_centered = X - self.mean_
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(X_centered.T)
        
        # ç‰¹å¾å€¼åˆ†è§£
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        
        # æŒ‰ç‰¹å¾å€¼å¤§å°æ’åºï¼ˆé™åºï¼‰
        idx = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        
        # é€‰æ‹©ä¸»æˆåˆ†æ•°é‡
        if self.n_components is None:
            self.n_components = len(eigenvalues)
        
        # ä¿å­˜ç»“æœ
        self.components_ = eigenvectors[:, :self.n_components].T
        self.explained_variance_ = eigenvalues[:self.n_components]
        self.explained_variance_ratio_ = self.explained_variance_ / np.sum(eigenvalues)
        
        return self
    
    def transform(self, X):
        """æ•°æ®å˜æ¢"""
        X_centered = X - self.mean_
        return np.dot(X_centered, self.components_.T)
    
    def fit_transform(self, X):
        """è®­ç»ƒå¹¶å˜æ¢"""
        return self.fit(X).transform(X)
    
    def inverse_transform(self, X_transformed):
        """é€†å˜æ¢"""
        return np.dot(X_transformed, self.components_) + self.mean_
    
    def get_covariance(self):
        """è·å–åæ–¹å·®çŸ©é˜µ"""
        return np.dot(self.components_.T * self.explained_variance_, self.components_)

# æµ‹è¯•SimplePCA
print("\n=== SimplePCA æµ‹è¯• ===")

# ç”Ÿæˆæµ‹è¯•æ•°æ®
np.random.seed(42)
X_test = np.random.multivariate_normal([0, 0, 0], 
                                      [[3, 1, 0.5], [1, 2, 0.3], [0.5, 0.3, 1]], 
                                      300)

# ä½¿ç”¨è‡ªå®ç°çš„PCA
simple_pca = SimplePCA(n_components=2)
X_simple_pca = simple_pca.fit_transform(X_test)

# ä½¿ç”¨sklearnçš„PCAå¯¹æ¯”
sklearn_pca = PCA(n_components=2)
X_sklearn_pca = sklearn_pca.fit_transform(X_test)

print("SimplePCAç»“æœï¼š")
print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {simple_pca.explained_variance_ratio_}")
print(f"ä¸»æˆåˆ†å½¢çŠ¶: {simple_pca.components_.shape}")

print("\nSklearn PCAç»“æœï¼š")
print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {sklearn_pca.explained_variance_ratio_}")
print(f"ä¸»æˆåˆ†å½¢çŠ¶: {sklearn_pca.components_.shape}")

# éªŒè¯ç»“æœä¸€è‡´æ€§ï¼ˆå¯èƒ½æœ‰ç¬¦å·å·®å¼‚ï¼‰
diff = np.abs(np.abs(simple_pca.explained_variance_ratio_) - 
              np.abs(sklearn_pca.explained_variance_ratio_))
print(f"\nè§£é‡Šæ–¹å·®æ¯”ä¾‹å·®å¼‚: {np.max(diff):.6f}")
```

## 3. PCAåº”ç”¨æ¡ˆä¾‹

### 3.1 å›¾åƒæ•°æ®é™ç»´

```python
# æ¨¡æ‹Ÿå›¾åƒæ•°æ®ï¼ˆæ‰‹å†™æ•°å­—ï¼‰
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†
digits = load_digits()
X_digits, y_digits = digits.data, digits.target

print("æ‰‹å†™æ•°å­—æ•°æ®é›†ä¿¡æ¯ï¼š")
print(f"æ ·æœ¬æ•°é‡: {X_digits.shape[0]}")
print(f"åŸå§‹ç‰¹å¾ç»´åº¦: {X_digits.shape[1]}")
print(f"ç±»åˆ«æ•°é‡: {len(np.unique(y_digits))}")

# å¯è§†åŒ–åŸå§‹å›¾åƒ
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
for i in range(10):
    row, col = i // 5, i % 5
    axes[row, col].imshow(digits.images[i], cmap='gray')
    axes[row, col].set_title(f'æ•°å­—: {y_digits[i]}')
    axes[row, col].axis('off')

plt.suptitle('åŸå§‹æ‰‹å†™æ•°å­—å›¾åƒ', fontsize=16)
plt.tight_layout()
plt.show()

# PCAé™ç»´åˆ†æ
class ImagePCAAnalysis:
    """å›¾åƒPCAåˆ†æç³»ç»Ÿ"""
    
    def __init__(self):
        self.pca_models = {}
        self.reconstruction_errors = {}
        self.classification_scores = {}
    
    def analyze_dimensions(self, X, y, max_components=100):
        """åˆ†æä¸åŒç»´åº¦çš„æ•ˆæœ"""
        # æµ‹è¯•ä¸åŒçš„ä¸»æˆåˆ†æ•°é‡
        n_components_list = [5, 10, 20, 30, 50, 100]
        
        print("\n=== PCAé™ç»´æ•ˆæœåˆ†æ ===")
        
        for n_comp in n_components_list:
            if n_comp > min(X.shape):
                continue
                
            # PCAé™ç»´
            pca = PCA(n_components=n_comp)
            X_pca = pca.fit_transform(X)
            
            # é‡æ„è¯¯å·®
            X_reconstructed = pca.inverse_transform(X_pca)
            reconstruction_error = np.mean((X - X_reconstructed) ** 2)
            
            # åˆ†ç±»æ€§èƒ½æµ‹è¯•
            X_train, X_test, y_train, y_test = train_test_split(
                X_pca, y, test_size=0.3, random_state=42, stratify=y
            )
            
            clf = LogisticRegression(max_iter=1000, random_state=42)
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            # ä¿å­˜ç»“æœ
            self.pca_models[n_comp] = pca
            self.reconstruction_errors[n_comp] = reconstruction_error
            self.classification_scores[n_comp] = accuracy
            
            print(f"ä¸»æˆåˆ†æ•°: {n_comp:3d} | "
                  f"è§£é‡Šæ–¹å·®: {np.sum(pca.explained_variance_ratio_):.3f} | "
                  f"é‡æ„è¯¯å·®: {reconstruction_error:.4f} | "
                  f"åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.3f}")
    
    def visualize_analysis(self):
        """å¯è§†åŒ–åˆ†æç»“æœ"""
        n_components = list(self.pca_models.keys())
        explained_variances = [np.sum(self.pca_models[n].explained_variance_ratio_) 
                             for n in n_components]
        reconstruction_errors = [self.reconstruction_errors[n] for n in n_components]
        classification_scores = [self.classification_scores[n] for n in n_components]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # è§£é‡Šæ–¹å·®æ¯”ä¾‹
        axes[0, 0].plot(n_components, explained_variances, 'bo-', linewidth=2, markersize=8)
        axes[0, 0].set_title('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹', fontsize=14)
        axes[0, 0].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
        axes[0, 0].set_ylabel('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95%é˜ˆå€¼')
        axes[0, 0].legend()
        
        # é‡æ„è¯¯å·®
        axes[0, 1].plot(n_components, reconstruction_errors, 'ro-', linewidth=2, markersize=8)
        axes[0, 1].set_title('é‡æ„è¯¯å·®', fontsize=14)
        axes[0, 1].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
        axes[0, 1].set_ylabel('å‡æ–¹é‡æ„è¯¯å·®')
        axes[0, 1].grid(True, alpha=0.3)
        
        # åˆ†ç±»å‡†ç¡®ç‡
        axes[1, 0].plot(n_components, classification_scores, 'go-', linewidth=2, markersize=8)
        axes[1, 0].set_title('åˆ†ç±»å‡†ç¡®ç‡', fontsize=14)
        axes[1, 0].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
        axes[1, 0].set_ylabel('å‡†ç¡®ç‡')
        axes[1, 0].grid(True, alpha=0.3)
        
        # ç»¼åˆåˆ†æï¼ˆå‡†ç¡®ç‡ vs å‹ç¼©æ¯”ï¼‰
        compression_ratios = [n/X_digits.shape[1] for n in n_components]
        axes[1, 1].scatter(compression_ratios, classification_scores, 
                          s=100, c=reconstruction_errors, cmap='viridis', alpha=0.7)
        axes[1, 1].set_title('å‡†ç¡®ç‡ vs å‹ç¼©æ¯”', fontsize=14)
        axes[1, 1].set_xlabel('å‹ç¼©æ¯” (ä¿ç•™ç»´åº¦/åŸå§‹ç»´åº¦)')
        axes[1, 1].set_ylabel('åˆ†ç±»å‡†ç¡®ç‡')
        axes[1, 1].grid(True, alpha=0.3)
        
        # æ·»åŠ é¢œè‰²æ¡
        scatter = axes[1, 1].collections[0]
        cbar = plt.colorbar(scatter, ax=axes[1, 1])
        cbar.set_label('é‡æ„è¯¯å·®')
        
        plt.tight_layout()
        plt.show()
    
    def visualize_components(self, n_components=20):
        """å¯è§†åŒ–ä¸»æˆåˆ†"""
        pca = self.pca_models[n_components]
        
        # é‡å¡‘ä¸»æˆåˆ†ä¸ºå›¾åƒå½¢çŠ¶
        components_images = pca.components_.reshape(n_components, 8, 8)
        
        fig, axes = plt.subplots(4, 5, figsize=(15, 12))
        axes = axes.ravel()
        
        for i in range(min(n_components, 20)):
            axes[i].imshow(components_images[i], cmap='RdBu_r')
            axes[i].set_title(f'PC{i+1}\næ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_[i]:.3f}')
            axes[i].axis('off')
        
        plt.suptitle(f'å‰{min(n_components, 20)}ä¸ªä¸»æˆåˆ†å¯è§†åŒ–', fontsize=16)
        plt.tight_layout()
        plt.show()
    
    def demonstrate_reconstruction(self, n_components=20, n_samples=10):
        """æ¼”ç¤ºå›¾åƒé‡æ„"""
        pca = self.pca_models[n_components]
        
        # é€‰æ‹©æ ·æœ¬è¿›è¡Œé‡æ„
        indices = np.random.choice(len(X_digits), n_samples, replace=False)
        original_images = X_digits[indices]
        
        # PCAå˜æ¢å’Œé€†å˜æ¢
        transformed = pca.transform(original_images)
        reconstructed = pca.inverse_transform(transformed)
        
        # å¯è§†åŒ–å¯¹æ¯”
        fig, axes = plt.subplots(3, n_samples, figsize=(2*n_samples, 6))
        
        for i in range(n_samples):
            # åŸå§‹å›¾åƒ
            axes[0, i].imshow(original_images[i].reshape(8, 8), cmap='gray')
            axes[0, i].set_title(f'åŸå§‹\næ ‡ç­¾: {y_digits[indices[i]]}')
            axes[0, i].axis('off')
            
            # é‡æ„å›¾åƒ
            axes[1, i].imshow(reconstructed[i].reshape(8, 8), cmap='gray')
            axes[1, i].set_title('é‡æ„')
            axes[1, i].axis('off')
            
            # å·®å¼‚å›¾åƒ
            diff = np.abs(original_images[i] - reconstructed[i])
            axes[2, i].imshow(diff.reshape(8, 8), cmap='Reds')
            axes[2, i].set_title(f'å·®å¼‚\nMSE: {np.mean(diff**2):.3f}')
            axes[2, i].axis('off')
        
        plt.suptitle(f'PCAå›¾åƒé‡æ„æ¼”ç¤º (ä¸»æˆåˆ†æ•°: {n_components})', fontsize=16)
        plt.tight_layout()
        plt.show()

# æ‰§è¡Œå›¾åƒPCAåˆ†æ
image_pca = ImagePCAAnalysis()
image_pca.analyze_dimensions(X_digits, y_digits)
image_pca.visualize_analysis()
image_pca.visualize_components(n_components=20)
image_pca.demonstrate_reconstruction(n_components=20, n_samples=8)
```

### 3.2 é«˜ç»´æ•°æ®å¯è§†åŒ–

```python
# é«˜ç»´æ•°æ®å¯è§†åŒ–æ¡ˆä¾‹
from sklearn.datasets import load_wine, load_breast_cancer
from mpl_toolkits.mplot3d import Axes3D

class HighDimVisualization:
    """é«˜ç»´æ•°æ®å¯è§†åŒ–ç³»ç»Ÿ"""
    
    def __init__(self):
        self.datasets = {
            'wine': load_wine(),
            'breast_cancer': load_breast_cancer()
        }
    
    def analyze_dataset(self, dataset_name):
        """åˆ†ææ•°æ®é›†"""
        data = self.datasets[dataset_name]
        X, y = data.data, data.target
        
        print(f"\n=== {dataset_name.upper()} æ•°æ®é›†åˆ†æ ===")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        print(f"ç±»åˆ«æ•°é‡: {len(np.unique(y))}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
        
        # æ ‡å‡†åŒ–æ•°æ®
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # PCAåˆ†æ
        pca_full = PCA()
        X_pca_full = pca_full.fit_transform(X_scaled)
        
        # å¯è§†åŒ–è§£é‡Šæ–¹å·®
        self._plot_explained_variance(pca_full, dataset_name)
        
        # 2Då’Œ3Då¯è§†åŒ–
        self._plot_2d_3d_visualization(X_scaled, y, data.target_names, dataset_name)
        
        return X_scaled, y, data.target_names
    
    def _plot_explained_variance(self, pca, dataset_name):
        """ç»˜åˆ¶è§£é‡Šæ–¹å·®å›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # ä¸ªä½“è§£é‡Šæ–¹å·®
        axes[0].bar(range(1, len(pca.explained_variance_ratio_) + 1), 
                   pca.explained_variance_ratio_, alpha=0.7)
        axes[0].set_title(f'{dataset_name} - å„ä¸»æˆåˆ†è§£é‡Šæ–¹å·®', fontsize=14)
        axes[0].set_xlabel('ä¸»æˆåˆ†')
        axes[0].set_ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
        axes[0].grid(True, alpha=0.3)
        
        # ç´¯ç§¯è§£é‡Šæ–¹å·®
        cumsum_var = np.cumsum(pca.explained_variance_ratio_)
        axes[1].plot(range(1, len(cumsum_var) + 1), cumsum_var, 'bo-', linewidth=2)
        axes[1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95%é˜ˆå€¼')
        axes[1].axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90%é˜ˆå€¼')
        axes[1].set_title(f'{dataset_name} - ç´¯ç§¯è§£é‡Šæ–¹å·®', fontsize=14)
        axes[1].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
        axes[1].set_ylabel('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # è¾“å‡ºå…³é”®ä¿¡æ¯
        n_95 = np.argmax(cumsum_var >= 0.95) + 1
        n_90 = np.argmax(cumsum_var >= 0.90) + 1
        print(f"è¾¾åˆ°90%è§£é‡Šæ–¹å·®éœ€è¦: {n_90} ä¸ªä¸»æˆåˆ†")
        print(f"è¾¾åˆ°95%è§£é‡Šæ–¹å·®éœ€è¦: {n_95} ä¸ªä¸»æˆåˆ†")
        print(f"å‰3ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {cumsum_var[2]:.3f}")
    
    def _plot_2d_3d_visualization(self, X, y, target_names, dataset_name):
        """2Då’Œ3Då¯è§†åŒ–"""
        # PCAé™ç»´
        pca_2d = PCA(n_components=2)
        pca_3d = PCA(n_components=3)
        
        X_2d = pca_2d.fit_transform(X)
        X_3d = pca_3d.fit_transform(X)
        
        fig = plt.figure(figsize=(18, 6))
        
        # 2Då¯è§†åŒ–
        ax1 = fig.add_subplot(131)
        colors = plt.cm.Set1(np.linspace(0, 1, len(target_names)))
        
        for i, (target, color) in enumerate(zip(target_names, colors)):
            mask = y == i
            ax1.scatter(X_2d[mask, 0], X_2d[mask, 1], 
                       c=[color], label=target, alpha=0.7, s=50)
        
        ax1.set_title(f'{dataset_name} - PCA 2Då¯è§†åŒ–\n'
                     f'è§£é‡Šæ–¹å·®: {np.sum(pca_2d.explained_variance_ratio_):.3f}', fontsize=14)
        ax1.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.3f})')
        ax1.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.3f})')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 3Då¯è§†åŒ–
        ax2 = fig.add_subplot(132, projection='3d')
        
        for i, (target, color) in enumerate(zip(target_names, colors)):
            mask = y == i
            ax2.scatter(X_3d[mask, 0], X_3d[mask, 1], X_3d[mask, 2],
                       c=[color], label=target, alpha=0.7, s=50)
        
        ax2.set_title(f'{dataset_name} - PCA 3Då¯è§†åŒ–\n'
                     f'è§£é‡Šæ–¹å·®: {np.sum(pca_3d.explained_variance_ratio_):.3f}', fontsize=14)
        ax2.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.3f})')
        ax2.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.3f})')
        ax2.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.3f})')
        ax2.legend()
        
        # ç‰¹å¾é‡è¦æ€§çƒ­å›¾
        ax3 = fig.add_subplot(133)
        
        # æ˜¾ç¤ºå‰10ä¸ªç‰¹å¾åœ¨å‰3ä¸ªä¸»æˆåˆ†ä¸­çš„æƒé‡
        n_features_show = min(10, X.shape[1])
        components_matrix = pca_3d.components_[:3, :n_features_show]
        
        im = ax3.imshow(components_matrix, cmap='RdBu_r', aspect='auto')
        ax3.set_title(f'{dataset_name} - ç‰¹å¾æƒé‡çƒ­å›¾', fontsize=14)
        ax3.set_xlabel('ç‰¹å¾ç´¢å¼•')
        ax3.set_ylabel('ä¸»æˆåˆ†')
        ax3.set_yticks(range(3))
        ax3.set_yticklabels(['PC1', 'PC2', 'PC3'])
        ax3.set_xticks(range(n_features_show))
        ax3.set_xticklabels(range(n_features_show))
        
        plt.colorbar(im, ax=ax3, label='æƒé‡')
        
        plt.tight_layout()
        plt.show()

# æ‰§è¡Œé«˜ç»´æ•°æ®å¯è§†åŒ–
vis_system = HighDimVisualization()

# åˆ†æWineæ•°æ®é›†
X_wine, y_wine, wine_names = vis_system.analyze_dataset('wine')

# åˆ†æBreast Canceræ•°æ®é›†
X_cancer, y_cancer, cancer_names = vis_system.analyze_dataset('breast_cancer')

### 3.3 PCAç”¨äºå™ªå£°å»é™¤

```python
class PCADenoising:
    """PCAå™ªå£°å»é™¤ç³»ç»Ÿ"""
    
    def __init__(self):
        self.pca_models = {}
    
    def add_noise_to_data(self, X, noise_level=0.1):
        """å‘æ•°æ®æ·»åŠ å™ªå£°"""
        noise = np.random.normal(0, noise_level, X.shape)
        return X + noise
    
    def denoise_comparison(self, X_clean, noise_levels=[0.05, 0.1, 0.2, 0.3]):
        """æ¯”è¾ƒä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„å»å™ªæ•ˆæœ"""
        print("\n=== PCAå™ªå£°å»é™¤æ•ˆæœåˆ†æ ===")
        
        results = {}
        
        for noise_level in noise_levels:
            # æ·»åŠ å™ªå£°
            X_noisy = self.add_noise_to_data(X_clean, noise_level)
            
            # æµ‹è¯•ä¸åŒä¸»æˆåˆ†æ•°é‡çš„å»å™ªæ•ˆæœ
            n_components_list = [5, 10, 20, 30]
            
            results[noise_level] = {}
            
            for n_comp in n_components_list:
                if n_comp > min(X_clean.shape):
                    continue
                
                # PCAå»å™ª
                pca = PCA(n_components=n_comp)
                X_pca = pca.fit_transform(X_noisy)
                X_denoised = pca.inverse_transform(X_pca)
                
                # è®¡ç®—å»å™ªæ•ˆæœ
                mse_noisy = np.mean((X_clean - X_noisy) ** 2)
                mse_denoised = np.mean((X_clean - X_denoised) ** 2)
                
                noise_reduction = (mse_noisy - mse_denoised) / mse_noisy * 100
                
                results[noise_level][n_comp] = {
                    'mse_noisy': mse_noisy,
                    'mse_denoised': mse_denoised,
                    'noise_reduction': noise_reduction
                }
                
                print(f"å™ªå£°æ°´å¹³: {noise_level:.2f} | ä¸»æˆåˆ†: {n_comp:2d} | "
                      f"å™ªå£°MSE: {mse_noisy:.4f} | å»å™ªMSE: {mse_denoised:.4f} | "
                      f"å™ªå£°å‡å°‘: {noise_reduction:.1f}%")
        
        return results
    
    def visualize_denoising(self, X_clean, noise_level=0.2, n_components=20):
        """å¯è§†åŒ–å»å™ªæ•ˆæœ"""
        # æ·»åŠ å™ªå£°
        X_noisy = self.add_noise_to_data(X_clean, noise_level)
        
        # PCAå»å™ª
        pca = PCA(n_components=n_components)
        X_pca = pca.fit_transform(X_noisy)
        X_denoised = pca.inverse_transform(X_pca)
        
        # é€‰æ‹©å‡ ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–ï¼ˆå‡è®¾æ˜¯å›¾åƒæ•°æ®ï¼‰
        if X_clean.shape[1] == 64:  # 8x8å›¾åƒ
            n_samples = 6
            indices = np.random.choice(len(X_clean), n_samples, replace=False)
            
            fig, axes = plt.subplots(4, n_samples, figsize=(2*n_samples, 8))
            
            for i, idx in enumerate(indices):
                # åŸå§‹å›¾åƒ
                axes[0, i].imshow(X_clean[idx].reshape(8, 8), cmap='gray')
                axes[0, i].set_title('åŸå§‹')
                axes[0, i].axis('off')
                
                # å™ªå£°å›¾åƒ
                axes[1, i].imshow(X_noisy[idx].reshape(8, 8), cmap='gray')
                axes[1, i].set_title(f'å™ªå£°\n(Ïƒ={noise_level})')
                axes[1, i].axis('off')
                
                # å»å™ªå›¾åƒ
                axes[2, i].imshow(X_denoised[idx].reshape(8, 8), cmap='gray')
                axes[2, i].set_title(f'å»å™ª\n(PC={n_components})')
                axes[2, i].axis('off')
                
                # å™ªå£°å·®å¼‚
                noise_diff = np.abs(X_clean[idx] - X_noisy[idx])
                denoised_diff = np.abs(X_clean[idx] - X_denoised[idx])
                
                axes[3, i].imshow((noise_diff - denoised_diff).reshape(8, 8), 
                                 cmap='RdBu_r')
                axes[3, i].set_title('æ”¹å–„ç¨‹åº¦')
                axes[3, i].axis('off')
            
            plt.suptitle(f'PCAå™ªå£°å»é™¤æ•ˆæœæ¼”ç¤º', fontsize=16)
            plt.tight_layout()
            plt.show()
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        mse_noisy = np.mean((X_clean - X_noisy) ** 2)
        mse_denoised = np.mean((X_clean - X_denoised) ** 2)
        noise_reduction = (mse_noisy - mse_denoised) / mse_noisy * 100
        
        print(f"\nå»å™ªæ•ˆæœç»Ÿè®¡:")
        print(f"åŸå§‹å™ªå£°MSE: {mse_noisy:.4f}")
        print(f"å»å™ªåMSE: {mse_denoised:.4f}")
        print(f"å™ªå£°å‡å°‘: {noise_reduction:.1f}%")
        print(f"ä¿¡å™ªæ¯”æ”¹å–„: {10*np.log10(mse_noisy/mse_denoised):.2f} dB")

# æ‰§è¡Œå™ªå£°å»é™¤æ¼”ç¤º
denoising_system = PCADenoising()

# ä½¿ç”¨æ‰‹å†™æ•°å­—æ•°æ®è¿›è¡Œå»å™ªæµ‹è¯•
denoising_results = denoising_system.denoise_comparison(X_digits[:100])  # ä½¿ç”¨éƒ¨åˆ†æ•°æ®
denoising_system.visualize_denoising(X_digits[:50], noise_level=0.15, n_components=20)

## 4. PCAçš„å˜ä½“å’Œæ‰©å±•

### 4.1 æ ¸PCA (Kernel PCA)

```python
from sklearn.decomposition import KernelPCA
from sklearn.datasets import make_circles, make_moons

class KernelPCADemo:
    """æ ¸PCAæ¼”ç¤º"""
    
    def __init__(self):
        self.kernels = ['linear', 'poly', 'rbf', 'sigmoid']
    
    def compare_pca_methods(self):
        """æ¯”è¾ƒçº¿æ€§PCAå’Œæ ¸PCA"""
        # ç”Ÿæˆéçº¿æ€§æ•°æ®
        datasets = {
            'circles': make_circles(n_samples=300, factor=0.3, noise=0.1, random_state=42),
            'moons': make_moons(n_samples=300, noise=0.1, random_state=42)
        }
        
        fig, axes = plt.subplots(len(datasets), len(self.kernels) + 1, 
                                figsize=(20, 8))
        
        for row, (dataset_name, (X, y)) in enumerate(datasets.items()):
            # åŸå§‹æ•°æ®
            axes[row, 0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
            axes[row, 0].set_title(f'{dataset_name} - åŸå§‹æ•°æ®')
            axes[row, 0].grid(True, alpha=0.3)
            
            # ä¸åŒæ ¸å‡½æ•°çš„PCA
            for col, kernel in enumerate(self.kernels):
                if kernel == 'linear':
                    # æ ‡å‡†PCA
                    pca = PCA(n_components=1)
                    X_transformed = pca.fit_transform(X)
                    # ä¸ºäº†å¯è§†åŒ–ï¼Œæ·»åŠ éšæœºyåæ ‡
                    X_plot = np.column_stack([X_transformed, 
                                            np.random.normal(0, 0.1, len(X_transformed))])
                else:
                    # æ ¸PCA
                    kpca = KernelPCA(n_components=2, kernel=kernel, gamma=1.0)
                    X_plot = kpca.fit_transform(X)
                
                axes[row, col + 1].scatter(X_plot[:, 0], X_plot[:, 1], 
                                         c=y, cmap='viridis', alpha=0.7)
                axes[row, col + 1].set_title(f'{kernel.upper()} PCA')
                axes[row, col + 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# æ‰§è¡Œæ ¸PCAæ¼”ç¤º
kernel_pca_demo = KernelPCADemo()
kernel_pca_demo.compare_pca_methods()
```

### 4.2 å¢é‡PCA (Incremental PCA)

```python
from sklearn.decomposition import IncrementalPCA

class IncrementalPCADemo:
    """å¢é‡PCAæ¼”ç¤º"""
    
    def __init__(self):
        self.batch_sizes = [50, 100, 200]
    
    def compare_batch_learning(self, X, n_components=10):
        """æ¯”è¾ƒæ‰¹é‡å­¦ä¹ å’Œå¢é‡å­¦ä¹ """
        print("\n=== å¢é‡PCA vs æ ‡å‡†PCA æ¯”è¾ƒ ===")
        
        # æ ‡å‡†PCA
        start_time = time.time()
        pca_standard = PCA(n_components=n_components)
        X_pca_standard = pca_standard.fit_transform(X)
        time_standard = time.time() - start_time
        
        results = {'standard': {
            'time': time_standard,
            'explained_variance_ratio': pca_standard.explained_variance_ratio_
        }}
        
        # å¢é‡PCA with different batch sizes
        for batch_size in self.batch_sizes:
            start_time = time.time()
            
            ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)
            
            # åˆ†æ‰¹è®­ç»ƒ
            for i in range(0, len(X), batch_size):
                batch = X[i:i+batch_size]
                ipca.partial_fit(batch)
            
            # å˜æ¢æ•°æ®
            X_ipca = ipca.transform(X)
            time_incremental = time.time() - start_time
            
            results[f'incremental_{batch_size}'] = {
                'time': time_incremental,
                'explained_variance_ratio': ipca.explained_variance_ratio_
            }
            
            print(f"æ‰¹å¤§å° {batch_size:3d}: æ—¶é—´ {time_incremental:.3f}ç§’")
        
        print(f"æ ‡å‡†PCA:     æ—¶é—´ {time_standard:.3f}ç§’")
        
        # æ¯”è¾ƒè§£é‡Šæ–¹å·®
        print("\nè§£é‡Šæ–¹å·®æ¯”ä¾‹å¯¹æ¯”:")
        for method, result in results.items():
            variance_sum = np.sum(result['explained_variance_ratio'])
            print(f"{method:20s}: {variance_sum:.4f}")
        
        return results
    
    def simulate_streaming_data(self, n_batches=10, batch_size=100, n_features=50):
        """æ¨¡æ‹Ÿæµå¼æ•°æ®å¤„ç†"""
        print("\n=== æµå¼æ•°æ®PCAå¤„ç†æ¼”ç¤º ===")
        
        # åˆå§‹åŒ–å¢é‡PCA
        ipca = IncrementalPCA(n_components=10, batch_size=batch_size)
        
        explained_variances = []
        
        for batch_idx in range(n_batches):
            # ç”Ÿæˆæ–°æ‰¹æ¬¡æ•°æ®
            batch_data = np.random.randn(batch_size, n_features)
            
            # å¢é‡å­¦ä¹ 
            ipca.partial_fit(batch_data)
            
            # è®°å½•è§£é‡Šæ–¹å·®
            if hasattr(ipca, 'explained_variance_ratio_'):
                explained_variances.append(np.sum(ipca.explained_variance_ratio_))
            
            print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx+1:2d}: "
                  f"ç´¯ç§¯æ ·æœ¬ {(batch_idx+1)*batch_size:4d}, "
                  f"è§£é‡Šæ–¹å·® {explained_variances[-1]:.4f}" if explained_variances else "åˆå§‹åŒ–ä¸­...")
        
        # å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹
        if explained_variances:
            plt.figure(figsize=(10, 6))
            plt.plot(range(1, len(explained_variances)+1), explained_variances, 
                    'bo-', linewidth=2, markersize=8)
            plt.title('å¢é‡PCAå­¦ä¹ è¿‡ç¨‹', fontsize=14)
            plt.xlabel('æ‰¹æ¬¡æ•°')
            plt.ylabel('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹')
            plt.grid(True, alpha=0.3)
            plt.show()

# æ‰§è¡Œå¢é‡PCAæ¼”ç¤º
incremental_demo = IncrementalPCADemo()
incremental_results = incremental_demo.compare_batch_learning(X_digits)
incremental_demo.simulate_streaming_data()

## 5. Traeé£æ ¼PCAå®ç°

```python
class TraePCA:
    """Traeé£æ ¼çš„PCAå®ç°"""
    
    def __init__(self, n_components=None, whiten=False, verbose=True):
        self.n_components = n_components
        self.whiten = whiten
        self.verbose = verbose
        
        # Traeç‰¹è‰²ï¼šè¯¦ç»†çš„åˆ†æå†å²
        self.analysis_history = {
            'fit_time': None,
            'transform_time': None,
            'explained_variance_progression': [],
            'component_stability': [],
            'reconstruction_quality': {}
        }
    
    def trae_fit(self, X, feature_names=None, sample_names=None):
        """Traeé£æ ¼çš„è®­ç»ƒæ–¹æ³•"""
        if self.verbose:
            print("ğŸ” Trae PCA åˆ†æå¼€å§‹")
            print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        
        self.X_original_ = X.copy()
        self.feature_names_ = feature_names or [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]
        self.sample_names_ = sample_names or [f'æ ·æœ¬_{i+1}' for i in range(X.shape[0])]
        
        # æ•°æ®é¢„å¤„ç†
        start_time = time.time()
        self.mean_ = np.mean(X, axis=0)
        X_centered = X - self.mean_
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        if self.verbose:
            print("ğŸ§® è®¡ç®—åæ–¹å·®çŸ©é˜µ...")
        
        cov_matrix = np.cov(X_centered.T)
        
        # ç‰¹å¾å€¼åˆ†è§£
        if self.verbose:
            print("ğŸ”¢ æ‰§è¡Œç‰¹å¾å€¼åˆ†è§£...")
        
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        
        # æ’åº
        idx = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        
        # ç¡®å®šä¸»æˆåˆ†æ•°é‡
        if self.n_components is None:
            # è‡ªåŠ¨é€‰æ‹©ï¼ˆä¿ç•™95%æ–¹å·®ï¼‰
            cumsum_var = np.cumsum(eigenvalues) / np.sum(eigenvalues)
            self.n_components = np.argmax(cumsum_var >= 0.95) + 1
            if self.verbose:
                print(f"ğŸ¯ è‡ªåŠ¨é€‰æ‹©ä¸»æˆåˆ†æ•°: {self.n_components} (ä¿ç•™95%æ–¹å·®)")
        
        # ä¿å­˜ç»“æœ
        self.components_ = eigenvectors[:, :self.n_components].T
        self.explained_variance_ = eigenvalues[:self.n_components]
        self.explained_variance_ratio_ = self.explained_variance_ / np.sum(eigenvalues)
        
        # ç™½åŒ–å¤„ç†
        if self.whiten:
            self.components_ = self.components_ / np.sqrt(self.explained_variance_[:, np.newaxis])
        
        self.analysis_history['fit_time'] = time.time() - start_time
        
        if self.verbose:
            print(f"âœ… PCAè®­ç»ƒå®Œæˆ! è®­ç»ƒæ—¶é—´: {self.analysis_history['fit_time']:.3f}ç§’")
            print(f"ğŸ“ˆ ä¿ç•™ä¸»æˆåˆ†æ•°: {self.n_components}")
            print(f"ğŸ“Š ç´¯ç§¯è§£é‡Šæ–¹å·®: {np.sum(self.explained_variance_ratio_):.4f}")
        
        return self
    
    def trae_transform(self, X):
        """Traeé£æ ¼çš„å˜æ¢æ–¹æ³•"""
        start_time = time.time()
        
        X_centered = X - self.mean_
        X_transformed = np.dot(X_centered, self.components_.T)
        
        if self.whiten:
            X_transformed = X_transformed / np.sqrt(self.explained_variance_)
        
        self.analysis_history['transform_time'] = time.time() - start_time
        
        if self.verbose:
            print(f"ğŸ”„ æ•°æ®å˜æ¢å®Œæˆ! å˜æ¢æ—¶é—´: {self.analysis_history['transform_time']:.3f}ç§’")
        
        return X_transformed
    
    def trae_fit_transform(self, X, feature_names=None, sample_names=None):
        """è®­ç»ƒå¹¶å˜æ¢"""
        return self.trae_fit(X, feature_names, sample_names).trae_transform(X)
    
    def trae_analyze_components(self, top_features=5):
        """åˆ†æä¸»æˆåˆ†"""
        print("\nğŸ” Trae ä¸»æˆåˆ†è¯¦ç»†åˆ†æ")
        print("=" * 60)
        
        component_analysis = {}
        
        for i in range(self.n_components):
            print(f"\nğŸ“‹ ä¸»æˆåˆ† {i+1}:")
            print(f"   è§£é‡Šæ–¹å·®: {self.explained_variance_[i]:.4f}")
            print(f"   è§£é‡Šæ–¹å·®æ¯”ä¾‹: {self.explained_variance_ratio_[i]:.4f} ({self.explained_variance_ratio_[i]*100:.1f}%)")
            
            # æ‰¾å‡ºè´¡çŒ®æœ€å¤§çš„ç‰¹å¾
            component_weights = np.abs(self.components_[i])
            top_indices = np.argsort(component_weights)[::-1][:top_features]
            
            print(f"   ä¸»è¦è´¡çŒ®ç‰¹å¾:")
            for j, idx in enumerate(top_indices):
                weight = self.components_[i, idx]
                print(f"     {j+1}. {self.feature_names_[idx]}: {weight:+.4f}")
            
            component_analysis[f'PC{i+1}'] = {
                'explained_variance': self.explained_variance_[i],
                'explained_variance_ratio': self.explained_variance_ratio_[i],
                'top_features': [(self.feature_names_[idx], self.components_[i, idx]) 
                               for idx in top_indices]
            }
        
        return component_analysis
    
    def trae_reconstruction_analysis(self, X, n_samples=5):
        """é‡æ„è´¨é‡åˆ†æ"""
        print("\nğŸ”§ Trae é‡æ„è´¨é‡åˆ†æ")
        print("=" * 50)
        
        # å˜æ¢å’Œé€†å˜æ¢
        X_transformed = self.trae_transform(X)
        X_reconstructed = self.trae_inverse_transform(X_transformed)
        
        # è®¡ç®—é‡æ„è¯¯å·®
        reconstruction_errors = np.mean((X - X_reconstructed) ** 2, axis=1)
        overall_mse = np.mean(reconstruction_errors)
        
        print(f"ğŸ“Š æ•´ä½“é‡æ„è´¨é‡:")
        print(f"   å¹³å‡é‡æ„è¯¯å·® (MSE): {overall_mse:.6f}")
        print(f"   é‡æ„è¯¯å·®æ ‡å‡†å·®: {np.std(reconstruction_errors):.6f}")
        print(f"   æœ€å¤§é‡æ„è¯¯å·®: {np.max(reconstruction_errors):.6f}")
        print(f"   æœ€å°é‡æ„è¯¯å·®: {np.min(reconstruction_errors):.6f}")
        
        # åˆ†æé‡æ„è¯¯å·®åˆ†å¸ƒ
        percentiles = [50, 75, 90, 95, 99]
        print(f"\nğŸ“ˆ é‡æ„è¯¯å·®åˆ†ä½æ•°:")
        for p in percentiles:
            value = np.percentile(reconstruction_errors, p)
            print(f"   {p:2d}%: {value:.6f}")
        
        # æ‰¾å‡ºé‡æ„è´¨é‡æœ€å¥½å’Œæœ€å·®çš„æ ·æœ¬
        best_indices = np.argsort(reconstruction_errors)[:n_samples]
        worst_indices = np.argsort(reconstruction_errors)[-n_samples:]
        
        print(f"\nğŸ† é‡æ„è´¨é‡æœ€å¥½çš„{n_samples}ä¸ªæ ·æœ¬:")
        for i, idx in enumerate(best_indices):
            print(f"   {i+1}. {self.sample_names_[idx]}: MSE = {reconstruction_errors[idx]:.6f}")
        
        print(f"\nâš ï¸  é‡æ„è´¨é‡æœ€å·®çš„{n_samples}ä¸ªæ ·æœ¬:")
        for i, idx in enumerate(worst_indices):
            print(f"   {i+1}. {self.sample_names_[idx]}: MSE = {reconstruction_errors[idx]:.6f}")
        
        self.analysis_history['reconstruction_quality'] = {
            'overall_mse': overall_mse,
            'mse_std': np.std(reconstruction_errors),
            'best_samples': [(self.sample_names_[idx], reconstruction_errors[idx]) 
                           for idx in best_indices],
            'worst_samples': [(self.sample_names_[idx], reconstruction_errors[idx]) 
                            for idx in worst_indices]
        }
        
        return reconstruction_errors
    
    def trae_inverse_transform(self, X_transformed):
        """é€†å˜æ¢"""
        if self.whiten:
            X_transformed = X_transformed * np.sqrt(self.explained_variance_)
        
        return np.dot(X_transformed, self.components_) + self.mean_
    
    def trae_visualize_analysis(self, X, max_components=10):
        """å¯è§†åŒ–åˆ†æç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # è§£é‡Šæ–¹å·®å›¾
        n_show = min(max_components, len(self.explained_variance_ratio_))
        axes[0, 0].bar(range(1, n_show + 1), self.explained_variance_ratio_[:n_show], 
                      alpha=0.7, color='skyblue')
        axes[0, 0].set_title('ğŸ” å„ä¸»æˆåˆ†è§£é‡Šæ–¹å·®', fontsize=14)
        axes[0, 0].set_xlabel('ä¸»æˆåˆ†')
        axes[0, 0].set_ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ç´¯ç§¯è§£é‡Šæ–¹å·®
        cumsum_var = np.cumsum(self.explained_variance_ratio_)
        axes[0, 1].plot(range(1, len(cumsum_var) + 1), cumsum_var, 
                       'bo-', linewidth=2, markersize=6)
        axes[0, 1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95%')
        axes[0, 1].axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90%')
        axes[0, 1].set_title('ğŸ“ˆ ç´¯ç§¯è§£é‡Šæ–¹å·®', fontsize=14)
        axes[0, 1].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
        axes[0, 1].set_ylabel('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # ä¸»æˆåˆ†æƒé‡çƒ­å›¾
        n_features_show = min(20, len(self.feature_names_))
        n_components_show = min(5, self.n_components)
        
        weights_matrix = self.components_[:n_components_show, :n_features_show]
        im = axes[1, 0].imshow(weights_matrix, cmap='RdBu_r', aspect='auto')
        axes[1, 0].set_title('ğŸ¯ ä¸»æˆåˆ†æƒé‡çƒ­å›¾', fontsize=14)
        axes[1, 0].set_xlabel('ç‰¹å¾')
        axes[1, 0].set_ylabel('ä¸»æˆåˆ†')
        axes[1, 0].set_yticks(range(n_components_show))
        axes[1, 0].set_yticklabels([f'PC{i+1}' for i in range(n_components_show)])
        plt.colorbar(im, ax=axes[1, 0], label='æƒé‡')
        
        # é‡æ„è¯¯å·®åˆ†æ
        reconstruction_errors = self.trae_reconstruction_analysis(X, n_samples=3)
        axes[1, 1].hist(reconstruction_errors, bins=30, alpha=0.7, color='lightcoral')
        axes[1, 1].axvline(np.mean(reconstruction_errors), color='red', 
                          linestyle='--', label=f'å‡å€¼: {np.mean(reconstruction_errors):.4f}')
        axes[1, 1].set_title('ğŸ”§ é‡æ„è¯¯å·®åˆ†å¸ƒ', fontsize=14)
        axes[1, 1].set_xlabel('é‡æ„è¯¯å·® (MSE)')
        axes[1, 1].set_ylabel('é¢‘æ¬¡')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def trae_export_results(self, filename=None):
        """å¯¼å‡ºåˆ†æç»“æœ"""
        results = {
            'analysis_summary': {
                'n_components': self.n_components,
                'total_explained_variance': float(np.sum(self.explained_variance_ratio_)),
                'fit_time': self.analysis_history['fit_time'],
                'transform_time': self.analysis_history.get('transform_time', None)
            },
            'components': {
                f'PC{i+1}': {
                    'explained_variance': float(self.explained_variance_[i]),
                    'explained_variance_ratio': float(self.explained_variance_ratio_[i]),
                    'weights': self.components_[i].tolist()
                } for i in range(self.n_components)
            },
            'feature_names': self.feature_names_,
            'reconstruction_quality': self.analysis_history.get('reconstruction_quality', {})
        }
        
        if filename:
            import json
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“ åˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
        
        return results

# Trae PCAæ¼”ç¤º
print("\nğŸ” Trae PCA ç³»ç»Ÿæ¼”ç¤º")
print("=" * 60)

# ä½¿ç”¨Wineæ•°æ®é›†è¿›è¡Œæ¼”ç¤º
trae_pca = TraePCA(n_components=None, verbose=True)

# è®­ç»ƒæ¨¡å‹
X_wine_transformed = trae_pca.trae_fit_transform(
    X_wine, 
    feature_names=[f'ç‰¹å¾_{i+1}' for i in range(X_wine.shape[1])],
    sample_names=[f'æ ·æœ¬_{i+1}' for i in range(X_wine.shape[0])]
)

# è¯¦ç»†åˆ†æ
component_analysis = trae_pca.trae_analyze_components(top_features=3)
reconstruction_errors = trae_pca.trae_reconstruction_analysis(X_wine, n_samples=3)

# å¯è§†åŒ–åˆ†æ
trae_pca.trae_visualize_analysis(X_wine)

# å¯¼å‡ºç»“æœ
results = trae_pca.trae_export_results()
print(f"\nğŸ“‹ åˆ†æç»“æœæ‘˜è¦:")
print(f"ä¸»æˆåˆ†æ•°é‡: {results['analysis_summary']['n_components']}")
print(f"è§£é‡Šæ–¹å·®: {results['analysis_summary']['total_explained_variance']:.4f}")
print(f"è®­ç»ƒæ—¶é—´: {results['analysis_summary']['fit_time']:.3f}ç§’")

## 6. æ€è€ƒé¢˜

1. **ä¸»æˆåˆ†é€‰æ‹©**: å¦‚ä½•ç¡®å®šæœ€ä¼˜çš„ä¸»æˆåˆ†æ•°é‡ï¼Ÿé™¤äº†è§£é‡Šæ–¹å·®æ¯”ä¾‹ï¼Œè¿˜æœ‰å“ªäº›åˆ¤æ–­æ ‡å‡†ï¼Ÿ

2. **æ•°æ®é¢„å¤„ç†**: ä¸ºä»€ä¹ˆPCAå‰éœ€è¦å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Ÿä»€ä¹ˆæƒ…å†µä¸‹å¯ä»¥ä¸æ ‡å‡†åŒ–ï¼Ÿ

3. **å‡ ä½•è§£é‡Š**: PCAçš„å‡ ä½•æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿä¸»æˆåˆ†æ–¹å‘ä¸æ•°æ®åˆ†å¸ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ

4. **åº”ç”¨åœºæ™¯**: PCAé€‚ç”¨äºå“ªäº›åœºæ™¯ï¼Ÿä»€ä¹ˆæƒ…å†µä¸‹ä¸é€‚åˆä½¿ç”¨PCAï¼Ÿ

5. **æ ¸PCAä¼˜åŠ¿**: æ ¸PCAç›¸æ¯”çº¿æ€§PCAæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿå¦‚ä½•é€‰æ‹©åˆé€‚çš„æ ¸å‡½æ•°ï¼Ÿ

## 7. å°ç»“

### 7.1 æ ¸å¿ƒä¼˜åŠ¿

- **é™ç»´æ•ˆæœ**: æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼Œä¿ç•™ä¸»è¦ä¿¡æ¯
- **å™ªå£°å»é™¤**: é€šè¿‡ä¿ç•™ä¸»è¦æˆåˆ†å»é™¤æ•°æ®å™ªå£°
- **æ•°æ®å¯è§†åŒ–**: å°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°2D/3Dç©ºé—´è¿›è¡Œå¯è§†åŒ–
- **è®¡ç®—æ•ˆç‡**: é™ä½åç»­ç®—æ³•çš„è®¡ç®—å¤æ‚åº¦
- **ç‰¹å¾æå–**: å‘ç°æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼å’Œç»“æ„

### 7.2 å…³é”®æŠ€æœ¯

- **åæ–¹å·®çŸ©é˜µ**: æè¿°ç‰¹å¾é—´çš„çº¿æ€§å…³ç³»
- **ç‰¹å¾å€¼åˆ†è§£**: æ‰¾åˆ°æ•°æ®çš„ä¸»è¦å˜åŒ–æ–¹å‘
- **æ–¹å·®æœ€å¤§åŒ–**: ä¿ç•™æ•°æ®ä¸­çš„æœ€å¤§æ–¹å·®ä¿¡æ¯
- **æ­£äº¤å˜æ¢**: ç¡®ä¿ä¸»æˆåˆ†ä¹‹é—´ç›¸äº’ç‹¬ç«‹
- **ç™½åŒ–å¤„ç†**: ä½¿å˜æ¢åçš„æ•°æ®å…·æœ‰å•ä½æ–¹å·®

### 7.3 å®é™…åº”ç”¨

- **å›¾åƒå¤„ç†**: å›¾åƒå‹ç¼©ã€ç‰¹å¾æå–ã€äººè„¸è¯†åˆ«
- **æ•°æ®å¯è§†åŒ–**: é«˜ç»´æ•°æ®çš„2D/3Då¯è§†åŒ–
- **ç‰¹å¾å·¥ç¨‹**: é™ç»´ã€å»å™ªã€ç‰¹å¾é€‰æ‹©
- **ç”Ÿç‰©ä¿¡æ¯å­¦**: åŸºå› è¡¨è¾¾åˆ†æã€è›‹ç™½è´¨ç»“æ„åˆ†æ
- **é‡‘èåˆ†æ**: é£é™©å› å­åˆ†æã€æŠ•èµ„ç»„åˆä¼˜åŒ–

### 7.4 å±€é™æ€§

- **çº¿æ€§å‡è®¾**: åªèƒ½æ•è·çº¿æ€§å…³ç³»ï¼Œå¯¹éçº¿æ€§ç»“æ„æ•ˆæœæœ‰é™
- **è§£é‡Šæ€§**: ä¸»æˆåˆ†é€šå¸¸éš¾ä»¥ç›´æ¥è§£é‡Šå…¶ç‰©ç†æ„ä¹‰
- **å…¨å±€æ–¹æ³•**: éœ€è¦è®¿é—®å…¨éƒ¨æ•°æ®ï¼Œä¸é€‚åˆåœ¨çº¿å­¦ä¹ 
- **å¼‚å¸¸å€¼æ•æ„Ÿ**: å®¹æ˜“å—åˆ°å¼‚å¸¸å€¼å½±å“
- **æ–¹å·®å¯¼å‘**: å¯èƒ½ä¿ç•™å™ªå£°è€Œä¸¢å¤±æœ‰ç”¨çš„ä½æ–¹å·®ä¿¡æ¯

### 7.5 ä½¿ç”¨å»ºè®®

1. **æ•°æ®é¢„å¤„ç†**: æ ¹æ®ç‰¹å¾å°ºåº¦å†³å®šæ˜¯å¦æ ‡å‡†åŒ–
2. **ä¸»æˆåˆ†é€‰æ‹©**: ç»“åˆè§£é‡Šæ–¹å·®ã€è‚˜éƒ¨æ³•åˆ™å’Œä¸šåŠ¡éœ€æ±‚
3. **ç»“æœéªŒè¯**: é€šè¿‡é‡æ„è¯¯å·®å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½éªŒè¯æ•ˆæœ
4. **å¯è§†åŒ–åˆ†æ**: å……åˆ†åˆ©ç”¨ä¸»æˆåˆ†æƒé‡å’ŒæŠ•å½±ç»“æœ
5. **å˜ä½“é€‰æ‹©**: æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„PCAå˜ä½“

### 7.6 ä¸‹ä¸€æ­¥å­¦ä¹ 

- **éçº¿æ€§é™ç»´**: t-SNEã€UMAPç­‰éçº¿æ€§é™ç»´æ–¹æ³•
- **æ¦‚ç‡PCA**: åŸºäºæ¦‚ç‡æ¨¡å‹çš„PCAæ‰©å±•
- **ç¨€ç–PCA**: å…·æœ‰ç¨€ç–æ€§çº¦æŸçš„ä¸»æˆåˆ†åˆ†æ
- **ç›‘ç£é™ç»´**: LDAã€CCAç­‰ç›‘ç£å­¦ä¹ é™ç»´æ–¹æ³•
- **æ·±åº¦å­¦ä¹ **: è‡ªç¼–ç å™¨ç­‰æ·±åº¦å­¦ä¹ é™ç»´æŠ€æœ¯

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å·²ç»æŒæ¡äº†PCAçš„æ ¸å¿ƒåŸç†å’Œå®è·µæŠ€èƒ½ã€‚PCAä½œä¸ºæœ€ç»å…¸çš„é™ç»´æŠ€æœ¯ï¼Œåœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚
```