# 1.2.2 逻辑回归

## 学习目标
理解逻辑回归的数学原理，掌握分类问题的建模方法，学会评估分类模型的性能。

## 引言：从回归到分类

假设你是一家银行的风控专家，需要判断一个贷款申请是否会违约：

- **输入**：客户年龄、收入、信用历史、负债比例等
- **输出**：违约（1）或不违约（0）

这不再是预测连续数值，而是预测**类别**。线性回归直接输出连续值，但我们需要的是概率和类别。

**逻辑回归**就是解决这个问题的经典算法！

## 什么是逻辑回归？

**逻辑回归 (Logistic Regression)** 是一种用于分类问题的线性模型。尽管名字叫"回归"，但它实际上是分类算法。

### 核心思想

1. **线性组合**：像线性回归一样计算特征的线性组合
2. **Sigmoid变换**：将线性输出转换为0-1之间的概率
3. **概率解释**：输出可以解释为属于正类的概率

### 数学表达

**线性部分**：
```
z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ
```

**Sigmoid函数**：
```
p = σ(z) = 1 / (1 + e^(-z))
```

**预测规则**：
```
ŷ = 1 if p ≥ 0.5 else 0
```

### Sigmoid函数的特性

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def sigmoid(z):
    """Sigmoid函数"""
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # 防止溢出

# 可视化Sigmoid函数
z = np.linspace(-10, 10, 100)
p = sigmoid(z)

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Sigmoid函数图像
axes[0].plot(z, p, 'b-', linewidth=3, label='σ(z) = 1/(1+e^(-z))')
axes[0].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='决策边界')
axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
axes[0].set_xlabel('z (线性输出)')
axes[0].set_ylabel('p (概率)')
axes[0].set_title('Sigmoid函数')
axes[0].grid(True, alpha=0.3)
axes[0].legend()
axes[0].set_ylim(-0.1, 1.1)

# Sigmoid函数的导数
def sigmoid_derivative(z):
    s = sigmoid(z)
    return s * (1 - s)

derivative = sigmoid_derivative(z)
axes[1].plot(z, derivative, 'g-', linewidth=3, label="σ'(z) = σ(z)(1-σ(z))")
axes[1].set_xlabel('z')
axes[1].set_ylabel("σ'(z)")
axes[1].set_title('Sigmoid函数的导数')
axes[1].grid(True, alpha=0.3)
axes[1].legend()

# 不同参数的影响
z_demo = np.linspace(-5, 5, 100)
for beta in [0.5, 1, 2, 5]:
    p_demo = sigmoid(beta * z_demo)
    axes[2].plot(z_demo, p_demo, linewidth=2, label=f'β = {beta}')

axes[2].axhline(y=0.5, color='red', linestyle='--', alpha=0.7)
axes[2].set_xlabel('z')
axes[2].set_ylabel('p')
axes[2].set_title('不同系数对Sigmoid的影响')
axes[2].grid(True, alpha=0.3)
axes[2].legend()

plt.tight_layout()
plt.show()

print("Sigmoid函数特性:")
print("1. 输出范围: (0, 1)")
print("2. 单调递增")
print("3. 在z=0处，p=0.5")
print("4. 关于点(0, 0.5)中心对称")
print("5. 导数最大值在z=0处")
```

## 逻辑回归的推导

### 从线性回归到逻辑回归

**问题**：线性回归的输出可能超出[0,1]范围，无法表示概率。

**解决方案**：使用**几率 (Odds)** 和**对数几率 (Log-Odds)**。

### 几率和对数几率

**几率**：
```
Odds = p / (1 - p)
```

**对数几率 (Logit)**：
```
logit(p) = ln(p / (1 - p)) = z
```

**逆变换**：
```
p = 1 / (1 + e^(-z))
```

```python
# 可视化几率和对数几率
p_values = np.linspace(0.01, 0.99, 100)
odds = p_values / (1 - p_values)
log_odds = np.log(odds)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# 概率 vs 几率
axes[0].plot(p_values, odds, 'b-', linewidth=2)
axes[0].set_xlabel('概率 p')
axes[0].set_ylabel('几率 p/(1-p)')
axes[0].set_title('概率与几率的关系')
axes[0].grid(True, alpha=0.3)
axes[0].set_yscale('log')

# 概率 vs 对数几率
axes[1].plot(p_values, log_odds, 'r-', linewidth=2)
axes[1].set_xlabel('概率 p')
axes[1].set_ylabel('对数几率 ln(p/(1-p))')
axes[1].set_title('概率与对数几率的关系')
axes[1].grid(True, alpha=0.3)
axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
axes[1].axvline(x=0.5, color='black', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

print("几率解释:")
print("- 几率 = 1: 正负类概率相等 (p = 0.5)")
print("- 几率 > 1: 正类概率更大")
print("- 几率 < 1: 负类概率更大")
print("\n对数几率解释:")
print("- 对数几率 = 0: 正负类概率相等")
print("- 对数几率 > 0: 正类概率更大")
print("- 对数几率 < 0: 负类概率更大")
```

## 最大似然估计

逻辑回归使用**最大似然估计 (Maximum Likelihood Estimation, MLE)** 来求解参数。

### 似然函数

对于单个样本：
```
P(y|x) = p^y × (1-p)^(1-y)
```

对于所有样本：
```
L(β) = ∏ᵢ P(yᵢ|xᵢ) = ∏ᵢ pᵢ^yᵢ × (1-pᵢ)^(1-yᵢ)
```

### 对数似然函数

```
ℓ(β) = ln L(β) = Σᵢ [yᵢ ln(pᵢ) + (1-yᵢ) ln(1-pᵢ)]
```

### 损失函数（负对数似然）

```
J(β) = -ℓ(β) = -Σᵢ [yᵢ ln(pᵢ) + (1-yᵢ) ln(1-pᵢ)]
```

这就是**交叉熵损失函数**！

```python
class LogisticRegressionFromScratch:
    """从零实现逻辑回归"""
    
    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.tolerance = tolerance
        self.cost_history = []
        
    def _add_bias(self, X):
        """添加偏置项"""
        return np.column_stack([np.ones(X.shape[0]), X])
    
    def _sigmoid(self, z):
        """Sigmoid函数"""
        # 防止数值溢出
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    
    def _compute_cost(self, y_true, y_prob):
        """计算交叉熵损失"""
        # 防止log(0)
        y_prob = np.clip(y_prob, 1e-15, 1 - 1e-15)
        cost = -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))
        return cost
    
    def _compute_gradients(self, X, y_true, y_prob):
        """计算梯度"""
        m = X.shape[0]
        gradients = (1 / m) * X.T @ (y_prob - y_true)
        return gradients
    
    def fit(self, X, y):
        """训练模型"""
        # 添加偏置项
        X_with_bias = self._add_bias(X)
        
        # 初始化参数
        n_features = X_with_bias.shape[1]
        self.theta = np.random.normal(0, 0.01, n_features)
        
        # 梯度下降
        for i in range(self.max_iterations):
            # 前向传播
            z = X_with_bias @ self.theta
            y_prob = self._sigmoid(z)
            
            # 计算损失
            cost = self._compute_cost(y, y_prob)
            self.cost_history.append(cost)
            
            # 计算梯度
            gradients = self._compute_gradients(X_with_bias, y, y_prob)
            
            # 更新参数
            new_theta = self.theta - self.learning_rate * gradients
            
            # 检查收敛
            if np.linalg.norm(new_theta - self.theta) < self.tolerance:
                print(f"在第 {i+1} 次迭代时收敛")
                break
                
            self.theta = new_theta
        
        # 分离截距和系数
        self.intercept_ = self.theta[0]
        self.coef_ = self.theta[1:]
        
        return self
    
    def predict_proba(self, X):
        """预测概率"""
        X_with_bias = self._add_bias(X)
        z = X_with_bias @ self.theta
        return self._sigmoid(z)
    
    def predict(self, X, threshold=0.5):
        """预测类别"""
        probabilities = self.predict_proba(X)
        return (probabilities >= threshold).astype(int)
    
    def plot_cost_history(self):
        """绘制损失函数变化"""
        plt.figure(figsize=(10, 6))
        plt.plot(self.cost_history, linewidth=2)
        plt.xlabel('迭代次数')
        plt.ylabel('交叉熵损失')
        plt.title('逻辑回归收敛过程')
        plt.grid(True, alpha=0.3)
        plt.show()

# 生成二分类数据
np.random.seed(42)
n_samples = 1000

# 特征：年龄、收入
age = np.random.uniform(18, 80, n_samples)
income = np.random.uniform(20, 200, n_samples)  # 收入(千元)

# 标签：是否违约 (年龄越大、收入越低，违约概率越高)
logit = -2 + 0.05 * age - 0.02 * income + np.random.normal(0, 1, n_samples)
default_prob = sigmoid(logit)
default = (np.random.random(n_samples) < default_prob).astype(int)

# 构建数据集
X_binary = np.column_stack([age, income])
y_binary = default

print(f"数据集信息:")
print(f"样本数量: {len(y_binary)}")
print(f"违约率: {y_binary.mean():.3f}")
print(f"特征: 年龄, 收入")
```

## 模型训练与评估

```python
# 数据标准化
scaler = StandardScaler()
X_binary_scaled = scaler.fit_transform(X_binary)

# 训练测试分割
X_train, X_test, y_train, y_test = train_test_split(
    X_binary_scaled, y_binary, test_size=0.2, random_state=42, stratify=y_binary
)

# 使用自实现的逻辑回归
model_scratch = LogisticRegressionFromScratch(learning_rate=0.1, max_iterations=1000)
model_scratch.fit(X_train, y_train)

# 使用sklearn的逻辑回归对比
model_sklearn = LogisticRegression(random_state=42)
model_sklearn.fit(X_train, y_train)

# 预测
y_prob_scratch = model_scratch.predict_proba(X_test)
y_pred_scratch = model_scratch.predict(X_test)

y_prob_sklearn = model_sklearn.predict_proba(X_test)[:, 1]
y_pred_sklearn = model_sklearn.predict(X_test)

# 比较结果
print("模型参数对比:")
print(f"自实现 - 截距: {model_scratch.intercept_:.4f}, 系数: {model_scratch.coef_}")
print(f"sklearn - 截距: {model_sklearn.intercept_[0]:.4f}, 系数: {model_sklearn.coef_[0]}")

print(f"\n准确率对比:")
print(f"自实现: {accuracy_score(y_test, y_pred_scratch):.4f}")
print(f"sklearn: {accuracy_score(y_test, y_pred_sklearn):.4f}")

# 绘制收敛过程
model_scratch.plot_cost_history()
```

### 决策边界可视化

```python
def plot_decision_boundary(X, y, model, scaler, title):
    """绘制决策边界"""
    plt.figure(figsize=(10, 8))
    
    # 创建网格
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # 预测网格点
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    grid_points_scaled = scaler.transform(grid_points)
    
    if hasattr(model, 'predict_proba') and hasattr(model.predict_proba(grid_points_scaled), '__len__'):
        # sklearn模型
        Z = model.predict_proba(grid_points_scaled)[:, 1]
    else:
        # 自实现模型
        Z = model.predict_proba(grid_points_scaled)
    
    Z = Z.reshape(xx.shape)
    
    # 绘制等高线
    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
    plt.colorbar(label='违约概率')
    
    # 绘制决策边界
    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)
    
    # 绘制数据点
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
    plt.colorbar(scatter, label='实际标签')
    
    plt.xlabel('年龄 (标准化)')
    plt.ylabel('收入 (标准化)')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()

# 绘制决策边界
plot_decision_boundary(X_test, y_test, model_sklearn, scaler, '逻辑回归决策边界')
```

## 分类评估指标

### 混淆矩阵

```python
def plot_confusion_matrix(y_true, y_pred, title):
    """绘制混淆矩阵"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['不违约', '违约'],
                yticklabels=['不违约', '违约'])
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.title(title)
    plt.show()
    
    # 计算各项指标
    tn, fp, fn, tp = cm.ravel()
    
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    print(f"\n{title} - 详细指标:")
    print(f"准确率 (Accuracy): {accuracy:.4f}")
    print(f"精确率 (Precision): {precision:.4f}")
    print(f"召回率 (Recall): {recall:.4f}")
    print(f"F1分数: {f1:.4f}")
    print(f"特异性 (Specificity): {specificity:.4f}")
    
    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# 绘制混淆矩阵
metrics = plot_confusion_matrix(y_test, y_pred_sklearn, '逻辑回归混淆矩阵')

# 详细分类报告
print("\n详细分类报告:")
print(classification_report(y_test, y_pred_sklearn, target_names=['不违约', '违约']))
```

### ROC曲线和AUC

```python
from sklearn.metrics import roc_curve, auc, precision_recall_curve

def plot_roc_and_pr_curves(y_true, y_prob, title):
    """绘制ROC曲线和PR曲线"""
    # ROC曲线
    fpr, tpr, roc_thresholds = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)
    
    # PR曲线
    precision, recall, pr_thresholds = precision_recall_curve(y_true, y_prob)
    pr_auc = auc(recall, precision)
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # ROC曲线
    axes[0].plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROC曲线 (AUC = {roc_auc:.3f})')
    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='随机分类器')
    axes[0].set_xlim([0.0, 1.0])
    axes[0].set_ylim([0.0, 1.05])
    axes[0].set_xlabel('假正率 (FPR)')
    axes[0].set_ylabel('真正率 (TPR)')
    axes[0].set_title('ROC曲线')
    axes[0].legend(loc="lower right")
    axes[0].grid(True, alpha=0.3)
    
    # PR曲线
    axes[1].plot(recall, precision, color='blue', lw=2,
                label=f'PR曲线 (AUC = {pr_auc:.3f})')
    axes[1].axhline(y=y_true.mean(), color='red', linestyle='--', 
                   label=f'随机分类器 (基线 = {y_true.mean():.3f})')
    axes[1].set_xlim([0.0, 1.0])
    axes[1].set_ylim([0.0, 1.05])
    axes[1].set_xlabel('召回率 (Recall)')
    axes[1].set_ylabel('精确率 (Precision)')
    axes[1].set_title('精确率-召回率曲线')
    axes[1].legend(loc="lower left")
    axes[1].grid(True, alpha=0.3)
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()
    
    return roc_auc, pr_auc

# 绘制ROC和PR曲线
roc_auc, pr_auc = plot_roc_and_pr_curves(y_test, y_prob_sklearn, '逻辑回归性能评估')

print(f"\nAUC指标:")
print(f"ROC-AUC: {roc_auc:.4f}")
print(f"PR-AUC: {pr_auc:.4f}")
```

### 阈值选择

```python
def find_optimal_threshold(y_true, y_prob):
    """寻找最优阈值"""
    thresholds = np.linspace(0, 1, 101)
    
    accuracies = []
    f1_scores = []
    precisions = []
    recalls = []
    
    for threshold in thresholds:
        y_pred_thresh = (y_prob >= threshold).astype(int)
        
        if len(np.unique(y_pred_thresh)) == 1:
            # 如果只预测一个类别，跳过
            accuracies.append(0)
            f1_scores.append(0)
            precisions.append(0)
            recalls.append(0)
            continue
            
        acc = accuracy_score(y_true, y_pred_thresh)
        
        # 计算其他指标
        cm = confusion_matrix(y_true, y_pred_thresh)
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        else:
            precision = recall = f1 = 0
        
        accuracies.append(acc)
        f1_scores.append(f1)
        precisions.append(precision)
        recalls.append(recall)
    
    # 可视化
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot(thresholds, accuracies, 'b-', linewidth=2, label='准确率')
    plt.xlabel('阈值')
    plt.ylabel('准确率')
    plt.title('准确率 vs 阈值')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.subplot(2, 2, 2)
    plt.plot(thresholds, f1_scores, 'g-', linewidth=2, label='F1分数')
    plt.xlabel('阈值')
    plt.ylabel('F1分数')
    plt.title('F1分数 vs 阈值')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.subplot(2, 2, 3)
    plt.plot(thresholds, precisions, 'r-', linewidth=2, label='精确率')
    plt.plot(thresholds, recalls, 'orange', linewidth=2, label='召回率')
    plt.xlabel('阈值')
    plt.ylabel('分数')
    plt.title('精确率和召回率 vs 阈值')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.subplot(2, 2, 4)
    plt.plot(thresholds, accuracies, 'b-', linewidth=2, label='准确率')
    plt.plot(thresholds, f1_scores, 'g-', linewidth=2, label='F1分数')
    plt.xlabel('阈值')
    plt.ylabel('分数')
    plt.title('综合指标对比')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    # 找到最优阈值
    best_f1_idx = np.argmax(f1_scores)
    best_acc_idx = np.argmax(accuracies)
    
    print(f"\n最优阈值分析:")
    print(f"最大F1分数阈值: {thresholds[best_f1_idx]:.3f} (F1 = {f1_scores[best_f1_idx]:.4f})")
    print(f"最大准确率阈值: {thresholds[best_acc_idx]:.3f} (Acc = {accuracies[best_acc_idx]:.4f})")
    
    return thresholds[best_f1_idx], thresholds[best_acc_idx]

# 寻找最优阈值
best_f1_threshold, best_acc_threshold = find_optimal_threshold(y_test, y_prob_sklearn)

# 使用最优阈值重新预测
y_pred_optimal = (y_prob_sklearn >= best_f1_threshold).astype(int)
print(f"\n使用最优阈值 {best_f1_threshold:.3f} 的结果:")
plot_confusion_matrix(y_test, y_pred_optimal, f'优化阈值 ({best_f1_threshold:.3f}) 混淆矩阵')
```

## 多分类逻辑回归

逻辑回归可以扩展到多分类问题：

### One-vs-Rest (OvR) 策略

```python
from sklearn.datasets import make_classification
from sklearn.multiclass import OneVsRestClassifier

# 生成多分类数据
X_multi, y_multi = make_classification(
    n_samples=1000, n_features=2, n_redundant=0, n_informative=2,
    n_clusters_per_class=1, n_classes=3, random_state=42
)

# 数据标准化
scaler_multi = StandardScaler()
X_multi_scaled = scaler_multi.fit_transform(X_multi)

# 训练测试分割
X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(
    X_multi_scaled, y_multi, test_size=0.2, random_state=42, stratify=y_multi
)

# One-vs-Rest逻辑回归
model_ovr = OneVsRestClassifier(LogisticRegression(random_state=42))
model_ovr.fit(X_train_multi, y_train_multi)

# 多项式逻辑回归
model_multinomial = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)
model_multinomial.fit(X_train_multi, y_train_multi)

# 预测
y_pred_ovr = model_ovr.predict(X_test_multi)
y_pred_multi = model_multinomial.predict(X_test_multi)

print(f"多分类结果:")
print(f"OvR准确率: {accuracy_score(y_test_multi, y_pred_ovr):.4f}")
print(f"多项式准确率: {accuracy_score(y_test_multi, y_pred_multi):.4f}")

# 可视化多分类决策边界
def plot_multiclass_decision_boundary(X, y, model, title):
    """绘制多分类决策边界"""
    plt.figure(figsize=(10, 8))
    
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black')
    plt.colorbar(scatter)
    plt.xlabel('特征 1')
    plt.ylabel('特征 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()

plot_multiclass_decision_boundary(X_test_multi, y_test_multi, model_multinomial, '多项式逻辑回归决策边界')
```

## 逻辑回归的优缺点

### 优点

1. **概率输出**：提供类别概率，便于风险评估
2. **线性可分**：对线性可分数据效果很好
3. **无需调参**：几乎没有超参数
4. **训练快速**：计算效率高
5. **可解释性强**：系数有明确的统计意义
6. **不需要特征缩放**：对特征尺度不敏感（但建议标准化）

### 缺点

1. **线性假设**：只能处理线性可分问题
2. **对异常值敏感**：极值会影响决策边界
3. **需要大样本**：小样本时可能不稳定
4. **特征独立性假设**：假设特征之间相互独立

```python
# 演示逻辑回归的局限性
def demonstrate_logistic_limitations():
    """演示逻辑回归的局限性"""
    
    # 生成非线性可分数据（XOR问题）
    np.random.seed(42)
    n = 200
    
    # XOR数据
    X_xor = np.random.randn(n, 2)
    y_xor = ((X_xor[:, 0] > 0) ^ (X_xor[:, 1] > 0)).astype(int)
    
    # 环形数据
    theta = np.random.uniform(0, 2*np.pi, n//2)
    r1 = np.random.uniform(0.5, 1.5, n//2)
    r2 = np.random.uniform(2.5, 3.5, n//2)
    
    X_circle = np.vstack([
        np.column_stack([r1 * np.cos(theta), r1 * np.sin(theta)]),
        np.column_stack([r2 * np.cos(theta), r2 * np.sin(theta)])
    ])
    y_circle = np.hstack([np.zeros(n//2), np.ones(n//2)])
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    datasets = [
        (X_xor, y_xor, 'XOR问题'),
        (X_circle, y_circle, '环形分布')
    ]
    
    for i, (X, y, name) in enumerate(datasets):
        # 原始数据
        axes[i, 0].scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
        axes[i, 0].set_title(f'{name} - 原始数据')
        axes[i, 0].grid(True, alpha=0.3)
        
        # 逻辑回归尝试
        model_limit = LogisticRegression(random_state=42)
        model_limit.fit(X, y)
        
        # 绘制决策边界
        h = 0.02
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        
        Z = model_limit.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
        Z = Z.reshape(xx.shape)
        
        axes[i, 1].contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
        axes[i, 1].contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)
        axes[i, 1].scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
        
        acc = accuracy_score(y, model_limit.predict(X))
        axes[i, 1].set_title(f'{name} - 逻辑回归 (准确率: {acc:.3f})')
        axes[i, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

demonstrate_logistic_limitations()
```

## 实际应用案例

### 信用卡欺诈检测

```python
class FraudDetectionSystem:
    """信用卡欺诈检测系统"""
    
    def __init__(self):
        self.model = None
        self.scaler = None
        self.threshold = 0.5
        self.feature_names = None
        
    def generate_fraud_data(self, n_samples=10000):
        """生成模拟欺诈数据"""
        np.random.seed(42)
        
        # 特征：交易金额、时间、商户类型、地理位置等
        amount = np.random.lognormal(3, 1.5, n_samples)  # 交易金额
        hour = np.random.randint(0, 24, n_samples)       # 交易时间
        merchant_risk = np.random.uniform(0, 1, n_samples)  # 商户风险评分
        location_risk = np.random.uniform(0, 1, n_samples)  # 地理位置风险
        frequency = np.random.poisson(5, n_samples)      # 当日交易频次
        
        # 欺诈概率模型
        fraud_logit = (-3 +                              # 基础概率
                      0.0001 * amount +                  # 金额越大风险越高
                      0.1 * (hour < 6).astype(int) +     # 凌晨交易风险高
                      2 * merchant_risk +                # 商户风险
                      1.5 * location_risk +              # 地理风险
                      0.1 * frequency +                  # 频次风险
                      np.random.normal(0, 0.5, n_samples))  # 噪声
        
        fraud_prob = sigmoid(fraud_logit)
        is_fraud = (np.random.random(n_samples) < fraud_prob).astype(int)
        
        # 构建特征矩阵
        X = np.column_stack([amount, hour, merchant_risk, location_risk, frequency])
        self.feature_names = ['交易金额', '交易时间', '商户风险', '地理风险', '交易频次']
        
        return X, is_fraud
    
    def train(self, X, y):
        """训练欺诈检测模型"""
        # 数据预处理
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # 处理类别不平衡
        from sklearn.utils.class_weight import compute_class_weight
        
        classes = np.unique(y)
        class_weights = compute_class_weight('balanced', classes=classes, y=y)
        class_weight_dict = dict(zip(classes, class_weights))
        
        # 训练测试分割
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # 训练模型
        self.model = LogisticRegression(
            class_weight=class_weight_dict,
            random_state=42
        )
        self.model.fit(X_train, y_train)
        
        # 评估模型
        y_prob_train = self.model.predict_proba(X_train)[:, 1]
        y_prob_test = self.model.predict_proba(X_test)[:, 1]
        
        self.X_test = X_test
        self.y_test = y_test
        self.y_prob_test = y_prob_test
        
        # 选择最优阈值（最大化F1分数）
        from sklearn.metrics import f1_score
        
        thresholds = np.linspace(0.01, 0.99, 99)
        f1_scores = []
        
        for thresh in thresholds:
            y_pred_thresh = (y_prob_test >= thresh).astype(int)
            f1 = f1_score(y_test, y_pred_thresh)
            f1_scores.append(f1)
        
        best_threshold_idx = np.argmax(f1_scores)
        self.threshold = thresholds[best_threshold_idx]
        
        print(f"训练完成！")
        print(f"数据集大小: {len(y)}")
        print(f"欺诈率: {y.mean():.4f}")
        print(f"最优阈值: {self.threshold:.4f}")
        print(f"最大F1分数: {max(f1_scores):.4f}")
        
        return self
    
    def predict_fraud(self, X):
        """预测欺诈概率"""
        X_scaled = self.scaler.transform(X)
        fraud_prob = self.model.predict_proba(X_scaled)[:, 1]
        is_fraud = (fraud_prob >= self.threshold).astype(int)
        
        return fraud_prob, is_fraud
    
    def evaluate_model(self):
        """评估模型性能"""
        y_pred = (self.y_prob_test >= self.threshold).astype(int)
        
        print("\n=== 欺诈检测模型评估 ===")
        print(classification_report(self.y_test, y_pred, 
                                  target_names=['正常', '欺诈']))
        
        # 绘制评估图表
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 混淆矩阵
        cm = confusion_matrix(self.y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['正常', '欺诈'],
                   yticklabels=['正常', '欺诈'], ax=axes[0, 0])
        axes[0, 0].set_title('混淆矩阵')
        axes[0, 0].set_xlabel('预测标签')
        axes[0, 0].set_ylabel('真实标签')
        
        # ROC曲线
        fpr, tpr, _ = roc_curve(self.y_test, self.y_prob_test)
        roc_auc = auc(fpr, tpr)
        axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2,
                       label=f'ROC (AUC = {roc_auc:.3f})')
        axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[0, 1].set_xlabel('假正率')
        axes[0, 1].set_ylabel('真正率')
        axes[0, 1].set_title('ROC曲线')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 特征重要性
        feature_importance = np.abs(self.model.coef_[0])
        axes[1, 0].barh(self.feature_names, feature_importance)
        axes[1, 0].set_xlabel('特征重要性 (|系数|)')
        axes[1, 0].set_title('特征重要性分析')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 概率分布
        axes[1, 1].hist(self.y_prob_test[self.y_test == 0], bins=50, alpha=0.7,
                       label='正常交易', density=True, color='blue')
        axes[1, 1].hist(self.y_prob_test[self.y_test == 1], bins=50, alpha=0.7,
                       label='欺诈交易', density=True, color='red')
        axes[1, 1].axvline(x=self.threshold, color='black', linestyle='--',
                          label=f'阈值 = {self.threshold:.3f}')
        axes[1, 1].set_xlabel('欺诈概率')
        axes[1, 1].set_ylabel('密度')
        axes[1, 1].set_title('欺诈概率分布')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def analyze_transaction(self, amount, hour, merchant_risk, location_risk, frequency):
        """分析单笔交易"""
        features = np.array([[amount, hour, merchant_risk, location_risk, frequency]])
        fraud_prob, is_fraud = self.predict_fraud(features)
        
        print(f"\n=== 交易风险分析 ===")
        print(f"交易金额: {amount:.2f} 元")
        print(f"交易时间: {hour}:00")
        print(f"商户风险评分: {merchant_risk:.3f}")
        print(f"地理风险评分: {location_risk:.3f}")
        print(f"当日交易频次: {frequency}")
        print(f"\n欺诈概率: {fraud_prob[0]:.4f}")
        print(f"风险等级: {'🔴 高风险' if is_fraud[0] else '🟢 低风险'}")
        
        if fraud_prob[0] > 0.8:
            print("⚠️ 建议：立即冻结交易，人工审核")
        elif fraud_prob[0] > 0.5:
            print("⚠️ 建议：增强验证，短信确认")
        else:
            print("✅ 建议：正常放行")
        
        return fraud_prob[0], is_fraud[0]

# 使用欺诈检测系统
fraud_detector = FraudDetectionSystem()

# 生成数据并训练
X_fraud, y_fraud = fraud_detector.generate_fraud_data(10000)
fraud_detector.train(X_fraud, y_fraud)

# 评估模型
fraud_detector.evaluate_model()

# 分析示例交易
print("\n" + "="*50)
print("交易风险分析示例:")

# 正常交易
fraud_detector.analyze_transaction(
    amount=150.0,      # 150元
    hour=14,           # 下午2点
    merchant_risk=0.2, # 低风险商户
    location_risk=0.1, # 常用地点
    frequency=3        # 当日第3笔交易
)

# 可疑交易
fraud_detector.analyze_transaction(
    amount=5000.0,     # 5000元大额交易
    hour=3,            # 凌晨3点
    merchant_risk=0.8, # 高风险商户
    location_risk=0.9, # 异常地点
    frequency=15       # 当日第15笔交易
)
```

## 小结

逻辑回归是分类问题的基础算法：

### 关键要点

1. **核心思想**：通过Sigmoid函数将线性输出转换为概率
2. **数学基础**：最大似然估计，交叉熵损失函数
3. **输出解释**：概率值，便于风险评估和决策
4. **扩展能力**：可处理多分类问题
5. **实用技巧**：阈值调优，类别平衡处理

### 评估指标

- **准确率**：整体正确率
- **精确率**：预测为正类中真正为正类的比例
- **召回率**：真正为正类中被正确预测的比例
- **F1分数**：精确率和召回率的调和平均
- **AUC-ROC**：模型区分能力
- **AUC-PR**：不平衡数据集的评估

### 实用建议

1. **数据预处理**：标准化特征，处理缺失值
2. **类别平衡**：使用class_weight处理不平衡数据
3. **阈值选择**：根据业务需求选择合适阈值
4. **特征工程**：多项式特征，交互项
5. **模型诊断**：ROC曲线，混淆矩阵分析

### 适用场景

- ✅ 二分类和多分类问题
- ✅ 需要概率输出的场景
- ✅ 线性可分或近似线性可分数据
- ✅ 风险评估和决策支持
- ❌ 复杂非线性关系
- ❌ 高维稀疏数据

## 思考题

1. 为什么逻辑回归使用Sigmoid函数而不是其他函数？

2. 在不平衡数据集中，如何选择合适的评估指标？

3. 逻辑回归的系数如何解释？正系数和负系数分别意味着什么？

4. 什么情况下应该调整分类阈值？如何选择最优阈值？

---

**下一节预告**：我们将学习决策树算法，了解如何构建可解释的非线性分类模型。