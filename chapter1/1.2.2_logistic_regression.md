# 1.2.2 é€»è¾‘å›å½’

## å­¦ä¹ ç›®æ ‡
ç†è§£é€»è¾‘å›å½’çš„æ•°å­¦åŸç†ï¼ŒæŒæ¡åˆ†ç±»é—®é¢˜çš„å»ºæ¨¡æ–¹æ³•ï¼Œå­¦ä¼šè¯„ä¼°åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½ã€‚

## å¼•è¨€ï¼šä»å›å½’åˆ°åˆ†ç±»

å‡è®¾ä½ æ˜¯ä¸€å®¶é“¶è¡Œçš„é£æ§ä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­ä¸€ä¸ªè´·æ¬¾ç”³è¯·æ˜¯å¦ä¼šè¿çº¦ï¼š

- **è¾“å…¥**ï¼šå®¢æˆ·å¹´é¾„ã€æ”¶å…¥ã€ä¿¡ç”¨å†å²ã€è´Ÿå€ºæ¯”ä¾‹ç­‰
- **è¾“å‡º**ï¼šè¿çº¦ï¼ˆ1ï¼‰æˆ–ä¸è¿çº¦ï¼ˆ0ï¼‰

è¿™ä¸å†æ˜¯é¢„æµ‹è¿ç»­æ•°å€¼ï¼Œè€Œæ˜¯é¢„æµ‹**ç±»åˆ«**ã€‚çº¿æ€§å›å½’ç›´æ¥è¾“å‡ºè¿ç»­å€¼ï¼Œä½†æˆ‘ä»¬éœ€è¦çš„æ˜¯æ¦‚ç‡å’Œç±»åˆ«ã€‚

**é€»è¾‘å›å½’**å°±æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ç»å…¸ç®—æ³•ï¼

## ä»€ä¹ˆæ˜¯é€»è¾‘å›å½’ï¼Ÿ

**é€»è¾‘å›å½’ (Logistic Regression)** æ˜¯ä¸€ç§ç”¨äºåˆ†ç±»é—®é¢˜çš„çº¿æ€§æ¨¡å‹ã€‚å°½ç®¡åå­—å«"å›å½’"ï¼Œä½†å®ƒå®é™…ä¸Šæ˜¯åˆ†ç±»ç®—æ³•ã€‚

### æ ¸å¿ƒæ€æƒ³

1. **çº¿æ€§ç»„åˆ**ï¼šåƒçº¿æ€§å›å½’ä¸€æ ·è®¡ç®—ç‰¹å¾çš„çº¿æ€§ç»„åˆ
2. **Sigmoidå˜æ¢**ï¼šå°†çº¿æ€§è¾“å‡ºè½¬æ¢ä¸º0-1ä¹‹é—´çš„æ¦‚ç‡
3. **æ¦‚ç‡è§£é‡Š**ï¼šè¾“å‡ºå¯ä»¥è§£é‡Šä¸ºå±äºæ­£ç±»çš„æ¦‚ç‡

### æ•°å­¦è¡¨è¾¾

**çº¿æ€§éƒ¨åˆ†**ï¼š
```
z = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™
```

**Sigmoidå‡½æ•°**ï¼š
```
p = Ïƒ(z) = 1 / (1 + e^(-z))
```

**é¢„æµ‹è§„åˆ™**ï¼š
```
Å· = 1 if p â‰¥ 0.5 else 0
```

### Sigmoidå‡½æ•°çš„ç‰¹æ€§

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def sigmoid(z):
    """Sigmoidå‡½æ•°"""
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # é˜²æ­¢æº¢å‡º

# å¯è§†åŒ–Sigmoidå‡½æ•°
z = np.linspace(-10, 10, 100)
p = sigmoid(z)

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Sigmoidå‡½æ•°å›¾åƒ
axes[0].plot(z, p, 'b-', linewidth=3, label='Ïƒ(z) = 1/(1+e^(-z))')
axes[0].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='å†³ç­–è¾¹ç•Œ')
axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
axes[0].set_xlabel('z (çº¿æ€§è¾“å‡º)')
axes[0].set_ylabel('p (æ¦‚ç‡)')
axes[0].set_title('Sigmoidå‡½æ•°')
axes[0].grid(True, alpha=0.3)
axes[0].legend()
axes[0].set_ylim(-0.1, 1.1)

# Sigmoidå‡½æ•°çš„å¯¼æ•°
def sigmoid_derivative(z):
    s = sigmoid(z)
    return s * (1 - s)

derivative = sigmoid_derivative(z)
axes[1].plot(z, derivative, 'g-', linewidth=3, label="Ïƒ'(z) = Ïƒ(z)(1-Ïƒ(z))")
axes[1].set_xlabel('z')
axes[1].set_ylabel("Ïƒ'(z)")
axes[1].set_title('Sigmoidå‡½æ•°çš„å¯¼æ•°')
axes[1].grid(True, alpha=0.3)
axes[1].legend()

# ä¸åŒå‚æ•°çš„å½±å“
z_demo = np.linspace(-5, 5, 100)
for beta in [0.5, 1, 2, 5]:
    p_demo = sigmoid(beta * z_demo)
    axes[2].plot(z_demo, p_demo, linewidth=2, label=f'Î² = {beta}')

axes[2].axhline(y=0.5, color='red', linestyle='--', alpha=0.7)
axes[2].set_xlabel('z')
axes[2].set_ylabel('p')
axes[2].set_title('ä¸åŒç³»æ•°å¯¹Sigmoidçš„å½±å“')
axes[2].grid(True, alpha=0.3)
axes[2].legend()

plt.tight_layout()
plt.show()

print("Sigmoidå‡½æ•°ç‰¹æ€§:")
print("1. è¾“å‡ºèŒƒå›´: (0, 1)")
print("2. å•è°ƒé€’å¢")
print("3. åœ¨z=0å¤„ï¼Œp=0.5")
print("4. å…³äºç‚¹(0, 0.5)ä¸­å¿ƒå¯¹ç§°")
print("5. å¯¼æ•°æœ€å¤§å€¼åœ¨z=0å¤„")
```

## é€»è¾‘å›å½’çš„æ¨å¯¼

### ä»çº¿æ€§å›å½’åˆ°é€»è¾‘å›å½’

**é—®é¢˜**ï¼šçº¿æ€§å›å½’çš„è¾“å‡ºå¯èƒ½è¶…å‡º[0,1]èŒƒå›´ï¼Œæ— æ³•è¡¨ç¤ºæ¦‚ç‡ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨**å‡ ç‡ (Odds)** å’Œ**å¯¹æ•°å‡ ç‡ (Log-Odds)**ã€‚

### å‡ ç‡å’Œå¯¹æ•°å‡ ç‡

**å‡ ç‡**ï¼š
```
Odds = p / (1 - p)
```

**å¯¹æ•°å‡ ç‡ (Logit)**ï¼š
```
logit(p) = ln(p / (1 - p)) = z
```

**é€†å˜æ¢**ï¼š
```
p = 1 / (1 + e^(-z))
```

```python
# å¯è§†åŒ–å‡ ç‡å’Œå¯¹æ•°å‡ ç‡
p_values = np.linspace(0.01, 0.99, 100)
odds = p_values / (1 - p_values)
log_odds = np.log(odds)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# æ¦‚ç‡ vs å‡ ç‡
axes[0].plot(p_values, odds, 'b-', linewidth=2)
axes[0].set_xlabel('æ¦‚ç‡ p')
axes[0].set_ylabel('å‡ ç‡ p/(1-p)')
axes[0].set_title('æ¦‚ç‡ä¸å‡ ç‡çš„å…³ç³»')
axes[0].grid(True, alpha=0.3)
axes[0].set_yscale('log')

# æ¦‚ç‡ vs å¯¹æ•°å‡ ç‡
axes[1].plot(p_values, log_odds, 'r-', linewidth=2)
axes[1].set_xlabel('æ¦‚ç‡ p')
axes[1].set_ylabel('å¯¹æ•°å‡ ç‡ ln(p/(1-p))')
axes[1].set_title('æ¦‚ç‡ä¸å¯¹æ•°å‡ ç‡çš„å…³ç³»')
axes[1].grid(True, alpha=0.3)
axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
axes[1].axvline(x=0.5, color='black', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

print("å‡ ç‡è§£é‡Š:")
print("- å‡ ç‡ = 1: æ­£è´Ÿç±»æ¦‚ç‡ç›¸ç­‰ (p = 0.5)")
print("- å‡ ç‡ > 1: æ­£ç±»æ¦‚ç‡æ›´å¤§")
print("- å‡ ç‡ < 1: è´Ÿç±»æ¦‚ç‡æ›´å¤§")
print("\nå¯¹æ•°å‡ ç‡è§£é‡Š:")
print("- å¯¹æ•°å‡ ç‡ = 0: æ­£è´Ÿç±»æ¦‚ç‡ç›¸ç­‰")
print("- å¯¹æ•°å‡ ç‡ > 0: æ­£ç±»æ¦‚ç‡æ›´å¤§")
print("- å¯¹æ•°å‡ ç‡ < 0: è´Ÿç±»æ¦‚ç‡æ›´å¤§")
```

## æœ€å¤§ä¼¼ç„¶ä¼°è®¡

é€»è¾‘å›å½’ä½¿ç”¨**æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (Maximum Likelihood Estimation, MLE)** æ¥æ±‚è§£å‚æ•°ã€‚

### ä¼¼ç„¶å‡½æ•°

å¯¹äºå•ä¸ªæ ·æœ¬ï¼š
```
P(y|x) = p^y Ã— (1-p)^(1-y)
```

å¯¹äºæ‰€æœ‰æ ·æœ¬ï¼š
```
L(Î²) = âˆáµ¢ P(yáµ¢|xáµ¢) = âˆáµ¢ páµ¢^yáµ¢ Ã— (1-páµ¢)^(1-yáµ¢)
```

### å¯¹æ•°ä¼¼ç„¶å‡½æ•°

```
â„“(Î²) = ln L(Î²) = Î£áµ¢ [yáµ¢ ln(páµ¢) + (1-yáµ¢) ln(1-páµ¢)]
```

### æŸå¤±å‡½æ•°ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰

```
J(Î²) = -â„“(Î²) = -Î£áµ¢ [yáµ¢ ln(páµ¢) + (1-yáµ¢) ln(1-páµ¢)]
```

è¿™å°±æ˜¯**äº¤å‰ç†µæŸå¤±å‡½æ•°**ï¼

```python
class LogisticRegressionFromScratch:
    """ä»é›¶å®ç°é€»è¾‘å›å½’"""
    
    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.tolerance = tolerance
        self.cost_history = []
        
    def _add_bias(self, X):
        """æ·»åŠ åç½®é¡¹"""
        return np.column_stack([np.ones(X.shape[0]), X])
    
    def _sigmoid(self, z):
        """Sigmoidå‡½æ•°"""
        # é˜²æ­¢æ•°å€¼æº¢å‡º
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    
    def _compute_cost(self, y_true, y_prob):
        """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
        # é˜²æ­¢log(0)
        y_prob = np.clip(y_prob, 1e-15, 1 - 1e-15)
        cost = -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))
        return cost
    
    def _compute_gradients(self, X, y_true, y_prob):
        """è®¡ç®—æ¢¯åº¦"""
        m = X.shape[0]
        gradients = (1 / m) * X.T @ (y_prob - y_true)
        return gradients
    
    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        # æ·»åŠ åç½®é¡¹
        X_with_bias = self._add_bias(X)
        
        # åˆå§‹åŒ–å‚æ•°
        n_features = X_with_bias.shape[1]
        self.theta = np.random.normal(0, 0.01, n_features)
        
        # æ¢¯åº¦ä¸‹é™
        for i in range(self.max_iterations):
            # å‰å‘ä¼ æ’­
            z = X_with_bias @ self.theta
            y_prob = self._sigmoid(z)
            
            # è®¡ç®—æŸå¤±
            cost = self._compute_cost(y, y_prob)
            self.cost_history.append(cost)
            
            # è®¡ç®—æ¢¯åº¦
            gradients = self._compute_gradients(X_with_bias, y, y_prob)
            
            # æ›´æ–°å‚æ•°
            new_theta = self.theta - self.learning_rate * gradients
            
            # æ£€æŸ¥æ”¶æ•›
            if np.linalg.norm(new_theta - self.theta) < self.tolerance:
                print(f"åœ¨ç¬¬ {i+1} æ¬¡è¿­ä»£æ—¶æ”¶æ•›")
                break
                
            self.theta = new_theta
        
        # åˆ†ç¦»æˆªè·å’Œç³»æ•°
        self.intercept_ = self.theta[0]
        self.coef_ = self.theta[1:]
        
        return self
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        X_with_bias = self._add_bias(X)
        z = X_with_bias @ self.theta
        return self._sigmoid(z)
    
    def predict(self, X, threshold=0.5):
        """é¢„æµ‹ç±»åˆ«"""
        probabilities = self.predict_proba(X)
        return (probabilities >= threshold).astype(int)
    
    def plot_cost_history(self):
        """ç»˜åˆ¶æŸå¤±å‡½æ•°å˜åŒ–"""
        plt.figure(figsize=(10, 6))
        plt.plot(self.cost_history, linewidth=2)
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('äº¤å‰ç†µæŸå¤±')
        plt.title('é€»è¾‘å›å½’æ”¶æ•›è¿‡ç¨‹')
        plt.grid(True, alpha=0.3)
        plt.show()

# ç”ŸæˆäºŒåˆ†ç±»æ•°æ®
np.random.seed(42)
n_samples = 1000

# ç‰¹å¾ï¼šå¹´é¾„ã€æ”¶å…¥
age = np.random.uniform(18, 80, n_samples)
income = np.random.uniform(20, 200, n_samples)  # æ”¶å…¥(åƒå…ƒ)

# æ ‡ç­¾ï¼šæ˜¯å¦è¿çº¦ (å¹´é¾„è¶Šå¤§ã€æ”¶å…¥è¶Šä½ï¼Œè¿çº¦æ¦‚ç‡è¶Šé«˜)
logit = -2 + 0.05 * age - 0.02 * income + np.random.normal(0, 1, n_samples)
default_prob = sigmoid(logit)
default = (np.random.random(n_samples) < default_prob).astype(int)

# æ„å»ºæ•°æ®é›†
X_binary = np.column_stack([age, income])
y_binary = default

print(f"æ•°æ®é›†ä¿¡æ¯:")
print(f"æ ·æœ¬æ•°é‡: {len(y_binary)}")
print(f"è¿çº¦ç‡: {y_binary.mean():.3f}")
print(f"ç‰¹å¾: å¹´é¾„, æ”¶å…¥")
```

## æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°

```python
# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_binary_scaled = scaler.fit_transform(X_binary)

# è®­ç»ƒæµ‹è¯•åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X_binary_scaled, y_binary, test_size=0.2, random_state=42, stratify=y_binary
)

# ä½¿ç”¨è‡ªå®ç°çš„é€»è¾‘å›å½’
model_scratch = LogisticRegressionFromScratch(learning_rate=0.1, max_iterations=1000)
model_scratch.fit(X_train, y_train)

# ä½¿ç”¨sklearnçš„é€»è¾‘å›å½’å¯¹æ¯”
model_sklearn = LogisticRegression(random_state=42)
model_sklearn.fit(X_train, y_train)

# é¢„æµ‹
y_prob_scratch = model_scratch.predict_proba(X_test)
y_pred_scratch = model_scratch.predict(X_test)

y_prob_sklearn = model_sklearn.predict_proba(X_test)[:, 1]
y_pred_sklearn = model_sklearn.predict(X_test)

# æ¯”è¾ƒç»“æœ
print("æ¨¡å‹å‚æ•°å¯¹æ¯”:")
print(f"è‡ªå®ç° - æˆªè·: {model_scratch.intercept_:.4f}, ç³»æ•°: {model_scratch.coef_}")
print(f"sklearn - æˆªè·: {model_sklearn.intercept_[0]:.4f}, ç³»æ•°: {model_sklearn.coef_[0]}")

print(f"\nå‡†ç¡®ç‡å¯¹æ¯”:")
print(f"è‡ªå®ç°: {accuracy_score(y_test, y_pred_scratch):.4f}")
print(f"sklearn: {accuracy_score(y_test, y_pred_sklearn):.4f}")

# ç»˜åˆ¶æ”¶æ•›è¿‡ç¨‹
model_scratch.plot_cost_history()
```

### å†³ç­–è¾¹ç•Œå¯è§†åŒ–

```python
def plot_decision_boundary(X, y, model, scaler, title):
    """ç»˜åˆ¶å†³ç­–è¾¹ç•Œ"""
    plt.figure(figsize=(10, 8))
    
    # åˆ›å»ºç½‘æ ¼
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # é¢„æµ‹ç½‘æ ¼ç‚¹
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    grid_points_scaled = scaler.transform(grid_points)
    
    if hasattr(model, 'predict_proba') and hasattr(model.predict_proba(grid_points_scaled), '__len__'):
        # sklearnæ¨¡å‹
        Z = model.predict_proba(grid_points_scaled)[:, 1]
    else:
        # è‡ªå®ç°æ¨¡å‹
        Z = model.predict_proba(grid_points_scaled)
    
    Z = Z.reshape(xx.shape)
    
    # ç»˜åˆ¶ç­‰é«˜çº¿
    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
    plt.colorbar(label='è¿çº¦æ¦‚ç‡')
    
    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)
    
    # ç»˜åˆ¶æ•°æ®ç‚¹
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
    plt.colorbar(scatter, label='å®é™…æ ‡ç­¾')
    
    plt.xlabel('å¹´é¾„ (æ ‡å‡†åŒ–)')
    plt.ylabel('æ”¶å…¥ (æ ‡å‡†åŒ–)')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()

# ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
plot_decision_boundary(X_test, y_test, model_sklearn, scaler, 'é€»è¾‘å›å½’å†³ç­–è¾¹ç•Œ')
```

## åˆ†ç±»è¯„ä¼°æŒ‡æ ‡

### æ··æ·†çŸ©é˜µ

```python
def plot_confusion_matrix(y_true, y_pred, title):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['ä¸è¿çº¦', 'è¿çº¦'],
                yticklabels=['ä¸è¿çº¦', 'è¿çº¦'])
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title(title)
    plt.show()
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    tn, fp, fn, tp = cm.ravel()
    
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    print(f"\n{title} - è¯¦ç»†æŒ‡æ ‡:")
    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"å¬å›ç‡ (Recall): {recall:.4f}")
    print(f"F1åˆ†æ•°: {f1:.4f}")
    print(f"ç‰¹å¼‚æ€§ (Specificity): {specificity:.4f}")
    
    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}

# ç»˜åˆ¶æ··æ·†çŸ©é˜µ
metrics = plot_confusion_matrix(y_test, y_pred_sklearn, 'é€»è¾‘å›å½’æ··æ·†çŸ©é˜µ')

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred_sklearn, target_names=['ä¸è¿çº¦', 'è¿çº¦']))
```

### ROCæ›²çº¿å’ŒAUC

```python
from sklearn.metrics import roc_curve, auc, precision_recall_curve

def plot_roc_and_pr_curves(y_true, y_prob, title):
    """ç»˜åˆ¶ROCæ›²çº¿å’ŒPRæ›²çº¿"""
    # ROCæ›²çº¿
    fpr, tpr, roc_thresholds = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)
    
    # PRæ›²çº¿
    precision, recall, pr_thresholds = precision_recall_curve(y_true, y_prob)
    pr_auc = auc(recall, precision)
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # ROCæ›²çº¿
    axes[0].plot(fpr, tpr, color='darkorange', lw=2, 
                label=f'ROCæ›²çº¿ (AUC = {roc_auc:.3f})')
    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='éšæœºåˆ†ç±»å™¨')
    axes[0].set_xlim([0.0, 1.0])
    axes[0].set_ylim([0.0, 1.05])
    axes[0].set_xlabel('å‡æ­£ç‡ (FPR)')
    axes[0].set_ylabel('çœŸæ­£ç‡ (TPR)')
    axes[0].set_title('ROCæ›²çº¿')
    axes[0].legend(loc="lower right")
    axes[0].grid(True, alpha=0.3)
    
    # PRæ›²çº¿
    axes[1].plot(recall, precision, color='blue', lw=2,
                label=f'PRæ›²çº¿ (AUC = {pr_auc:.3f})')
    axes[1].axhline(y=y_true.mean(), color='red', linestyle='--', 
                   label=f'éšæœºåˆ†ç±»å™¨ (åŸºçº¿ = {y_true.mean():.3f})')
    axes[1].set_xlim([0.0, 1.0])
    axes[1].set_ylim([0.0, 1.05])
    axes[1].set_xlabel('å¬å›ç‡ (Recall)')
    axes[1].set_ylabel('ç²¾ç¡®ç‡ (Precision)')
    axes[1].set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')
    axes[1].legend(loc="lower left")
    axes[1].grid(True, alpha=0.3)
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()
    
    return roc_auc, pr_auc

# ç»˜åˆ¶ROCå’ŒPRæ›²çº¿
roc_auc, pr_auc = plot_roc_and_pr_curves(y_test, y_prob_sklearn, 'é€»è¾‘å›å½’æ€§èƒ½è¯„ä¼°')

print(f"\nAUCæŒ‡æ ‡:")
print(f"ROC-AUC: {roc_auc:.4f}")
print(f"PR-AUC: {pr_auc:.4f}")
```

### é˜ˆå€¼é€‰æ‹©

```python
def find_optimal_threshold(y_true, y_prob):
    """å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
    thresholds = np.linspace(0, 1, 101)
    
    accuracies = []
    f1_scores = []
    precisions = []
    recalls = []
    
    for threshold in thresholds:
        y_pred_thresh = (y_prob >= threshold).astype(int)
        
        if len(np.unique(y_pred_thresh)) == 1:
            # å¦‚æœåªé¢„æµ‹ä¸€ä¸ªç±»åˆ«ï¼Œè·³è¿‡
            accuracies.append(0)
            f1_scores.append(0)
            precisions.append(0)
            recalls.append(0)
            continue
            
        acc = accuracy_score(y_true, y_pred_thresh)
        
        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
        cm = confusion_matrix(y_true, y_pred_thresh)
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        else:
            precision = recall = f1 = 0
        
        accuracies.append(acc)
        f1_scores.append(f1)
        precisions.append(precision)
        recalls.append(recall)
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot(thresholds, accuracies, 'b-', linewidth=2, label='å‡†ç¡®ç‡')
    plt.xlabel('é˜ˆå€¼')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('å‡†ç¡®ç‡ vs é˜ˆå€¼')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.subplot(2, 2, 2)
    plt.plot(thresholds, f1_scores, 'g-', linewidth=2, label='F1åˆ†æ•°')
    plt.xlabel('é˜ˆå€¼')
    plt.ylabel('F1åˆ†æ•°')
    plt.title('F1åˆ†æ•° vs é˜ˆå€¼')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.subplot(2, 2, 3)
    plt.plot(thresholds, precisions, 'r-', linewidth=2, label='ç²¾ç¡®ç‡')
    plt.plot(thresholds, recalls, 'orange', linewidth=2, label='å¬å›ç‡')
    plt.xlabel('é˜ˆå€¼')
    plt.ylabel('åˆ†æ•°')
    plt.title('ç²¾ç¡®ç‡å’Œå¬å›ç‡ vs é˜ˆå€¼')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.subplot(2, 2, 4)
    plt.plot(thresholds, accuracies, 'b-', linewidth=2, label='å‡†ç¡®ç‡')
    plt.plot(thresholds, f1_scores, 'g-', linewidth=2, label='F1åˆ†æ•°')
    plt.xlabel('é˜ˆå€¼')
    plt.ylabel('åˆ†æ•°')
    plt.title('ç»¼åˆæŒ‡æ ‡å¯¹æ¯”')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
    best_f1_idx = np.argmax(f1_scores)
    best_acc_idx = np.argmax(accuracies)
    
    print(f"\næœ€ä¼˜é˜ˆå€¼åˆ†æ:")
    print(f"æœ€å¤§F1åˆ†æ•°é˜ˆå€¼: {thresholds[best_f1_idx]:.3f} (F1 = {f1_scores[best_f1_idx]:.4f})")
    print(f"æœ€å¤§å‡†ç¡®ç‡é˜ˆå€¼: {thresholds[best_acc_idx]:.3f} (Acc = {accuracies[best_acc_idx]:.4f})")
    
    return thresholds[best_f1_idx], thresholds[best_acc_idx]

# å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
best_f1_threshold, best_acc_threshold = find_optimal_threshold(y_test, y_prob_sklearn)

# ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼é‡æ–°é¢„æµ‹
y_pred_optimal = (y_prob_sklearn >= best_f1_threshold).astype(int)
print(f"\nä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ {best_f1_threshold:.3f} çš„ç»“æœ:")
plot_confusion_matrix(y_test, y_pred_optimal, f'ä¼˜åŒ–é˜ˆå€¼ ({best_f1_threshold:.3f}) æ··æ·†çŸ©é˜µ')
```

## å¤šåˆ†ç±»é€»è¾‘å›å½’

é€»è¾‘å›å½’å¯ä»¥æ‰©å±•åˆ°å¤šåˆ†ç±»é—®é¢˜ï¼š

### One-vs-Rest (OvR) ç­–ç•¥

```python
from sklearn.datasets import make_classification
from sklearn.multiclass import OneVsRestClassifier

# ç”Ÿæˆå¤šåˆ†ç±»æ•°æ®
X_multi, y_multi = make_classification(
    n_samples=1000, n_features=2, n_redundant=0, n_informative=2,
    n_clusters_per_class=1, n_classes=3, random_state=42
)

# æ•°æ®æ ‡å‡†åŒ–
scaler_multi = StandardScaler()
X_multi_scaled = scaler_multi.fit_transform(X_multi)

# è®­ç»ƒæµ‹è¯•åˆ†å‰²
X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(
    X_multi_scaled, y_multi, test_size=0.2, random_state=42, stratify=y_multi
)

# One-vs-Resté€»è¾‘å›å½’
model_ovr = OneVsRestClassifier(LogisticRegression(random_state=42))
model_ovr.fit(X_train_multi, y_train_multi)

# å¤šé¡¹å¼é€»è¾‘å›å½’
model_multinomial = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)
model_multinomial.fit(X_train_multi, y_train_multi)

# é¢„æµ‹
y_pred_ovr = model_ovr.predict(X_test_multi)
y_pred_multi = model_multinomial.predict(X_test_multi)

print(f"å¤šåˆ†ç±»ç»“æœ:")
print(f"OvRå‡†ç¡®ç‡: {accuracy_score(y_test_multi, y_pred_ovr):.4f}")
print(f"å¤šé¡¹å¼å‡†ç¡®ç‡: {accuracy_score(y_test_multi, y_pred_multi):.4f}")

# å¯è§†åŒ–å¤šåˆ†ç±»å†³ç­–è¾¹ç•Œ
def plot_multiclass_decision_boundary(X, y, model, title):
    """ç»˜åˆ¶å¤šåˆ†ç±»å†³ç­–è¾¹ç•Œ"""
    plt.figure(figsize=(10, 8))
    
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black')
    plt.colorbar(scatter)
    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.show()

plot_multiclass_decision_boundary(X_test_multi, y_test_multi, model_multinomial, 'å¤šé¡¹å¼é€»è¾‘å›å½’å†³ç­–è¾¹ç•Œ')
```

## é€»è¾‘å›å½’çš„ä¼˜ç¼ºç‚¹

### ä¼˜ç‚¹

1. **æ¦‚ç‡è¾“å‡º**ï¼šæä¾›ç±»åˆ«æ¦‚ç‡ï¼Œä¾¿äºé£é™©è¯„ä¼°
2. **çº¿æ€§å¯åˆ†**ï¼šå¯¹çº¿æ€§å¯åˆ†æ•°æ®æ•ˆæœå¾ˆå¥½
3. **æ— éœ€è°ƒå‚**ï¼šå‡ ä¹æ²¡æœ‰è¶…å‚æ•°
4. **è®­ç»ƒå¿«é€Ÿ**ï¼šè®¡ç®—æ•ˆç‡é«˜
5. **å¯è§£é‡Šæ€§å¼º**ï¼šç³»æ•°æœ‰æ˜ç¡®çš„ç»Ÿè®¡æ„ä¹‰
6. **ä¸éœ€è¦ç‰¹å¾ç¼©æ”¾**ï¼šå¯¹ç‰¹å¾å°ºåº¦ä¸æ•æ„Ÿï¼ˆä½†å»ºè®®æ ‡å‡†åŒ–ï¼‰

### ç¼ºç‚¹

1. **çº¿æ€§å‡è®¾**ï¼šåªèƒ½å¤„ç†çº¿æ€§å¯åˆ†é—®é¢˜
2. **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ**ï¼šæå€¼ä¼šå½±å“å†³ç­–è¾¹ç•Œ
3. **éœ€è¦å¤§æ ·æœ¬**ï¼šå°æ ·æœ¬æ—¶å¯èƒ½ä¸ç¨³å®š
4. **ç‰¹å¾ç‹¬ç«‹æ€§å‡è®¾**ï¼šå‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹

```python
# æ¼”ç¤ºé€»è¾‘å›å½’çš„å±€é™æ€§
def demonstrate_logistic_limitations():
    """æ¼”ç¤ºé€»è¾‘å›å½’çš„å±€é™æ€§"""
    
    # ç”Ÿæˆéçº¿æ€§å¯åˆ†æ•°æ®ï¼ˆXORé—®é¢˜ï¼‰
    np.random.seed(42)
    n = 200
    
    # XORæ•°æ®
    X_xor = np.random.randn(n, 2)
    y_xor = ((X_xor[:, 0] > 0) ^ (X_xor[:, 1] > 0)).astype(int)
    
    # ç¯å½¢æ•°æ®
    theta = np.random.uniform(0, 2*np.pi, n//2)
    r1 = np.random.uniform(0.5, 1.5, n//2)
    r2 = np.random.uniform(2.5, 3.5, n//2)
    
    X_circle = np.vstack([
        np.column_stack([r1 * np.cos(theta), r1 * np.sin(theta)]),
        np.column_stack([r2 * np.cos(theta), r2 * np.sin(theta)])
    ])
    y_circle = np.hstack([np.zeros(n//2), np.ones(n//2)])
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    datasets = [
        (X_xor, y_xor, 'XORé—®é¢˜'),
        (X_circle, y_circle, 'ç¯å½¢åˆ†å¸ƒ')
    ]
    
    for i, (X, y, name) in enumerate(datasets):
        # åŸå§‹æ•°æ®
        axes[i, 0].scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
        axes[i, 0].set_title(f'{name} - åŸå§‹æ•°æ®')
        axes[i, 0].grid(True, alpha=0.3)
        
        # é€»è¾‘å›å½’å°è¯•
        model_limit = LogisticRegression(random_state=42)
        model_limit.fit(X, y)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        h = 0.02
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        
        Z = model_limit.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
        Z = Z.reshape(xx.shape)
        
        axes[i, 1].contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
        axes[i, 1].contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)
        axes[i, 1].scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
        
        acc = accuracy_score(y, model_limit.predict(X))
        axes[i, 1].set_title(f'{name} - é€»è¾‘å›å½’ (å‡†ç¡®ç‡: {acc:.3f})')
        axes[i, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

demonstrate_logistic_limitations()
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹

```python
class FraudDetectionSystem:
    """ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.model = None
        self.scaler = None
        self.threshold = 0.5
        self.feature_names = None
        
    def generate_fraud_data(self, n_samples=10000):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ¬ºè¯ˆæ•°æ®"""
        np.random.seed(42)
        
        # ç‰¹å¾ï¼šäº¤æ˜“é‡‘é¢ã€æ—¶é—´ã€å•†æˆ·ç±»å‹ã€åœ°ç†ä½ç½®ç­‰
        amount = np.random.lognormal(3, 1.5, n_samples)  # äº¤æ˜“é‡‘é¢
        hour = np.random.randint(0, 24, n_samples)       # äº¤æ˜“æ—¶é—´
        merchant_risk = np.random.uniform(0, 1, n_samples)  # å•†æˆ·é£é™©è¯„åˆ†
        location_risk = np.random.uniform(0, 1, n_samples)  # åœ°ç†ä½ç½®é£é™©
        frequency = np.random.poisson(5, n_samples)      # å½“æ—¥äº¤æ˜“é¢‘æ¬¡
        
        # æ¬ºè¯ˆæ¦‚ç‡æ¨¡å‹
        fraud_logit = (-3 +                              # åŸºç¡€æ¦‚ç‡
                      0.0001 * amount +                  # é‡‘é¢è¶Šå¤§é£é™©è¶Šé«˜
                      0.1 * (hour < 6).astype(int) +     # å‡Œæ™¨äº¤æ˜“é£é™©é«˜
                      2 * merchant_risk +                # å•†æˆ·é£é™©
                      1.5 * location_risk +              # åœ°ç†é£é™©
                      0.1 * frequency +                  # é¢‘æ¬¡é£é™©
                      np.random.normal(0, 0.5, n_samples))  # å™ªå£°
        
        fraud_prob = sigmoid(fraud_logit)
        is_fraud = (np.random.random(n_samples) < fraud_prob).astype(int)
        
        # æ„å»ºç‰¹å¾çŸ©é˜µ
        X = np.column_stack([amount, hour, merchant_risk, location_risk, frequency])
        self.feature_names = ['äº¤æ˜“é‡‘é¢', 'äº¤æ˜“æ—¶é—´', 'å•†æˆ·é£é™©', 'åœ°ç†é£é™©', 'äº¤æ˜“é¢‘æ¬¡']
        
        return X, is_fraud
    
    def train(self, X, y):
        """è®­ç»ƒæ¬ºè¯ˆæ£€æµ‹æ¨¡å‹"""
        # æ•°æ®é¢„å¤„ç†
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        from sklearn.utils.class_weight import compute_class_weight
        
        classes = np.unique(y)
        class_weights = compute_class_weight('balanced', classes=classes, y=y)
        class_weight_dict = dict(zip(classes, class_weights))
        
        # è®­ç»ƒæµ‹è¯•åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.model = LogisticRegression(
            class_weight=class_weight_dict,
            random_state=42
        )
        self.model.fit(X_train, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        y_prob_train = self.model.predict_proba(X_train)[:, 1]
        y_prob_test = self.model.predict_proba(X_test)[:, 1]
        
        self.X_test = X_test
        self.y_test = y_test
        self.y_prob_test = y_prob_test
        
        # é€‰æ‹©æœ€ä¼˜é˜ˆå€¼ï¼ˆæœ€å¤§åŒ–F1åˆ†æ•°ï¼‰
        from sklearn.metrics import f1_score
        
        thresholds = np.linspace(0.01, 0.99, 99)
        f1_scores = []
        
        for thresh in thresholds:
            y_pred_thresh = (y_prob_test >= thresh).astype(int)
            f1 = f1_score(y_test, y_pred_thresh)
            f1_scores.append(f1)
        
        best_threshold_idx = np.argmax(f1_scores)
        self.threshold = thresholds[best_threshold_idx]
        
        print(f"è®­ç»ƒå®Œæˆï¼")
        print(f"æ•°æ®é›†å¤§å°: {len(y)}")
        print(f"æ¬ºè¯ˆç‡: {y.mean():.4f}")
        print(f"æœ€ä¼˜é˜ˆå€¼: {self.threshold:.4f}")
        print(f"æœ€å¤§F1åˆ†æ•°: {max(f1_scores):.4f}")
        
        return self
    
    def predict_fraud(self, X):
        """é¢„æµ‹æ¬ºè¯ˆæ¦‚ç‡"""
        X_scaled = self.scaler.transform(X)
        fraud_prob = self.model.predict_proba(X_scaled)[:, 1]
        is_fraud = (fraud_prob >= self.threshold).astype(int)
        
        return fraud_prob, is_fraud
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        y_pred = (self.y_prob_test >= self.threshold).astype(int)
        
        print("\n=== æ¬ºè¯ˆæ£€æµ‹æ¨¡å‹è¯„ä¼° ===")
        print(classification_report(self.y_test, y_pred, 
                                  target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ']))
        
        # ç»˜åˆ¶è¯„ä¼°å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['æ­£å¸¸', 'æ¬ºè¯ˆ'],
                   yticklabels=['æ­£å¸¸', 'æ¬ºè¯ˆ'], ax=axes[0, 0])
        axes[0, 0].set_title('æ··æ·†çŸ©é˜µ')
        axes[0, 0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[0, 0].set_ylabel('çœŸå®æ ‡ç­¾')
        
        # ROCæ›²çº¿
        fpr, tpr, _ = roc_curve(self.y_test, self.y_prob_test)
        roc_auc = auc(fpr, tpr)
        axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2,
                       label=f'ROC (AUC = {roc_auc:.3f})')
        axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[0, 1].set_xlabel('å‡æ­£ç‡')
        axes[0, 1].set_ylabel('çœŸæ­£ç‡')
        axes[0, 1].set_title('ROCæ›²çº¿')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = np.abs(self.model.coef_[0])
        axes[1, 0].barh(self.feature_names, feature_importance)
        axes[1, 0].set_xlabel('ç‰¹å¾é‡è¦æ€§ (|ç³»æ•°|)')
        axes[1, 0].set_title('ç‰¹å¾é‡è¦æ€§åˆ†æ')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ¦‚ç‡åˆ†å¸ƒ
        axes[1, 1].hist(self.y_prob_test[self.y_test == 0], bins=50, alpha=0.7,
                       label='æ­£å¸¸äº¤æ˜“', density=True, color='blue')
        axes[1, 1].hist(self.y_prob_test[self.y_test == 1], bins=50, alpha=0.7,
                       label='æ¬ºè¯ˆäº¤æ˜“', density=True, color='red')
        axes[1, 1].axvline(x=self.threshold, color='black', linestyle='--',
                          label=f'é˜ˆå€¼ = {self.threshold:.3f}')
        axes[1, 1].set_xlabel('æ¬ºè¯ˆæ¦‚ç‡')
        axes[1, 1].set_ylabel('å¯†åº¦')
        axes[1, 1].set_title('æ¬ºè¯ˆæ¦‚ç‡åˆ†å¸ƒ')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def analyze_transaction(self, amount, hour, merchant_risk, location_risk, frequency):
        """åˆ†æå•ç¬”äº¤æ˜“"""
        features = np.array([[amount, hour, merchant_risk, location_risk, frequency]])
        fraud_prob, is_fraud = self.predict_fraud(features)
        
        print(f"\n=== äº¤æ˜“é£é™©åˆ†æ ===")
        print(f"äº¤æ˜“é‡‘é¢: {amount:.2f} å…ƒ")
        print(f"äº¤æ˜“æ—¶é—´: {hour}:00")
        print(f"å•†æˆ·é£é™©è¯„åˆ†: {merchant_risk:.3f}")
        print(f"åœ°ç†é£é™©è¯„åˆ†: {location_risk:.3f}")
        print(f"å½“æ—¥äº¤æ˜“é¢‘æ¬¡: {frequency}")
        print(f"\næ¬ºè¯ˆæ¦‚ç‡: {fraud_prob[0]:.4f}")
        print(f"é£é™©ç­‰çº§: {'ğŸ”´ é«˜é£é™©' if is_fraud[0] else 'ğŸŸ¢ ä½é£é™©'}")
        
        if fraud_prob[0] > 0.8:
            print("âš ï¸ å»ºè®®ï¼šç«‹å³å†»ç»“äº¤æ˜“ï¼Œäººå·¥å®¡æ ¸")
        elif fraud_prob[0] > 0.5:
            print("âš ï¸ å»ºè®®ï¼šå¢å¼ºéªŒè¯ï¼ŒçŸ­ä¿¡ç¡®è®¤")
        else:
            print("âœ… å»ºè®®ï¼šæ­£å¸¸æ”¾è¡Œ")
        
        return fraud_prob[0], is_fraud[0]

# ä½¿ç”¨æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿ
fraud_detector = FraudDetectionSystem()

# ç”Ÿæˆæ•°æ®å¹¶è®­ç»ƒ
X_fraud, y_fraud = fraud_detector.generate_fraud_data(10000)
fraud_detector.train(X_fraud, y_fraud)

# è¯„ä¼°æ¨¡å‹
fraud_detector.evaluate_model()

# åˆ†æç¤ºä¾‹äº¤æ˜“
print("\n" + "="*50)
print("äº¤æ˜“é£é™©åˆ†æç¤ºä¾‹:")

# æ­£å¸¸äº¤æ˜“
fraud_detector.analyze_transaction(
    amount=150.0,      # 150å…ƒ
    hour=14,           # ä¸‹åˆ2ç‚¹
    merchant_risk=0.2, # ä½é£é™©å•†æˆ·
    location_risk=0.1, # å¸¸ç”¨åœ°ç‚¹
    frequency=3        # å½“æ—¥ç¬¬3ç¬”äº¤æ˜“
)

# å¯ç–‘äº¤æ˜“
fraud_detector.analyze_transaction(
    amount=5000.0,     # 5000å…ƒå¤§é¢äº¤æ˜“
    hour=3,            # å‡Œæ™¨3ç‚¹
    merchant_risk=0.8, # é«˜é£é™©å•†æˆ·
    location_risk=0.9, # å¼‚å¸¸åœ°ç‚¹
    frequency=15       # å½“æ—¥ç¬¬15ç¬”äº¤æ˜“
)
```

## å°ç»“

é€»è¾‘å›å½’æ˜¯åˆ†ç±»é—®é¢˜çš„åŸºç¡€ç®—æ³•ï¼š

### å…³é”®è¦ç‚¹

1. **æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡Sigmoidå‡½æ•°å°†çº¿æ€§è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡
2. **æ•°å­¦åŸºç¡€**ï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°
3. **è¾“å‡ºè§£é‡Š**ï¼šæ¦‚ç‡å€¼ï¼Œä¾¿äºé£é™©è¯„ä¼°å’Œå†³ç­–
4. **æ‰©å±•èƒ½åŠ›**ï¼šå¯å¤„ç†å¤šåˆ†ç±»é—®é¢˜
5. **å®ç”¨æŠ€å·§**ï¼šé˜ˆå€¼è°ƒä¼˜ï¼Œç±»åˆ«å¹³è¡¡å¤„ç†

### è¯„ä¼°æŒ‡æ ‡

- **å‡†ç¡®ç‡**ï¼šæ•´ä½“æ­£ç¡®ç‡
- **ç²¾ç¡®ç‡**ï¼šé¢„æµ‹ä¸ºæ­£ç±»ä¸­çœŸæ­£ä¸ºæ­£ç±»çš„æ¯”ä¾‹
- **å¬å›ç‡**ï¼šçœŸæ­£ä¸ºæ­£ç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **F1åˆ†æ•°**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
- **AUC-ROC**ï¼šæ¨¡å‹åŒºåˆ†èƒ½åŠ›
- **AUC-PR**ï¼šä¸å¹³è¡¡æ•°æ®é›†çš„è¯„ä¼°

### å®ç”¨å»ºè®®

1. **æ•°æ®é¢„å¤„ç†**ï¼šæ ‡å‡†åŒ–ç‰¹å¾ï¼Œå¤„ç†ç¼ºå¤±å€¼
2. **ç±»åˆ«å¹³è¡¡**ï¼šä½¿ç”¨class_weightå¤„ç†ä¸å¹³è¡¡æ•°æ®
3. **é˜ˆå€¼é€‰æ‹©**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚é˜ˆå€¼
4. **ç‰¹å¾å·¥ç¨‹**ï¼šå¤šé¡¹å¼ç‰¹å¾ï¼Œäº¤äº’é¡¹
5. **æ¨¡å‹è¯Šæ–­**ï¼šROCæ›²çº¿ï¼Œæ··æ·†çŸ©é˜µåˆ†æ

### é€‚ç”¨åœºæ™¯

- âœ… äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»é—®é¢˜
- âœ… éœ€è¦æ¦‚ç‡è¾“å‡ºçš„åœºæ™¯
- âœ… çº¿æ€§å¯åˆ†æˆ–è¿‘ä¼¼çº¿æ€§å¯åˆ†æ•°æ®
- âœ… é£é™©è¯„ä¼°å’Œå†³ç­–æ”¯æŒ
- âŒ å¤æ‚éçº¿æ€§å…³ç³»
- âŒ é«˜ç»´ç¨€ç–æ•°æ®

## æ€è€ƒé¢˜

1. ä¸ºä»€ä¹ˆé€»è¾‘å›å½’ä½¿ç”¨Sigmoidå‡½æ•°è€Œä¸æ˜¯å…¶ä»–å‡½æ•°ï¼Ÿ

2. åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸­ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡ï¼Ÿ

3. é€»è¾‘å›å½’çš„ç³»æ•°å¦‚ä½•è§£é‡Šï¼Ÿæ­£ç³»æ•°å’Œè´Ÿç³»æ•°åˆ†åˆ«æ„å‘³ç€ä»€ä¹ˆï¼Ÿ

4. ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥è°ƒæ•´åˆ†ç±»é˜ˆå€¼ï¼Ÿå¦‚ä½•é€‰æ‹©æœ€ä¼˜é˜ˆå€¼ï¼Ÿ

---

**ä¸‹ä¸€èŠ‚é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ å†³ç­–æ ‘ç®—æ³•ï¼Œäº†è§£å¦‚ä½•æ„å»ºå¯è§£é‡Šçš„éçº¿æ€§åˆ†ç±»æ¨¡å‹ã€‚