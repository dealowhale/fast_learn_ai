# 1.4.3 激活函数与损失函数

## 1. 激活函数详解

### 1.1 激活函数的作用

激活函数是神经网络中的关键组件，它为网络引入非线性，使得神经网络能够学习和表示复杂的非线性关系。

```mermaid
graph TD
    subgraph "激活函数的作用"
        A[线性变换 Z = WX + b] --> B[激活函数 A = f(Z)]
        B --> C[非线性输出]
    end
    
    subgraph "为什么需要激活函数"
        D["没有激活函数"] --> E["只能学习线性关系"]
        F["有激活函数"] --> G["可以学习复杂非线性关系"]
    end
```

**数学表达：**
- 线性变换：$Z = WX + b$
- 激活函数：$A = f(Z)$
- 网络输出：$Y = f_L(W_L \cdot f_{L-1}(W_{L-1} \cdot ... \cdot f_1(W_1X + b_1) + ... + b_{L-1}) + b_L)$

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

class ActivationFunctionAnalyzer:
    """激活函数分析器"""
    
    def __init__(self):
        self.x_range = np.linspace(-10, 10, 1000)
        
    def sigmoid(self, x):
        """Sigmoid激活函数"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x):
        """Sigmoid导数"""
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def relu(self, x):
        """ReLU激活函数"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """ReLU导数"""
        return (x > 0).astype(float)
    
    def leaky_relu(self, x, alpha=0.01):
        """Leaky ReLU激活函数"""
        return np.where(x > 0, x, alpha * x)
    
    def leaky_relu_derivative(self, x, alpha=0.01):
        """Leaky ReLU导数"""
        return np.where(x > 0, 1, alpha)
    
    def tanh(self, x):
        """Tanh激活函数"""
        return np.tanh(x)
    
    def tanh_derivative(self, x):
        """Tanh导数"""
        return 1 - np.tanh(x) ** 2
    
    def elu(self, x, alpha=1.0):
        """ELU激活函数"""
        return np.where(x > 0, x, alpha * (np.exp(x) - 1))
    
    def elu_derivative(self, x, alpha=1.0):
        """ELU导数"""
        return np.where(x > 0, 1, alpha * np.exp(x))
    
    def swish(self, x):
        """Swish激活函数"""
        return x * self.sigmoid(x)
    
    def swish_derivative(self, x):
        """Swish导数"""
        s = self.sigmoid(x)
        return s + x * s * (1 - s)
    
    def gelu(self, x):
        """GELU激活函数"""
        return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))
    
    def gelu_derivative(self, x):
        """GELU导数（近似）"""
        # 使用数值导数近似
        h = 1e-7
        return (self.gelu(x + h) - self.gelu(x - h)) / (2 * h)
    
    def softmax(self, x):
        """Softmax激活函数"""
        if x.ndim == 1:
            x = x.reshape(1, -1)
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def visualize_activation_functions(self):
        """可视化激活函数"""
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        axes = axes.flatten()
        
        # 激活函数列表
        functions = [
            ('Sigmoid', self.sigmoid, self.sigmoid_derivative),
            ('ReLU', self.relu, self.relu_derivative),
            ('Leaky ReLU', self.leaky_relu, self.leaky_relu_derivative),
            ('Tanh', self.tanh, self.tanh_derivative),
            ('ELU', self.elu, self.elu_derivative),
            ('Swish', self.swish, self.swish_derivative),
            ('GELU', self.gelu, self.gelu_derivative)
        ]
        
        for i, (name, func, deriv_func) in enumerate(functions):
            if i >= len(axes) - 2:  # 保留最后两个位置
                break
                
            y = func(self.x_range)
            dy = deriv_func(self.x_range)
            
            axes[i].plot(self.x_range, y, label=f'{name}', linewidth=2, color='blue')
            axes[i].plot(self.x_range, dy, label=f'{name} 导数', linewidth=2, 
                        color='red', linestyle='--', alpha=0.7)
            
            axes[i].set_title(f'{name} 激活函数', fontweight='bold', fontsize=12)
            axes[i].set_xlabel('输入 x')
            axes[i].set_ylabel('输出')
            axes[i].grid(True, alpha=0.3)
            axes[i].legend()
            axes[i].axhline(y=0, color='black', linewidth=0.5)
            axes[i].axvline(x=0, color='black', linewidth=0.5)
        
        # Softmax特殊处理
        x_softmax = np.linspace(-5, 5, 100)
        softmax_input = np.array([x_softmax, -x_softmax, np.zeros_like(x_softmax)])
        softmax_output = self.softmax(softmax_input.T)
        
        axes[-2].plot(x_softmax, softmax_output[:, 0], label='类别1', linewidth=2)
        axes[-2].plot(x_softmax, softmax_output[:, 1], label='类别2', linewidth=2)
        axes[-2].plot(x_softmax, softmax_output[:, 2], label='类别3', linewidth=2)
        axes[-2].set_title('Softmax 激活函数', fontweight='bold', fontsize=12)
        axes[-2].set_xlabel('输入 x')
        axes[-2].set_ylabel('概率')
        axes[-2].legend()
        axes[-2].grid(True, alpha=0.3)
        
        # 激活函数特性对比
        properties = {
            'Sigmoid': {'范围': '[0,1]', '梯度消失': '严重', '计算复杂度': '中等', '零中心': '否'},
            'ReLU': {'范围': '[0,∞)', '梯度消失': '轻微', '计算复杂度': '低', '零中心': '否'},
            'Leaky ReLU': {'范围': '(-∞,∞)', '梯度消失': '轻微', '计算复杂度': '低', '零中心': '否'},
            'Tanh': {'范围': '[-1,1]', '梯度消失': '中等', '计算复杂度': '中等', '零中心': '是'},
            'ELU': {'范围': '(-α,∞)', '梯度消失': '轻微', '计算复杂度': '中等', '零中心': '近似'},
            'Swish': {'范围': '(-∞,∞)', '梯度消失': '轻微', '计算复杂度': '中等', '零中心': '否'},
            'GELU': {'范围': '(-∞,∞)', '梯度消失': '轻微', '计算复杂度': '高', '零中心': '近似'}
        }
        
        # 创建特性对比表
        axes[-1].axis('off')
        table_data = []
        headers = ['激活函数', '输出范围', '梯度消失', '计算复杂度', '零中心']
        
        for func_name, props in properties.items():
            row = [func_name, props['范围'], props['梯度消失'], props['计算复杂度'], props['零中心']]
            table_data.append(row)
        
        table = axes[-1].table(cellText=table_data, colLabels=headers, 
                              cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        axes[-1].set_title('激活函数特性对比', fontweight='bold', fontsize=12)
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def analyze_gradient_flow(self, depth=10):
        """分析不同激活函数的梯度流动"""
        print(f"\n{'='*80}")
        print(f"📊 梯度流动分析 (网络深度: {depth})")
        print(f"{'='*80}")
        
        # 模拟深层网络的梯度传播
        initial_gradient = 1.0
        x_test = 0.5  # 测试点
        
        functions = {
            'Sigmoid': self.sigmoid_derivative,
            'ReLU': self.relu_derivative,
            'Tanh': self.tanh_derivative,
            'Leaky ReLU': self.leaky_relu_derivative,
            'ELU': self.elu_derivative
        }
        
        results = {}
        
        for name, deriv_func in functions.items():
            gradient = initial_gradient
            gradients = [gradient]
            
            for layer in range(depth):
                # 假设每层的输入都是x_test
                layer_derivative = deriv_func(x_test)
                gradient *= layer_derivative
                gradients.append(gradient)
            
            results[name] = gradients
            
            print(f"\n🔸 {name}:")
            print(f"   初始梯度: {initial_gradient:.6f}")
            print(f"   第{depth}层梯度: {gradient:.6e}")
            print(f"   梯度衰减比: {gradient/initial_gradient:.6e}")
            
            if gradient < 1e-10:
                print(f"   ⚠️  严重梯度消失!")
            elif gradient < 1e-5:
                print(f"   ⚠️  轻微梯度消失")
            elif gradient > 10:
                print(f"   ⚠️  梯度爆炸!")
            else:
                print(f"   ✅ 梯度正常")
        
        # 可视化梯度流动
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 线性尺度
        for name, gradients in results.items():
            ax1.plot(range(len(gradients)), gradients, label=name, linewidth=2, marker='o')
        
        ax1.set_xlabel('网络层数')
        ax1.set_ylabel('梯度大小')
        ax1.set_title('梯度流动 (线性尺度)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 对数尺度
        for name, gradients in results.items():
            # 避免log(0)
            gradients_safe = [max(abs(g), 1e-15) for g in gradients]
            ax2.semilogy(range(len(gradients_safe)), gradients_safe, label=name, linewidth=2, marker='o')
        
        ax2.set_xlabel('网络层数')
        ax2.set_ylabel('梯度大小 (对数尺度)')
        ax2.set_title('梯度流动 (对数尺度)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return results
    
    def dead_neuron_analysis(self):
        """分析ReLU死神经元问题"""
        print(f"\n{'='*80}")
        print(f"🧠 ReLU死神经元分析")
        print(f"{'='*80}")
        
        # 模拟不同初始化对死神经元的影响
        np.random.seed(42)
        n_neurons = 1000
        n_samples = 100
        
        # 不同的权重初始化方法
        initializations = {
            '标准正态分布': np.random.randn(n_neurons, 1),
            '小随机值': np.random.randn(n_neurons, 1) * 0.01,
            'He初始化': np.random.randn(n_neurons, 1) * np.sqrt(2.0),
            '负偏置': np.random.randn(n_neurons, 1) - 2.0
        }
        
        # 随机输入数据
        X = np.random.randn(n_samples, 1)
        
        results = {}
        
        for init_name, weights in initializations.items():
            # 计算神经元输出
            z = np.dot(X, weights.T)  # (n_samples, n_neurons)
            activations = self.relu(z)
            
            # 统计死神经元
            dead_neurons = np.sum(np.all(activations == 0, axis=0))
            dead_ratio = dead_neurons / n_neurons
            
            # 统计活跃神经元的平均激活值
            active_neurons = activations[:, np.any(activations > 0, axis=0)]
            avg_activation = np.mean(active_neurons) if active_neurons.size > 0 else 0
            
            results[init_name] = {
                'dead_count': dead_neurons,
                'dead_ratio': dead_ratio,
                'avg_activation': avg_activation,
                'activations': activations
            }
            
            print(f"\n🔸 {init_name}:")
            print(f"   死神经元数量: {dead_neurons}/{n_neurons}")
            print(f"   死神经元比例: {dead_ratio:.2%}")
            print(f"   平均激活值: {avg_activation:.4f}")
        
        # 可视化死神经元分布
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        for i, (init_name, result) in enumerate(results.items()):
            # 激活值分布直方图
            activations_flat = result['activations'].flatten()
            activations_nonzero = activations_flat[activations_flat > 0]
            
            axes[i].hist(activations_nonzero, bins=50, alpha=0.7, color='blue', 
                        label=f'活跃神经元 ({len(activations_nonzero)})')
            axes[i].axvline(0, color='red', linestyle='--', linewidth=2, 
                           label=f'死神经元 ({result["dead_count"]})')
            
            axes[i].set_xlabel('激活值')
            axes[i].set_ylabel('频次')
            axes[i].set_title(f'{init_name}\n死神经元比例: {result["dead_ratio"]:.2%}')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return results

# 激活函数分析演示
print("\n" + "=" * 80)
print("🧠 激活函数详细分析")
print("=" * 80)

analyzer = ActivationFunctionAnalyzer()

# 可视化所有激活函数
analyzer.visualize_activation_functions()

# 梯度流动分析
gradient_results = analyzer.analyze_gradient_flow(depth=15)

# 死神经元分析
dead_neuron_results = analyzer.dead_neuron_analysis()
```

## 2. 损失函数详解

### 2.1 损失函数的作用

损失函数（Loss Function）衡量模型预测值与真实值之间的差异，是训练神经网络的目标函数。

```mermaid
graph TD
    subgraph "损失函数的作用"
        A[预测值 ŷ] --> C[损失函数 L(ŷ,y)]
        B[真实值 y] --> C
        C --> D[损失值]
        D --> E[反向传播]
        E --> F[参数更新]
    end
    
    subgraph "损失函数类型"
        G[回归任务] --> H[MSE, MAE, Huber]
        I[分类任务] --> J[交叉熵, Hinge]
        K[特殊任务] --> L[自定义损失]
    end
```

```python
class LossFunctionAnalyzer:
    """损失函数分析器"""
    
    def __init__(self):
        pass
    
    # 回归损失函数
    def mse_loss(self, y_pred, y_true):
        """均方误差损失"""
        return np.mean((y_pred - y_true) ** 2)
    
    def mse_derivative(self, y_pred, y_true):
        """MSE导数"""
        return 2 * (y_pred - y_true) / len(y_true)
    
    def mae_loss(self, y_pred, y_true):
        """平均绝对误差损失"""
        return np.mean(np.abs(y_pred - y_true))
    
    def mae_derivative(self, y_pred, y_true):
        """MAE导数"""
        return np.sign(y_pred - y_true) / len(y_true)
    
    def huber_loss(self, y_pred, y_true, delta=1.0):
        """Huber损失"""
        residual = np.abs(y_pred - y_true)
        condition = residual <= delta
        return np.mean(np.where(condition, 0.5 * residual**2, delta * residual - 0.5 * delta**2))
    
    def huber_derivative(self, y_pred, y_true, delta=1.0):
        """Huber损失导数"""
        residual = y_pred - y_true
        condition = np.abs(residual) <= delta
        return np.where(condition, residual, delta * np.sign(residual)) / len(y_true)
    
    # 分类损失函数
    def binary_cross_entropy(self, y_pred, y_true):
        """二元交叉熵损失"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # 避免log(0)
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    def binary_cross_entropy_derivative(self, y_pred, y_true):
        """二元交叉熵导数"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -(y_true / y_pred - (1 - y_true) / (1 - y_pred)) / len(y_true)
    
    def categorical_cross_entropy(self, y_pred, y_true):
        """多类交叉熵损失"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
    
    def categorical_cross_entropy_derivative(self, y_pred, y_true):
        """多类交叉熵导数"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -(y_true / y_pred) / len(y_true)
    
    def hinge_loss(self, y_pred, y_true):
        """Hinge损失（SVM）"""
        return np.mean(np.maximum(0, 1 - y_true * y_pred))
    
    def hinge_derivative(self, y_pred, y_true):
        """Hinge损失导数"""
        condition = y_true * y_pred < 1
        return -y_true * condition / len(y_true)
    
    def focal_loss(self, y_pred, y_true, alpha=1.0, gamma=2.0):
        """Focal损失（处理类别不平衡）"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        ce_loss = -y_true * np.log(y_pred)
        p_t = np.where(y_true == 1, y_pred, 1 - y_pred)
        focal_weight = alpha * (1 - p_t) ** gamma
        return np.mean(focal_weight * ce_loss)
    
    def visualize_regression_losses(self):
        """可视化回归损失函数"""
        # 创建测试数据
        y_true = 0  # 真实值设为0
        y_pred_range = np.linspace(-3, 3, 1000)
        
        # 计算不同损失函数的值
        mse_values = [(pred - y_true)**2 for pred in y_pred_range]
        mae_values = [abs(pred - y_true) for pred in y_pred_range]
        huber_values = [self.huber_loss(np.array([pred]), np.array([y_true]), delta=1.0) 
                       for pred in y_pred_range]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 损失函数值
        axes[0, 0].plot(y_pred_range, mse_values, label='MSE', linewidth=2, color='blue')
        axes[0, 0].plot(y_pred_range, mae_values, label='MAE', linewidth=2, color='red')
        axes[0, 0].plot(y_pred_range, huber_values, label='Huber (δ=1)', linewidth=2, color='green')
        axes[0, 0].set_xlabel('预测值')
        axes[0, 0].set_ylabel('损失值')
        axes[0, 0].set_title('回归损失函数对比')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].axvline(y_true, color='black', linestyle='--', alpha=0.5, label='真实值')
        
        # 损失函数导数
        mse_derivs = [2 * (pred - y_true) for pred in y_pred_range]
        mae_derivs = [np.sign(pred - y_true) for pred in y_pred_range]
        huber_derivs = [self.huber_derivative(np.array([pred]), np.array([y_true]), delta=1.0)[0] 
                       for pred in y_pred_range]
        
        axes[0, 1].plot(y_pred_range, mse_derivs, label='MSE导数', linewidth=2, color='blue')
        axes[0, 1].plot(y_pred_range, mae_derivs, label='MAE导数', linewidth=2, color='red')
        axes[0, 1].plot(y_pred_range, huber_derivs, label='Huber导数', linewidth=2, color='green')
        axes[0, 1].set_xlabel('预测值')
        axes[0, 1].set_ylabel('梯度')
        axes[0, 1].set_title('回归损失函数梯度对比')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].axvline(y_true, color='black', linestyle='--', alpha=0.5)
        axes[0, 1].axhline(0, color='black', linestyle='-', alpha=0.3)
        
        # 异常值敏感性分析
        outlier_errors = np.array([-5, -2, -1, -0.5, 0, 0.5, 1, 2, 5])
        mse_outlier = [error**2 for error in outlier_errors]
        mae_outlier = [abs(error) for error in outlier_errors]
        huber_outlier = [self.huber_loss(np.array([error]), np.array([0]), delta=1.0) 
                        for error in outlier_errors]
        
        x_pos = np.arange(len(outlier_errors))
        width = 0.25
        
        axes[1, 0].bar(x_pos - width, mse_outlier, width, label='MSE', alpha=0.8, color='blue')
        axes[1, 0].bar(x_pos, mae_outlier, width, label='MAE', alpha=0.8, color='red')
        axes[1, 0].bar(x_pos + width, huber_outlier, width, label='Huber', alpha=0.8, color='green')
        
        axes[1, 0].set_xlabel('预测误差')
        axes[1, 0].set_ylabel('损失值')
        axes[1, 0].set_title('异常值敏感性对比')
        axes[1, 0].set_xticks(x_pos)
        axes[1, 0].set_xticklabels(outlier_errors)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 损失函数特性对比表
        axes[1, 1].axis('off')
        properties = [
            ['损失函数', '对异常值敏感性', '梯度特性', '适用场景', '计算复杂度'],
            ['MSE', '高', '连续，随误差增大', '标准回归', '低'],
            ['MAE', '低', '常数，不连续', '鲁棒回归', '低'],
            ['Huber', '中等', '平滑过渡', '鲁棒回归', '中等'],
            ['Focal', '可调', '关注困难样本', '不平衡分类', '中等']
        ]
        
        table = axes[1, 1].table(cellText=properties[1:], colLabels=properties[0], 
                                cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        axes[1, 1].set_title('损失函数特性对比', fontweight='bold', fontsize=12)
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def visualize_classification_losses(self):
        """可视化分类损失函数"""
        # 二分类情况
        y_true = 1  # 正类
        y_pred_range = np.linspace(0.001, 0.999, 1000)
        
        # 计算不同损失
        bce_values = [-np.log(p) for p in y_pred_range]  # 正类的交叉熵
        hinge_values = [max(0, 1 - (2*p - 1)) for p in y_pred_range]  # 转换为[-1,1]标签
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 二分类损失对比
        axes[0, 0].plot(y_pred_range, bce_values, label='Binary Cross-Entropy', linewidth=2, color='blue')
        axes[0, 0].plot(y_pred_range, hinge_values, label='Hinge Loss', linewidth=2, color='red')
        axes[0, 0].set_xlabel('预测概率 (正类)')
        axes[0, 0].set_ylabel('损失值')
        axes[0, 0].set_title('二分类损失函数对比')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_yscale('log')
        
        # 梯度对比
        bce_grads = [-(1/p) for p in y_pred_range]
        hinge_grads = [-2 if (2*p - 1) < 1 else 0 for p in y_pred_range]
        
        axes[0, 1].plot(y_pred_range, bce_grads, label='BCE梯度', linewidth=2, color='blue')
        axes[0, 1].plot(y_pred_range, hinge_grads, label='Hinge梯度', linewidth=2, color='red')
        axes[0, 1].set_xlabel('预测概率 (正类)')
        axes[0, 1].set_ylabel('梯度')
        axes[0, 1].set_title('分类损失函数梯度对比')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_ylim(-50, 5)
        
        # Focal Loss演示
        gamma_values = [0, 0.5, 1, 2, 5]
        for gamma in gamma_values:
            focal_values = [-(1-p)**gamma * np.log(p) for p in y_pred_range]
            axes[1, 0].plot(y_pred_range, focal_values, label=f'γ={gamma}', linewidth=2)
        
        axes[1, 0].set_xlabel('预测概率 (正类)')
        axes[1, 0].set_ylabel('Focal Loss')
        axes[1, 0].set_title('Focal Loss (不同γ值)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_yscale('log')
        
        # 类别不平衡影响
        class_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]
        standard_ce = []
        weighted_ce = []
        
        for ratio in class_ratios:
            # 模拟预测结果
            n_samples = 1000
            n_positive = int(n_samples * ratio)
            n_negative = n_samples - n_positive
            
            # 假设模型预测概率
            y_true_sim = np.concatenate([np.ones(n_positive), np.zeros(n_negative)])
            y_pred_sim = np.concatenate([np.random.beta(3, 1, n_positive), 
                                       np.random.beta(1, 3, n_negative)])
            
            # 标准交叉熵
            std_loss = self.binary_cross_entropy(y_pred_sim, y_true_sim)
            standard_ce.append(std_loss)
            
            # 加权交叉熵
            pos_weight = n_negative / n_positive if n_positive > 0 else 1
            weighted_loss = self.binary_cross_entropy(y_pred_sim, y_true_sim) * \
                          (pos_weight if ratio < 0.5 else 1/pos_weight)
            weighted_ce.append(weighted_loss)
        
        axes[1, 1].plot(class_ratios, standard_ce, 'o-', label='标准交叉熵', linewidth=2, color='blue')
        axes[1, 1].plot(class_ratios, weighted_ce, 's-', label='加权交叉熵', linewidth=2, color='red')
        axes[1, 1].set_xlabel('正类比例')
        axes[1, 1].set_ylabel('损失值')
        axes[1, 1].set_title('类别不平衡对损失的影响')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def loss_landscape_analysis(self):
        """损失函数景观分析"""
        print(f"\n{'='*80}")
        print(f"🗺️ 损失函数景观分析")
        print(f"{'='*80}")
        
        # 创建简单的二维优化问题
        def rosenbrock(x, y, a=1, b=100):
            """Rosenbrock函数（香蕉函数）"""
            return (a - x)**2 + b * (y - x**2)**2
        
        def quadratic(x, y):
            """二次函数"""
            return x**2 + y**2
        
        def beale(x, y):
            """Beale函数"""
            return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2
        
        # 创建网格
        x = np.linspace(-2, 2, 100)
        y = np.linspace(-1, 3, 100)
        X, Y = np.meshgrid(x, y)
        
        functions = {
            'Rosenbrock': rosenbrock,
            'Quadratic': quadratic,
            'Beale': beale
        }
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        for i, (name, func) in enumerate(functions.items()):
            Z = func(X, Y)
            
            # 使用对数尺度以更好地显示等高线
            if name == 'Beale':
                Z = np.log(Z + 1)  # 避免log(0)
            
            contour = axes[i].contour(X, Y, Z, levels=20, alpha=0.6)
            axes[i].contourf(X, Y, Z, levels=20, alpha=0.4, cmap='viridis')
            axes[i].clabel(contour, inline=True, fontsize=8)
            
            # 标记全局最小值
            if name == 'Rosenbrock':
                axes[i].plot(1, 1, 'r*', markersize=15, label='全局最小值')
            elif name == 'Quadratic':
                axes[i].plot(0, 0, 'r*', markersize=15, label='全局最小值')
            elif name == 'Beale':
                axes[i].plot(3, 0.5, 'r*', markersize=15, label='全局最小值')
            
            axes[i].set_xlabel('参数 x')
            axes[i].set_ylabel('参数 y')
            axes[i].set_title(f'{name} 函数景观')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # 分析不同损失函数的优化特性
        print(f"\n📊 损失函数优化特性分析:")
        
        characteristics = {
            'MSE': {
                '凸性': '凸函数',
                '全局最小值': '唯一',
                '梯度特性': '连续可导',
                '收敛性': '快速',
                '鲁棒性': '对异常值敏感'
            },
            'MAE': {
                '凸性': '凸函数',
                '全局最小值': '可能多个',
                '梯度特性': '不可导(0点)',
                '收敛性': '较慢',
                '鲁棒性': '对异常值鲁棒'
            },
            'Cross-Entropy': {
                '凸性': '凸函数',
                '全局最小值': '唯一',
                '梯度特性': '连续可导',
                '收敛性': '快速',
                '鲁棒性': '对标签噪声敏感'
            },
            'Hinge': {
                '凸性': '凸函数',
                '全局最小值': '可能多个',
                '梯度特性': '分段线性',
                '收敛性': '中等',
                '鲁棒性': '对异常值较鲁棒'
            }
        }
        
        for loss_name, props in characteristics.items():
            print(f"\n🔸 {loss_name}:")
            for prop_name, prop_value in props.items():
                print(f"   {prop_name}: {prop_value}")
        
        return fig

# 损失函数分析演示
print("\n" + "=" * 80)
print("📊 损失函数详细分析")
print("=" * 80)

loss_analyzer = LossFunctionAnalyzer()

# 可视化回归损失函数
loss_analyzer.visualize_regression_losses()

# 可视化分类损失函数
loss_analyzer.visualize_classification_losses()

# 损失函数景观分析
loss_analyzer.loss_landscape_analysis()
```

## 3. 激活函数与损失函数的匹配

### 3.1 最佳实践组合

```python
class ActivationLossMatchingGuide:
    """激活函数与损失函数匹配指南"""
    
    def __init__(self):
        self.combinations = {
            '二分类': {
                '推荐组合': [('Sigmoid', 'Binary Cross-Entropy')],
                '原因': 'Sigmoid输出[0,1]概率，与BCE数学上完美匹配',
                '梯度': '简化为 σ(z) - y，计算高效'
            },
            '多分类': {
                '推荐组合': [('Softmax', 'Categorical Cross-Entropy')],
                '原因': 'Softmax输出概率分布，与CCE数学上完美匹配',
                '梯度': '简化为 softmax(z) - y，计算高效'
            },
            '回归': {
                '推荐组合': [('Linear', 'MSE'), ('Linear', 'MAE')],
                '原因': '线性输出适合连续值预测',
                '选择': 'MSE适合正态分布误差，MAE适合鲁棒回归'
            },
            '隐藏层': {
                '推荐组合': [('ReLU', 'Any'), ('Leaky ReLU', 'Any'), ('GELU', 'Any')],
                '原因': '避免梯度消失，计算效率高',
                '注意': '避免使用Sigmoid/Tanh在深层网络'
            }
        }
    
    def demonstrate_combinations(self):
        """演示不同组合的效果"""
        print(f"\n{'='*80}")
        print(f"🎯 激活函数与损失函数匹配演示")
        print(f"{'='*80}")
        
        # 创建测试数据
        np.random.seed(42)
        n_samples = 1000
        
        # 二分类数据
        X_binary = np.random.randn(n_samples, 2)
        y_binary = (X_binary[:, 0] + X_binary[:, 1] > 0).astype(int)
        
        # 多分类数据（3类）
        X_multi = np.random.randn(n_samples, 2)
        centers = np.array([[2, 2], [-2, -2], [2, -2]])
        y_multi = np.argmin(np.linalg.norm(X_multi[:, None] - centers, axis=2), axis=1)
        
        # 回归数据
        X_reg = np.random.randn(n_samples, 1)
        y_reg = 2 * X_reg.flatten() + 0.5 * np.random.randn(n_samples)
        
        datasets = {
            '二分类': (X_binary, y_binary),
            '多分类': (X_multi, y_multi),
            '回归': (X_reg, y_reg)
        }
        
        for task_name, (X, y) in datasets.items():
            print(f"\n🔸 {task_name}任务:")
            
            if task_name in self.combinations:
                combo_info = self.combinations[task_name]
                print(f"   推荐组合: {combo_info['推荐组合']}")
                print(f"   原因: {combo_info['原因']}")
                
                if '梯度' in combo_info:
                    print(f"   梯度简化: {combo_info['梯度']}")
                if '选择' in combo_info:
                    print(f"   选择建议: {combo_info['选择']}")
        
        return datasets
    
    def gradient_flow_comparison(self):
        """比较不同组合的梯度流动"""
        print(f"\n{'='*80}")
        print(f"🌊 梯度流动对比分析")
        print(f"{'='*80}")
        
        # 模拟不同激活函数在深层网络中的梯度传播
        def simulate_gradient_flow(activation_func, depth=10, input_val=0.5):
            """模拟梯度在深层网络中的传播"""
            gradient = 1.0
            gradients = [gradient]
            
            for layer in range(depth):
                if activation_func == 'sigmoid':
                    # Sigmoid导数: σ(x)(1-σ(x))
                    sigma = 1 / (1 + np.exp(-input_val))
                    derivative = sigma * (1 - sigma)
                elif activation_func == 'tanh':
                    # Tanh导数: 1 - tanh²(x)
                    derivative = 1 - np.tanh(input_val) ** 2
                elif activation_func == 'relu':
                    # ReLU导数: 1 if x > 0 else 0
                    derivative = 1.0 if input_val > 0 else 0.0
                elif activation_func == 'leaky_relu':
                    # Leaky ReLU导数: 1 if x > 0 else 0.01
                    derivative = 1.0 if input_val > 0 else 0.01
                
                gradient *= derivative
                gradients.append(gradient)
                
                # 更新输入值（简化模拟）
                input_val = input_val * 0.9
            
            return gradients
        
        # 测试不同激活函数
        activation_functions = ['sigmoid', 'tanh', 'relu', 'leaky_relu']
        depth = 15
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        gradient_results = {}
        
        for activation in activation_functions:
            gradients = simulate_gradient_flow(activation, depth)
            gradient_results[activation] = gradients
            
            # 线性尺度
            ax1.plot(range(len(gradients)), gradients, 
                    label=activation.replace('_', ' ').title(), 
                    linewidth=2, marker='o')
            
            # 对数尺度
            gradients_abs = [max(abs(g), 1e-15) for g in gradients]
            ax2.semilogy(range(len(gradients_abs)), gradients_abs, 
                        label=activation.replace('_', ' ').title(), 
                        linewidth=2, marker='o')
        
        ax1.set_xlabel('网络层数')
        ax1.set_ylabel('梯度大小')
        ax1.set_title('梯度传播 (线性尺度)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        ax2.set_xlabel('网络层数')
        ax2.set_ylabel('梯度大小 (对数尺度)')
        ax2.set_title('梯度传播 (对数尺度)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # 分析结果
        print(f"\n📊 梯度传播分析结果:")
        for activation, gradients in gradient_results.items():
            final_gradient = gradients[-1]
            print(f"\n🔸 {activation.replace('_', ' ').title()}:")
            print(f"   初始梯度: 1.000000")
            print(f"   第{depth}层梯度: {final_gradient:.6e}")
            print(f"   梯度保持率: {final_gradient:.6e}")
            
            if abs(final_gradient) < 1e-10:
                print(f"   状态: ❌ 严重梯度消失")
            elif abs(final_gradient) < 1e-5:
                print(f"   状态: ⚠️ 轻微梯度消失")
            elif abs(final_gradient) > 10:
                print(f"   状态: ⚠️ 梯度爆炸")
            else:
                print(f"   状态: ✅ 梯度正常")
        
        return gradient_results
    
    def create_best_practices_guide(self):
        """创建最佳实践指南"""
        print(f"\n{'='*80}")
        print(f"📋 激活函数与损失函数最佳实践指南")
        print(f"{'='*80}")
        
        best_practices = {
            '🎯 任务类型匹配': {
                '二分类': 'Sigmoid + Binary Cross-Entropy',
                '多分类': 'Softmax + Categorical Cross-Entropy',
                '回归': 'Linear + MSE/MAE',
                '多标签分类': 'Sigmoid + Binary Cross-Entropy (每个标签)'
            },
            
            '🏗️ 网络架构建议': {
                '隐藏层': 'ReLU/Leaky ReLU/GELU (避免梯度消失)',
                '输出层': '根据任务选择 (Sigmoid/Softmax/Linear)',
                '深层网络': '使用ReLU变体 + 批归一化',
                '残差网络': 'ReLU + 跳跃连接'
            },
            
            '⚡ 性能优化': {
                '计算效率': 'ReLU > Sigmoid/Tanh (计算简单)',
                '内存效率': '避免复杂激活函数在大网络中',
                '训练稳定性': '使用He/Xavier初始化配合相应激活函数',
                '收敛速度': 'Adam + ReLU通常收敛最快'
            },
            
            '🛠️ 调试技巧': {
                '梯度消失': '检查激活函数导数，使用ReLU变体',
                '梯度爆炸': '使用梯度裁剪，检查学习率',
                '死神经元': '使用Leaky ReLU替代ReLU',
                '训练不稳定': '检查激活函数与损失函数匹配'
            },
            
            '🚨 常见错误': {
                '错误1': 'Softmax + MSE (应该用Cross-Entropy)',
                '错误2': '深层网络使用Sigmoid (梯度消失)',
                '错误3': '回归任务使用Sigmoid输出 (范围受限)',
                '错误4': '忽略激活函数的数值稳定性'
            }
        }
        
        for category, items in best_practices.items():
            print(f"\n{category}:")
            for key, value in items.items():
                print(f"   • {key}: {value}")
        
        # 创建决策树
        print(f"\n🌳 激活函数选择决策树:")
        print(f"""   
   开始
    |
    ├─ 是输出层？
    │   ├─ 是 → 任务类型？
    │   │   ├─ 二分类 → Sigmoid
    │   │   ├─ 多分类 → Softmax  
    │   │   └─ 回归 → Linear
    │   └─ 否 → 隐藏层
    │       ├─ 深层网络？
    │       │   ├─ 是 → ReLU/Leaky ReLU
    │       │   └─ 否 → ReLU/Tanh
    │       └─ 特殊需求？
    │           ├─ 平滑性 → GELU/Swish
    │           └─ 标准选择 → ReLU
        """)
        
        return best_practices

# 激活函数与损失函数匹配演示
print("\n" + "=" * 80)
print("🎯 激活函数与损失函数匹配分析")
print("=" * 80)

matching_guide = ActivationLossMatchingGuide()

# 演示不同组合
datasets = matching_guide.demonstrate_combinations()

# 梯度流动对比
gradient_comparison = matching_guide.gradient_flow_comparison()

# 最佳实践指南
best_practices = matching_guide.create_best_practices_guide()
```

## 4. 思考题

1. **激活函数选择**: 为什么ReLU在深层网络中比Sigmoid表现更好？Leaky ReLU相比ReLU有什么优势？

2. **损失函数设计**: 在处理类别不平衡问题时，如何设计或修改损失函数？Focal Loss的核心思想是什么？

3. **梯度问题**: 什么情况下会出现梯度消失和梯度爆炸？如何通过激活函数和损失函数的选择来缓解这些问题？

4. **数值稳定性**: 在实现Softmax和交叉熵损失时，需要注意哪些数值稳定性问题？如何解决？

5. **自定义设计**: 如果要为特定任务设计新的激活函数或损失函数，需要考虑哪些因素？

## 5. 小结

### 5.1 核心概念

- **激活函数**: 为神经网络引入非线性的关键组件
- **损失函数**: 衡量模型预测与真实值差异的目标函数
- **梯度流动**: 反向传播中梯度在网络中的传播过程
- **函数匹配**: 激活函数与损失函数的合理搭配

### 5.2 关键技术

- **ReLU系列**: 解决梯度消失问题的有效激活函数
- **Softmax**: 多分类任务的标准输出激活函数
- **交叉熵**: 分类任务的标准损失函数
- **数值稳定性**: 防止计算溢出的重要技术

### 5.3 实践要点

- 根据任务类型选择合适的激活函数和损失函数组合
- 在深层网络中优先使用ReLU及其变体
- 注意数值稳定性，特别是在计算指数和对数时
- 监控梯度流动，及时发现和解决梯度问题
- 考虑特殊需求（如类别不平衡）选择或设计损失函数

通过本节的学习，你已经深入理解了激活函数和损失函数的原理、特性和应用场景，这为你设计和优化神经网络提供了重要的理论基础和实践指导。