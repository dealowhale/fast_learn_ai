# 1.4.3 æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°

## 1. æ¿€æ´»å‡½æ•°è¯¦è§£

### 1.1 æ¿€æ´»å‡½æ•°çš„ä½œç”¨

æ¿€æ´»å‡½æ•°æ˜¯ç¥ç»ç½‘ç»œä¸­çš„å…³é”®ç»„ä»¶ï¼Œå®ƒä¸ºç½‘ç»œå¼•å…¥éçº¿æ€§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å’Œè¡¨ç¤ºå¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚

```mermaid
graph TD
    subgraph "æ¿€æ´»å‡½æ•°çš„ä½œç”¨"
        A[çº¿æ€§å˜æ¢ Z = WX + b] --> B[æ¿€æ´»å‡½æ•° A = f(Z)]
        B --> C[éçº¿æ€§è¾“å‡º]
    end
    
    subgraph "ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°"
        D["æ²¡æœ‰æ¿€æ´»å‡½æ•°"] --> E["åªèƒ½å­¦ä¹ çº¿æ€§å…³ç³»"]
        F["æœ‰æ¿€æ´»å‡½æ•°"] --> G["å¯ä»¥å­¦ä¹ å¤æ‚éçº¿æ€§å…³ç³»"]
    end
```

**æ•°å­¦è¡¨è¾¾ï¼š**
- çº¿æ€§å˜æ¢ï¼š$Z = WX + b$
- æ¿€æ´»å‡½æ•°ï¼š$A = f(Z)$
- ç½‘ç»œè¾“å‡ºï¼š$Y = f_L(W_L \cdot f_{L-1}(W_{L-1} \cdot ... \cdot f_1(W_1X + b_1) + ... + b_{L-1}) + b_L)$

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

class ActivationFunctionAnalyzer:
    """æ¿€æ´»å‡½æ•°åˆ†æå™¨"""
    
    def __init__(self):
        self.x_range = np.linspace(-10, 10, 1000)
        
    def sigmoid(self, x):
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x):
        """Sigmoidå¯¼æ•°"""
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def relu(self, x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """ReLUå¯¼æ•°"""
        return (x > 0).astype(float)
    
    def leaky_relu(self, x, alpha=0.01):
        """Leaky ReLUæ¿€æ´»å‡½æ•°"""
        return np.where(x > 0, x, alpha * x)
    
    def leaky_relu_derivative(self, x, alpha=0.01):
        """Leaky ReLUå¯¼æ•°"""
        return np.where(x > 0, 1, alpha)
    
    def tanh(self, x):
        """Tanhæ¿€æ´»å‡½æ•°"""
        return np.tanh(x)
    
    def tanh_derivative(self, x):
        """Tanhå¯¼æ•°"""
        return 1 - np.tanh(x) ** 2
    
    def elu(self, x, alpha=1.0):
        """ELUæ¿€æ´»å‡½æ•°"""
        return np.where(x > 0, x, alpha * (np.exp(x) - 1))
    
    def elu_derivative(self, x, alpha=1.0):
        """ELUå¯¼æ•°"""
        return np.where(x > 0, 1, alpha * np.exp(x))
    
    def swish(self, x):
        """Swishæ¿€æ´»å‡½æ•°"""
        return x * self.sigmoid(x)
    
    def swish_derivative(self, x):
        """Swishå¯¼æ•°"""
        s = self.sigmoid(x)
        return s + x * s * (1 - s)
    
    def gelu(self, x):
        """GELUæ¿€æ´»å‡½æ•°"""
        return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))
    
    def gelu_derivative(self, x):
        """GELUå¯¼æ•°ï¼ˆè¿‘ä¼¼ï¼‰"""
        # ä½¿ç”¨æ•°å€¼å¯¼æ•°è¿‘ä¼¼
        h = 1e-7
        return (self.gelu(x + h) - self.gelu(x - h)) / (2 * h)
    
    def softmax(self, x):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        if x.ndim == 1:
            x = x.reshape(1, -1)
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def visualize_activation_functions(self):
        """å¯è§†åŒ–æ¿€æ´»å‡½æ•°"""
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        axes = axes.flatten()
        
        # æ¿€æ´»å‡½æ•°åˆ—è¡¨
        functions = [
            ('Sigmoid', self.sigmoid, self.sigmoid_derivative),
            ('ReLU', self.relu, self.relu_derivative),
            ('Leaky ReLU', self.leaky_relu, self.leaky_relu_derivative),
            ('Tanh', self.tanh, self.tanh_derivative),
            ('ELU', self.elu, self.elu_derivative),
            ('Swish', self.swish, self.swish_derivative),
            ('GELU', self.gelu, self.gelu_derivative)
        ]
        
        for i, (name, func, deriv_func) in enumerate(functions):
            if i >= len(axes) - 2:  # ä¿ç•™æœ€åä¸¤ä¸ªä½ç½®
                break
                
            y = func(self.x_range)
            dy = deriv_func(self.x_range)
            
            axes[i].plot(self.x_range, y, label=f'{name}', linewidth=2, color='blue')
            axes[i].plot(self.x_range, dy, label=f'{name} å¯¼æ•°', linewidth=2, 
                        color='red', linestyle='--', alpha=0.7)
            
            axes[i].set_title(f'{name} æ¿€æ´»å‡½æ•°', fontweight='bold', fontsize=12)
            axes[i].set_xlabel('è¾“å…¥ x')
            axes[i].set_ylabel('è¾“å‡º')
            axes[i].grid(True, alpha=0.3)
            axes[i].legend()
            axes[i].axhline(y=0, color='black', linewidth=0.5)
            axes[i].axvline(x=0, color='black', linewidth=0.5)
        
        # Softmaxç‰¹æ®Šå¤„ç†
        x_softmax = np.linspace(-5, 5, 100)
        softmax_input = np.array([x_softmax, -x_softmax, np.zeros_like(x_softmax)])
        softmax_output = self.softmax(softmax_input.T)
        
        axes[-2].plot(x_softmax, softmax_output[:, 0], label='ç±»åˆ«1', linewidth=2)
        axes[-2].plot(x_softmax, softmax_output[:, 1], label='ç±»åˆ«2', linewidth=2)
        axes[-2].plot(x_softmax, softmax_output[:, 2], label='ç±»åˆ«3', linewidth=2)
        axes[-2].set_title('Softmax æ¿€æ´»å‡½æ•°', fontweight='bold', fontsize=12)
        axes[-2].set_xlabel('è¾“å…¥ x')
        axes[-2].set_ylabel('æ¦‚ç‡')
        axes[-2].legend()
        axes[-2].grid(True, alpha=0.3)
        
        # æ¿€æ´»å‡½æ•°ç‰¹æ€§å¯¹æ¯”
        properties = {
            'Sigmoid': {'èŒƒå›´': '[0,1]', 'æ¢¯åº¦æ¶ˆå¤±': 'ä¸¥é‡', 'è®¡ç®—å¤æ‚åº¦': 'ä¸­ç­‰', 'é›¶ä¸­å¿ƒ': 'å¦'},
            'ReLU': {'èŒƒå›´': '[0,âˆ)', 'æ¢¯åº¦æ¶ˆå¤±': 'è½»å¾®', 'è®¡ç®—å¤æ‚åº¦': 'ä½', 'é›¶ä¸­å¿ƒ': 'å¦'},
            'Leaky ReLU': {'èŒƒå›´': '(-âˆ,âˆ)', 'æ¢¯åº¦æ¶ˆå¤±': 'è½»å¾®', 'è®¡ç®—å¤æ‚åº¦': 'ä½', 'é›¶ä¸­å¿ƒ': 'å¦'},
            'Tanh': {'èŒƒå›´': '[-1,1]', 'æ¢¯åº¦æ¶ˆå¤±': 'ä¸­ç­‰', 'è®¡ç®—å¤æ‚åº¦': 'ä¸­ç­‰', 'é›¶ä¸­å¿ƒ': 'æ˜¯'},
            'ELU': {'èŒƒå›´': '(-Î±,âˆ)', 'æ¢¯åº¦æ¶ˆå¤±': 'è½»å¾®', 'è®¡ç®—å¤æ‚åº¦': 'ä¸­ç­‰', 'é›¶ä¸­å¿ƒ': 'è¿‘ä¼¼'},
            'Swish': {'èŒƒå›´': '(-âˆ,âˆ)', 'æ¢¯åº¦æ¶ˆå¤±': 'è½»å¾®', 'è®¡ç®—å¤æ‚åº¦': 'ä¸­ç­‰', 'é›¶ä¸­å¿ƒ': 'å¦'},
            'GELU': {'èŒƒå›´': '(-âˆ,âˆ)', 'æ¢¯åº¦æ¶ˆå¤±': 'è½»å¾®', 'è®¡ç®—å¤æ‚åº¦': 'é«˜', 'é›¶ä¸­å¿ƒ': 'è¿‘ä¼¼'}
        }
        
        # åˆ›å»ºç‰¹æ€§å¯¹æ¯”è¡¨
        axes[-1].axis('off')
        table_data = []
        headers = ['æ¿€æ´»å‡½æ•°', 'è¾“å‡ºèŒƒå›´', 'æ¢¯åº¦æ¶ˆå¤±', 'è®¡ç®—å¤æ‚åº¦', 'é›¶ä¸­å¿ƒ']
        
        for func_name, props in properties.items():
            row = [func_name, props['èŒƒå›´'], props['æ¢¯åº¦æ¶ˆå¤±'], props['è®¡ç®—å¤æ‚åº¦'], props['é›¶ä¸­å¿ƒ']]
            table_data.append(row)
        
        table = axes[-1].table(cellText=table_data, colLabels=headers, 
                              cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        axes[-1].set_title('æ¿€æ´»å‡½æ•°ç‰¹æ€§å¯¹æ¯”', fontweight='bold', fontsize=12)
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def analyze_gradient_flow(self, depth=10):
        """åˆ†æä¸åŒæ¿€æ´»å‡½æ•°çš„æ¢¯åº¦æµåŠ¨"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ¢¯åº¦æµåŠ¨åˆ†æ (ç½‘ç»œæ·±åº¦: {depth})")
        print(f"{'='*80}")
        
        # æ¨¡æ‹Ÿæ·±å±‚ç½‘ç»œçš„æ¢¯åº¦ä¼ æ’­
        initial_gradient = 1.0
        x_test = 0.5  # æµ‹è¯•ç‚¹
        
        functions = {
            'Sigmoid': self.sigmoid_derivative,
            'ReLU': self.relu_derivative,
            'Tanh': self.tanh_derivative,
            'Leaky ReLU': self.leaky_relu_derivative,
            'ELU': self.elu_derivative
        }
        
        results = {}
        
        for name, deriv_func in functions.items():
            gradient = initial_gradient
            gradients = [gradient]
            
            for layer in range(depth):
                # å‡è®¾æ¯å±‚çš„è¾“å…¥éƒ½æ˜¯x_test
                layer_derivative = deriv_func(x_test)
                gradient *= layer_derivative
                gradients.append(gradient)
            
            results[name] = gradients
            
            print(f"\nğŸ”¸ {name}:")
            print(f"   åˆå§‹æ¢¯åº¦: {initial_gradient:.6f}")
            print(f"   ç¬¬{depth}å±‚æ¢¯åº¦: {gradient:.6e}")
            print(f"   æ¢¯åº¦è¡°å‡æ¯”: {gradient/initial_gradient:.6e}")
            
            if gradient < 1e-10:
                print(f"   âš ï¸  ä¸¥é‡æ¢¯åº¦æ¶ˆå¤±!")
            elif gradient < 1e-5:
                print(f"   âš ï¸  è½»å¾®æ¢¯åº¦æ¶ˆå¤±")
            elif gradient > 10:
                print(f"   âš ï¸  æ¢¯åº¦çˆ†ç‚¸!")
            else:
                print(f"   âœ… æ¢¯åº¦æ­£å¸¸")
        
        # å¯è§†åŒ–æ¢¯åº¦æµåŠ¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # çº¿æ€§å°ºåº¦
        for name, gradients in results.items():
            ax1.plot(range(len(gradients)), gradients, label=name, linewidth=2, marker='o')
        
        ax1.set_xlabel('ç½‘ç»œå±‚æ•°')
        ax1.set_ylabel('æ¢¯åº¦å¤§å°')
        ax1.set_title('æ¢¯åº¦æµåŠ¨ (çº¿æ€§å°ºåº¦)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å¯¹æ•°å°ºåº¦
        for name, gradients in results.items():
            # é¿å…log(0)
            gradients_safe = [max(abs(g), 1e-15) for g in gradients]
            ax2.semilogy(range(len(gradients_safe)), gradients_safe, label=name, linewidth=2, marker='o')
        
        ax2.set_xlabel('ç½‘ç»œå±‚æ•°')
        ax2.set_ylabel('æ¢¯åº¦å¤§å° (å¯¹æ•°å°ºåº¦)')
        ax2.set_title('æ¢¯åº¦æµåŠ¨ (å¯¹æ•°å°ºåº¦)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return results
    
    def dead_neuron_analysis(self):
        """åˆ†æReLUæ­»ç¥ç»å…ƒé—®é¢˜"""
        print(f"\n{'='*80}")
        print(f"ğŸ§  ReLUæ­»ç¥ç»å…ƒåˆ†æ")
        print(f"{'='*80}")
        
        # æ¨¡æ‹Ÿä¸åŒåˆå§‹åŒ–å¯¹æ­»ç¥ç»å…ƒçš„å½±å“
        np.random.seed(42)
        n_neurons = 1000
        n_samples = 100
        
        # ä¸åŒçš„æƒé‡åˆå§‹åŒ–æ–¹æ³•
        initializations = {
            'æ ‡å‡†æ­£æ€åˆ†å¸ƒ': np.random.randn(n_neurons, 1),
            'å°éšæœºå€¼': np.random.randn(n_neurons, 1) * 0.01,
            'Heåˆå§‹åŒ–': np.random.randn(n_neurons, 1) * np.sqrt(2.0),
            'è´Ÿåç½®': np.random.randn(n_neurons, 1) - 2.0
        }
        
        # éšæœºè¾“å…¥æ•°æ®
        X = np.random.randn(n_samples, 1)
        
        results = {}
        
        for init_name, weights in initializations.items():
            # è®¡ç®—ç¥ç»å…ƒè¾“å‡º
            z = np.dot(X, weights.T)  # (n_samples, n_neurons)
            activations = self.relu(z)
            
            # ç»Ÿè®¡æ­»ç¥ç»å…ƒ
            dead_neurons = np.sum(np.all(activations == 0, axis=0))
            dead_ratio = dead_neurons / n_neurons
            
            # ç»Ÿè®¡æ´»è·ƒç¥ç»å…ƒçš„å¹³å‡æ¿€æ´»å€¼
            active_neurons = activations[:, np.any(activations > 0, axis=0)]
            avg_activation = np.mean(active_neurons) if active_neurons.size > 0 else 0
            
            results[init_name] = {
                'dead_count': dead_neurons,
                'dead_ratio': dead_ratio,
                'avg_activation': avg_activation,
                'activations': activations
            }
            
            print(f"\nğŸ”¸ {init_name}:")
            print(f"   æ­»ç¥ç»å…ƒæ•°é‡: {dead_neurons}/{n_neurons}")
            print(f"   æ­»ç¥ç»å…ƒæ¯”ä¾‹: {dead_ratio:.2%}")
            print(f"   å¹³å‡æ¿€æ´»å€¼: {avg_activation:.4f}")
        
        # å¯è§†åŒ–æ­»ç¥ç»å…ƒåˆ†å¸ƒ
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        for i, (init_name, result) in enumerate(results.items()):
            # æ¿€æ´»å€¼åˆ†å¸ƒç›´æ–¹å›¾
            activations_flat = result['activations'].flatten()
            activations_nonzero = activations_flat[activations_flat > 0]
            
            axes[i].hist(activations_nonzero, bins=50, alpha=0.7, color='blue', 
                        label=f'æ´»è·ƒç¥ç»å…ƒ ({len(activations_nonzero)})')
            axes[i].axvline(0, color='red', linestyle='--', linewidth=2, 
                           label=f'æ­»ç¥ç»å…ƒ ({result["dead_count"]})')
            
            axes[i].set_xlabel('æ¿€æ´»å€¼')
            axes[i].set_ylabel('é¢‘æ¬¡')
            axes[i].set_title(f'{init_name}\næ­»ç¥ç»å…ƒæ¯”ä¾‹: {result["dead_ratio"]:.2%}')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return results

# æ¿€æ´»å‡½æ•°åˆ†ææ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ§  æ¿€æ´»å‡½æ•°è¯¦ç»†åˆ†æ")
print("=" * 80)

analyzer = ActivationFunctionAnalyzer()

# å¯è§†åŒ–æ‰€æœ‰æ¿€æ´»å‡½æ•°
analyzer.visualize_activation_functions()

# æ¢¯åº¦æµåŠ¨åˆ†æ
gradient_results = analyzer.analyze_gradient_flow(depth=15)

# æ­»ç¥ç»å…ƒåˆ†æ
dead_neuron_results = analyzer.dead_neuron_analysis()
```

## 2. æŸå¤±å‡½æ•°è¯¦è§£

### 2.1 æŸå¤±å‡½æ•°çš„ä½œç”¨

æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰è¡¡é‡æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œæ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„ç›®æ ‡å‡½æ•°ã€‚

```mermaid
graph TD
    subgraph "æŸå¤±å‡½æ•°çš„ä½œç”¨"
        A[é¢„æµ‹å€¼ Å·] --> C[æŸå¤±å‡½æ•° L(Å·,y)]
        B[çœŸå®å€¼ y] --> C
        C --> D[æŸå¤±å€¼]
        D --> E[åå‘ä¼ æ’­]
        E --> F[å‚æ•°æ›´æ–°]
    end
    
    subgraph "æŸå¤±å‡½æ•°ç±»å‹"
        G[å›å½’ä»»åŠ¡] --> H[MSE, MAE, Huber]
        I[åˆ†ç±»ä»»åŠ¡] --> J[äº¤å‰ç†µ, Hinge]
        K[ç‰¹æ®Šä»»åŠ¡] --> L[è‡ªå®šä¹‰æŸå¤±]
    end
```

```python
class LossFunctionAnalyzer:
    """æŸå¤±å‡½æ•°åˆ†æå™¨"""
    
    def __init__(self):
        pass
    
    # å›å½’æŸå¤±å‡½æ•°
    def mse_loss(self, y_pred, y_true):
        """å‡æ–¹è¯¯å·®æŸå¤±"""
        return np.mean((y_pred - y_true) ** 2)
    
    def mse_derivative(self, y_pred, y_true):
        """MSEå¯¼æ•°"""
        return 2 * (y_pred - y_true) / len(y_true)
    
    def mae_loss(self, y_pred, y_true):
        """å¹³å‡ç»å¯¹è¯¯å·®æŸå¤±"""
        return np.mean(np.abs(y_pred - y_true))
    
    def mae_derivative(self, y_pred, y_true):
        """MAEå¯¼æ•°"""
        return np.sign(y_pred - y_true) / len(y_true)
    
    def huber_loss(self, y_pred, y_true, delta=1.0):
        """HuberæŸå¤±"""
        residual = np.abs(y_pred - y_true)
        condition = residual <= delta
        return np.mean(np.where(condition, 0.5 * residual**2, delta * residual - 0.5 * delta**2))
    
    def huber_derivative(self, y_pred, y_true, delta=1.0):
        """HuberæŸå¤±å¯¼æ•°"""
        residual = y_pred - y_true
        condition = np.abs(residual) <= delta
        return np.where(condition, residual, delta * np.sign(residual)) / len(y_true)
    
    # åˆ†ç±»æŸå¤±å‡½æ•°
    def binary_cross_entropy(self, y_pred, y_true):
        """äºŒå…ƒäº¤å‰ç†µæŸå¤±"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)  # é¿å…log(0)
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    def binary_cross_entropy_derivative(self, y_pred, y_true):
        """äºŒå…ƒäº¤å‰ç†µå¯¼æ•°"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -(y_true / y_pred - (1 - y_true) / (1 - y_pred)) / len(y_true)
    
    def categorical_cross_entropy(self, y_pred, y_true):
        """å¤šç±»äº¤å‰ç†µæŸå¤±"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
    
    def categorical_cross_entropy_derivative(self, y_pred, y_true):
        """å¤šç±»äº¤å‰ç†µå¯¼æ•°"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -(y_true / y_pred) / len(y_true)
    
    def hinge_loss(self, y_pred, y_true):
        """HingeæŸå¤±ï¼ˆSVMï¼‰"""
        return np.mean(np.maximum(0, 1 - y_true * y_pred))
    
    def hinge_derivative(self, y_pred, y_true):
        """HingeæŸå¤±å¯¼æ•°"""
        condition = y_true * y_pred < 1
        return -y_true * condition / len(y_true)
    
    def focal_loss(self, y_pred, y_true, alpha=1.0, gamma=2.0):
        """FocalæŸå¤±ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰"""
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        ce_loss = -y_true * np.log(y_pred)
        p_t = np.where(y_true == 1, y_pred, 1 - y_pred)
        focal_weight = alpha * (1 - p_t) ** gamma
        return np.mean(focal_weight * ce_loss)
    
    def visualize_regression_losses(self):
        """å¯è§†åŒ–å›å½’æŸå¤±å‡½æ•°"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        y_true = 0  # çœŸå®å€¼è®¾ä¸º0
        y_pred_range = np.linspace(-3, 3, 1000)
        
        # è®¡ç®—ä¸åŒæŸå¤±å‡½æ•°çš„å€¼
        mse_values = [(pred - y_true)**2 for pred in y_pred_range]
        mae_values = [abs(pred - y_true) for pred in y_pred_range]
        huber_values = [self.huber_loss(np.array([pred]), np.array([y_true]), delta=1.0) 
                       for pred in y_pred_range]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # æŸå¤±å‡½æ•°å€¼
        axes[0, 0].plot(y_pred_range, mse_values, label='MSE', linewidth=2, color='blue')
        axes[0, 0].plot(y_pred_range, mae_values, label='MAE', linewidth=2, color='red')
        axes[0, 0].plot(y_pred_range, huber_values, label='Huber (Î´=1)', linewidth=2, color='green')
        axes[0, 0].set_xlabel('é¢„æµ‹å€¼')
        axes[0, 0].set_ylabel('æŸå¤±å€¼')
        axes[0, 0].set_title('å›å½’æŸå¤±å‡½æ•°å¯¹æ¯”')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].axvline(y_true, color='black', linestyle='--', alpha=0.5, label='çœŸå®å€¼')
        
        # æŸå¤±å‡½æ•°å¯¼æ•°
        mse_derivs = [2 * (pred - y_true) for pred in y_pred_range]
        mae_derivs = [np.sign(pred - y_true) for pred in y_pred_range]
        huber_derivs = [self.huber_derivative(np.array([pred]), np.array([y_true]), delta=1.0)[0] 
                       for pred in y_pred_range]
        
        axes[0, 1].plot(y_pred_range, mse_derivs, label='MSEå¯¼æ•°', linewidth=2, color='blue')
        axes[0, 1].plot(y_pred_range, mae_derivs, label='MAEå¯¼æ•°', linewidth=2, color='red')
        axes[0, 1].plot(y_pred_range, huber_derivs, label='Huberå¯¼æ•°', linewidth=2, color='green')
        axes[0, 1].set_xlabel('é¢„æµ‹å€¼')
        axes[0, 1].set_ylabel('æ¢¯åº¦')
        axes[0, 1].set_title('å›å½’æŸå¤±å‡½æ•°æ¢¯åº¦å¯¹æ¯”')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].axvline(y_true, color='black', linestyle='--', alpha=0.5)
        axes[0, 1].axhline(0, color='black', linestyle='-', alpha=0.3)
        
        # å¼‚å¸¸å€¼æ•æ„Ÿæ€§åˆ†æ
        outlier_errors = np.array([-5, -2, -1, -0.5, 0, 0.5, 1, 2, 5])
        mse_outlier = [error**2 for error in outlier_errors]
        mae_outlier = [abs(error) for error in outlier_errors]
        huber_outlier = [self.huber_loss(np.array([error]), np.array([0]), delta=1.0) 
                        for error in outlier_errors]
        
        x_pos = np.arange(len(outlier_errors))
        width = 0.25
        
        axes[1, 0].bar(x_pos - width, mse_outlier, width, label='MSE', alpha=0.8, color='blue')
        axes[1, 0].bar(x_pos, mae_outlier, width, label='MAE', alpha=0.8, color='red')
        axes[1, 0].bar(x_pos + width, huber_outlier, width, label='Huber', alpha=0.8, color='green')
        
        axes[1, 0].set_xlabel('é¢„æµ‹è¯¯å·®')
        axes[1, 0].set_ylabel('æŸå¤±å€¼')
        axes[1, 0].set_title('å¼‚å¸¸å€¼æ•æ„Ÿæ€§å¯¹æ¯”')
        axes[1, 0].set_xticks(x_pos)
        axes[1, 0].set_xticklabels(outlier_errors)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # æŸå¤±å‡½æ•°ç‰¹æ€§å¯¹æ¯”è¡¨
        axes[1, 1].axis('off')
        properties = [
            ['æŸå¤±å‡½æ•°', 'å¯¹å¼‚å¸¸å€¼æ•æ„Ÿæ€§', 'æ¢¯åº¦ç‰¹æ€§', 'é€‚ç”¨åœºæ™¯', 'è®¡ç®—å¤æ‚åº¦'],
            ['MSE', 'é«˜', 'è¿ç»­ï¼Œéšè¯¯å·®å¢å¤§', 'æ ‡å‡†å›å½’', 'ä½'],
            ['MAE', 'ä½', 'å¸¸æ•°ï¼Œä¸è¿ç»­', 'é²æ£’å›å½’', 'ä½'],
            ['Huber', 'ä¸­ç­‰', 'å¹³æ»‘è¿‡æ¸¡', 'é²æ£’å›å½’', 'ä¸­ç­‰'],
            ['Focal', 'å¯è°ƒ', 'å…³æ³¨å›°éš¾æ ·æœ¬', 'ä¸å¹³è¡¡åˆ†ç±»', 'ä¸­ç­‰']
        ]
        
        table = axes[1, 1].table(cellText=properties[1:], colLabels=properties[0], 
                                cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        axes[1, 1].set_title('æŸå¤±å‡½æ•°ç‰¹æ€§å¯¹æ¯”', fontweight='bold', fontsize=12)
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def visualize_classification_losses(self):
        """å¯è§†åŒ–åˆ†ç±»æŸå¤±å‡½æ•°"""
        # äºŒåˆ†ç±»æƒ…å†µ
        y_true = 1  # æ­£ç±»
        y_pred_range = np.linspace(0.001, 0.999, 1000)
        
        # è®¡ç®—ä¸åŒæŸå¤±
        bce_values = [-np.log(p) for p in y_pred_range]  # æ­£ç±»çš„äº¤å‰ç†µ
        hinge_values = [max(0, 1 - (2*p - 1)) for p in y_pred_range]  # è½¬æ¢ä¸º[-1,1]æ ‡ç­¾
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # äºŒåˆ†ç±»æŸå¤±å¯¹æ¯”
        axes[0, 0].plot(y_pred_range, bce_values, label='Binary Cross-Entropy', linewidth=2, color='blue')
        axes[0, 0].plot(y_pred_range, hinge_values, label='Hinge Loss', linewidth=2, color='red')
        axes[0, 0].set_xlabel('é¢„æµ‹æ¦‚ç‡ (æ­£ç±»)')
        axes[0, 0].set_ylabel('æŸå¤±å€¼')
        axes[0, 0].set_title('äºŒåˆ†ç±»æŸå¤±å‡½æ•°å¯¹æ¯”')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_yscale('log')
        
        # æ¢¯åº¦å¯¹æ¯”
        bce_grads = [-(1/p) for p in y_pred_range]
        hinge_grads = [-2 if (2*p - 1) < 1 else 0 for p in y_pred_range]
        
        axes[0, 1].plot(y_pred_range, bce_grads, label='BCEæ¢¯åº¦', linewidth=2, color='blue')
        axes[0, 1].plot(y_pred_range, hinge_grads, label='Hingeæ¢¯åº¦', linewidth=2, color='red')
        axes[0, 1].set_xlabel('é¢„æµ‹æ¦‚ç‡ (æ­£ç±»)')
        axes[0, 1].set_ylabel('æ¢¯åº¦')
        axes[0, 1].set_title('åˆ†ç±»æŸå¤±å‡½æ•°æ¢¯åº¦å¯¹æ¯”')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_ylim(-50, 5)
        
        # Focal Lossæ¼”ç¤º
        gamma_values = [0, 0.5, 1, 2, 5]
        for gamma in gamma_values:
            focal_values = [-(1-p)**gamma * np.log(p) for p in y_pred_range]
            axes[1, 0].plot(y_pred_range, focal_values, label=f'Î³={gamma}', linewidth=2)
        
        axes[1, 0].set_xlabel('é¢„æµ‹æ¦‚ç‡ (æ­£ç±»)')
        axes[1, 0].set_ylabel('Focal Loss')
        axes[1, 0].set_title('Focal Loss (ä¸åŒÎ³å€¼)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_yscale('log')
        
        # ç±»åˆ«ä¸å¹³è¡¡å½±å“
        class_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]
        standard_ce = []
        weighted_ce = []
        
        for ratio in class_ratios:
            # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
            n_samples = 1000
            n_positive = int(n_samples * ratio)
            n_negative = n_samples - n_positive
            
            # å‡è®¾æ¨¡å‹é¢„æµ‹æ¦‚ç‡
            y_true_sim = np.concatenate([np.ones(n_positive), np.zeros(n_negative)])
            y_pred_sim = np.concatenate([np.random.beta(3, 1, n_positive), 
                                       np.random.beta(1, 3, n_negative)])
            
            # æ ‡å‡†äº¤å‰ç†µ
            std_loss = self.binary_cross_entropy(y_pred_sim, y_true_sim)
            standard_ce.append(std_loss)
            
            # åŠ æƒäº¤å‰ç†µ
            pos_weight = n_negative / n_positive if n_positive > 0 else 1
            weighted_loss = self.binary_cross_entropy(y_pred_sim, y_true_sim) * \
                          (pos_weight if ratio < 0.5 else 1/pos_weight)
            weighted_ce.append(weighted_loss)
        
        axes[1, 1].plot(class_ratios, standard_ce, 'o-', label='æ ‡å‡†äº¤å‰ç†µ', linewidth=2, color='blue')
        axes[1, 1].plot(class_ratios, weighted_ce, 's-', label='åŠ æƒäº¤å‰ç†µ', linewidth=2, color='red')
        axes[1, 1].set_xlabel('æ­£ç±»æ¯”ä¾‹')
        axes[1, 1].set_ylabel('æŸå¤±å€¼')
        axes[1, 1].set_title('ç±»åˆ«ä¸å¹³è¡¡å¯¹æŸå¤±çš„å½±å“')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def loss_landscape_analysis(self):
        """æŸå¤±å‡½æ•°æ™¯è§‚åˆ†æ"""
        print(f"\n{'='*80}")
        print(f"ğŸ—ºï¸ æŸå¤±å‡½æ•°æ™¯è§‚åˆ†æ")
        print(f"{'='*80}")
        
        # åˆ›å»ºç®€å•çš„äºŒç»´ä¼˜åŒ–é—®é¢˜
        def rosenbrock(x, y, a=1, b=100):
            """Rosenbrockå‡½æ•°ï¼ˆé¦™è•‰å‡½æ•°ï¼‰"""
            return (a - x)**2 + b * (y - x**2)**2
        
        def quadratic(x, y):
            """äºŒæ¬¡å‡½æ•°"""
            return x**2 + y**2
        
        def beale(x, y):
            """Bealeå‡½æ•°"""
            return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2
        
        # åˆ›å»ºç½‘æ ¼
        x = np.linspace(-2, 2, 100)
        y = np.linspace(-1, 3, 100)
        X, Y = np.meshgrid(x, y)
        
        functions = {
            'Rosenbrock': rosenbrock,
            'Quadratic': quadratic,
            'Beale': beale
        }
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        for i, (name, func) in enumerate(functions.items()):
            Z = func(X, Y)
            
            # ä½¿ç”¨å¯¹æ•°å°ºåº¦ä»¥æ›´å¥½åœ°æ˜¾ç¤ºç­‰é«˜çº¿
            if name == 'Beale':
                Z = np.log(Z + 1)  # é¿å…log(0)
            
            contour = axes[i].contour(X, Y, Z, levels=20, alpha=0.6)
            axes[i].contourf(X, Y, Z, levels=20, alpha=0.4, cmap='viridis')
            axes[i].clabel(contour, inline=True, fontsize=8)
            
            # æ ‡è®°å…¨å±€æœ€å°å€¼
            if name == 'Rosenbrock':
                axes[i].plot(1, 1, 'r*', markersize=15, label='å…¨å±€æœ€å°å€¼')
            elif name == 'Quadratic':
                axes[i].plot(0, 0, 'r*', markersize=15, label='å…¨å±€æœ€å°å€¼')
            elif name == 'Beale':
                axes[i].plot(3, 0.5, 'r*', markersize=15, label='å…¨å±€æœ€å°å€¼')
            
            axes[i].set_xlabel('å‚æ•° x')
            axes[i].set_ylabel('å‚æ•° y')
            axes[i].set_title(f'{name} å‡½æ•°æ™¯è§‚')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # åˆ†æä¸åŒæŸå¤±å‡½æ•°çš„ä¼˜åŒ–ç‰¹æ€§
        print(f"\nğŸ“Š æŸå¤±å‡½æ•°ä¼˜åŒ–ç‰¹æ€§åˆ†æ:")
        
        characteristics = {
            'MSE': {
                'å‡¸æ€§': 'å‡¸å‡½æ•°',
                'å…¨å±€æœ€å°å€¼': 'å”¯ä¸€',
                'æ¢¯åº¦ç‰¹æ€§': 'è¿ç»­å¯å¯¼',
                'æ”¶æ•›æ€§': 'å¿«é€Ÿ',
                'é²æ£’æ€§': 'å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ'
            },
            'MAE': {
                'å‡¸æ€§': 'å‡¸å‡½æ•°',
                'å…¨å±€æœ€å°å€¼': 'å¯èƒ½å¤šä¸ª',
                'æ¢¯åº¦ç‰¹æ€§': 'ä¸å¯å¯¼(0ç‚¹)',
                'æ”¶æ•›æ€§': 'è¾ƒæ…¢',
                'é²æ£’æ€§': 'å¯¹å¼‚å¸¸å€¼é²æ£’'
            },
            'Cross-Entropy': {
                'å‡¸æ€§': 'å‡¸å‡½æ•°',
                'å…¨å±€æœ€å°å€¼': 'å”¯ä¸€',
                'æ¢¯åº¦ç‰¹æ€§': 'è¿ç»­å¯å¯¼',
                'æ”¶æ•›æ€§': 'å¿«é€Ÿ',
                'é²æ£’æ€§': 'å¯¹æ ‡ç­¾å™ªå£°æ•æ„Ÿ'
            },
            'Hinge': {
                'å‡¸æ€§': 'å‡¸å‡½æ•°',
                'å…¨å±€æœ€å°å€¼': 'å¯èƒ½å¤šä¸ª',
                'æ¢¯åº¦ç‰¹æ€§': 'åˆ†æ®µçº¿æ€§',
                'æ”¶æ•›æ€§': 'ä¸­ç­‰',
                'é²æ£’æ€§': 'å¯¹å¼‚å¸¸å€¼è¾ƒé²æ£’'
            }
        }
        
        for loss_name, props in characteristics.items():
            print(f"\nğŸ”¸ {loss_name}:")
            for prop_name, prop_value in props.items():
                print(f"   {prop_name}: {prop_value}")
        
        return fig

# æŸå¤±å‡½æ•°åˆ†ææ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ“Š æŸå¤±å‡½æ•°è¯¦ç»†åˆ†æ")
print("=" * 80)

loss_analyzer = LossFunctionAnalyzer()

# å¯è§†åŒ–å›å½’æŸå¤±å‡½æ•°
loss_analyzer.visualize_regression_losses()

# å¯è§†åŒ–åˆ†ç±»æŸå¤±å‡½æ•°
loss_analyzer.visualize_classification_losses()

# æŸå¤±å‡½æ•°æ™¯è§‚åˆ†æ
loss_analyzer.loss_landscape_analysis()
```

## 3. æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°çš„åŒ¹é…

### 3.1 æœ€ä½³å®è·µç»„åˆ

```python
class ActivationLossMatchingGuide:
    """æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°åŒ¹é…æŒ‡å—"""
    
    def __init__(self):
        self.combinations = {
            'äºŒåˆ†ç±»': {
                'æ¨èç»„åˆ': [('Sigmoid', 'Binary Cross-Entropy')],
                'åŸå› ': 'Sigmoidè¾“å‡º[0,1]æ¦‚ç‡ï¼Œä¸BCEæ•°å­¦ä¸Šå®Œç¾åŒ¹é…',
                'æ¢¯åº¦': 'ç®€åŒ–ä¸º Ïƒ(z) - yï¼Œè®¡ç®—é«˜æ•ˆ'
            },
            'å¤šåˆ†ç±»': {
                'æ¨èç»„åˆ': [('Softmax', 'Categorical Cross-Entropy')],
                'åŸå› ': 'Softmaxè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œä¸CCEæ•°å­¦ä¸Šå®Œç¾åŒ¹é…',
                'æ¢¯åº¦': 'ç®€åŒ–ä¸º softmax(z) - yï¼Œè®¡ç®—é«˜æ•ˆ'
            },
            'å›å½’': {
                'æ¨èç»„åˆ': [('Linear', 'MSE'), ('Linear', 'MAE')],
                'åŸå› ': 'çº¿æ€§è¾“å‡ºé€‚åˆè¿ç»­å€¼é¢„æµ‹',
                'é€‰æ‹©': 'MSEé€‚åˆæ­£æ€åˆ†å¸ƒè¯¯å·®ï¼ŒMAEé€‚åˆé²æ£’å›å½’'
            },
            'éšè—å±‚': {
                'æ¨èç»„åˆ': [('ReLU', 'Any'), ('Leaky ReLU', 'Any'), ('GELU', 'Any')],
                'åŸå› ': 'é¿å…æ¢¯åº¦æ¶ˆå¤±ï¼Œè®¡ç®—æ•ˆç‡é«˜',
                'æ³¨æ„': 'é¿å…ä½¿ç”¨Sigmoid/Tanhåœ¨æ·±å±‚ç½‘ç»œ'
            }
        }
    
    def demonstrate_combinations(self):
        """æ¼”ç¤ºä¸åŒç»„åˆçš„æ•ˆæœ"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°åŒ¹é…æ¼”ç¤º")
        print(f"{'='*80}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        np.random.seed(42)
        n_samples = 1000
        
        # äºŒåˆ†ç±»æ•°æ®
        X_binary = np.random.randn(n_samples, 2)
        y_binary = (X_binary[:, 0] + X_binary[:, 1] > 0).astype(int)
        
        # å¤šåˆ†ç±»æ•°æ®ï¼ˆ3ç±»ï¼‰
        X_multi = np.random.randn(n_samples, 2)
        centers = np.array([[2, 2], [-2, -2], [2, -2]])
        y_multi = np.argmin(np.linalg.norm(X_multi[:, None] - centers, axis=2), axis=1)
        
        # å›å½’æ•°æ®
        X_reg = np.random.randn(n_samples, 1)
        y_reg = 2 * X_reg.flatten() + 0.5 * np.random.randn(n_samples)
        
        datasets = {
            'äºŒåˆ†ç±»': (X_binary, y_binary),
            'å¤šåˆ†ç±»': (X_multi, y_multi),
            'å›å½’': (X_reg, y_reg)
        }
        
        for task_name, (X, y) in datasets.items():
            print(f"\nğŸ”¸ {task_name}ä»»åŠ¡:")
            
            if task_name in self.combinations:
                combo_info = self.combinations[task_name]
                print(f"   æ¨èç»„åˆ: {combo_info['æ¨èç»„åˆ']}")
                print(f"   åŸå› : {combo_info['åŸå› ']}")
                
                if 'æ¢¯åº¦' in combo_info:
                    print(f"   æ¢¯åº¦ç®€åŒ–: {combo_info['æ¢¯åº¦']}")
                if 'é€‰æ‹©' in combo_info:
                    print(f"   é€‰æ‹©å»ºè®®: {combo_info['é€‰æ‹©']}")
        
        return datasets
    
    def gradient_flow_comparison(self):
        """æ¯”è¾ƒä¸åŒç»„åˆçš„æ¢¯åº¦æµåŠ¨"""
        print(f"\n{'='*80}")
        print(f"ğŸŒŠ æ¢¯åº¦æµåŠ¨å¯¹æ¯”åˆ†æ")
        print(f"{'='*80}")
        
        # æ¨¡æ‹Ÿä¸åŒæ¿€æ´»å‡½æ•°åœ¨æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦ä¼ æ’­
        def simulate_gradient_flow(activation_func, depth=10, input_val=0.5):
            """æ¨¡æ‹Ÿæ¢¯åº¦åœ¨æ·±å±‚ç½‘ç»œä¸­çš„ä¼ æ’­"""
            gradient = 1.0
            gradients = [gradient]
            
            for layer in range(depth):
                if activation_func == 'sigmoid':
                    # Sigmoidå¯¼æ•°: Ïƒ(x)(1-Ïƒ(x))
                    sigma = 1 / (1 + np.exp(-input_val))
                    derivative = sigma * (1 - sigma)
                elif activation_func == 'tanh':
                    # Tanhå¯¼æ•°: 1 - tanhÂ²(x)
                    derivative = 1 - np.tanh(input_val) ** 2
                elif activation_func == 'relu':
                    # ReLUå¯¼æ•°: 1 if x > 0 else 0
                    derivative = 1.0 if input_val > 0 else 0.0
                elif activation_func == 'leaky_relu':
                    # Leaky ReLUå¯¼æ•°: 1 if x > 0 else 0.01
                    derivative = 1.0 if input_val > 0 else 0.01
                
                gradient *= derivative
                gradients.append(gradient)
                
                # æ›´æ–°è¾“å…¥å€¼ï¼ˆç®€åŒ–æ¨¡æ‹Ÿï¼‰
                input_val = input_val * 0.9
            
            return gradients
        
        # æµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°
        activation_functions = ['sigmoid', 'tanh', 'relu', 'leaky_relu']
        depth = 15
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        gradient_results = {}
        
        for activation in activation_functions:
            gradients = simulate_gradient_flow(activation, depth)
            gradient_results[activation] = gradients
            
            # çº¿æ€§å°ºåº¦
            ax1.plot(range(len(gradients)), gradients, 
                    label=activation.replace('_', ' ').title(), 
                    linewidth=2, marker='o')
            
            # å¯¹æ•°å°ºåº¦
            gradients_abs = [max(abs(g), 1e-15) for g in gradients]
            ax2.semilogy(range(len(gradients_abs)), gradients_abs, 
                        label=activation.replace('_', ' ').title(), 
                        linewidth=2, marker='o')
        
        ax1.set_xlabel('ç½‘ç»œå±‚æ•°')
        ax1.set_ylabel('æ¢¯åº¦å¤§å°')
        ax1.set_title('æ¢¯åº¦ä¼ æ’­ (çº¿æ€§å°ºåº¦)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        ax2.set_xlabel('ç½‘ç»œå±‚æ•°')
        ax2.set_ylabel('æ¢¯åº¦å¤§å° (å¯¹æ•°å°ºåº¦)')
        ax2.set_title('æ¢¯åº¦ä¼ æ’­ (å¯¹æ•°å°ºåº¦)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # åˆ†æç»“æœ
        print(f"\nğŸ“Š æ¢¯åº¦ä¼ æ’­åˆ†æç»“æœ:")
        for activation, gradients in gradient_results.items():
            final_gradient = gradients[-1]
            print(f"\nğŸ”¸ {activation.replace('_', ' ').title()}:")
            print(f"   åˆå§‹æ¢¯åº¦: 1.000000")
            print(f"   ç¬¬{depth}å±‚æ¢¯åº¦: {final_gradient:.6e}")
            print(f"   æ¢¯åº¦ä¿æŒç‡: {final_gradient:.6e}")
            
            if abs(final_gradient) < 1e-10:
                print(f"   çŠ¶æ€: âŒ ä¸¥é‡æ¢¯åº¦æ¶ˆå¤±")
            elif abs(final_gradient) < 1e-5:
                print(f"   çŠ¶æ€: âš ï¸ è½»å¾®æ¢¯åº¦æ¶ˆå¤±")
            elif abs(final_gradient) > 10:
                print(f"   çŠ¶æ€: âš ï¸ æ¢¯åº¦çˆ†ç‚¸")
            else:
                print(f"   çŠ¶æ€: âœ… æ¢¯åº¦æ­£å¸¸")
        
        return gradient_results
    
    def create_best_practices_guide(self):
        """åˆ›å»ºæœ€ä½³å®è·µæŒ‡å—"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°æœ€ä½³å®è·µæŒ‡å—")
        print(f"{'='*80}")
        
        best_practices = {
            'ğŸ¯ ä»»åŠ¡ç±»å‹åŒ¹é…': {
                'äºŒåˆ†ç±»': 'Sigmoid + Binary Cross-Entropy',
                'å¤šåˆ†ç±»': 'Softmax + Categorical Cross-Entropy',
                'å›å½’': 'Linear + MSE/MAE',
                'å¤šæ ‡ç­¾åˆ†ç±»': 'Sigmoid + Binary Cross-Entropy (æ¯ä¸ªæ ‡ç­¾)'
            },
            
            'ğŸ—ï¸ ç½‘ç»œæ¶æ„å»ºè®®': {
                'éšè—å±‚': 'ReLU/Leaky ReLU/GELU (é¿å…æ¢¯åº¦æ¶ˆå¤±)',
                'è¾“å‡ºå±‚': 'æ ¹æ®ä»»åŠ¡é€‰æ‹© (Sigmoid/Softmax/Linear)',
                'æ·±å±‚ç½‘ç»œ': 'ä½¿ç”¨ReLUå˜ä½“ + æ‰¹å½’ä¸€åŒ–',
                'æ®‹å·®ç½‘ç»œ': 'ReLU + è·³è·ƒè¿æ¥'
            },
            
            'âš¡ æ€§èƒ½ä¼˜åŒ–': {
                'è®¡ç®—æ•ˆç‡': 'ReLU > Sigmoid/Tanh (è®¡ç®—ç®€å•)',
                'å†…å­˜æ•ˆç‡': 'é¿å…å¤æ‚æ¿€æ´»å‡½æ•°åœ¨å¤§ç½‘ç»œä¸­',
                'è®­ç»ƒç¨³å®šæ€§': 'ä½¿ç”¨He/Xavieråˆå§‹åŒ–é…åˆç›¸åº”æ¿€æ´»å‡½æ•°',
                'æ”¶æ•›é€Ÿåº¦': 'Adam + ReLUé€šå¸¸æ”¶æ•›æœ€å¿«'
            },
            
            'ğŸ› ï¸ è°ƒè¯•æŠ€å·§': {
                'æ¢¯åº¦æ¶ˆå¤±': 'æ£€æŸ¥æ¿€æ´»å‡½æ•°å¯¼æ•°ï¼Œä½¿ç”¨ReLUå˜ä½“',
                'æ¢¯åº¦çˆ†ç‚¸': 'ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼Œæ£€æŸ¥å­¦ä¹ ç‡',
                'æ­»ç¥ç»å…ƒ': 'ä½¿ç”¨Leaky ReLUæ›¿ä»£ReLU',
                'è®­ç»ƒä¸ç¨³å®š': 'æ£€æŸ¥æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°åŒ¹é…'
            },
            
            'ğŸš¨ å¸¸è§é”™è¯¯': {
                'é”™è¯¯1': 'Softmax + MSE (åº”è¯¥ç”¨Cross-Entropy)',
                'é”™è¯¯2': 'æ·±å±‚ç½‘ç»œä½¿ç”¨Sigmoid (æ¢¯åº¦æ¶ˆå¤±)',
                'é”™è¯¯3': 'å›å½’ä»»åŠ¡ä½¿ç”¨Sigmoidè¾“å‡º (èŒƒå›´å—é™)',
                'é”™è¯¯4': 'å¿½ç•¥æ¿€æ´»å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§'
            }
        }
        
        for category, items in best_practices.items():
            print(f"\n{category}:")
            for key, value in items.items():
                print(f"   â€¢ {key}: {value}")
        
        # åˆ›å»ºå†³ç­–æ ‘
        print(f"\nğŸŒ³ æ¿€æ´»å‡½æ•°é€‰æ‹©å†³ç­–æ ‘:")
        print(f"""   
   å¼€å§‹
    |
    â”œâ”€ æ˜¯è¾“å‡ºå±‚ï¼Ÿ
    â”‚   â”œâ”€ æ˜¯ â†’ ä»»åŠ¡ç±»å‹ï¼Ÿ
    â”‚   â”‚   â”œâ”€ äºŒåˆ†ç±» â†’ Sigmoid
    â”‚   â”‚   â”œâ”€ å¤šåˆ†ç±» â†’ Softmax  
    â”‚   â”‚   â””â”€ å›å½’ â†’ Linear
    â”‚   â””â”€ å¦ â†’ éšè—å±‚
    â”‚       â”œâ”€ æ·±å±‚ç½‘ç»œï¼Ÿ
    â”‚       â”‚   â”œâ”€ æ˜¯ â†’ ReLU/Leaky ReLU
    â”‚       â”‚   â””â”€ å¦ â†’ ReLU/Tanh
    â”‚       â””â”€ ç‰¹æ®Šéœ€æ±‚ï¼Ÿ
    â”‚           â”œâ”€ å¹³æ»‘æ€§ â†’ GELU/Swish
    â”‚           â””â”€ æ ‡å‡†é€‰æ‹© â†’ ReLU
        """)
        
        return best_practices

# æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°åŒ¹é…æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ¯ æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°åŒ¹é…åˆ†æ")
print("=" * 80)

matching_guide = ActivationLossMatchingGuide()

# æ¼”ç¤ºä¸åŒç»„åˆ
datasets = matching_guide.demonstrate_combinations()

# æ¢¯åº¦æµåŠ¨å¯¹æ¯”
gradient_comparison = matching_guide.gradient_flow_comparison()

# æœ€ä½³å®è·µæŒ‡å—
best_practices = matching_guide.create_best_practices_guide()
```

## 4. æ€è€ƒé¢˜

1. **æ¿€æ´»å‡½æ•°é€‰æ‹©**: ä¸ºä»€ä¹ˆReLUåœ¨æ·±å±‚ç½‘ç»œä¸­æ¯”Sigmoidè¡¨ç°æ›´å¥½ï¼ŸLeaky ReLUç›¸æ¯”ReLUæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

2. **æŸå¤±å‡½æ•°è®¾è®¡**: åœ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æ—¶ï¼Œå¦‚ä½•è®¾è®¡æˆ–ä¿®æ”¹æŸå¤±å‡½æ•°ï¼ŸFocal Lossçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ

3. **æ¢¯åº¦é—®é¢˜**: ä»€ä¹ˆæƒ…å†µä¸‹ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸ï¼Ÿå¦‚ä½•é€šè¿‡æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°çš„é€‰æ‹©æ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Ÿ

4. **æ•°å€¼ç¨³å®šæ€§**: åœ¨å®ç°Softmaxå’Œäº¤å‰ç†µæŸå¤±æ—¶ï¼Œéœ€è¦æ³¨æ„å“ªäº›æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ

5. **è‡ªå®šä¹‰è®¾è®¡**: å¦‚æœè¦ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡æ–°çš„æ¿€æ´»å‡½æ•°æˆ–æŸå¤±å‡½æ•°ï¼Œéœ€è¦è€ƒè™‘å“ªäº›å› ç´ ï¼Ÿ

## 5. å°ç»“

### 5.1 æ ¸å¿ƒæ¦‚å¿µ

- **æ¿€æ´»å‡½æ•°**: ä¸ºç¥ç»ç½‘ç»œå¼•å…¥éçº¿æ€§çš„å…³é”®ç»„ä»¶
- **æŸå¤±å‡½æ•°**: è¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼å·®å¼‚çš„ç›®æ ‡å‡½æ•°
- **æ¢¯åº¦æµåŠ¨**: åå‘ä¼ æ’­ä¸­æ¢¯åº¦åœ¨ç½‘ç»œä¸­çš„ä¼ æ’­è¿‡ç¨‹
- **å‡½æ•°åŒ¹é…**: æ¿€æ´»å‡½æ•°ä¸æŸå¤±å‡½æ•°çš„åˆç†æ­é…

### 5.2 å…³é”®æŠ€æœ¯

- **ReLUç³»åˆ—**: è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„æœ‰æ•ˆæ¿€æ´»å‡½æ•°
- **Softmax**: å¤šåˆ†ç±»ä»»åŠ¡çš„æ ‡å‡†è¾“å‡ºæ¿€æ´»å‡½æ•°
- **äº¤å‰ç†µ**: åˆ†ç±»ä»»åŠ¡çš„æ ‡å‡†æŸå¤±å‡½æ•°
- **æ•°å€¼ç¨³å®šæ€§**: é˜²æ­¢è®¡ç®—æº¢å‡ºçš„é‡è¦æŠ€æœ¯

### 5.3 å®è·µè¦ç‚¹

- æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°ç»„åˆ
- åœ¨æ·±å±‚ç½‘ç»œä¸­ä¼˜å…ˆä½¿ç”¨ReLUåŠå…¶å˜ä½“
- æ³¨æ„æ•°å€¼ç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—æŒ‡æ•°å’Œå¯¹æ•°æ—¶
- ç›‘æ§æ¢¯åº¦æµåŠ¨ï¼ŒåŠæ—¶å‘ç°å’Œè§£å†³æ¢¯åº¦é—®é¢˜
- è€ƒè™‘ç‰¹æ®Šéœ€æ±‚ï¼ˆå¦‚ç±»åˆ«ä¸å¹³è¡¡ï¼‰é€‰æ‹©æˆ–è®¾è®¡æŸå¤±å‡½æ•°

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œä½ å·²ç»æ·±å…¥ç†è§£äº†æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°çš„åŸç†ã€ç‰¹æ€§å’Œåº”ç”¨åœºæ™¯ï¼Œè¿™ä¸ºä½ è®¾è®¡å’Œä¼˜åŒ–ç¥ç»ç½‘ç»œæä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚