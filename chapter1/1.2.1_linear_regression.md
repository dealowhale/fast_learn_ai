# 1.2.1 线性回归

## 学习目标
理解线性回归的数学原理，掌握其实现方法，学会在实际项目中应用线性回归解决问题。

## 引言：从直线说起

想象你是一个房地产经纪人，客户问你："这套100平米的房子应该卖多少钱？"

你可能会这样思考：
- 看看周围类似房子的价格
- 发现房价和面积之间似乎有规律
- 面积越大，价格越高
- 这种关系可以用一条直线来描述

这就是线性回归的核心思想：**用一条直线来描述两个变量之间的关系**。

## 什么是线性回归？

**线性回归 (Linear Regression)** 是机器学习中最基础也是最重要的算法之一。它假设目标变量与特征变量之间存在线性关系。

### 数学表达

**一元线性回归**：
```
y = β₀ + β₁x + ε
```

**多元线性回归**：
```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```

其中：
- `y`：目标变量（因变量）
- `x₁, x₂, ..., xₙ`：特征变量（自变量）
- `β₀`：截距（偏置项）
- `β₁, β₂, ..., βₙ`：回归系数（权重）
- `ε`：误差项（噪声）

### 几何直观

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns

# 设置中文字体和样式
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# 生成示例数据：房屋面积 vs 价格
np.random.seed(42)
n_samples = 100

# 房屋面积 (50-200平米)
area = np.random.uniform(50, 200, n_samples)

# 价格 = 基础价格 + 面积系数 * 面积 + 噪声
base_price = 50  # 50万基础价格
area_coefficient = 0.8  # 每平米0.8万
noise = np.random.normal(0, 10, n_samples)  # 噪声
price = base_price + area_coefficient * area + noise

# 可视化数据和线性关系
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 原始数据散点图
axes[0].scatter(area, price, alpha=0.6, color='blue')
axes[0].set_xlabel('房屋面积 (平米)')
axes[0].set_ylabel('房价 (万元)')
axes[0].set_title('房屋面积 vs 价格\n(原始数据)')
axes[0].grid(True, alpha=0.3)

# 拟合线性回归
X = area.reshape(-1, 1)
y = price

model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)

# 绘制拟合直线
axes[1].scatter(area, price, alpha=0.6, color='blue', label='实际数据')
axes[1].plot(area, y_pred, color='red', linewidth=2, label='拟合直线')
axes[1].set_xlabel('房屋面积 (平米)')
axes[1].set_ylabel('房价 (万元)')
axes[1].set_title(f'线性回归拟合\ny = {model.intercept_:.1f} + {model.coef_[0]:.2f}x')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

# 残差图
residuals = price - y_pred
axes[2].scatter(y_pred, residuals, alpha=0.6, color='green')
axes[2].axhline(y=0, color='red', linestyle='--')
axes[2].set_xlabel('预测值')
axes[2].set_ylabel('残差')
axes[2].set_title('残差分析')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"拟合结果:")
print(f"截距 (β₀): {model.intercept_:.3f}")
print(f"斜率 (β₁): {model.coef_[0]:.3f}")
print(f"R² 分数: {r2_score(y, y_pred):.3f}")
print(f"均方误差: {mean_squared_error(y, y_pred):.3f}")
```

## 线性回归的数学原理

### 最小二乘法 (Ordinary Least Squares, OLS)

线性回归的目标是找到最佳的参数 β，使得预测值与真实值之间的误差最小。

**损失函数**：
```
J(β) = (1/2m) × Σ(hβ(x⁽ⁱ⁾) - y⁽ⁱ⁾)²
```

其中：
- `m`：样本数量
- `hβ(x⁽ⁱ⁾)`：第i个样本的预测值
- `y⁽ⁱ⁾`：第i个样本的真实值

### 解析解 (Normal Equation)

对于线性回归，我们可以直接求出最优解：

**矩阵形式**：
```
β = (XᵀX)⁻¹Xᵀy
```

```python
def linear_regression_normal_equation(X, y):
    """使用正规方程求解线性回归"""
    # 添加偏置项（截距）
    X_with_bias = np.column_stack([np.ones(X.shape[0]), X])
    
    # 正规方程: β = (X^T X)^(-1) X^T y
    XtX = X_with_bias.T @ X_with_bias
    Xty = X_with_bias.T @ y
    beta = np.linalg.solve(XtX, Xty)  # 更稳定的求解方式
    
    return beta

# 使用正规方程求解
beta_normal = linear_regression_normal_equation(X, y)
print(f"\n正规方程求解结果:")
print(f"截距: {beta_normal[0]:.3f}")
print(f"斜率: {beta_normal[1]:.3f}")

# 与sklearn结果对比
print(f"\nsklearn结果:")
print(f"截距: {model.intercept_:.3f}")
print(f"斜率: {model.coef_[0]:.3f}")

# 验证结果一致性
print(f"\n结果差异:")
print(f"截距差异: {abs(beta_normal[0] - model.intercept_):.6f}")
print(f"斜率差异: {abs(beta_normal[1] - model.coef_[0]):.6f}")
```

### 梯度下降法求解

当数据量很大或特征很多时，正规方程可能计算困难，这时可以使用梯度下降法：

```python
class LinearRegressionGD:
    """使用梯度下降的线性回归实现"""
    
    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.tolerance = tolerance
        self.cost_history = []
        
    def _add_bias(self, X):
        """添加偏置项"""
        return np.column_stack([np.ones(X.shape[0]), X])
    
    def _compute_cost(self, X, y, theta):
        """计算损失函数"""
        m = X.shape[0]
        predictions = X @ theta
        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
        return cost
    
    def _compute_gradients(self, X, y, theta):
        """计算梯度"""
        m = X.shape[0]
        predictions = X @ theta
        gradients = (1 / m) * X.T @ (predictions - y)
        return gradients
    
    def fit(self, X, y):
        """训练模型"""
        # 添加偏置项
        X_with_bias = self._add_bias(X)
        
        # 初始化参数
        n_features = X_with_bias.shape[1]
        self.theta = np.random.normal(0, 0.01, n_features)
        
        # 梯度下降
        for i in range(self.max_iterations):
            # 计算当前损失
            cost = self._compute_cost(X_with_bias, y, self.theta)
            self.cost_history.append(cost)
            
            # 计算梯度
            gradients = self._compute_gradients(X_with_bias, y, self.theta)
            
            # 更新参数
            new_theta = self.theta - self.learning_rate * gradients
            
            # 检查收敛
            if np.linalg.norm(new_theta - self.theta) < self.tolerance:
                print(f"在第 {i+1} 次迭代时收敛")
                break
                
            self.theta = new_theta
        
        # 分离截距和系数
        self.intercept_ = self.theta[0]
        self.coef_ = self.theta[1:]
        
        return self
    
    def predict(self, X):
        """预测"""
        X_with_bias = self._add_bias(X)
        return X_with_bias @ self.theta
    
    def plot_cost_history(self):
        """绘制损失函数变化"""
        plt.figure(figsize=(10, 6))
        plt.plot(self.cost_history, linewidth=2)
        plt.xlabel('迭代次数')
        plt.ylabel('损失函数值')
        plt.title('梯度下降收敛过程')
        plt.grid(True, alpha=0.3)
        plt.show()

# 使用梯度下降训练
model_gd = LinearRegressionGD(learning_rate=0.0001, max_iterations=10000)
model_gd.fit(X, y)

print(f"\n梯度下降求解结果:")
print(f"截距: {model_gd.intercept_:.3f}")
print(f"斜率: {model_gd.coef_[0]:.3f}")

# 绘制收敛过程
model_gd.plot_cost_history()

# 预测并评估
y_pred_gd = model_gd.predict(X)
print(f"\n梯度下降模型性能:")
print(f"R² 分数: {r2_score(y, y_pred_gd):.3f}")
print(f"均方误差: {mean_squared_error(y, y_pred_gd):.3f}")
```

## 多元线性回归

现实中，房价不仅仅取决于面积，还受到位置、楼层、装修等多个因素影响：

```python
# 生成多元回归数据
np.random.seed(42)
n_samples = 500

# 特征：面积、楼层、距离市中心距离、房龄
area = np.random.uniform(50, 200, n_samples)  # 面积
floor = np.random.randint(1, 31, n_samples)   # 楼层
distance = np.random.uniform(1, 50, n_samples)  # 距离市中心距离(km)
age = np.random.uniform(0, 30, n_samples)     # 房龄

# 价格模型：基础价格 + 面积效应 + 楼层效应 - 距离效应 - 房龄效应 + 噪声
price_multi = (30 +                    # 基础价格
               0.8 * area +            # 面积系数
               0.5 * floor +           # 楼层系数
               -0.3 * distance +       # 距离系数（负相关）
               -0.2 * age +            # 房龄系数（负相关）
               np.random.normal(0, 8, n_samples))  # 噪声

# 构建特征矩阵
X_multi = np.column_stack([area, floor, distance, age])
feature_names = ['面积', '楼层', '距离', '房龄']

# 数据标准化（重要！）
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_multi_scaled = scaler.fit_transform(X_multi)

# 训练测试分割
X_train, X_test, y_train, y_test = train_test_split(
    X_multi_scaled, price_multi, test_size=0.2, random_state=42
)

# 训练多元线性回归
model_multi = LinearRegression()
model_multi.fit(X_train, y_train)

# 预测和评估
y_pred_train = model_multi.predict(X_train)
y_pred_test = model_multi.predict(X_test)

print("多元线性回归结果:")
print(f"训练集 R²: {r2_score(y_train, y_pred_train):.3f}")
print(f"测试集 R²: {r2_score(y_test, y_pred_test):.3f}")
print(f"训练集 RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.3f}")
print(f"测试集 RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.3f}")

# 分析特征重要性
print(f"\n特征系数分析:")
print(f"截距: {model_multi.intercept_:.3f}")
for i, (name, coef) in enumerate(zip(feature_names, model_multi.coef_)):
    print(f"{name}: {coef:.3f}")
```

### 特征重要性可视化

```python
# 可视化特征重要性
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 特征系数柱状图
axes[0, 0].bar(feature_names, model_multi.coef_, color=['skyblue', 'lightgreen', 'salmon', 'gold'])
axes[0, 0].set_title('特征系数')
axes[0, 0].set_ylabel('系数值')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].grid(True, alpha=0.3)

# 预测值 vs 真实值
axes[0, 1].scatter(y_test, y_pred_test, alpha=0.6)
axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0, 1].set_xlabel('真实价格')
axes[0, 1].set_ylabel('预测价格')
axes[0, 1].set_title(f'预测 vs 真实\nR² = {r2_score(y_test, y_pred_test):.3f}')
axes[0, 1].grid(True, alpha=0.3)

# 残差分布
residuals_multi = y_test - y_pred_test
axes[1, 0].hist(residuals_multi, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
axes[1, 0].axvline(x=0, color='red', linestyle='--')
axes[1, 0].set_xlabel('残差')
axes[1, 0].set_ylabel('频数')
axes[1, 0].set_title('残差分布')
axes[1, 0].grid(True, alpha=0.3)

# 残差 vs 预测值
axes[1, 1].scatter(y_pred_test, residuals_multi, alpha=0.6, color='purple')
axes[1, 1].axhline(y=0, color='red', linestyle='--')
axes[1, 1].set_xlabel('预测值')
axes[1, 1].set_ylabel('残差')
axes[1, 1].set_title('残差 vs 预测值')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## 线性回归的假设条件

线性回归基于几个重要假设，违反这些假设会影响模型性能：

### 1. 线性关系假设

```python
# 检验线性关系
def check_linearity(X, y, feature_names):
    """检查特征与目标变量的线性关系"""
    n_features = X.shape[1]
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.ravel()
    
    for i in range(min(n_features, 4)):
        axes[i].scatter(X[:, i], y, alpha=0.6)
        
        # 拟合线性趋势线
        z = np.polyfit(X[:, i], y, 1)
        p = np.poly1d(z)
        axes[i].plot(X[:, i], p(X[:, i]), "r--", alpha=0.8)
        
        axes[i].set_xlabel(feature_names[i])
        axes[i].set_ylabel('价格')
        axes[i].set_title(f'{feature_names[i]} vs 价格')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

check_linearity(X_multi, price_multi, feature_names)
```

### 2. 残差正态性假设

```python
from scipy import stats

def check_residual_normality(residuals):
    """检查残差正态性"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # Q-Q图
    stats.probplot(residuals, dist="norm", plot=axes[0])
    axes[0].set_title('Q-Q图：残差正态性检验')
    axes[0].grid(True, alpha=0.3)
    
    # 残差直方图
    axes[1].hist(residuals, bins=20, alpha=0.7, density=True, color='lightblue', edgecolor='black')
    
    # 叠加正态分布曲线
    mu, sigma = stats.norm.fit(residuals)
    x = np.linspace(residuals.min(), residuals.max(), 100)
    axes[1].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='正态分布拟合')
    
    axes[1].set_xlabel('残差')
    axes[1].set_ylabel('密度')
    axes[1].set_title('残差分布直方图')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Shapiro-Wilk正态性检验
    statistic, p_value = stats.shapiro(residuals)
    print(f"Shapiro-Wilk检验:")
    print(f"统计量: {statistic:.4f}")
    print(f"p值: {p_value:.4f}")
    if p_value > 0.05:
        print("✅ 残差符合正态分布 (p > 0.05)")
    else:
        print("❌ 残差不符合正态分布 (p ≤ 0.05)")

check_residual_normality(residuals_multi)
```

### 3. 同方差性假设

```python
def check_homoscedasticity(y_pred, residuals):
    """检查同方差性"""
    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='red', linestyle='--')
    
    # 添加趋势线
    z = np.polyfit(y_pred, residuals, 1)
    p = np.poly1d(z)
    plt.plot(y_pred, p(y_pred), "g--", alpha=0.8, linewidth=2)
    
    plt.xlabel('预测值')
    plt.ylabel('残差')
    plt.title('同方差性检验\n(残差应随机分布在0附近)')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # Breusch-Pagan检验
    from scipy.stats import pearsonr
    correlation, p_value = pearsonr(y_pred, np.abs(residuals))
    print(f"残差绝对值与预测值的相关性: {correlation:.4f}")
    print(f"p值: {p_value:.4f}")
    if p_value > 0.05:
        print("✅ 满足同方差性假设 (p > 0.05)")
    else:
        print("❌ 存在异方差性 (p ≤ 0.05)")

check_homoscedasticity(y_pred_test, residuals_multi)
```

### 4. 多重共线性检查

```python
def check_multicollinearity(X, feature_names):
    """检查多重共线性"""
    # 计算相关系数矩阵
    correlation_matrix = np.corrcoef(X.T)
    
    # 可视化相关系数矩阵
    plt.figure(figsize=(10, 8))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                xticklabels=feature_names, yticklabels=feature_names,
                square=True, linewidths=0.5)
    plt.title('特征相关系数矩阵')
    plt.show()
    
    # 计算方差膨胀因子 (VIF)
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    
    vif_data = []
    for i in range(X.shape[1]):
        vif = variance_inflation_factor(X, i)
        vif_data.append([feature_names[i], vif])
    
    print("\n方差膨胀因子 (VIF):")
    print("-" * 30)
    for name, vif in vif_data:
        status = "✅ 良好" if vif < 5 else "⚠️ 注意" if vif < 10 else "❌ 严重"
        print(f"{name:<10}: {vif:6.2f} {status}")
    
    print("\nVIF解释:")
    print("< 5: 无多重共线性问题")
    print("5-10: 中等多重共线性")
    print("> 10: 严重多重共线性")

check_multicollinearity(X_multi_scaled, feature_names)
```

## 线性回归的优缺点

### 优点

1. **简单易懂**：模型解释性强
2. **计算效率高**：训练和预测都很快
3. **无需调参**：没有超参数需要调整
4. **理论基础扎实**：有完整的统计学理论支撑
5. **适合小数据集**：在数据量较少时表现良好

### 缺点

1. **线性假设限制**：只能建模线性关系
2. **对异常值敏感**：极值会显著影响结果
3. **特征工程要求高**：需要手动处理非线性关系
4. **多重共线性问题**：相关特征会影响模型稳定性

```python
# 演示线性回归的局限性
def demonstrate_limitations():
    """演示线性回归的局限性"""
    
    # 生成非线性数据
    np.random.seed(42)
    x_nonlinear = np.linspace(0, 4, 100)
    y_nonlinear = x_nonlinear**2 + np.random.normal(0, 2, 100)
    
    # 生成含异常值的数据
    x_outlier = np.random.uniform(0, 10, 50)
    y_outlier = 2 * x_outlier + 1 + np.random.normal(0, 1, 50)
    # 添加异常值
    x_outlier = np.append(x_outlier, [8, 9])
    y_outlier = np.append(y_outlier, [50, 60])
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # 非线性关系
    model_nonlinear = LinearRegression()
    model_nonlinear.fit(x_nonlinear.reshape(-1, 1), y_nonlinear)
    y_pred_nonlinear = model_nonlinear.predict(x_nonlinear.reshape(-1, 1))
    
    axes[0].scatter(x_nonlinear, y_nonlinear, alpha=0.6, label='实际数据')
    axes[0].plot(x_nonlinear, y_pred_nonlinear, 'r-', linewidth=2, label='线性拟合')
    axes[0].set_xlabel('X')
    axes[0].set_ylabel('Y')
    axes[0].set_title(f'非线性数据的线性拟合\nR² = {r2_score(y_nonlinear, y_pred_nonlinear):.3f}')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # 异常值影响
    model_outlier = LinearRegression()
    model_outlier.fit(x_outlier.reshape(-1, 1), y_outlier)
    y_pred_outlier = model_outlier.predict(x_outlier.reshape(-1, 1))
    
    # 不含异常值的拟合
    x_clean = x_outlier[:-2]
    y_clean = y_outlier[:-2]
    model_clean = LinearRegression()
    model_clean.fit(x_clean.reshape(-1, 1), y_clean)
    y_pred_clean = model_clean.predict(x_outlier.reshape(-1, 1))
    
    axes[1].scatter(x_clean, y_clean, alpha=0.6, color='blue', label='正常数据')
    axes[1].scatter(x_outlier[-2:], y_outlier[-2:], color='red', s=100, label='异常值')
    axes[1].plot(x_outlier, y_pred_outlier, 'r-', linewidth=2, label='含异常值拟合')
    axes[1].plot(x_outlier, y_pred_clean, 'g--', linewidth=2, label='无异常值拟合')
    axes[1].set_xlabel('X')
    axes[1].set_ylabel('Y')
    axes[1].set_title('异常值对线性回归的影响')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

demonstrate_limitations()
```

## 线性回归的改进方法

### 1. 特征工程

```python
# 多项式特征
from sklearn.preprocessing import PolynomialFeatures

# 生成非线性数据
np.random.seed(42)
x_poly = np.linspace(0, 4, 100)
y_poly = x_poly**2 + 0.5*x_poly + np.random.normal(0, 2, 100)

# 原始线性拟合
model_linear = LinearRegression()
model_linear.fit(x_poly.reshape(-1, 1), y_poly)
y_pred_linear = model_linear.predict(x_poly.reshape(-1, 1))

# 多项式特征拟合
poly_features = PolynomialFeatures(degree=2)
X_poly = poly_features.fit_transform(x_poly.reshape(-1, 1))
model_poly = LinearRegression()
model_poly.fit(X_poly, y_poly)
y_pred_poly = model_poly.predict(X_poly)

# 可视化比较
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(x_poly, y_poly, alpha=0.6, label='实际数据')
plt.plot(x_poly, y_pred_linear, 'r-', linewidth=2, label='线性拟合')
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f'线性拟合\nR² = {r2_score(y_poly, y_pred_linear):.3f}')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(x_poly, y_poly, alpha=0.6, label='实际数据')
plt.plot(x_poly, y_pred_poly, 'g-', linewidth=2, label='多项式拟合')
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f'多项式拟合\nR² = {r2_score(y_poly, y_pred_poly):.3f}')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"线性拟合 R²: {r2_score(y_poly, y_pred_linear):.3f}")
print(f"多项式拟合 R²: {r2_score(y_poly, y_pred_poly):.3f}")
```

### 2. 鲁棒回归

```python
from sklearn.linear_model import HuberRegressor, RANSACRegressor

# 使用含异常值的数据
X_robust = x_outlier.reshape(-1, 1)
y_robust = y_outlier

# 不同的鲁棒回归方法
models_robust = {
    '普通线性回归': LinearRegression(),
    'Huber回归': HuberRegressor(epsilon=1.35),
    'RANSAC回归': RANSACRegressor(random_state=42)
}

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for i, (name, model) in enumerate(models_robust.items()):
    model.fit(X_robust, y_robust)
    y_pred_robust = model.predict(X_robust)
    
    axes[i].scatter(X_robust[:-2], y_robust[:-2], alpha=0.6, color='blue', label='正常数据')
    axes[i].scatter(X_robust[-2:], y_robust[-2:], color='red', s=100, label='异常值')
    axes[i].plot(X_robust, y_pred_robust, 'g-', linewidth=2, label='拟合直线')
    
    # 标记RANSAC的内点
    if hasattr(model, 'inlier_mask_'):
        inlier_mask = model.inlier_mask_
        axes[i].scatter(X_robust[~inlier_mask], y_robust[~inlier_mask], 
                       color='orange', s=50, marker='x', label='被忽略的点')
    
    axes[i].set_xlabel('X')
    axes[i].set_ylabel('Y')
    axes[i].set_title(f'{name}\nR² = {r2_score(y_robust, y_pred_robust):.3f}')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## 实际项目应用

### 完整的房价预测项目

```python
class HousePricePrediction:
    """房价预测完整项目"""
    
    def __init__(self):
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.is_fitted = False
    
    def generate_synthetic_data(self, n_samples=1000):
        """生成合成房价数据"""
        np.random.seed(42)
        
        # 特征生成
        area = np.random.uniform(50, 300, n_samples)
        bedrooms = np.random.randint(1, 6, n_samples)
        bathrooms = np.random.randint(1, 4, n_samples)
        age = np.random.uniform(0, 50, n_samples)
        distance_to_center = np.random.uniform(1, 30, n_samples)
        school_rating = np.random.uniform(1, 10, n_samples)
        
        # 价格生成（更复杂的关系）
        price = (50 +                                    # 基础价格
                0.6 * area +                            # 面积效应
                8 * bedrooms +                          # 卧室效应
                5 * bathrooms +                         # 浴室效应
                -0.3 * age +                            # 房龄效应
                -0.8 * distance_to_center +             # 距离效应
                3 * school_rating +                     # 学区效应
                0.001 * area * school_rating +          # 交互效应
                np.random.normal(0, 10, n_samples))     # 噪声
        
        # 构建数据集
        X = np.column_stack([area, bedrooms, bathrooms, age, distance_to_center, school_rating])
        self.feature_names = ['面积', '卧室数', '浴室数', '房龄', '距离市中心', '学区评分']
        
        return X, price
    
    def preprocess_data(self, X, y=None, fit_scaler=True):
        """数据预处理"""
        if fit_scaler:
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X)
        else:
            X_scaled = self.scaler.transform(X)
        
        return X_scaled
    
    def train(self, X, y):
        """训练模型"""
        # 数据预处理
        X_scaled = self.preprocess_data(X, y, fit_scaler=True)
        
        # 训练测试分割
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        
        # 训练模型
        self.model = LinearRegression()
        self.model.fit(X_train, y_train)
        
        # 评估模型
        y_pred_train = self.model.predict(X_train)
        y_pred_test = self.model.predict(X_test)
        
        self.train_score = r2_score(y_train, y_pred_train)
        self.test_score = r2_score(y_test, y_pred_test)
        self.train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
        self.test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        
        self.X_test = X_test
        self.y_test = y_test
        self.y_pred_test = y_pred_test
        
        self.is_fitted = True
        
        return self
    
    def predict(self, X):
        """预测"""
        if not self.is_fitted:
            raise ValueError("模型尚未训练")
        
        X_scaled = self.preprocess_data(X, fit_scaler=False)
        return self.model.predict(X_scaled)
    
    def print_model_summary(self):
        """打印模型摘要"""
        if not self.is_fitted:
            raise ValueError("模型尚未训练")
        
        print("房价预测模型摘要")
        print("=" * 50)
        print(f"训练集 R²: {self.train_score:.3f}")
        print(f"测试集 R²: {self.test_score:.3f}")
        print(f"训练集 RMSE: {self.train_rmse:.3f} 万元")
        print(f"测试集 RMSE: {self.test_rmse:.3f} 万元")
        
        print(f"\n模型参数:")
        print(f"截距: {self.model.intercept_:.3f}")
        print("\n特征系数:")
        for name, coef in zip(self.feature_names, self.model.coef_):
            print(f"{name}: {coef:8.3f}")
    
    def plot_results(self):
        """可视化结果"""
        if not self.is_fitted:
            raise ValueError("模型尚未训练")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 预测 vs 真实
        axes[0, 0].scatter(self.y_test, self.y_pred_test, alpha=0.6)
        axes[0, 0].plot([self.y_test.min(), self.y_test.max()], 
                       [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('真实价格 (万元)')
        axes[0, 0].set_ylabel('预测价格 (万元)')
        axes[0, 0].set_title(f'预测 vs 真实\nR² = {self.test_score:.3f}')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 特征重要性
        axes[0, 1].barh(self.feature_names, self.model.coef_)
        axes[0, 1].set_xlabel('系数值')
        axes[0, 1].set_title('特征重要性')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 残差分布
        residuals = self.y_test - self.y_pred_test
        axes[1, 0].hist(residuals, bins=20, alpha=0.7, edgecolor='black')
        axes[1, 0].axvline(x=0, color='red', linestyle='--')
        axes[1, 0].set_xlabel('残差 (万元)')
        axes[1, 0].set_ylabel('频数')
        axes[1, 0].set_title('残差分布')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 残差 vs 预测值
        axes[1, 1].scatter(self.y_pred_test, residuals, alpha=0.6)
        axes[1, 1].axhline(y=0, color='red', linestyle='--')
        axes[1, 1].set_xlabel('预测价格 (万元)')
        axes[1, 1].set_ylabel('残差 (万元)')
        axes[1, 1].set_title('残差 vs 预测值')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def predict_single_house(self, area, bedrooms, bathrooms, age, distance, school_rating):
        """预测单个房屋价格"""
        if not self.is_fitted:
            raise ValueError("模型尚未训练")
        
        # 构建特征向量
        features = np.array([[area, bedrooms, bathrooms, age, distance, school_rating]])
        
        # 预测
        predicted_price = self.predict(features)[0]
        
        print(f"房屋特征:")
        print(f"面积: {area} 平米")
        print(f"卧室: {bedrooms} 间")
        print(f"浴室: {bathrooms} 间")
        print(f"房龄: {age} 年")
        print(f"距离市中心: {distance} 公里")
        print(f"学区评分: {school_rating} 分")
        print(f"\n预测价格: {predicted_price:.1f} 万元")
        
        return predicted_price

# 使用完整项目
house_predictor = HousePricePrediction()

# 生成数据并训练
X_house, y_house = house_predictor.generate_synthetic_data(1000)
house_predictor.train(X_house, y_house)

# 查看结果
house_predictor.print_model_summary()
house_predictor.plot_results()

# 预测示例
print("\n" + "="*50)
print("单个房屋价格预测示例:")
house_predictor.predict_single_house(
    area=120,           # 120平米
    bedrooms=3,         # 3室
    bathrooms=2,        # 2卫
    age=5,             # 5年房龄
    distance=8,        # 距离市中心8公里
    school_rating=8.5  # 学区评分8.5分
)
```

## 小结

线性回归是机器学习的基石算法：

### 关键要点

1. **核心思想**：用线性函数建模特征与目标的关系
2. **求解方法**：正规方程（小数据）或梯度下降（大数据）
3. **重要假设**：线性关系、正态性、同方差性、无多重共线性
4. **评估指标**：R²、MSE、RMSE、MAE
5. **改进方法**：特征工程、正则化、鲁棒回归

### 实用建议

1. **数据预处理很重要**：标准化、处理异常值
2. **检查模型假设**：残差分析、共线性检验
3. **特征工程**：多项式特征、交互项
4. **模型诊断**：学习曲线、验证曲线
5. **业务理解**：系数的实际意义

### 适用场景

- ✅ 特征与目标呈线性关系
- ✅ 需要模型可解释性
- ✅ 数据量较小
- ✅ 作为基线模型
- ❌ 复杂非线性关系
- ❌ 高维稀疏数据

## 思考题

1. 为什么在多元线性回归中需要对特征进行标准化？

2. 如果线性回归的R²很高但预测效果很差，可能的原因是什么？

3. 在什么情况下应该选择梯度下降而不是正规方程来求解线性回归？

4. 如何判断一个特征对房价预测是正向还是负向影响？

---

**下一节预告**：我们将学习逻辑回归，了解如何将线性模型扩展到分类问题。