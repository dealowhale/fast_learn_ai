# 1.2.5 æœ´ç´ è´å¶æ–¯ç®—æ³•

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å°†æŒæ¡ï¼š
- æœ´ç´ è´å¶æ–¯ç®—æ³•çš„æ•°å­¦åŸç†å’Œè´å¶æ–¯å®šç†
- ä¸‰ç§ä¸»è¦çš„æœ´ç´ è´å¶æ–¯å˜ä½“ï¼šé«˜æ–¯ã€å¤šé¡¹å¼å’Œä¼¯åŠªåˆ©
- æœ´ç´ è´å¶æ–¯åœ¨æ–‡æœ¬åˆ†ç±»å’Œåƒåœ¾é‚®ä»¶æ£€æµ‹ä¸­çš„åº”ç”¨
- æ¡ä»¶ç‹¬ç«‹å‡è®¾çš„å«ä¹‰å’Œå½±å“
- æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æŠ€æœ¯å¤„ç†é›¶æ¦‚ç‡é—®é¢˜

## ç®—æ³•åŸç†

### è´å¶æ–¯å®šç†åŸºç¡€

æœ´ç´ è´å¶æ–¯ç®—æ³•åŸºäºè´å¶æ–¯å®šç†ï¼Œè¿™æ˜¯æ¦‚ç‡è®ºä¸­çš„ä¸€ä¸ªåŸºæœ¬å®šç†ï¼š

$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬è¦è®¡ç®—ç»™å®šç‰¹å¾ $X$ çš„æƒ…å†µä¸‹ï¼Œæ ·æœ¬å±äºç±»åˆ« $C_k$ çš„æ¦‚ç‡ï¼š

$$P(C_k|X) = \frac{P(X|C_k) \cdot P(C_k)}{P(X)}$$

å…¶ä¸­ï¼š
- $P(C_k|X)$ï¼šåéªŒæ¦‚ç‡ï¼ˆæˆ‘ä»¬è¦æ±‚çš„ï¼‰
- $P(X|C_k)$ï¼šä¼¼ç„¶æ¦‚ç‡
- $P(C_k)$ï¼šå…ˆéªŒæ¦‚ç‡
- $P(X)$ï¼šè¾¹ç¼˜æ¦‚ç‡ï¼ˆå½’ä¸€åŒ–å¸¸æ•°ï¼‰

### æœ´ç´ å‡è®¾

"æœ´ç´ "å‡è®¾æŒ‡çš„æ˜¯**æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼šç»™å®šç±»åˆ«çš„æƒ…å†µä¸‹ï¼Œå„ä¸ªç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚

å¯¹äºç‰¹å¾å‘é‡ $X = (x_1, x_2, ..., x_n)$ï¼š

$$P(X|C_k) = P(x_1, x_2, ..., x_n|C_k) = \prod_{i=1}^{n} P(x_i|C_k)$$

å› æ­¤ï¼Œåˆ†ç±»å†³ç­–å˜ä¸ºï¼š

$$\hat{y} = \arg\max_{k} P(C_k) \prod_{i=1}^{n} P(x_i|C_k)$$

## ç®—æ³•å®ç°

### åŸºç¡€æœ´ç´ è´å¶æ–¯å®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification, fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import defaultdict
import pandas as pd

class SimpleNaiveBayes:
    """ç®€å•çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨å®ç°"""
    
    def __init__(self, alpha=1.0):
        """
        å‚æ•°:
        alpha: æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•°
        """
        self.alpha = alpha
        self.class_priors = {}  # å…ˆéªŒæ¦‚ç‡ P(C_k)
        self.feature_probs = {}  # æ¡ä»¶æ¦‚ç‡ P(x_i|C_k)
        self.classes = None
        self.n_features = None
        
    def fit(self, X, y):
        """è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨"""
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        self.n_features = X.shape[1]
        n_samples = len(y)
        
        print(f"ğŸ¤– è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨...")
        print(f"æ ·æœ¬æ•°: {n_samples}, ç‰¹å¾æ•°: {self.n_features}, ç±»åˆ«æ•°: {len(self.classes)}")
        
        # è®¡ç®—å…ˆéªŒæ¦‚ç‡ P(C_k)
        for class_label in self.classes:
            class_count = np.sum(y == class_label)
            self.class_priors[class_label] = class_count / n_samples
            
        # è®¡ç®—æ¡ä»¶æ¦‚ç‡ P(x_i|C_k)
        self.feature_probs = {}
        
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            self.feature_probs[class_label] = {}
            
            for feature_idx in range(self.n_features):
                feature_values = class_samples[:, feature_idx]
                unique_values = np.unique(X[:, feature_idx])  # æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å€¼
                
                # è®¡ç®—æ¯ä¸ªç‰¹å¾å€¼çš„æ¡ä»¶æ¦‚ç‡ï¼ˆå¸¦æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰
                feature_prob_dict = {}
                for value in unique_values:
                    count = np.sum(feature_values == value)
                    # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
                    prob = (count + self.alpha) / (len(class_samples) + self.alpha * len(unique_values))
                    feature_prob_dict[value] = prob
                    
                self.feature_probs[class_label][feature_idx] = feature_prob_dict
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # è®¡ç®— P(C_k) * âˆP(x_i|C_k)
                prob = self.class_priors[class_label]
                
                for feature_idx, feature_value in enumerate(sample):
                    if feature_value in self.feature_probs[class_label][feature_idx]:
                        prob *= self.feature_probs[class_label][feature_idx][feature_value]
                    else:
                        # æœªè§è¿‡çš„ç‰¹å¾å€¼ï¼Œä½¿ç”¨å¹³æ»‘æ¦‚ç‡
                        prob *= self.alpha / (sum(self.class_priors.values()) + self.alpha * len(self.classes))
                
                probabilities[i, j] = prob
        
        # å½’ä¸€åŒ–
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / row_sums
        
        return probabilities
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
    
    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        predictions = self.predict(X)
        return accuracy_score(y, predictions)

# æ¼”ç¤ºåŸºç¡€æœ´ç´ è´å¶æ–¯
def demonstrate_basic_naive_bayes():
    """æ¼”ç¤ºåŸºç¡€æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨"""
    
    # ç”Ÿæˆç¦»æ•£ç‰¹å¾æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    # åˆ›å»ºç¦»æ•£ç‰¹å¾æ•°æ®
    X = np.random.randint(0, 3, size=(n_samples, 4))  # 4ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾æœ‰0,1,2ä¸‰ä¸ªå€¼
    
    # åˆ›å»ºæ ‡ç­¾ï¼ˆåŸºäºç‰¹å¾çš„ç®€å•è§„åˆ™ï¼‰
    y = ((X[:, 0] + X[:, 1]) > (X[:, 2] + X[:, 3])).astype(int)
    
    print("ğŸ“Š ç¦»æ•£ç‰¹å¾æœ´ç´ è´å¶æ–¯æ¼”ç¤º")
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # è®­ç»ƒè‡ªå®ç°çš„æœ´ç´ è´å¶æ–¯
    nb_custom = SimpleNaiveBayes(alpha=1.0)
    nb_custom.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred_custom = nb_custom.predict(X_test)
    accuracy_custom = nb_custom.score(X_test, y_test)
    
    print(f"\nğŸ¯ è‡ªå®ç°æœ´ç´ è´å¶æ–¯ç»“æœ:")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy_custom:.4f}")
    
    # æ˜¾ç¤ºå…ˆéªŒæ¦‚ç‡
    print(f"\nğŸ“ˆ å…ˆéªŒæ¦‚ç‡:")
    for class_label, prior in nb_custom.class_priors.items():
        print(f"  P(ç±»åˆ«={class_label}) = {prior:.4f}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†æ¡ä»¶æ¦‚ç‡
    print(f"\nğŸ” æ¡ä»¶æ¦‚ç‡ç¤ºä¾‹ (ç‰¹å¾0):")
    for class_label in nb_custom.classes:
        print(f"  ç±»åˆ« {class_label}:")
        for value, prob in nb_custom.feature_probs[class_label][0].items():
            print(f"    P(ç‰¹å¾0={value}|ç±»åˆ«={class_label}) = {prob:.4f}")
    
    return nb_custom, X_test, y_test, y_pred_custom

nb_model, X_test, y_test, y_pred = demonstrate_basic_naive_bayes()
```

### é«˜æ–¯æœ´ç´ è´å¶æ–¯

```python
class GaussianNaiveBayes:
    """é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼ˆé€‚ç”¨äºè¿ç»­ç‰¹å¾ï¼‰"""
    
    def __init__(self):
        self.class_priors = {}
        self.feature_means = {}  # æ¯ä¸ªç±»åˆ«æ¯ä¸ªç‰¹å¾çš„å‡å€¼
        self.feature_vars = {}   # æ¯ä¸ªç±»åˆ«æ¯ä¸ªç‰¹å¾çš„æ–¹å·®
        self.classes = None
        
    def fit(self, X, y):
        """è®­ç»ƒé«˜æ–¯æœ´ç´ è´å¶æ–¯"""
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        n_samples = len(y)
        
        print(f"ğŸ¤– è®­ç»ƒé«˜æ–¯æœ´ç´ è´å¶æ–¯...")
        
        for class_label in self.classes:
            # å…ˆéªŒæ¦‚ç‡
            class_mask = (y == class_label)
            self.class_priors[class_label] = np.sum(class_mask) / n_samples
            
            # è¯¥ç±»åˆ«çš„æ ·æœ¬
            class_samples = X[class_mask]
            
            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®
            self.feature_means[class_label] = np.mean(class_samples, axis=0)
            self.feature_vars[class_label] = np.var(class_samples, axis=0)
            
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self
    
    def _gaussian_pdf(self, x, mean, var):
        """è®¡ç®—é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°"""
        # é¿å…é™¤é›¶
        var = np.maximum(var, 1e-9)
        coeff = 1.0 / np.sqrt(2 * np.pi * var)
        exponent = -0.5 * ((x - mean) ** 2) / var
        return coeff * np.exp(exponent)
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # å…ˆéªŒæ¦‚ç‡
                prob = self.class_priors[class_label]
                
                # ä¼¼ç„¶æ¦‚ç‡ï¼ˆå„ç‰¹å¾æ¦‚ç‡çš„ä¹˜ç§¯ï¼‰
                mean = self.feature_means[class_label]
                var = self.feature_vars[class_label]
                
                likelihood = np.prod(self._gaussian_pdf(sample, mean, var))
                prob *= likelihood
                
                probabilities[i, j] = prob
        
        # å½’ä¸€åŒ–
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / (row_sums + 1e-10)
        
        return probabilities
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
    
    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        predictions = self.predict(X)
        return accuracy_score(y, predictions)

# æ¼”ç¤ºé«˜æ–¯æœ´ç´ è´å¶æ–¯
def demonstrate_gaussian_naive_bayes():
    """æ¼”ç¤ºé«˜æ–¯æœ´ç´ è´å¶æ–¯"""
    
    # ç”Ÿæˆè¿ç»­ç‰¹å¾æ•°æ®
    X, y = make_classification(
        n_samples=1000, n_features=2, n_redundant=0, 
        n_informative=2, n_clusters_per_class=1, random_state=42
    )
    
    print("\nğŸ“Š é«˜æ–¯æœ´ç´ è´å¶æ–¯æ¼”ç¤º")
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # è‡ªå®ç°çš„é«˜æ–¯æœ´ç´ è´å¶æ–¯
    gnb_custom = GaussianNaiveBayes()
    gnb_custom.fit(X_train, y_train)
    
    # sklearnçš„é«˜æ–¯æœ´ç´ è´å¶æ–¯
    gnb_sklearn = GaussianNB()
    gnb_sklearn.fit(X_train, y_train)
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred_custom = gnb_custom.predict(X_test)
    y_pred_sklearn = gnb_sklearn.predict(X_test)
    
    accuracy_custom = gnb_custom.score(X_test, y_test)
    accuracy_sklearn = gnb_sklearn.score(X_test, y_test)
    
    print(f"\nğŸ¯ ç»“æœå¯¹æ¯”:")
    print(f"è‡ªå®ç°å‡†ç¡®ç‡: {accuracy_custom:.4f}")
    print(f"sklearnå‡†ç¡®ç‡: {accuracy_sklearn:.4f}")
    
    # å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
    plt.figure(figsize=(15, 5))
    
    # åŸå§‹æ•°æ®
    plt.subplot(1, 3, 1)
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    plt.title('åŸå§‹æ•°æ®åˆ†å¸ƒ')
    plt.colorbar(scatter)
    plt.grid(True, alpha=0.3)
    
    # è‡ªå®ç°æ¨¡å‹å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 3, 2)
    plot_decision_boundary(gnb_custom, X, y, "è‡ªå®ç°é«˜æ–¯æœ´ç´ è´å¶æ–¯")
    
    # sklearnæ¨¡å‹å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 3, 3)
    plot_decision_boundary(gnb_sklearn, X, y, "sklearné«˜æ–¯æœ´ç´ è´å¶æ–¯")
    
    plt.tight_layout()
    plt.show()
    
    # æ˜¾ç¤ºå­¦ä¹ åˆ°çš„å‚æ•°
    print(f"\nğŸ“ˆ å­¦ä¹ åˆ°çš„å‚æ•°:")
    for class_label in gnb_custom.classes:
        print(f"ç±»åˆ« {class_label}:")
        print(f"  å…ˆéªŒæ¦‚ç‡: {gnb_custom.class_priors[class_label]:.4f}")
        print(f"  ç‰¹å¾å‡å€¼: {gnb_custom.feature_means[class_label]}")
        print(f"  ç‰¹å¾æ–¹å·®: {gnb_custom.feature_vars[class_label]}")
    
    return gnb_custom, gnb_sklearn

def plot_decision_boundary(model, X, y, title):
    """ç»˜åˆ¶å†³ç­–è¾¹ç•Œ"""
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black')
    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)

gnb_custom, gnb_sklearn = demonstrate_gaussian_naive_bayes()
```

## æ–‡æœ¬åˆ†ç±»åº”ç”¨

### å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ç”¨äºæ–‡æœ¬åˆ†ç±»

```python
class TextClassificationNB:
    """åŸºäºæœ´ç´ è´å¶æ–¯çš„æ–‡æœ¬åˆ†ç±»ç³»ç»Ÿ"""
    
    def __init__(self, alpha=1.0, use_tfidf=False):
        self.alpha = alpha
        self.use_tfidf = use_tfidf
        self.vectorizer = None
        self.nb_model = None
        self.classes = None
        
    def generate_text_data(self, n_samples=1000):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æœ¬æ•°æ®"""
        np.random.seed(42)
        
        # å®šä¹‰ä¸åŒç±»åˆ«çš„å…³é”®è¯
        categories = {
            'sports': ['football', 'basketball', 'soccer', 'game', 'team', 'player', 'score', 'match', 'win', 'championship'],
            'technology': ['computer', 'software', 'internet', 'data', 'algorithm', 'programming', 'AI', 'machine learning', 'code', 'digital'],
            'politics': ['government', 'election', 'policy', 'president', 'congress', 'vote', 'democracy', 'law', 'political', 'campaign']
        }
        
        texts = []
        labels = []
        
        for category, keywords in categories.items():
            for _ in range(n_samples // len(categories)):
                # éšæœºé€‰æ‹©3-8ä¸ªå…³é”®è¯
                n_words = np.random.randint(3, 9)
                selected_words = np.random.choice(keywords, n_words, replace=True)
                
                # æ·»åŠ ä¸€äº›å™ªå£°è¯
                noise_words = ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of']
                n_noise = np.random.randint(1, 4)
                noise = np.random.choice(noise_words, n_noise, replace=True)
                
                # ç»„åˆæˆæ–‡æœ¬
                all_words = np.concatenate([selected_words, noise])
                np.random.shuffle(all_words)
                text = ' '.join(all_words)
                
                texts.append(text)
                labels.append(category)
        
        return texts, labels
    
    def fit(self, texts, labels):
        """è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹"""
        print(f"ğŸ“š å¼€å§‹è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹...")
        print(f"æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(pd.Series(labels).value_counts())}")
        
        # æ–‡æœ¬å‘é‡åŒ–
        if self.use_tfidf:
            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        else:
            self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')
        
        X = self.vectorizer.fit_transform(texts)
        
        print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        
        # è®­ç»ƒæœ´ç´ è´å¶æ–¯æ¨¡å‹
        self.nb_model = MultinomialNB(alpha=self.alpha)
        self.nb_model.fit(X, labels)
        
        self.classes = self.nb_model.classes_
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self
    
    def predict(self, texts):
        """é¢„æµ‹æ–‡æœ¬ç±»åˆ«"""
        X = self.vectorizer.transform(texts)
        return self.nb_model.predict(X)
    
    def predict_proba(self, texts):
        """é¢„æµ‹æ–‡æœ¬ç±»åˆ«æ¦‚ç‡"""
        X = self.vectorizer.transform(texts)
        return self.nb_model.predict_proba(X)
    
    def analyze_features(self, top_n=10):
        """åˆ†æé‡è¦ç‰¹å¾"""
        feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"\nğŸ” å„ç±»åˆ«æœ€é‡è¦çš„{top_n}ä¸ªç‰¹å¾:")
        print("=" * 60)
        
        for i, class_name in enumerate(self.classes):
            # è·å–è¯¥ç±»åˆ«çš„ç‰¹å¾æƒé‡
            feature_log_probs = self.nb_model.feature_log_prob_[i]
            
            # è·å–topç‰¹å¾
            top_indices = np.argsort(feature_log_probs)[::-1][:top_n]
            
            print(f"\nğŸ“‚ {class_name.upper()}:")
            print("-" * 30)
            
            for j, idx in enumerate(top_indices):
                feature_name = feature_names[idx]
                log_prob = feature_log_probs[idx]
                prob = np.exp(log_prob)
                
                print(f"{j+1:2d}. {feature_name:<15} (æ¦‚ç‡: {prob:.6f})")
    
    def classify_text(self, text):
        """åˆ†ç±»å•ä¸ªæ–‡æœ¬å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"""
        prediction = self.predict([text])[0]
        probabilities = self.predict_proba([text])[0]
        
        print(f"\nğŸ“„ æ–‡æœ¬: '{text}'")
        print(f"ğŸ¯ é¢„æµ‹ç±»åˆ«: {prediction}")
        print(f"ğŸ“Š å„ç±»åˆ«æ¦‚ç‡:")
        
        for class_name, prob in zip(self.classes, probabilities):
            print(f"  {class_name:<12}: {prob:.4f} {'â˜…' if class_name == prediction else ''}")
        
        return prediction, probabilities

# æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»
def demonstrate_text_classification():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»"""
    
    # åˆ›å»ºæ–‡æœ¬åˆ†ç±»å™¨
    text_classifier = TextClassificationNB(alpha=1.0, use_tfidf=True)
    
    # ç”Ÿæˆæ•°æ®
    texts, labels = text_classifier.generate_text_data(900)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.3, random_state=42, stratify=labels
    )
    
    # è®­ç»ƒæ¨¡å‹
    text_classifier.fit(X_train, y_train)
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred = text_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\nğŸ¯ æ–‡æœ¬åˆ†ç±»ç»“æœ:")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f"\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=text_classifier.classes, 
                yticklabels=text_classifier.classes)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title(f'æ–‡æœ¬åˆ†ç±»æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {accuracy:.4f}')
    plt.show()
    
    # åˆ†æé‡è¦ç‰¹å¾
    text_classifier.analyze_features()
    
    # æµ‹è¯•æ–°æ–‡æœ¬
    test_texts = [
        "football team won the championship game",
        "new AI algorithm improves machine learning performance",
        "president announced new government policy",
        "basketball player scored in the match",
        "computer software programming code"
    ]
    
    print(f"\nğŸ§ª æµ‹è¯•æ–°æ–‡æœ¬åˆ†ç±»:")
    for text in test_texts:
        text_classifier.classify_text(text)
    
    return text_classifier

text_model = demonstrate_text_classification()
```

### åƒåœ¾é‚®ä»¶æ£€æµ‹å®æˆ˜

```python
class SpamDetectorNB:
    """åŸºäºæœ´ç´ è´å¶æ–¯çš„åƒåœ¾é‚®ä»¶æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.vectorizer = None
        self.nb_model = None
        self.feature_names = None
        
    def generate_email_dataset(self, n_samples=2000):
        """ç”Ÿæˆæ›´çœŸå®çš„é‚®ä»¶æ•°æ®é›†"""
        np.random.seed(42)
        
        # åƒåœ¾é‚®ä»¶æ¨¡å¼
        spam_patterns = {
            'promotional': ['free', 'win', 'prize', 'offer', 'discount', 'sale', 'deal', 'limited time'],
            'urgent': ['urgent', 'act now', 'hurry', 'expires', 'deadline', 'immediate'],
            'money': ['money', 'cash', 'earn', 'income', 'profit', 'rich', 'wealthy'],
            'suspicious': ['click here', 'guarantee', 'no risk', 'amazing', 'incredible']
        }
        
        # æ­£å¸¸é‚®ä»¶æ¨¡å¼
        normal_patterns = {
            'work': ['meeting', 'project', 'deadline', 'report', 'presentation', 'team', 'colleague'],
            'personal': ['family', 'friend', 'birthday', 'vacation', 'dinner', 'weekend'],
            'formal': ['regards', 'sincerely', 'thank you', 'please', 'kindly', 'appreciate'],
            'business': ['company', 'client', 'contract', 'proposal', 'budget', 'schedule']
        }
        
        emails = []
        labels = []
        
        # ç”Ÿæˆåƒåœ¾é‚®ä»¶
        for _ in range(n_samples // 2):
            email_parts = []
            
            # éšæœºé€‰æ‹©1-3ä¸ªåƒåœ¾é‚®ä»¶æ¨¡å¼
            n_patterns = np.random.randint(1, 4)
            selected_patterns = np.random.choice(list(spam_patterns.keys()), n_patterns, replace=False)
            
            for pattern in selected_patterns:
                n_words = np.random.randint(2, 5)
                words = np.random.choice(spam_patterns[pattern], n_words, replace=True)
                email_parts.extend(words)
            
            # æ·»åŠ ä¸€äº›æ­£å¸¸è¯æ±‡ä½œä¸ºå™ªå£°
            if np.random.random() < 0.3:
                normal_pattern = np.random.choice(list(normal_patterns.keys()))
                noise_words = np.random.choice(normal_patterns[normal_pattern], 2, replace=True)
                email_parts.extend(noise_words)
            
            email = ' '.join(email_parts)
            emails.append(email)
            labels.append('spam')
        
        # ç”Ÿæˆæ­£å¸¸é‚®ä»¶
        for _ in range(n_samples // 2):
            email_parts = []
            
            # éšæœºé€‰æ‹©1-2ä¸ªæ­£å¸¸é‚®ä»¶æ¨¡å¼
            n_patterns = np.random.randint(1, 3)
            selected_patterns = np.random.choice(list(normal_patterns.keys()), n_patterns, replace=False)
            
            for pattern in selected_patterns:
                n_words = np.random.randint(3, 7)
                words = np.random.choice(normal_patterns[pattern], n_words, replace=True)
                email_parts.extend(words)
            
            # æ·»åŠ ä¸€äº›åƒåœ¾è¯æ±‡ä½œä¸ºå™ªå£°
            if np.random.random() < 0.1:
                spam_pattern = np.random.choice(list(spam_patterns.keys()))
                noise_words = np.random.choice(spam_patterns[spam_pattern], 1, replace=True)
                email_parts.extend(noise_words)
            
            email = ' '.join(email_parts)
            emails.append(email)
            labels.append('ham')
        
        return emails, labels
    
    def train(self, emails, labels):
        """è®­ç»ƒåƒåœ¾é‚®ä»¶æ£€æµ‹æ¨¡å‹"""
        print(f"ğŸ“§ è®­ç»ƒåƒåœ¾é‚®ä»¶æ£€æµ‹å™¨...")
        print(f"é‚®ä»¶æ€»æ•°: {len(emails)}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(pd.Series(labels).value_counts())}")
        
        # ç‰¹å¾æå–ï¼šä½¿ç”¨TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=2000,
            stop_words='english',
            ngram_range=(1, 2),  # 1-2gram
            min_df=2,  # è‡³å°‘å‡ºç°2æ¬¡
            max_df=0.95  # æœ€å¤šå‡ºç°åœ¨95%çš„æ–‡æ¡£ä¸­
        )
        
        X = self.vectorizer.fit_transform(emails)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        
        # è®­ç»ƒå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯
        self.nb_model = MultinomialNB(alpha=1.0)
        self.nb_model.fit(X, labels)
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self
    
    def predict_email(self, email_text):
        """é¢„æµ‹å•å°é‚®ä»¶"""
        X = self.vectorizer.transform([email_text])
        prediction = self.nb_model.predict(X)[0]
        probabilities = self.nb_model.predict_proba(X)[0]
        
        return prediction, probabilities
    
    def analyze_spam_indicators(self, top_n=15):
        """åˆ†æåƒåœ¾é‚®ä»¶æŒ‡ç¤ºè¯"""
        # è·å–ç±»åˆ«ç´¢å¼•
        classes = self.nb_model.classes_
        spam_idx = list(classes).index('spam')
        ham_idx = list(classes).index('ham')
        
        # è®¡ç®—ç‰¹å¾çš„å¯¹æ•°æ¦‚ç‡æ¯”
        spam_log_probs = self.nb_model.feature_log_prob_[spam_idx]
        ham_log_probs = self.nb_model.feature_log_prob_[ham_idx]
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡å·®ï¼ˆspam - hamï¼‰
        log_prob_diff = spam_log_probs - ham_log_probs
        
        # è·å–æœ€å¼ºçš„åƒåœ¾é‚®ä»¶æŒ‡ç¤ºè¯
        spam_indicators = np.argsort(log_prob_diff)[::-1][:top_n]
        
        # è·å–æœ€å¼ºçš„æ­£å¸¸é‚®ä»¶æŒ‡ç¤ºè¯
        ham_indicators = np.argsort(log_prob_diff)[:top_n]
        
        print(f"\nğŸš¨ æœ€å¼ºåƒåœ¾é‚®ä»¶æŒ‡ç¤ºè¯ (Top {top_n}):")
        print("-" * 50)
        for i, idx in enumerate(spam_indicators):
            word = self.feature_names[idx]
            score = log_prob_diff[idx]
            print(f"{i+1:2d}. {word:<20} (åˆ†æ•°: {score:.4f})")
        
        print(f"\nâœ… æœ€å¼ºæ­£å¸¸é‚®ä»¶æŒ‡ç¤ºè¯ (Top {top_n}):")
        print("-" * 50)
        for i, idx in enumerate(ham_indicators):
            word = self.feature_names[idx]
            score = log_prob_diff[idx]
            print(f"{i+1:2d}. {word:<20} (åˆ†æ•°: {score:.4f})")
    
    def detailed_prediction(self, email_text):
        """è¯¦ç»†é¢„æµ‹åˆ†æ"""
        prediction, probabilities = self.predict_email(email_text)
        
        print(f"\nğŸ“§ é‚®ä»¶å†…å®¹: '{email_text}'")
        print(f"ğŸ¯ é¢„æµ‹ç»“æœ: {'åƒåœ¾é‚®ä»¶' if prediction == 'spam' else 'æ­£å¸¸é‚®ä»¶'}")
        print(f"ğŸ“Š æ¦‚ç‡åˆ†å¸ƒ:")
        
        classes = self.nb_model.classes_
        for class_name, prob in zip(classes, probabilities):
            emoji = "ğŸš¨" if class_name == 'spam' else "âœ…"
            star = "â˜…" if class_name == prediction else " "
            print(f"  {emoji} {class_name:<8}: {prob:.4f} {star}")
        
        # åˆ†æå…³é”®è¯è´¡çŒ®
        X = self.vectorizer.transform([email_text])
        feature_indices = X.nonzero()[1]
        
        if len(feature_indices) > 0:
            print(f"\nğŸ” å…³é”®è¯åˆ†æ:")
            
            spam_idx = list(classes).index('spam')
            ham_idx = list(classes).index('ham')
            
            word_contributions = []
            
            for idx in feature_indices:
                word = self.feature_names[idx]
                tfidf_score = X[0, idx]
                
                spam_log_prob = self.nb_model.feature_log_prob_[spam_idx, idx]
                ham_log_prob = self.nb_model.feature_log_prob_[ham_idx, idx]
                
                contribution = (spam_log_prob - ham_log_prob) * tfidf_score
                word_contributions.append((word, contribution, tfidf_score))
            
            # æŒ‰è´¡çŒ®åº¦æ’åº
            word_contributions.sort(key=lambda x: abs(x[1]), reverse=True)
            
            print("  è¯æ±‡           è´¡çŒ®åº¦    TF-IDF   ç±»å‹")
            print("  " + "-" * 45)
            
            for word, contrib, tfidf in word_contributions[:8]:
                contrib_type = "åƒåœ¾" if contrib > 0 else "æ­£å¸¸"
                print(f"  {word:<12} {contrib:>8.4f} {tfidf:>8.4f}   {contrib_type}")
        
        return prediction, probabilities

# æ¼”ç¤ºåƒåœ¾é‚®ä»¶æ£€æµ‹
def demonstrate_spam_detection():
    """æ¼”ç¤ºåƒåœ¾é‚®ä»¶æ£€æµ‹"""
    
    # åˆ›å»ºåƒåœ¾é‚®ä»¶æ£€æµ‹å™¨
    spam_detector = SpamDetectorNB()
    
    # ç”Ÿæˆæ•°æ®
    emails, labels = spam_detector.generate_email_dataset(1600)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        emails, labels, test_size=0.25, random_state=42, stratify=labels
    )
    
    # è®­ç»ƒæ¨¡å‹
    spam_detector.train(X_train, y_train)
    
    # è¯„ä¼°æ¨¡å‹
    y_pred = []
    y_proba = []
    
    for email in X_test:
        pred, prob = spam_detector.predict_email(email)
        y_pred.append(pred)
        y_proba.append(prob[1] if spam_detector.nb_model.classes_[1] == 'spam' else prob[0])
    
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\nğŸ¯ åƒåœ¾é‚®ä»¶æ£€æµ‹ç»“æœ:")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f"\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    # ROCæ›²çº¿
    from sklearn.metrics import roc_curve, auc
    
    # å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
    y_test_binary = [1 if label == 'spam' else 0 for label in y_test]
    
    fpr, tpr, _ = roc_curve(y_test_binary, y_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(12, 5))
    
    # ROCæ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡æ­£ç‡ (FPR)')
    plt.ylabel('çœŸæ­£ç‡ (TPR)')
    plt.title('ROCæ›²çº¿')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # æ··æ·†çŸ©é˜µ
    plt.subplot(1, 2, 2)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['æ­£å¸¸', 'åƒåœ¾'], yticklabels=['æ­£å¸¸', 'åƒåœ¾'])
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title(f'æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {accuracy:.4f}')
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†æåƒåœ¾é‚®ä»¶æŒ‡ç¤ºè¯
    spam_detector.analyze_spam_indicators()
    
    # æµ‹è¯•å…·ä½“é‚®ä»¶
    test_emails = [
        "free money win prize click here urgent offer",
        "meeting tomorrow project deadline report",
        "amazing deal limited time act now guarantee",
        "thank you for the presentation regards team",
        "earn cash incredible profit no risk",
        "family dinner weekend birthday celebration"
    ]
    
    print(f"\nğŸ§ª æµ‹è¯•é‚®ä»¶åˆ†ç±»:")
    for email in test_emails:
        spam_detector.detailed_prediction(email)
    
    return spam_detector

spam_model = demonstrate_spam_detection()
```

## æœ´ç´ è´å¶æ–¯å˜ä½“å¯¹æ¯”

```python
def compare_naive_bayes_variants():
    """å¯¹æ¯”ä¸åŒæœ´ç´ è´å¶æ–¯å˜ä½“"""
    
    print("\nğŸ”¬ æœ´ç´ è´å¶æ–¯å˜ä½“å¯¹æ¯”å®éªŒ")
    print("=" * 50)
    
    # å‡†å¤‡ä¸åŒç±»å‹çš„æ•°æ®
    datasets = {}
    
    # 1. è¿ç»­ç‰¹å¾æ•°æ®ï¼ˆé€‚åˆé«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼‰
    X_continuous, y_continuous = make_classification(
        n_samples=1000, n_features=4, n_redundant=0, 
        n_informative=4, n_clusters_per_class=1, random_state=42
    )
    datasets['è¿ç»­ç‰¹å¾'] = (X_continuous, y_continuous)
    
    # 2. ç¦»æ•£è®¡æ•°æ•°æ®ï¼ˆé€‚åˆå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ï¼‰
    np.random.seed(42)
    X_counts = np.random.poisson(3, size=(1000, 4))  # æ³Šæ¾åˆ†å¸ƒç”Ÿæˆè®¡æ•°æ•°æ®
    y_counts = (X_counts.sum(axis=1) > X_counts.mean()).astype(int)
    datasets['è®¡æ•°ç‰¹å¾'] = (X_counts, y_counts)
    
    # 3. äºŒå€¼ç‰¹å¾æ•°æ®ï¼ˆé€‚åˆä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼‰
    X_binary = np.random.binomial(1, 0.3, size=(1000, 4))  # äºŒé¡¹åˆ†å¸ƒç”ŸæˆäºŒå€¼æ•°æ®
    y_binary = (X_binary.sum(axis=1) > 2).astype(int)
    datasets['äºŒå€¼ç‰¹å¾'] = (X_binary, y_binary)
    
    # æœ´ç´ è´å¶æ–¯å˜ä½“
    models = {
        'é«˜æ–¯æœ´ç´ è´å¶æ–¯': GaussianNB(),
        'å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯': MultinomialNB(),
        'ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯': BernoulliNB()
    }
    
    results = []
    
    for data_name, (X, y) in datasets.items():
        print(f"\nğŸ“Š æ•°æ®é›†: {data_name}")
        print(f"æ•°æ®å½¢çŠ¶: {X.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # åˆ’åˆ†æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        print(f"\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        print("-" * 40)
        
        for model_name, model in models.items():
            try:
                # è®­ç»ƒå’Œé¢„æµ‹
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                
                print(f"{model_name:<15}: {accuracy:.4f}")
                
                results.append({
                    'æ•°æ®ç±»å‹': data_name,
                    'æ¨¡å‹': model_name,
                    'å‡†ç¡®ç‡': accuracy
                })
                
            except Exception as e:
                print(f"{model_name:<15}: é”™è¯¯ - {str(e)[:30]}...")
                results.append({
                    'æ•°æ®ç±»å‹': data_name,
                    'æ¨¡å‹': model_name,
                    'å‡†ç¡®ç‡': 0.0
                })
    
    # ç»“æœå¯è§†åŒ–
    results_df = pd.DataFrame(results)
    
    plt.figure(figsize=(12, 8))
    
    # çƒ­åŠ›å›¾
    pivot_table = results_df.pivot(index='æ¨¡å‹', columns='æ•°æ®ç±»å‹', values='å‡†ç¡®ç‡')
    
    plt.subplot(2, 1, 1)
    sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='YlOrRd', 
                cbar_kws={'label': 'å‡†ç¡®ç‡'})
    plt.title('æœ´ç´ è´å¶æ–¯å˜ä½“åœ¨ä¸åŒæ•°æ®ç±»å‹ä¸Šçš„æ€§èƒ½')
    
    # æŸ±çŠ¶å›¾
    plt.subplot(2, 1, 2)
    x_pos = np.arange(len(datasets))
    width = 0.25
    
    for i, model_name in enumerate(models.keys()):
        model_results = results_df[results_df['æ¨¡å‹'] == model_name]['å‡†ç¡®ç‡'].values
        plt.bar(x_pos + i * width, model_results, width, label=model_name)
    
    plt.xlabel('æ•°æ®ç±»å‹')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('æœ´ç´ è´å¶æ–¯å˜ä½“æ€§èƒ½å¯¹æ¯”')
    plt.xticks(x_pos + width, list(datasets.keys()))
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ€»ç»“å»ºè®®
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("=" * 30)
    print("ğŸ“ˆ é«˜æ–¯æœ´ç´ è´å¶æ–¯: é€‚ç”¨äºè¿ç»­ç‰¹å¾ï¼Œå‡è®¾ç‰¹å¾æœä»é«˜æ–¯åˆ†å¸ƒ")
    print("ğŸ“Š å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯: é€‚ç”¨äºè®¡æ•°ç‰¹å¾ï¼Œå¦‚æ–‡æœ¬ä¸­çš„è¯é¢‘")
    print("ğŸ”¢ ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯: é€‚ç”¨äºäºŒå€¼ç‰¹å¾ï¼Œå¦‚æ–‡æ¡£ä¸­è¯æ±‡çš„å‡ºç°ä¸å¦")
    
    return results_df

comparison_results = compare_naive_bayes_variants()
```