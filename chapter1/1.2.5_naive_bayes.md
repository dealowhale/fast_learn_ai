# 1.2.5 朴素贝叶斯算法

## 学习目标

通过本节学习，你将掌握：
- 朴素贝叶斯算法的数学原理和贝叶斯定理
- 三种主要的朴素贝叶斯变体：高斯、多项式和伯努利
- 朴素贝叶斯在文本分类和垃圾邮件检测中的应用
- 条件独立假设的含义和影响
- 拉普拉斯平滑技术处理零概率问题

## 算法原理

### 贝叶斯定理基础

朴素贝叶斯算法基于贝叶斯定理，这是概率论中的一个基本定理：

$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

在分类问题中，我们要计算给定特征 $X$ 的情况下，样本属于类别 $C_k$ 的概率：

$$P(C_k|X) = \frac{P(X|C_k) \cdot P(C_k)}{P(X)}$$

其中：
- $P(C_k|X)$：后验概率（我们要求的）
- $P(X|C_k)$：似然概率
- $P(C_k)$：先验概率
- $P(X)$：边缘概率（归一化常数）

### 朴素假设

"朴素"假设指的是**条件独立假设**：给定类别的情况下，各个特征之间相互独立。

对于特征向量 $X = (x_1, x_2, ..., x_n)$：

$$P(X|C_k) = P(x_1, x_2, ..., x_n|C_k) = \prod_{i=1}^{n} P(x_i|C_k)$$

因此，分类决策变为：

$$\hat{y} = \arg\max_{k} P(C_k) \prod_{i=1}^{n} P(x_i|C_k)$$

## 算法实现

### 基础朴素贝叶斯实现

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification, fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import defaultdict
import pandas as pd

class SimpleNaiveBayes:
    """简单的朴素贝叶斯分类器实现"""
    
    def __init__(self, alpha=1.0):
        """
        参数:
        alpha: 拉普拉斯平滑参数
        """
        self.alpha = alpha
        self.class_priors = {}  # 先验概率 P(C_k)
        self.feature_probs = {}  # 条件概率 P(x_i|C_k)
        self.classes = None
        self.n_features = None
        
    def fit(self, X, y):
        """训练朴素贝叶斯分类器"""
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        self.n_features = X.shape[1]
        n_samples = len(y)
        
        print(f"🤖 训练朴素贝叶斯分类器...")
        print(f"样本数: {n_samples}, 特征数: {self.n_features}, 类别数: {len(self.classes)}")
        
        # 计算先验概率 P(C_k)
        for class_label in self.classes:
            class_count = np.sum(y == class_label)
            self.class_priors[class_label] = class_count / n_samples
            
        # 计算条件概率 P(x_i|C_k)
        self.feature_probs = {}
        
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            self.feature_probs[class_label] = {}
            
            for feature_idx in range(self.n_features):
                feature_values = class_samples[:, feature_idx]
                unique_values = np.unique(X[:, feature_idx])  # 所有可能的特征值
                
                # 计算每个特征值的条件概率（带拉普拉斯平滑）
                feature_prob_dict = {}
                for value in unique_values:
                    count = np.sum(feature_values == value)
                    # 拉普拉斯平滑
                    prob = (count + self.alpha) / (len(class_samples) + self.alpha * len(unique_values))
                    feature_prob_dict[value] = prob
                    
                self.feature_probs[class_label][feature_idx] = feature_prob_dict
        
        print("✅ 训练完成!")
        return self
    
    def predict_proba(self, X):
        """预测概率"""
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # 计算 P(C_k) * ∏P(x_i|C_k)
                prob = self.class_priors[class_label]
                
                for feature_idx, feature_value in enumerate(sample):
                    if feature_value in self.feature_probs[class_label][feature_idx]:
                        prob *= self.feature_probs[class_label][feature_idx][feature_value]
                    else:
                        # 未见过的特征值，使用平滑概率
                        prob *= self.alpha / (sum(self.class_priors.values()) + self.alpha * len(self.classes))
                
                probabilities[i, j] = prob
        
        # 归一化
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / row_sums
        
        return probabilities
    
    def predict(self, X):
        """预测类别"""
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
    
    def score(self, X, y):
        """计算准确率"""
        predictions = self.predict(X)
        return accuracy_score(y, predictions)

# 演示基础朴素贝叶斯
def demonstrate_basic_naive_bayes():
    """演示基础朴素贝叶斯分类器"""
    
    # 生成离散特征数据
    np.random.seed(42)
    n_samples = 1000
    
    # 创建离散特征数据
    X = np.random.randint(0, 3, size=(n_samples, 4))  # 4个特征，每个特征有0,1,2三个值
    
    # 创建标签（基于特征的简单规则）
    y = ((X[:, 0] + X[:, 1]) > (X[:, 2] + X[:, 3])).astype(int)
    
    print("📊 离散特征朴素贝叶斯演示")
    print(f"数据形状: {X.shape}")
    print(f"类别分布: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 训练自实现的朴素贝叶斯
    nb_custom = SimpleNaiveBayes(alpha=1.0)
    nb_custom.fit(X_train, y_train)
    
    # 预测
    y_pred_custom = nb_custom.predict(X_test)
    accuracy_custom = nb_custom.score(X_test, y_test)
    
    print(f"\n🎯 自实现朴素贝叶斯结果:")
    print(f"测试准确率: {accuracy_custom:.4f}")
    
    # 显示先验概率
    print(f"\n📈 先验概率:")
    for class_label, prior in nb_custom.class_priors.items():
        print(f"  P(类别={class_label}) = {prior:.4f}")
    
    # 显示部分条件概率
    print(f"\n🔍 条件概率示例 (特征0):")
    for class_label in nb_custom.classes:
        print(f"  类别 {class_label}:")
        for value, prob in nb_custom.feature_probs[class_label][0].items():
            print(f"    P(特征0={value}|类别={class_label}) = {prob:.4f}")
    
    return nb_custom, X_test, y_test, y_pred_custom

nb_model, X_test, y_test, y_pred = demonstrate_basic_naive_bayes()
```

### 高斯朴素贝叶斯

```python
class GaussianNaiveBayes:
    """高斯朴素贝叶斯分类器（适用于连续特征）"""
    
    def __init__(self):
        self.class_priors = {}
        self.feature_means = {}  # 每个类别每个特征的均值
        self.feature_vars = {}   # 每个类别每个特征的方差
        self.classes = None
        
    def fit(self, X, y):
        """训练高斯朴素贝叶斯"""
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        n_samples = len(y)
        
        print(f"🤖 训练高斯朴素贝叶斯...")
        
        for class_label in self.classes:
            # 先验概率
            class_mask = (y == class_label)
            self.class_priors[class_label] = np.sum(class_mask) / n_samples
            
            # 该类别的样本
            class_samples = X[class_mask]
            
            # 计算每个特征的均值和方差
            self.feature_means[class_label] = np.mean(class_samples, axis=0)
            self.feature_vars[class_label] = np.var(class_samples, axis=0)
            
        print("✅ 训练完成!")
        return self
    
    def _gaussian_pdf(self, x, mean, var):
        """计算高斯概率密度函数"""
        # 避免除零
        var = np.maximum(var, 1e-9)
        coeff = 1.0 / np.sqrt(2 * np.pi * var)
        exponent = -0.5 * ((x - mean) ** 2) / var
        return coeff * np.exp(exponent)
    
    def predict_proba(self, X):
        """预测概率"""
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # 先验概率
                prob = self.class_priors[class_label]
                
                # 似然概率（各特征概率的乘积）
                mean = self.feature_means[class_label]
                var = self.feature_vars[class_label]
                
                likelihood = np.prod(self._gaussian_pdf(sample, mean, var))
                prob *= likelihood
                
                probabilities[i, j] = prob
        
        # 归一化
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / (row_sums + 1e-10)
        
        return probabilities
    
    def predict(self, X):
        """预测类别"""
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
    
    def score(self, X, y):
        """计算准确率"""
        predictions = self.predict(X)
        return accuracy_score(y, predictions)

# 演示高斯朴素贝叶斯
def demonstrate_gaussian_naive_bayes():
    """演示高斯朴素贝叶斯"""
    
    # 生成连续特征数据
    X, y = make_classification(
        n_samples=1000, n_features=2, n_redundant=0, 
        n_informative=2, n_clusters_per_class=1, random_state=42
    )
    
    print("\n📊 高斯朴素贝叶斯演示")
    print(f"数据形状: {X.shape}")
    print(f"类别分布: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 自实现的高斯朴素贝叶斯
    gnb_custom = GaussianNaiveBayes()
    gnb_custom.fit(X_train, y_train)
    
    # sklearn的高斯朴素贝叶斯
    gnb_sklearn = GaussianNB()
    gnb_sklearn.fit(X_train, y_train)
    
    # 预测和评估
    y_pred_custom = gnb_custom.predict(X_test)
    y_pred_sklearn = gnb_sklearn.predict(X_test)
    
    accuracy_custom = gnb_custom.score(X_test, y_test)
    accuracy_sklearn = gnb_sklearn.score(X_test, y_test)
    
    print(f"\n🎯 结果对比:")
    print(f"自实现准确率: {accuracy_custom:.4f}")
    print(f"sklearn准确率: {accuracy_sklearn:.4f}")
    
    # 可视化决策边界
    plt.figure(figsize=(15, 5))
    
    # 原始数据
    plt.subplot(1, 3, 1)
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
    plt.xlabel('特征 1')
    plt.ylabel('特征 2')
    plt.title('原始数据分布')
    plt.colorbar(scatter)
    plt.grid(True, alpha=0.3)
    
    # 自实现模型决策边界
    plt.subplot(1, 3, 2)
    plot_decision_boundary(gnb_custom, X, y, "自实现高斯朴素贝叶斯")
    
    # sklearn模型决策边界
    plt.subplot(1, 3, 3)
    plot_decision_boundary(gnb_sklearn, X, y, "sklearn高斯朴素贝叶斯")
    
    plt.tight_layout()
    plt.show()
    
    # 显示学习到的参数
    print(f"\n📈 学习到的参数:")
    for class_label in gnb_custom.classes:
        print(f"类别 {class_label}:")
        print(f"  先验概率: {gnb_custom.class_priors[class_label]:.4f}")
        print(f"  特征均值: {gnb_custom.feature_means[class_label]}")
        print(f"  特征方差: {gnb_custom.feature_vars[class_label]}")
    
    return gnb_custom, gnb_sklearn

def plot_decision_boundary(model, X, y, title):
    """绘制决策边界"""
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black')
    plt.xlabel('特征 1')
    plt.ylabel('特征 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)

gnb_custom, gnb_sklearn = demonstrate_gaussian_naive_bayes()
```

## 文本分类应用

### 多项式朴素贝叶斯用于文本分类

```python
class TextClassificationNB:
    """基于朴素贝叶斯的文本分类系统"""
    
    def __init__(self, alpha=1.0, use_tfidf=False):
        self.alpha = alpha
        self.use_tfidf = use_tfidf
        self.vectorizer = None
        self.nb_model = None
        self.classes = None
        
    def generate_text_data(self, n_samples=1000):
        """生成模拟文本数据"""
        np.random.seed(42)
        
        # 定义不同类别的关键词
        categories = {
            'sports': ['football', 'basketball', 'soccer', 'game', 'team', 'player', 'score', 'match', 'win', 'championship'],
            'technology': ['computer', 'software', 'internet', 'data', 'algorithm', 'programming', 'AI', 'machine learning', 'code', 'digital'],
            'politics': ['government', 'election', 'policy', 'president', 'congress', 'vote', 'democracy', 'law', 'political', 'campaign']
        }
        
        texts = []
        labels = []
        
        for category, keywords in categories.items():
            for _ in range(n_samples // len(categories)):
                # 随机选择3-8个关键词
                n_words = np.random.randint(3, 9)
                selected_words = np.random.choice(keywords, n_words, replace=True)
                
                # 添加一些噪声词
                noise_words = ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of']
                n_noise = np.random.randint(1, 4)
                noise = np.random.choice(noise_words, n_noise, replace=True)
                
                # 组合成文本
                all_words = np.concatenate([selected_words, noise])
                np.random.shuffle(all_words)
                text = ' '.join(all_words)
                
                texts.append(text)
                labels.append(category)
        
        return texts, labels
    
    def fit(self, texts, labels):
        """训练文本分类模型"""
        print(f"📚 开始训练文本分类模型...")
        print(f"文本数量: {len(texts)}")
        print(f"类别分布: {dict(pd.Series(labels).value_counts())}")
        
        # 文本向量化
        if self.use_tfidf:
            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        else:
            self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')
        
        X = self.vectorizer.fit_transform(texts)
        
        print(f"特征维度: {X.shape[1]}")
        
        # 训练朴素贝叶斯模型
        self.nb_model = MultinomialNB(alpha=self.alpha)
        self.nb_model.fit(X, labels)
        
        self.classes = self.nb_model.classes_
        
        print("✅ 训练完成!")
        return self
    
    def predict(self, texts):
        """预测文本类别"""
        X = self.vectorizer.transform(texts)
        return self.nb_model.predict(X)
    
    def predict_proba(self, texts):
        """预测文本类别概率"""
        X = self.vectorizer.transform(texts)
        return self.nb_model.predict_proba(X)
    
    def analyze_features(self, top_n=10):
        """分析重要特征"""
        feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"\n🔍 各类别最重要的{top_n}个特征:")
        print("=" * 60)
        
        for i, class_name in enumerate(self.classes):
            # 获取该类别的特征权重
            feature_log_probs = self.nb_model.feature_log_prob_[i]
            
            # 获取top特征
            top_indices = np.argsort(feature_log_probs)[::-1][:top_n]
            
            print(f"\n📂 {class_name.upper()}:")
            print("-" * 30)
            
            for j, idx in enumerate(top_indices):
                feature_name = feature_names[idx]
                log_prob = feature_log_probs[idx]
                prob = np.exp(log_prob)
                
                print(f"{j+1:2d}. {feature_name:<15} (概率: {prob:.6f})")
    
    def classify_text(self, text):
        """分类单个文本并显示详细信息"""
        prediction = self.predict([text])[0]
        probabilities = self.predict_proba([text])[0]
        
        print(f"\n📄 文本: '{text}'")
        print(f"🎯 预测类别: {prediction}")
        print(f"📊 各类别概率:")
        
        for class_name, prob in zip(self.classes, probabilities):
            print(f"  {class_name:<12}: {prob:.4f} {'★' if class_name == prediction else ''}")
        
        return prediction, probabilities

# 演示文本分类
def demonstrate_text_classification():
    """演示文本分类"""
    
    # 创建文本分类器
    text_classifier = TextClassificationNB(alpha=1.0, use_tfidf=True)
    
    # 生成数据
    texts, labels = text_classifier.generate_text_data(900)
    
    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.3, random_state=42, stratify=labels
    )
    
    # 训练模型
    text_classifier.fit(X_train, y_train)
    
    # 预测和评估
    y_pred = text_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\n🎯 文本分类结果:")
    print(f"测试准确率: {accuracy:.4f}")
    
    # 详细分类报告
    print(f"\n📊 详细分类报告:")
    print(classification_report(y_test, y_pred))
    
    # 混淆矩阵
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=text_classifier.classes, 
                yticklabels=text_classifier.classes)
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.title(f'文本分类混淆矩阵\n准确率: {accuracy:.4f}')
    plt.show()
    
    # 分析重要特征
    text_classifier.analyze_features()
    
    # 测试新文本
    test_texts = [
        "football team won the championship game",
        "new AI algorithm improves machine learning performance",
        "president announced new government policy",
        "basketball player scored in the match",
        "computer software programming code"
    ]
    
    print(f"\n🧪 测试新文本分类:")
    for text in test_texts:
        text_classifier.classify_text(text)
    
    return text_classifier

text_model = demonstrate_text_classification()
```

### 垃圾邮件检测实战

```python
class SpamDetectorNB:
    """基于朴素贝叶斯的垃圾邮件检测器"""
    
    def __init__(self):
        self.vectorizer = None
        self.nb_model = None
        self.feature_names = None
        
    def generate_email_dataset(self, n_samples=2000):
        """生成更真实的邮件数据集"""
        np.random.seed(42)
        
        # 垃圾邮件模式
        spam_patterns = {
            'promotional': ['free', 'win', 'prize', 'offer', 'discount', 'sale', 'deal', 'limited time'],
            'urgent': ['urgent', 'act now', 'hurry', 'expires', 'deadline', 'immediate'],
            'money': ['money', 'cash', 'earn', 'income', 'profit', 'rich', 'wealthy'],
            'suspicious': ['click here', 'guarantee', 'no risk', 'amazing', 'incredible']
        }
        
        # 正常邮件模式
        normal_patterns = {
            'work': ['meeting', 'project', 'deadline', 'report', 'presentation', 'team', 'colleague'],
            'personal': ['family', 'friend', 'birthday', 'vacation', 'dinner', 'weekend'],
            'formal': ['regards', 'sincerely', 'thank you', 'please', 'kindly', 'appreciate'],
            'business': ['company', 'client', 'contract', 'proposal', 'budget', 'schedule']
        }
        
        emails = []
        labels = []
        
        # 生成垃圾邮件
        for _ in range(n_samples // 2):
            email_parts = []
            
            # 随机选择1-3个垃圾邮件模式
            n_patterns = np.random.randint(1, 4)
            selected_patterns = np.random.choice(list(spam_patterns.keys()), n_patterns, replace=False)
            
            for pattern in selected_patterns:
                n_words = np.random.randint(2, 5)
                words = np.random.choice(spam_patterns[pattern], n_words, replace=True)
                email_parts.extend(words)
            
            # 添加一些正常词汇作为噪声
            if np.random.random() < 0.3:
                normal_pattern = np.random.choice(list(normal_patterns.keys()))
                noise_words = np.random.choice(normal_patterns[normal_pattern], 2, replace=True)
                email_parts.extend(noise_words)
            
            email = ' '.join(email_parts)
            emails.append(email)
            labels.append('spam')
        
        # 生成正常邮件
        for _ in range(n_samples // 2):
            email_parts = []
            
            # 随机选择1-2个正常邮件模式
            n_patterns = np.random.randint(1, 3)
            selected_patterns = np.random.choice(list(normal_patterns.keys()), n_patterns, replace=False)
            
            for pattern in selected_patterns:
                n_words = np.random.randint(3, 7)
                words = np.random.choice(normal_patterns[pattern], n_words, replace=True)
                email_parts.extend(words)
            
            # 添加一些垃圾词汇作为噪声
            if np.random.random() < 0.1:
                spam_pattern = np.random.choice(list(spam_patterns.keys()))
                noise_words = np.random.choice(spam_patterns[spam_pattern], 1, replace=True)
                email_parts.extend(noise_words)
            
            email = ' '.join(email_parts)
            emails.append(email)
            labels.append('ham')
        
        return emails, labels
    
    def train(self, emails, labels):
        """训练垃圾邮件检测模型"""
        print(f"📧 训练垃圾邮件检测器...")
        print(f"邮件总数: {len(emails)}")
        print(f"标签分布: {dict(pd.Series(labels).value_counts())}")
        
        # 特征提取：使用TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=2000,
            stop_words='english',
            ngram_range=(1, 2),  # 1-2gram
            min_df=2,  # 至少出现2次
            max_df=0.95  # 最多出现在95%的文档中
        )
        
        X = self.vectorizer.fit_transform(emails)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"特征维度: {X.shape[1]}")
        
        # 训练多项式朴素贝叶斯
        self.nb_model = MultinomialNB(alpha=1.0)
        self.nb_model.fit(X, labels)
        
        print("✅ 训练完成!")
        return self
    
    def predict_email(self, email_text):
        """预测单封邮件"""
        X = self.vectorizer.transform([email_text])
        prediction = self.nb_model.predict(X)[0]
        probabilities = self.nb_model.predict_proba(X)[0]
        
        return prediction, probabilities
    
    def analyze_spam_indicators(self, top_n=15):
        """分析垃圾邮件指示词"""
        # 获取类别索引
        classes = self.nb_model.classes_
        spam_idx = list(classes).index('spam')
        ham_idx = list(classes).index('ham')
        
        # 计算特征的对数概率比
        spam_log_probs = self.nb_model.feature_log_prob_[spam_idx]
        ham_log_probs = self.nb_model.feature_log_prob_[ham_idx]
        
        # 计算对数概率差（spam - ham）
        log_prob_diff = spam_log_probs - ham_log_probs
        
        # 获取最强的垃圾邮件指示词
        spam_indicators = np.argsort(log_prob_diff)[::-1][:top_n]
        
        # 获取最强的正常邮件指示词
        ham_indicators = np.argsort(log_prob_diff)[:top_n]
        
        print(f"\n🚨 最强垃圾邮件指示词 (Top {top_n}):")
        print("-" * 50)
        for i, idx in enumerate(spam_indicators):
            word = self.feature_names[idx]
            score = log_prob_diff[idx]
            print(f"{i+1:2d}. {word:<20} (分数: {score:.4f})")
        
        print(f"\n✅ 最强正常邮件指示词 (Top {top_n}):")
        print("-" * 50)
        for i, idx in enumerate(ham_indicators):
            word = self.feature_names[idx]
            score = log_prob_diff[idx]
            print(f"{i+1:2d}. {word:<20} (分数: {score:.4f})")
    
    def detailed_prediction(self, email_text):
        """详细预测分析"""
        prediction, probabilities = self.predict_email(email_text)
        
        print(f"\n📧 邮件内容: '{email_text}'")
        print(f"🎯 预测结果: {'垃圾邮件' if prediction == 'spam' else '正常邮件'}")
        print(f"📊 概率分布:")
        
        classes = self.nb_model.classes_
        for class_name, prob in zip(classes, probabilities):
            emoji = "🚨" if class_name == 'spam' else "✅"
            star = "★" if class_name == prediction else " "
            print(f"  {emoji} {class_name:<8}: {prob:.4f} {star}")
        
        # 分析关键词贡献
        X = self.vectorizer.transform([email_text])
        feature_indices = X.nonzero()[1]
        
        if len(feature_indices) > 0:
            print(f"\n🔍 关键词分析:")
            
            spam_idx = list(classes).index('spam')
            ham_idx = list(classes).index('ham')
            
            word_contributions = []
            
            for idx in feature_indices:
                word = self.feature_names[idx]
                tfidf_score = X[0, idx]
                
                spam_log_prob = self.nb_model.feature_log_prob_[spam_idx, idx]
                ham_log_prob = self.nb_model.feature_log_prob_[ham_idx, idx]
                
                contribution = (spam_log_prob - ham_log_prob) * tfidf_score
                word_contributions.append((word, contribution, tfidf_score))
            
            # 按贡献度排序
            word_contributions.sort(key=lambda x: abs(x[1]), reverse=True)
            
            print("  词汇           贡献度    TF-IDF   类型")
            print("  " + "-" * 45)
            
            for word, contrib, tfidf in word_contributions[:8]:
                contrib_type = "垃圾" if contrib > 0 else "正常"
                print(f"  {word:<12} {contrib:>8.4f} {tfidf:>8.4f}   {contrib_type}")
        
        return prediction, probabilities

# 演示垃圾邮件检测
def demonstrate_spam_detection():
    """演示垃圾邮件检测"""
    
    # 创建垃圾邮件检测器
    spam_detector = SpamDetectorNB()
    
    # 生成数据
    emails, labels = spam_detector.generate_email_dataset(1600)
    
    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        emails, labels, test_size=0.25, random_state=42, stratify=labels
    )
    
    # 训练模型
    spam_detector.train(X_train, y_train)
    
    # 评估模型
    y_pred = []
    y_proba = []
    
    for email in X_test:
        pred, prob = spam_detector.predict_email(email)
        y_pred.append(pred)
        y_proba.append(prob[1] if spam_detector.nb_model.classes_[1] == 'spam' else prob[0])
    
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\n🎯 垃圾邮件检测结果:")
    print(f"测试准确率: {accuracy:.4f}")
    
    # 详细分类报告
    print(f"\n📊 详细分类报告:")
    print(classification_report(y_test, y_pred))
    
    # ROC曲线
    from sklearn.metrics import roc_curve, auc
    
    # 将标签转换为数值
    y_test_binary = [1 if label == 'spam' else 0 for label in y_test]
    
    fpr, tpr, _ = roc_curve(y_test_binary, y_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(12, 5))
    
    # ROC曲线
    plt.subplot(1, 2, 1)
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC曲线 (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('假正率 (FPR)')
    plt.ylabel('真正率 (TPR)')
    plt.title('ROC曲线')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # 混淆矩阵
    plt.subplot(1, 2, 2)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['正常', '垃圾'], yticklabels=['正常', '垃圾'])
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.title(f'混淆矩阵\n准确率: {accuracy:.4f}')
    
    plt.tight_layout()
    plt.show()
    
    # 分析垃圾邮件指示词
    spam_detector.analyze_spam_indicators()
    
    # 测试具体邮件
    test_emails = [
        "free money win prize click here urgent offer",
        "meeting tomorrow project deadline report",
        "amazing deal limited time act now guarantee",
        "thank you for the presentation regards team",
        "earn cash incredible profit no risk",
        "family dinner weekend birthday celebration"
    ]
    
    print(f"\n🧪 测试邮件分类:")
    for email in test_emails:
        spam_detector.detailed_prediction(email)
    
    return spam_detector

spam_model = demonstrate_spam_detection()
```

## 朴素贝叶斯变体对比

```python
def compare_naive_bayes_variants():
    """对比不同朴素贝叶斯变体"""
    
    print("\n🔬 朴素贝叶斯变体对比实验")
    print("=" * 50)
    
    # 准备不同类型的数据
    datasets = {}
    
    # 1. 连续特征数据（适合高斯朴素贝叶斯）
    X_continuous, y_continuous = make_classification(
        n_samples=1000, n_features=4, n_redundant=0, 
        n_informative=4, n_clusters_per_class=1, random_state=42
    )
    datasets['连续特征'] = (X_continuous, y_continuous)
    
    # 2. 离散计数数据（适合多项式朴素贝叶斯）
    np.random.seed(42)
    X_counts = np.random.poisson(3, size=(1000, 4))  # 泊松分布生成计数数据
    y_counts = (X_counts.sum(axis=1) > X_counts.mean()).astype(int)
    datasets['计数特征'] = (X_counts, y_counts)
    
    # 3. 二值特征数据（适合伯努利朴素贝叶斯）
    X_binary = np.random.binomial(1, 0.3, size=(1000, 4))  # 二项分布生成二值数据
    y_binary = (X_binary.sum(axis=1) > 2).astype(int)
    datasets['二值特征'] = (X_binary, y_binary)
    
    # 朴素贝叶斯变体
    models = {
        '高斯朴素贝叶斯': GaussianNB(),
        '多项式朴素贝叶斯': MultinomialNB(),
        '伯努利朴素贝叶斯': BernoulliNB()
    }
    
    results = []
    
    for data_name, (X, y) in datasets.items():
        print(f"\n📊 数据集: {data_name}")
        print(f"数据形状: {X.shape}, 类别分布: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # 划分数据
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        print(f"\n模型性能对比:")
        print("-" * 40)
        
        for model_name, model in models.items():
            try:
                # 训练和预测
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                
                print(f"{model_name:<15}: {accuracy:.4f}")
                
                results.append({
                    '数据类型': data_name,
                    '模型': model_name,
                    '准确率': accuracy
                })
                
            except Exception as e:
                print(f"{model_name:<15}: 错误 - {str(e)[:30]}...")
                results.append({
                    '数据类型': data_name,
                    '模型': model_name,
                    '准确率': 0.0
                })
    
    # 结果可视化
    results_df = pd.DataFrame(results)
    
    plt.figure(figsize=(12, 8))
    
    # 热力图
    pivot_table = results_df.pivot(index='模型', columns='数据类型', values='准确率')
    
    plt.subplot(2, 1, 1)
    sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='YlOrRd', 
                cbar_kws={'label': '准确率'})
    plt.title('朴素贝叶斯变体在不同数据类型上的性能')
    
    # 柱状图
    plt.subplot(2, 1, 2)
    x_pos = np.arange(len(datasets))
    width = 0.25
    
    for i, model_name in enumerate(models.keys()):
        model_results = results_df[results_df['模型'] == model_name]['准确率'].values
        plt.bar(x_pos + i * width, model_results, width, label=model_name)
    
    plt.xlabel('数据类型')
    plt.ylabel('准确率')
    plt.title('朴素贝叶斯变体性能对比')
    plt.xticks(x_pos + width, list(datasets.keys()))
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 总结建议
    print(f"\n💡 使用建议:")
    print("=" * 30)
    print("📈 高斯朴素贝叶斯: 适用于连续特征，假设特征服从高斯分布")
    print("📊 多项式朴素贝叶斯: 适用于计数特征，如文本中的词频")
    print("🔢 伯努利朴素贝叶斯: 适用于二值特征，如文档中词汇的出现与否")
    
    return results_df

comparison_results = compare_naive_bayes_variants()
```

## Trae实践环节

### 使用Trae构建朴素贝叶斯

```python
class TraeNaiveBayes:
    """Trae风格的朴素贝叶斯实现"""
    
    def __init__(self, variant='gaussian', alpha=1.0):
        """
        参数:
        variant: 'gaussian', 'multinomial', 'bernoulli'
        alpha: 拉普拉斯平滑参数
        """
        self.variant = variant
        self.alpha = alpha
        self.class_priors = {}
        self.feature_params = {}  # 存储特征参数
        self.classes = None
        self.n_features = None
        
    def trae_fit(self, X, y):
        """Trae风格的训练方法"""
        print(f"🚀 Trae朴素贝叶斯开始训练...")
        print(f"📊 变体类型: {self.variant.upper()}")
        
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        self.n_features = X.shape[1]
        n_samples = len(y)
        
        print(f"📈 数据信息:")
        print(f"  • 样本数量: {n_samples}")
        print(f"  • 特征维度: {self.n_features}")
        print(f"  • 类别数量: {len(self.classes)}")
        print(f"  • 类别分布: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # 计算先验概率
        print("🔧 计算先验概率...")
        for class_label in self.classes:
            class_count = np.sum(y == class_label)
            self.class_priors[class_label] = class_count / n_samples
        
        # 根据变体类型计算特征参数
        print(f"⚙️ 计算{self.variant}特征参数...")
        
        if self.variant == 'gaussian':
            self._fit_gaussian(X, y)
        elif self.variant == 'multinomial':
            self._fit_multinomial(X, y)
        elif self.variant == 'bernoulli':
            self._fit_bernoulli(X, y)
        else:
            raise ValueError(f"不支持的变体: {self.variant}")
        
        print("✅ 训练完成!")
        
        # 显示学习到的参数
        self._display_learned_parameters()
        
        return self
    
    def _fit_gaussian(self, X, y):
        """拟合高斯朴素贝叶斯"""
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            # 计算均值和方差
            means = np.mean(class_samples, axis=0)
            variances = np.var(class_samples, axis=0)
            
            # 避免方差为0
            variances = np.maximum(variances, 1e-9)
            
            self.feature_params[class_label] = {
                'means': means,
                'variances': variances
            }
    
    def _fit_multinomial(self, X, y):
        """拟合多项式朴素贝叶斯"""
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            # 计算特征计数
            feature_counts = np.sum(class_samples, axis=0)
            total_count = np.sum(feature_counts)
            
            # 拉普拉斯平滑
            smoothed_counts = feature_counts + self.alpha
            smoothed_total = total_count + self.alpha * self.n_features
            
            # 计算概率
            probabilities = smoothed_counts / smoothed_total
            
            self.feature_params[class_label] = {
                'probabilities': probabilities,
                'counts': feature_counts
            }
    
    def _fit_bernoulli(self, X, y):
        """拟合伯努利朴素贝叶斯"""
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            # 计算特征出现概率
            feature_counts = np.sum(class_samples, axis=0)
            n_class_samples = len(class_samples)
            
            # 拉普拉斯平滑
            smoothed_probs = (feature_counts + self.alpha) / (n_class_samples + 2 * self.alpha)
            
            self.feature_params[class_label] = {
                'probabilities': smoothed_probs,
                'counts': feature_counts
            }
    
    def _display_learned_parameters(self):
        """显示学习到的参数"""
        print(f"\n📋 学习到的参数:")
        print("-" * 40)
        
        # 先验概率
        print("🎯 先验概率:")
        for class_label, prior in self.class_priors.items():
            print(f"  P(类别={class_label}) = {prior:.4f}")
        
        # 特征参数（只显示前几个特征）
        print(f"\n🔍 特征参数示例 (前3个特征):")
        
        if self.variant == 'gaussian':
            for class_label in self.classes:
                print(f"  类别 {class_label}:")
                means = self.feature_params[class_label]['means'][:3]
                variances = self.feature_params[class_label]['variances'][:3]
                for i, (mean, var) in enumerate(zip(means, variances)):
                    print(f"    特征{i}: μ={mean:.4f}, σ²={var:.4f}")
        
        elif self.variant in ['multinomial', 'bernoulli']:
            for class_label in self.classes:
                print(f"  类别 {class_label}:")
                probs = self.feature_params[class_label]['probabilities'][:3]
                for i, prob in enumerate(probs):
                    print(f"    P(特征{i}=1|类别={class_label}) = {prob:.4f}")
    
    def trae_predict_proba(self, X):
        """Trae风格的概率预测"""
        print(f"🔮 预测 {len(X)} 个样本的概率...")
        
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # 先验概率
                log_prob = np.log(self.class_priors[class_label])
                
                # 似然概率
                if self.variant == 'gaussian':
                    log_prob += self._gaussian_log_likelihood(sample, class_label)
                elif self.variant == 'multinomial':
                    log_prob += self._multinomial_log_likelihood(sample, class_label)
                elif self.variant == 'bernoulli':
                    log_prob += self._bernoulli_log_likelihood(sample, class_label)
                
                probabilities[i, j] = log_prob
        
        # 转换回概率空间并归一化
        probabilities = np.exp(probabilities)
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / row_sums
        
        print("✅ 概率预测完成!")
        return probabilities
    
    def _gaussian_log_likelihood(self, sample, class_label):
        """计算高斯对数似然"""
        means = self.feature_params[class_label]['means']
        variances = self.feature_params[class_label]['variances']
        
        # 计算对数概率密度
        log_prob = -0.5 * np.sum(np.log(2 * np.pi * variances))
        log_prob -= 0.5 * np.sum((sample - means) ** 2 / variances)
        
        return log_prob
    
    def _multinomial_log_likelihood(self, sample, class_label):
        """计算多项式对数似然"""
        probs = self.feature_params[class_label]['probabilities']
        
        # 避免log(0)
        probs = np.maximum(probs, 1e-10)
        
        # 计算对数似然
        log_prob = np.sum(sample * np.log(probs))
        
        return log_prob
    
    def _bernoulli_log_likelihood(self, sample, class_label):
        """计算伯努利对数似然"""
        probs = self.feature_params[class_label]['probabilities']
        
        # 避免log(0)
        probs = np.maximum(probs, 1e-10)
        probs = np.minimum(probs, 1 - 1e-10)
        
        # 计算对数似然
        log_prob = np.sum(sample * np.log(probs) + (1 - sample) * np.log(1 - probs))
        
        return log_prob
    
    def trae_predict(self, X):
        """Trae风格的类别预测"""
        probabilities = self.trae_predict_proba(X)
        predictions = self.classes[np.argmax(probabilities, axis=1)]
        
        print(f"🎯 预测完成! 预测了 {len(predictions)} 个样本")
        return predictions
    
    def trae_evaluate(self, X, y):
        """Trae风格的模型评估"""
        print("📊 开始模型评估...")
        
        predictions = self.trae_predict(X)
        accuracy = accuracy_score(y, predictions)
        
        print(f"🎯 评估结果:")
        print(f"  • 准确率: {accuracy:.4f}")
        
        # 详细分类报告
        unique_classes = np.unique(y)
        if len(unique_classes) <= 5:  # 只在类别不太多时显示详细报告
            print(f"\n📋 详细分类报告:")
            print(classification_report(y, predictions))
        
        return accuracy
    
    def trae_analyze_sample(self, sample, top_features=5):
        """Trae风格的样本分析"""
        print(f"\n🔍 样本分析: {sample[:min(5, len(sample))]}...")
        
        probabilities = self.trae_predict_proba([sample])[0]
        prediction = self.classes[np.argmax(probabilities)]
        
        print(f"🎯 预测类别: {prediction}")
        print(f"📊 各类别概率:")
        
        for class_label, prob in zip(self.classes, probabilities):
            star = "★" if class_label == prediction else " "
            print(f"  {class_label}: {prob:.4f} {star}")
        
        # 分析最重要的特征
        if self.variant == 'gaussian':
            self._analyze_gaussian_features(sample, prediction, top_features)
        elif self.variant in ['multinomial', 'bernoulli']:
            self._analyze_discrete_features(sample, prediction, top_features)
        
        return prediction, probabilities
    
    def _analyze_gaussian_features(self, sample, prediction, top_features):
        """分析高斯特征贡献"""
        print(f"\n🔬 特征贡献分析 (Top {top_features}):")
        
        means = self.feature_params[prediction]['means']
        variances = self.feature_params[prediction]['variances']
        
        # 计算每个特征的标准化距离
        distances = np.abs(sample - means) / np.sqrt(variances)
        
        # 获取最重要的特征
        important_indices = np.argsort(distances)[::-1][:top_features]
        
        print("  特征   值      均值    方差    标准化距离")
        print("  " + "-" * 45)
        
        for idx in important_indices:
            value = sample[idx]
            mean = means[idx]
            var = variances[idx]
            dist = distances[idx]
            
            print(f"  {idx:<5} {value:<7.3f} {mean:<7.3f} {var:<7.3f} {dist:<7.3f}")
    
    def _analyze_discrete_features(self, sample, prediction, top_features):
        """分析离散特征贡献"""
        print(f"\n🔬 特征贡献分析 (Top {top_features}):")
        
        probs = self.feature_params[prediction]['probabilities']
        
        # 计算特征重要性（基于概率和特征值）
        if self.variant == 'multinomial':
            importance = sample * np.log(probs + 1e-10)
        else:  # bernoulli
            importance = sample * np.log(probs + 1e-10) + (1 - sample) * np.log(1 - probs + 1e-10)
        
        # 获取最重要的特征
        important_indices = np.argsort(np.abs(importance))[::-1][:top_features]
        
        print("  特征   值    概率    重要性")
        print("  " + "-" * 30)
        
        for idx in important_indices:
            value = sample[idx]
            prob = probs[idx]
            imp = importance[idx]
            
            print(f"  {idx:<5} {value:<5.0f} {prob:<7.3f} {imp:<7.3f}")

# Trae朴素贝叶斯演示
def demonstrate_trae_naive_bayes():
    """演示Trae朴素贝叶斯"""
    
    print("\n🌟 === Trae朴素贝叶斯演示 === 🌟")
    
    # 演示1: 高斯朴素贝叶斯
    print("\n📊 演示1: 高斯朴素贝叶斯")
    print("=" * 40)
    
    X_gaussian, y_gaussian = make_classification(
        n_samples=800, n_features=4, n_redundant=0, 
        n_informative=4, n_clusters_per_class=1, random_state=42
    )
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_gaussian, y_gaussian, test_size=0.3, random_state=42
    )
    
    # 创建和训练Trae高斯朴素贝叶斯
    trae_gaussian = TraeNaiveBayes(variant='gaussian')
    trae_gaussian.trae_fit(X_train, y_train)
    
    # 评估
    accuracy_gaussian = trae_gaussian.trae_evaluate(X_test, y_test)
    
    # 分析样本
    sample_idx = 0
    trae_gaussian.trae_analyze_sample(X_test[sample_idx])
    
    # 演示2: 多项式朴素贝叶斯
    print("\n\n📊 演示2: 多项式朴素贝叶斯")
    print("=" * 40)
    
    # 生成计数数据
    np.random.seed(42)
    X_counts = np.random.poisson(2, size=(800, 6))
    y_counts = (X_counts.sum(axis=1) > X_counts.mean()).astype(int)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_counts, y_counts, test_size=0.3, random_state=42
    )
    
    # 创建和训练Trae多项式朴素贝叶斯
    trae_multinomial = TraeNaiveBayes(variant='multinomial', alpha=1.0)
    trae_multinomial.trae_fit(X_train, y_train)
    
    # 评估
    accuracy_multinomial = trae_multinomial.trae_evaluate(X_test, y_test)
    
    # 分析样本
    trae_multinomial.trae_analyze_sample(X_test[0])
    
    # 演示3: 伯努利朴素贝叶斯
    print("\n\n📊 演示3: 伯努利朴素贝叶斯")
    print("=" * 40)
    
    # 生成二值数据
    np.random.seed(42)
    X_binary = np.random.binomial(1, 0.4, size=(800, 6))
    y_binary = (X_binary.sum(axis=1) > 3).astype(int)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_binary, y_binary, test_size=0.3, random_state=42
    )
    
    # 创建和训练Trae伯努利朴素贝叶斯
    trae_bernoulli = TraeNaiveBayes(variant='bernoulli', alpha=1.0)
    trae_bernoulli.trae_fit(X_train, y_train)
    
    # 评估
    accuracy_bernoulli = trae_bernoulli.trae_evaluate(X_test, y_test)
    
    # 分析样本
    trae_bernoulli.trae_analyze_sample(X_test[0])
    
    # 性能总结
    print(f"\n\n🏆 Trae朴素贝叶斯性能总结:")
    print("=" * 50)
    print(f"📈 高斯朴素贝叶斯准确率:     {accuracy_gaussian:.4f}")
    print(f"📊 多项式朴素贝叶斯准确率:   {accuracy_multinomial:.4f}")
    print(f"🔢 伯努利朴素贝叶斯准确率:   {accuracy_bernoulli:.4f}")
    
    return trae_gaussian, trae_multinomial, trae_bernoulli

trae_models = demonstrate_trae_naive_bayes()
```

## 实际应用技巧

### 处理数据不平衡

```python
def handle_imbalanced_data():
    """处理不平衡数据的朴素贝叶斯技巧"""
    
    print("⚖️ 处理不平衡数据演示")
    print("=" * 30)
    
    # 生成不平衡数据
    from sklearn.datasets import make_classification
    
    X, y = make_classification(
        n_samples=1000, n_features=4, n_redundant=0,
        n_informative=4, n_clusters_per_class=1,
        weights=[0.9, 0.1], random_state=42  # 90% vs 10%
    )
    
    print(f"📊 原始数据分布: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 方法1: 调整类别权重
    from sklearn.utils.class_weight import compute_class_weight
    
    class_weights = compute_class_weight(
        'balanced', classes=np.unique(y_train), y=y_train
    )
    
    print(f"\n🎯 计算的类别权重: {dict(zip(np.unique(y_train), class_weights))}")
    
    # 方法2: 使用不同的评估指标
    from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
    
    # 标准朴素贝叶斯
    nb_standard = GaussianNB()
    nb_standard.fit(X_train, y_train)
    
    y_pred_standard = nb_standard.predict(X_test)
    y_proba_standard = nb_standard.predict_proba(X_test)[:, 1]
    
    # 调整决策阈值的朴素贝叶斯
    def predict_with_threshold(model, X, threshold=0.5):
        probabilities = model.predict_proba(X)[:, 1]
        return (probabilities >= threshold).astype(int)
    
    # 寻找最佳阈值
    thresholds = np.arange(0.1, 0.9, 0.1)
    best_threshold = 0.5
    best_f1 = 0
    
    for threshold in thresholds:
        y_pred_thresh = predict_with_threshold(nb_standard, X_test, threshold)
        _, _, f1, _ = precision_recall_fscore_support(y_test, y_pred_thresh, average='binary')
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"\n🎯 最佳决策阈值: {best_threshold:.2f} (F1分数: {best_f1:.4f})")
    
    # 使用最佳阈值预测
    y_pred_optimized = predict_with_threshold(nb_standard, X_test, best_threshold)
    
    # 评估结果对比
    from sklearn.metrics import classification_report
    
    print(f"\n📊 标准阈值(0.5)结果:")
    print(classification_report(y_test, y_pred_standard))
    
    print(f"\n📊 优化阈值({best_threshold:.2f})结果:")
    print(classification_report(y_test, y_pred_optimized))
    
    # ROC-AUC对比
    auc_standard = roc_auc_score(y_test, y_proba_standard)
    print(f"\n🎯 ROC-AUC分数: {auc_standard:.4f}")
    
    return nb_standard, best_threshold

nb_imbalanced, optimal_threshold = handle_imbalanced_data()
```

### 特征选择和降维

```python
def feature_selection_naive_bayes():
    """朴素贝叶斯的特征选择技巧"""
    
    print("\n🔍 朴素贝叶斯特征选择演示")
    print("=" * 35)
    
    # 生成高维数据
    X, y = make_classification(
        n_samples=1000, n_features=50, n_informative=10,
        n_redundant=10, n_clusters_per_class=1, random_state=42
    )
    
    print(f"📊 原始数据: {X.shape[0]} 样本, {X.shape[1]} 特征")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # 基准模型（使用所有特征）
    nb_baseline = GaussianNB()
    nb_baseline.fit(X_train, y_train)
    baseline_accuracy = nb_baseline.score(X_test, y_test)
    
    print(f"\n🎯 基准准确率 (所有特征): {baseline_accuracy:.4f}")
    
    # 方法1: 基于方差的特征选择
    from sklearn.feature_selection import VarianceThreshold
    
    variance_selector = VarianceThreshold(threshold=0.1)
    X_train_var = variance_selector.fit_transform(X_train)
    X_test_var = variance_selector.transform(X_test)
    
    nb_variance = GaussianNB()
    nb_variance.fit(X_train_var, y_train)
    variance_accuracy = nb_variance.score(X_test_var, y_test)
    
    print(f"📉 方差选择后: {X_train_var.shape[1]} 特征, 准确率: {variance_accuracy:.4f}")
    
    # 方法2: 基于卡方检验的特征选择
    from sklearn.feature_selection import SelectKBest, chi2
    from sklearn.preprocessing import MinMaxScaler
    
    # 标准化到非负值（卡方检验要求）
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    chi2_selector = SelectKBest(chi2, k=20)
    X_train_chi2 = chi2_selector.fit_transform(X_train_scaled, y_train)
    X_test_chi2 = chi2_selector.transform(X_test_scaled)
    
    nb_chi2 = GaussianNB()
    nb_chi2.fit(X_train_chi2, y_train)
    chi2_accuracy = nb_chi2.score(X_test_chi2, y_test)
    
    print(f"🔬 卡方选择后: {X_train_chi2.shape[1]} 特征, 准确率: {chi2_accuracy:.4f}")
    
    # 方法3: 基于互信息的特征选择
    from sklearn.feature_selection import mutual_info_classif
    
    mi_scores = mutual_info_classif(X_train, y_train, random_state=42)
    
    # 选择互信息最高的特征
    k_best_mi = 15
    mi_indices = np.argsort(mi_scores)[::-1][:k_best_mi]
    
    X_train_mi = X_train[:, mi_indices]
    X_test_mi = X_test[:, mi_indices]
    
    nb_mi = GaussianNB()
    nb_mi.fit(X_train_mi, y_train)
    mi_accuracy = nb_mi.score(X_test_mi, y_test)
    
    print(f"🧠 互信息选择后: {X_train_mi.shape[1]} 特征, 准确率: {mi_accuracy:.4f}")
    
    # 方法4: 递归特征消除
    from sklearn.feature_selection import RFE
    
    rfe_selector = RFE(estimator=GaussianNB(), n_features_to_select=12)
    X_train_rfe = rfe_selector.fit_transform(X_train, y_train)
    X_test_rfe = rfe_selector.transform(X_test)
    
    nb_rfe = GaussianNB()
    nb_rfe.fit(X_train_rfe, y_train)
    rfe_accuracy = nb_rfe.score(X_test_rfe, y_test)
    
    print(f"🔄 RFE选择后: {X_train_rfe.shape[1]} 特征, 准确率: {rfe_accuracy:.4f}")
    
    # 结果可视化
    methods = ['基准', '方差选择', '卡方选择', '互信息选择', 'RFE选择']
    accuracies = [baseline_accuracy, variance_accuracy, chi2_accuracy, mi_accuracy, rfe_accuracy]
    feature_counts = [X.shape[1], X_train_var.shape[1], X_train_chi2.shape[1], 
                     X_train_mi.shape[1], X_train_rfe.shape[1]]
    
    plt.figure(figsize=(12, 5))
    
    # 准确率对比
    plt.subplot(1, 2, 1)
    bars = plt.bar(methods, accuracies, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
    plt.ylabel('准确率')
    plt.title('不同特征选择方法的准确率对比')
    plt.xticks(rotation=45)
    
    # 在柱状图上显示数值
    for bar, acc in zip(bars, accuracies):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                f'{acc:.3f}', ha='center', va='bottom')
    
    # 特征数量对比
    plt.subplot(1, 2, 2)
    bars2 = plt.bar(methods, feature_counts, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
    plt.ylabel('特征数量')
    plt.title('不同方法选择的特征数量')
    plt.xticks(rotation=45)
    
    # 在柱状图上显示数值
    for bar, count in zip(bars2, feature_counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                f'{count}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # 总结
    print(f"\n💡 特征选择总结:")
    print("=" * 25)
    best_method_idx = np.argmax(accuracies)
    best_method = methods[best_method_idx]
    best_accuracy = accuracies[best_method_idx]
    best_features = feature_counts[best_method_idx]
    
    print(f"🏆 最佳方法: {best_method}")
    print(f"📈 最佳准确率: {best_accuracy:.4f}")
    print(f"🔢 使用特征数: {best_features}")
    print(f"📉 特征减少: {(X.shape[1] - best_features) / X.shape[1] * 100:.1f}%")
    
    return nb_baseline, (nb_variance, nb_chi2, nb_mi, nb_rfe)

baseline_model, selected_models = feature_selection_naive_bayes()
```

## 思考题

1. **条件独立假设**：朴素贝叶斯的"朴素"假设在什么情况下会严重影响性能？如何检验这个假设？

2. **拉普拉斯平滑**：为什么需要拉普拉斯平滑？平滑参数α如何影响模型性能？

3. **变体选择**：在什么情况下应该选择高斯、多项式或伯努利朴素贝叶斯？

4. **零概率问题**：除了拉普拉斯平滑，还有哪些方法可以处理零概率问题？

5. **朴素贝叶斯 vs 其他算法**：朴素贝叶斯相比逻辑回归和SVM有什么优缺点？

## 本节小结

朴素贝叶斯是一种基于概率论的简单而强大的分类算法，具有以下特点：

### 核心优势
- **简单高效**：算法简单，训练和预测速度快
- **理论基础**：基于贝叶斯定理，有坚实的概率论基础
- **处理多分类**：天然支持多分类问题
- **小样本友好**：在小数据集上表现良好
- **特征独立**：不需要特征之间的相关性假设

### 关键技术
- **条件独立假设**：简化计算但可能影响性能
- **拉普拉斯平滑**：解决零概率问题
- **三种变体**：高斯、多项式、伯努利适用于不同数据类型

### 实际应用
- **文本分类**：垃圾邮件检测、情感分析、新闻分类
- **推荐系统**：基于用户行为的推荐
- **医疗诊断**：基于症状的疾病预测
- **实时分类**：需要快速响应的在线分类任务

### 使用建议
- **数据预处理**：根据变体类型准备合适的数据格式
- **特征选择**：移除冗余和不相关特征
- **参数调优**：调整平滑参数和决策阈值
- **性能评估**：使用适合的评估指标，特别是不平衡数据

### 局限性
- **独立性假设**：现实中特征往往相关
- **数值稳定性**：需要处理概率计算中的数值问题
- **连续特征**：高斯假设可能不适合所有连续特征

### 下一步学习
- **贝叶斯网络**：放松独立性假设的贝叶斯方法
- **集成方法**：将朴素贝叶斯与其他算法结合
- **深度学习**：了解神经网络在文本分类中的应用

朴素贝叶斯虽然"朴素"，但在许多实际应用中表现出色，特别是在文本分类和需要快速训练的场景中。它为我们展示了概率论在机器学习中的强大应用。
```