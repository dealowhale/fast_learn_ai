# 1.2.5 æœ´ç´ è´å¶æ–¯ç®—æ³•

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å°†æŒæ¡ï¼š
- æœ´ç´ è´å¶æ–¯ç®—æ³•çš„æ•°å­¦åŸç†å’Œè´å¶æ–¯å®šç†
- ä¸‰ç§ä¸»è¦çš„æœ´ç´ è´å¶æ–¯å˜ä½“ï¼šé«˜æ–¯ã€å¤šé¡¹å¼å’Œä¼¯åŠªåˆ©
- æœ´ç´ è´å¶æ–¯åœ¨æ–‡æœ¬åˆ†ç±»å’Œåƒåœ¾é‚®ä»¶æ£€æµ‹ä¸­çš„åº”ç”¨
- æ¡ä»¶ç‹¬ç«‹å‡è®¾çš„å«ä¹‰å’Œå½±å“
- æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æŠ€æœ¯å¤„ç†é›¶æ¦‚ç‡é—®é¢˜

## ç®—æ³•åŸç†

### è´å¶æ–¯å®šç†åŸºç¡€

æœ´ç´ è´å¶æ–¯ç®—æ³•åŸºäºè´å¶æ–¯å®šç†ï¼Œè¿™æ˜¯æ¦‚ç‡è®ºä¸­çš„ä¸€ä¸ªåŸºæœ¬å®šç†ï¼š

$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬è¦è®¡ç®—ç»™å®šç‰¹å¾ $X$ çš„æƒ…å†µä¸‹ï¼Œæ ·æœ¬å±äºç±»åˆ« $C_k$ çš„æ¦‚ç‡ï¼š

$$P(C_k|X) = \frac{P(X|C_k) \cdot P(C_k)}{P(X)}$$

å…¶ä¸­ï¼š
- $P(C_k|X)$ï¼šåéªŒæ¦‚ç‡ï¼ˆæˆ‘ä»¬è¦æ±‚çš„ï¼‰
- $P(X|C_k)$ï¼šä¼¼ç„¶æ¦‚ç‡
- $P(C_k)$ï¼šå…ˆéªŒæ¦‚ç‡
- $P(X)$ï¼šè¾¹ç¼˜æ¦‚ç‡ï¼ˆå½’ä¸€åŒ–å¸¸æ•°ï¼‰

### æœ´ç´ å‡è®¾

"æœ´ç´ "å‡è®¾æŒ‡çš„æ˜¯**æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼šç»™å®šç±»åˆ«çš„æƒ…å†µä¸‹ï¼Œå„ä¸ªç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚

å¯¹äºç‰¹å¾å‘é‡ $X = (x_1, x_2, ..., x_n)$ï¼š

$$P(X|C_k) = P(x_1, x_2, ..., x_n|C_k) = \prod_{i=1}^{n} P(x_i|C_k)$$

å› æ­¤ï¼Œåˆ†ç±»å†³ç­–å˜ä¸ºï¼š

$$\hat{y} = \arg\max_{k} P(C_k) \prod_{i=1}^{n} P(x_i|C_k)$$

## ç®—æ³•å®ç°

### åŸºç¡€æœ´ç´ è´å¶æ–¯å®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification, fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import defaultdict
import pandas as pd

class SimpleNaiveBayes:
    """ç®€å•çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨å®ç°"""
    
    def __init__(self, alpha=1.0):
        """
        å‚æ•°:
        alpha: æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•°
        """
        self.alpha = alpha
        self.class_priors = {}  # å…ˆéªŒæ¦‚ç‡ P(C_k)
        self.feature_probs = {}  # æ¡ä»¶æ¦‚ç‡ P(x_i|C_k)
        self.classes = None
        self.n_features = None
        
    def fit(self, X, y):
        """è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨"""
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        self.n_features = X.shape[1]
        n_samples = len(y)
        
        print(f"ğŸ¤– è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨...")
        print(f"æ ·æœ¬æ•°: {n_samples}, ç‰¹å¾æ•°: {self.n_features}, ç±»åˆ«æ•°: {len(self.classes)}")
        
        # è®¡ç®—å…ˆéªŒæ¦‚ç‡ P(C_k)
        for class_label in self.classes:
            class_count = np.sum(y == class_label)
            self.class_priors[class_label] = class_count / n_samples
            
        # è®¡ç®—æ¡ä»¶æ¦‚ç‡ P(x_i|C_k)
        self.feature_probs = {}
        
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            self.feature_probs[class_label] = {}
            
            for feature_idx in range(self.n_features):
                feature_values = class_samples[:, feature_idx]
                unique_values = np.unique(X[:, feature_idx])  # æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å€¼
                
                # è®¡ç®—æ¯ä¸ªç‰¹å¾å€¼çš„æ¡ä»¶æ¦‚ç‡ï¼ˆå¸¦æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰
                feature_prob_dict = {}
                for value in unique_values:
                    count = np.sum(feature_values == value)
                    # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
                    prob = (count + self.alpha) / (len(class_samples) + self.alpha * len(unique_values))
                    feature_prob_dict[value] = prob
                    
                self.feature_probs[class_label][feature_idx] = feature_prob_dict
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # è®¡ç®— P(C_k) * âˆP(x_i|C_k)
                prob = self.class_priors[class_label]
                
                for feature_idx, feature_value in enumerate(sample):
                    if feature_value in self.feature_probs[class_label][feature_idx]:
                        prob *= self.feature_probs[class_label][feature_idx][feature_value]
                    else:
                        # æœªè§è¿‡çš„ç‰¹å¾å€¼ï¼Œä½¿ç”¨å¹³æ»‘æ¦‚ç‡
                        prob *= self.alpha / (sum(self.class_priors.values()) + self.alpha * len(self.classes))
                
                probabilities[i, j] = prob
        
        # å½’ä¸€åŒ–
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / row_sums
        
        return probabilities
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
    
    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        predictions = self.predict(X)
        return accuracy_score(y, predictions)

# æ¼”ç¤ºåŸºç¡€æœ´ç´ è´å¶æ–¯
def demonstrate_basic_naive_bayes():
    """æ¼”ç¤ºåŸºç¡€æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨"""
    
    # ç”Ÿæˆç¦»æ•£ç‰¹å¾æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    
    # åˆ›å»ºç¦»æ•£ç‰¹å¾æ•°æ®
    X = np.random.randint(0, 3, size=(n_samples, 4))  # 4ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾æœ‰0,1,2ä¸‰ä¸ªå€¼
    
    # åˆ›å»ºæ ‡ç­¾ï¼ˆåŸºäºç‰¹å¾çš„ç®€å•è§„åˆ™ï¼‰
    y = ((X[:, 0] + X[:, 1]) > (X[:, 2] + X[:, 3])).astype(int)
    
    print("ğŸ“Š ç¦»æ•£ç‰¹å¾æœ´ç´ è´å¶æ–¯æ¼”ç¤º")
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # è®­ç»ƒè‡ªå®ç°çš„æœ´ç´ è´å¶æ–¯
    nb_custom = SimpleNaiveBayes(alpha=1.0)
    nb_custom.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred_custom = nb_custom.predict(X_test)
    accuracy_custom = nb_custom.score(X_test, y_test)
    
    print(f"\nğŸ¯ è‡ªå®ç°æœ´ç´ è´å¶æ–¯ç»“æœ:")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy_custom:.4f}")
    
    # æ˜¾ç¤ºå…ˆéªŒæ¦‚ç‡
    print(f"\nğŸ“ˆ å…ˆéªŒæ¦‚ç‡:")
    for class_label, prior in nb_custom.class_priors.items():
        print(f"  P(ç±»åˆ«={class_label}) = {prior:.4f}")
    
    # æ˜¾ç¤ºéƒ¨åˆ†æ¡ä»¶æ¦‚ç‡
    print(f"\nğŸ” æ¡ä»¶æ¦‚ç‡ç¤ºä¾‹ (ç‰¹å¾0):")
    for class_label in nb_custom.classes:
        print(f"  ç±»åˆ« {class_label}:")
        for value, prob in nb_custom.feature_probs[class_label][0].items():
            print(f"    P(ç‰¹å¾0={value}|ç±»åˆ«={class_label}) = {prob:.4f}")
    
    return nb_custom, X_test, y_test, y_pred_custom

nb_model, X_test, y_test, y_pred = demonstrate_basic_naive_bayes()
```

### é«˜æ–¯æœ´ç´ è´å¶æ–¯

```python
class GaussianNaiveBayes:
    """é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼ˆé€‚ç”¨äºè¿ç»­ç‰¹å¾ï¼‰"""
    
    def __init__(self):
        self.class_priors = {}
        self.feature_means = {}  # æ¯ä¸ªç±»åˆ«æ¯ä¸ªç‰¹å¾çš„å‡å€¼
        self.feature_vars = {}   # æ¯ä¸ªç±»åˆ«æ¯ä¸ªç‰¹å¾çš„æ–¹å·®
        self.classes = None
        
    def fit(self, X, y):
        """è®­ç»ƒé«˜æ–¯æœ´ç´ è´å¶æ–¯"""
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        n_samples = len(y)
        
        print(f"ğŸ¤– è®­ç»ƒé«˜æ–¯æœ´ç´ è´å¶æ–¯...")
        
        for class_label in self.classes:
            # å…ˆéªŒæ¦‚ç‡
            class_mask = (y == class_label)
            self.class_priors[class_label] = np.sum(class_mask) / n_samples
            
            # è¯¥ç±»åˆ«çš„æ ·æœ¬
            class_samples = X[class_mask]
            
            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®
            self.feature_means[class_label] = np.mean(class_samples, axis=0)
            self.feature_vars[class_label] = np.var(class_samples, axis=0)
            
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self
    
    def _gaussian_pdf(self, x, mean, var):
        """è®¡ç®—é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°"""
        # é¿å…é™¤é›¶
        var = np.maximum(var, 1e-9)
        coeff = 1.0 / np.sqrt(2 * np.pi * var)
        exponent = -0.5 * ((x - mean) ** 2) / var
        return coeff * np.exp(exponent)
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # å…ˆéªŒæ¦‚ç‡
                prob = self.class_priors[class_label]
                
                # ä¼¼ç„¶æ¦‚ç‡ï¼ˆå„ç‰¹å¾æ¦‚ç‡çš„ä¹˜ç§¯ï¼‰
                mean = self.feature_means[class_label]
                var = self.feature_vars[class_label]
                
                likelihood = np.prod(self._gaussian_pdf(sample, mean, var))
                prob *= likelihood
                
                probabilities[i, j] = prob
        
        # å½’ä¸€åŒ–
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / (row_sums + 1e-10)
        
        return probabilities
    
    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
    
    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        predictions = self.predict(X)
        return accuracy_score(y, predictions)

# æ¼”ç¤ºé«˜æ–¯æœ´ç´ è´å¶æ–¯
def demonstrate_gaussian_naive_bayes():
    """æ¼”ç¤ºé«˜æ–¯æœ´ç´ è´å¶æ–¯"""
    
    # ç”Ÿæˆè¿ç»­ç‰¹å¾æ•°æ®
    X, y = make_classification(
        n_samples=1000, n_features=2, n_redundant=0, 
        n_informative=2, n_clusters_per_class=1, random_state=42
    )
    
    print("\nğŸ“Š é«˜æ–¯æœ´ç´ è´å¶æ–¯æ¼”ç¤º")
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # è‡ªå®ç°çš„é«˜æ–¯æœ´ç´ è´å¶æ–¯
    gnb_custom = GaussianNaiveBayes()
    gnb_custom.fit(X_train, y_train)
    
    # sklearnçš„é«˜æ–¯æœ´ç´ è´å¶æ–¯
    gnb_sklearn = GaussianNB()
    gnb_sklearn.fit(X_train, y_train)
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred_custom = gnb_custom.predict(X_test)
    y_pred_sklearn = gnb_sklearn.predict(X_test)
    
    accuracy_custom = gnb_custom.score(X_test, y_test)
    accuracy_sklearn = gnb_sklearn.score(X_test, y_test)
    
    print(f"\nğŸ¯ ç»“æœå¯¹æ¯”:")
    print(f"è‡ªå®ç°å‡†ç¡®ç‡: {accuracy_custom:.4f}")
    print(f"sklearnå‡†ç¡®ç‡: {accuracy_sklearn:.4f}")
    
    # å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
    plt.figure(figsize=(15, 5))
    
    # åŸå§‹æ•°æ®
    plt.subplot(1, 3, 1)
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    plt.title('åŸå§‹æ•°æ®åˆ†å¸ƒ')
    plt.colorbar(scatter)
    plt.grid(True, alpha=0.3)
    
    # è‡ªå®ç°æ¨¡å‹å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 3, 2)
    plot_decision_boundary(gnb_custom, X, y, "è‡ªå®ç°é«˜æ–¯æœ´ç´ è´å¶æ–¯")
    
    # sklearnæ¨¡å‹å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 3, 3)
    plot_decision_boundary(gnb_sklearn, X, y, "sklearné«˜æ–¯æœ´ç´ è´å¶æ–¯")
    
    plt.tight_layout()
    plt.show()
    
    # æ˜¾ç¤ºå­¦ä¹ åˆ°çš„å‚æ•°
    print(f"\nğŸ“ˆ å­¦ä¹ åˆ°çš„å‚æ•°:")
    for class_label in gnb_custom.classes:
        print(f"ç±»åˆ« {class_label}:")
        print(f"  å…ˆéªŒæ¦‚ç‡: {gnb_custom.class_priors[class_label]:.4f}")
        print(f"  ç‰¹å¾å‡å€¼: {gnb_custom.feature_means[class_label]}")
        print(f"  ç‰¹å¾æ–¹å·®: {gnb_custom.feature_vars[class_label]}")
    
    return gnb_custom, gnb_sklearn

def plot_decision_boundary(model, X, y, title):
    """ç»˜åˆ¶å†³ç­–è¾¹ç•Œ"""
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black')
    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)

gnb_custom, gnb_sklearn = demonstrate_gaussian_naive_bayes()
```

## æ–‡æœ¬åˆ†ç±»åº”ç”¨

### å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ç”¨äºæ–‡æœ¬åˆ†ç±»

```python
class TextClassificationNB:
    """åŸºäºæœ´ç´ è´å¶æ–¯çš„æ–‡æœ¬åˆ†ç±»ç³»ç»Ÿ"""
    
    def __init__(self, alpha=1.0, use_tfidf=False):
        self.alpha = alpha
        self.use_tfidf = use_tfidf
        self.vectorizer = None
        self.nb_model = None
        self.classes = None
        
    def generate_text_data(self, n_samples=1000):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æœ¬æ•°æ®"""
        np.random.seed(42)
        
        # å®šä¹‰ä¸åŒç±»åˆ«çš„å…³é”®è¯
        categories = {
            'sports': ['football', 'basketball', 'soccer', 'game', 'team', 'player', 'score', 'match', 'win', 'championship'],
            'technology': ['computer', 'software', 'internet', 'data', 'algorithm', 'programming', 'AI', 'machine learning', 'code', 'digital'],
            'politics': ['government', 'election', 'policy', 'president', 'congress', 'vote', 'democracy', 'law', 'political', 'campaign']
        }
        
        texts = []
        labels = []
        
        for category, keywords in categories.items():
            for _ in range(n_samples // len(categories)):
                # éšæœºé€‰æ‹©3-8ä¸ªå…³é”®è¯
                n_words = np.random.randint(3, 9)
                selected_words = np.random.choice(keywords, n_words, replace=True)
                
                # æ·»åŠ ä¸€äº›å™ªå£°è¯
                noise_words = ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of']
                n_noise = np.random.randint(1, 4)
                noise = np.random.choice(noise_words, n_noise, replace=True)
                
                # ç»„åˆæˆæ–‡æœ¬
                all_words = np.concatenate([selected_words, noise])
                np.random.shuffle(all_words)
                text = ' '.join(all_words)
                
                texts.append(text)
                labels.append(category)
        
        return texts, labels
    
    def fit(self, texts, labels):
        """è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹"""
        print(f"ğŸ“š å¼€å§‹è®­ç»ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹...")
        print(f"æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(pd.Series(labels).value_counts())}")
        
        # æ–‡æœ¬å‘é‡åŒ–
        if self.use_tfidf:
            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        else:
            self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')
        
        X = self.vectorizer.fit_transform(texts)
        
        print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        
        # è®­ç»ƒæœ´ç´ è´å¶æ–¯æ¨¡å‹
        self.nb_model = MultinomialNB(alpha=self.alpha)
        self.nb_model.fit(X, labels)
        
        self.classes = self.nb_model.classes_
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self
    
    def predict(self, texts):
        """é¢„æµ‹æ–‡æœ¬ç±»åˆ«"""
        X = self.vectorizer.transform(texts)
        return self.nb_model.predict(X)
    
    def predict_proba(self, texts):
        """é¢„æµ‹æ–‡æœ¬ç±»åˆ«æ¦‚ç‡"""
        X = self.vectorizer.transform(texts)
        return self.nb_model.predict_proba(X)
    
    def analyze_features(self, top_n=10):
        """åˆ†æé‡è¦ç‰¹å¾"""
        feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"\nğŸ” å„ç±»åˆ«æœ€é‡è¦çš„{top_n}ä¸ªç‰¹å¾:")
        print("=" * 60)
        
        for i, class_name in enumerate(self.classes):
            # è·å–è¯¥ç±»åˆ«çš„ç‰¹å¾æƒé‡
            feature_log_probs = self.nb_model.feature_log_prob_[i]
            
            # è·å–topç‰¹å¾
            top_indices = np.argsort(feature_log_probs)[::-1][:top_n]
            
            print(f"\nğŸ“‚ {class_name.upper()}:")
            print("-" * 30)
            
            for j, idx in enumerate(top_indices):
                feature_name = feature_names[idx]
                log_prob = feature_log_probs[idx]
                prob = np.exp(log_prob)
                
                print(f"{j+1:2d}. {feature_name:<15} (æ¦‚ç‡: {prob:.6f})")
    
    def classify_text(self, text):
        """åˆ†ç±»å•ä¸ªæ–‡æœ¬å¹¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"""
        prediction = self.predict([text])[0]
        probabilities = self.predict_proba([text])[0]
        
        print(f"\nğŸ“„ æ–‡æœ¬: '{text}'")
        print(f"ğŸ¯ é¢„æµ‹ç±»åˆ«: {prediction}")
        print(f"ğŸ“Š å„ç±»åˆ«æ¦‚ç‡:")
        
        for class_name, prob in zip(self.classes, probabilities):
            print(f"  {class_name:<12}: {prob:.4f} {'â˜…' if class_name == prediction else ''}")
        
        return prediction, probabilities

# æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»
def demonstrate_text_classification():
    """æ¼”ç¤ºæ–‡æœ¬åˆ†ç±»"""
    
    # åˆ›å»ºæ–‡æœ¬åˆ†ç±»å™¨
    text_classifier = TextClassificationNB(alpha=1.0, use_tfidf=True)
    
    # ç”Ÿæˆæ•°æ®
    texts, labels = text_classifier.generate_text_data(900)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.3, random_state=42, stratify=labels
    )
    
    # è®­ç»ƒæ¨¡å‹
    text_classifier.fit(X_train, y_train)
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred = text_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\nğŸ¯ æ–‡æœ¬åˆ†ç±»ç»“æœ:")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f"\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=text_classifier.classes, 
                yticklabels=text_classifier.classes)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title(f'æ–‡æœ¬åˆ†ç±»æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {accuracy:.4f}')
    plt.show()
    
    # åˆ†æé‡è¦ç‰¹å¾
    text_classifier.analyze_features()
    
    # æµ‹è¯•æ–°æ–‡æœ¬
    test_texts = [
        "football team won the championship game",
        "new AI algorithm improves machine learning performance",
        "president announced new government policy",
        "basketball player scored in the match",
        "computer software programming code"
    ]
    
    print(f"\nğŸ§ª æµ‹è¯•æ–°æ–‡æœ¬åˆ†ç±»:")
    for text in test_texts:
        text_classifier.classify_text(text)
    
    return text_classifier

text_model = demonstrate_text_classification()
```

### åƒåœ¾é‚®ä»¶æ£€æµ‹å®æˆ˜

```python
class SpamDetectorNB:
    """åŸºäºæœ´ç´ è´å¶æ–¯çš„åƒåœ¾é‚®ä»¶æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.vectorizer = None
        self.nb_model = None
        self.feature_names = None
        
    def generate_email_dataset(self, n_samples=2000):
        """ç”Ÿæˆæ›´çœŸå®çš„é‚®ä»¶æ•°æ®é›†"""
        np.random.seed(42)
        
        # åƒåœ¾é‚®ä»¶æ¨¡å¼
        spam_patterns = {
            'promotional': ['free', 'win', 'prize', 'offer', 'discount', 'sale', 'deal', 'limited time'],
            'urgent': ['urgent', 'act now', 'hurry', 'expires', 'deadline', 'immediate'],
            'money': ['money', 'cash', 'earn', 'income', 'profit', 'rich', 'wealthy'],
            'suspicious': ['click here', 'guarantee', 'no risk', 'amazing', 'incredible']
        }
        
        # æ­£å¸¸é‚®ä»¶æ¨¡å¼
        normal_patterns = {
            'work': ['meeting', 'project', 'deadline', 'report', 'presentation', 'team', 'colleague'],
            'personal': ['family', 'friend', 'birthday', 'vacation', 'dinner', 'weekend'],
            'formal': ['regards', 'sincerely', 'thank you', 'please', 'kindly', 'appreciate'],
            'business': ['company', 'client', 'contract', 'proposal', 'budget', 'schedule']
        }
        
        emails = []
        labels = []
        
        # ç”Ÿæˆåƒåœ¾é‚®ä»¶
        for _ in range(n_samples // 2):
            email_parts = []
            
            # éšæœºé€‰æ‹©1-3ä¸ªåƒåœ¾é‚®ä»¶æ¨¡å¼
            n_patterns = np.random.randint(1, 4)
            selected_patterns = np.random.choice(list(spam_patterns.keys()), n_patterns, replace=False)
            
            for pattern in selected_patterns:
                n_words = np.random.randint(2, 5)
                words = np.random.choice(spam_patterns[pattern], n_words, replace=True)
                email_parts.extend(words)
            
            # æ·»åŠ ä¸€äº›æ­£å¸¸è¯æ±‡ä½œä¸ºå™ªå£°
            if np.random.random() < 0.3:
                normal_pattern = np.random.choice(list(normal_patterns.keys()))
                noise_words = np.random.choice(normal_patterns[normal_pattern], 2, replace=True)
                email_parts.extend(noise_words)
            
            email = ' '.join(email_parts)
            emails.append(email)
            labels.append('spam')
        
        # ç”Ÿæˆæ­£å¸¸é‚®ä»¶
        for _ in range(n_samples // 2):
            email_parts = []
            
            # éšæœºé€‰æ‹©1-2ä¸ªæ­£å¸¸é‚®ä»¶æ¨¡å¼
            n_patterns = np.random.randint(1, 3)
            selected_patterns = np.random.choice(list(normal_patterns.keys()), n_patterns, replace=False)
            
            for pattern in selected_patterns:
                n_words = np.random.randint(3, 7)
                words = np.random.choice(normal_patterns[pattern], n_words, replace=True)
                email_parts.extend(words)
            
            # æ·»åŠ ä¸€äº›åƒåœ¾è¯æ±‡ä½œä¸ºå™ªå£°
            if np.random.random() < 0.1:
                spam_pattern = np.random.choice(list(spam_patterns.keys()))
                noise_words = np.random.choice(spam_patterns[spam_pattern], 1, replace=True)
                email_parts.extend(noise_words)
            
            email = ' '.join(email_parts)
            emails.append(email)
            labels.append('ham')
        
        return emails, labels
    
    def train(self, emails, labels):
        """è®­ç»ƒåƒåœ¾é‚®ä»¶æ£€æµ‹æ¨¡å‹"""
        print(f"ğŸ“§ è®­ç»ƒåƒåœ¾é‚®ä»¶æ£€æµ‹å™¨...")
        print(f"é‚®ä»¶æ€»æ•°: {len(emails)}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(pd.Series(labels).value_counts())}")
        
        # ç‰¹å¾æå–ï¼šä½¿ç”¨TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=2000,
            stop_words='english',
            ngram_range=(1, 2),  # 1-2gram
            min_df=2,  # è‡³å°‘å‡ºç°2æ¬¡
            max_df=0.95  # æœ€å¤šå‡ºç°åœ¨95%çš„æ–‡æ¡£ä¸­
        )
        
        X = self.vectorizer.fit_transform(emails)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        
        # è®­ç»ƒå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯
        self.nb_model = MultinomialNB(alpha=1.0)
        self.nb_model.fit(X, labels)
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self
    
    def predict_email(self, email_text):
        """é¢„æµ‹å•å°é‚®ä»¶"""
        X = self.vectorizer.transform([email_text])
        prediction = self.nb_model.predict(X)[0]
        probabilities = self.nb_model.predict_proba(X)[0]
        
        return prediction, probabilities
    
    def analyze_spam_indicators(self, top_n=15):
        """åˆ†æåƒåœ¾é‚®ä»¶æŒ‡ç¤ºè¯"""
        # è·å–ç±»åˆ«ç´¢å¼•
        classes = self.nb_model.classes_
        spam_idx = list(classes).index('spam')
        ham_idx = list(classes).index('ham')
        
        # è®¡ç®—ç‰¹å¾çš„å¯¹æ•°æ¦‚ç‡æ¯”
        spam_log_probs = self.nb_model.feature_log_prob_[spam_idx]
        ham_log_probs = self.nb_model.feature_log_prob_[ham_idx]
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡å·®ï¼ˆspam - hamï¼‰
        log_prob_diff = spam_log_probs - ham_log_probs
        
        # è·å–æœ€å¼ºçš„åƒåœ¾é‚®ä»¶æŒ‡ç¤ºè¯
        spam_indicators = np.argsort(log_prob_diff)[::-1][:top_n]
        
        # è·å–æœ€å¼ºçš„æ­£å¸¸é‚®ä»¶æŒ‡ç¤ºè¯
        ham_indicators = np.argsort(log_prob_diff)[:top_n]
        
        print(f"\nğŸš¨ æœ€å¼ºåƒåœ¾é‚®ä»¶æŒ‡ç¤ºè¯ (Top {top_n}):")
        print("-" * 50)
        for i, idx in enumerate(spam_indicators):
            word = self.feature_names[idx]
            score = log_prob_diff[idx]
            print(f"{i+1:2d}. {word:<20} (åˆ†æ•°: {score:.4f})")
        
        print(f"\nâœ… æœ€å¼ºæ­£å¸¸é‚®ä»¶æŒ‡ç¤ºè¯ (Top {top_n}):")
        print("-" * 50)
        for i, idx in enumerate(ham_indicators):
            word = self.feature_names[idx]
            score = log_prob_diff[idx]
            print(f"{i+1:2d}. {word:<20} (åˆ†æ•°: {score:.4f})")
    
    def detailed_prediction(self, email_text):
        """è¯¦ç»†é¢„æµ‹åˆ†æ"""
        prediction, probabilities = self.predict_email(email_text)
        
        print(f"\nğŸ“§ é‚®ä»¶å†…å®¹: '{email_text}'")
        print(f"ğŸ¯ é¢„æµ‹ç»“æœ: {'åƒåœ¾é‚®ä»¶' if prediction == 'spam' else 'æ­£å¸¸é‚®ä»¶'}")
        print(f"ğŸ“Š æ¦‚ç‡åˆ†å¸ƒ:")
        
        classes = self.nb_model.classes_
        for class_name, prob in zip(classes, probabilities):
            emoji = "ğŸš¨" if class_name == 'spam' else "âœ…"
            star = "â˜…" if class_name == prediction else " "
            print(f"  {emoji} {class_name:<8}: {prob:.4f} {star}")
        
        # åˆ†æå…³é”®è¯è´¡çŒ®
        X = self.vectorizer.transform([email_text])
        feature_indices = X.nonzero()[1]
        
        if len(feature_indices) > 0:
            print(f"\nğŸ” å…³é”®è¯åˆ†æ:")
            
            spam_idx = list(classes).index('spam')
            ham_idx = list(classes).index('ham')
            
            word_contributions = []
            
            for idx in feature_indices:
                word = self.feature_names[idx]
                tfidf_score = X[0, idx]
                
                spam_log_prob = self.nb_model.feature_log_prob_[spam_idx, idx]
                ham_log_prob = self.nb_model.feature_log_prob_[ham_idx, idx]
                
                contribution = (spam_log_prob - ham_log_prob) * tfidf_score
                word_contributions.append((word, contribution, tfidf_score))
            
            # æŒ‰è´¡çŒ®åº¦æ’åº
            word_contributions.sort(key=lambda x: abs(x[1]), reverse=True)
            
            print("  è¯æ±‡           è´¡çŒ®åº¦    TF-IDF   ç±»å‹")
            print("  " + "-" * 45)
            
            for word, contrib, tfidf in word_contributions[:8]:
                contrib_type = "åƒåœ¾" if contrib > 0 else "æ­£å¸¸"
                print(f"  {word:<12} {contrib:>8.4f} {tfidf:>8.4f}   {contrib_type}")
        
        return prediction, probabilities

# æ¼”ç¤ºåƒåœ¾é‚®ä»¶æ£€æµ‹
def demonstrate_spam_detection():
    """æ¼”ç¤ºåƒåœ¾é‚®ä»¶æ£€æµ‹"""
    
    # åˆ›å»ºåƒåœ¾é‚®ä»¶æ£€æµ‹å™¨
    spam_detector = SpamDetectorNB()
    
    # ç”Ÿæˆæ•°æ®
    emails, labels = spam_detector.generate_email_dataset(1600)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        emails, labels, test_size=0.25, random_state=42, stratify=labels
    )
    
    # è®­ç»ƒæ¨¡å‹
    spam_detector.train(X_train, y_train)
    
    # è¯„ä¼°æ¨¡å‹
    y_pred = []
    y_proba = []
    
    for email in X_test:
        pred, prob = spam_detector.predict_email(email)
        y_pred.append(pred)
        y_proba.append(prob[1] if spam_detector.nb_model.classes_[1] == 'spam' else prob[0])
    
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\nğŸ¯ åƒåœ¾é‚®ä»¶æ£€æµ‹ç»“æœ:")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f"\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    # ROCæ›²çº¿
    from sklearn.metrics import roc_curve, auc
    
    # å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼
    y_test_binary = [1 if label == 'spam' else 0 for label in y_test]
    
    fpr, tpr, _ = roc_curve(y_test_binary, y_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(12, 5))
    
    # ROCæ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROCæ›²çº¿ (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡æ­£ç‡ (FPR)')
    plt.ylabel('çœŸæ­£ç‡ (TPR)')
    plt.title('ROCæ›²çº¿')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # æ··æ·†çŸ©é˜µ
    plt.subplot(1, 2, 2)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['æ­£å¸¸', 'åƒåœ¾'], yticklabels=['æ­£å¸¸', 'åƒåœ¾'])
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title(f'æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {accuracy:.4f}')
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†æåƒåœ¾é‚®ä»¶æŒ‡ç¤ºè¯
    spam_detector.analyze_spam_indicators()
    
    # æµ‹è¯•å…·ä½“é‚®ä»¶
    test_emails = [
        "free money win prize click here urgent offer",
        "meeting tomorrow project deadline report",
        "amazing deal limited time act now guarantee",
        "thank you for the presentation regards team",
        "earn cash incredible profit no risk",
        "family dinner weekend birthday celebration"
    ]
    
    print(f"\nğŸ§ª æµ‹è¯•é‚®ä»¶åˆ†ç±»:")
    for email in test_emails:
        spam_detector.detailed_prediction(email)
    
    return spam_detector

spam_model = demonstrate_spam_detection()
```

## æœ´ç´ è´å¶æ–¯å˜ä½“å¯¹æ¯”

```python
def compare_naive_bayes_variants():
    """å¯¹æ¯”ä¸åŒæœ´ç´ è´å¶æ–¯å˜ä½“"""
    
    print("\nğŸ”¬ æœ´ç´ è´å¶æ–¯å˜ä½“å¯¹æ¯”å®éªŒ")
    print("=" * 50)
    
    # å‡†å¤‡ä¸åŒç±»å‹çš„æ•°æ®
    datasets = {}
    
    # 1. è¿ç»­ç‰¹å¾æ•°æ®ï¼ˆé€‚åˆé«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼‰
    X_continuous, y_continuous = make_classification(
        n_samples=1000, n_features=4, n_redundant=0, 
        n_informative=4, n_clusters_per_class=1, random_state=42
    )
    datasets['è¿ç»­ç‰¹å¾'] = (X_continuous, y_continuous)
    
    # 2. ç¦»æ•£è®¡æ•°æ•°æ®ï¼ˆé€‚åˆå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ï¼‰
    np.random.seed(42)
    X_counts = np.random.poisson(3, size=(1000, 4))  # æ³Šæ¾åˆ†å¸ƒç”Ÿæˆè®¡æ•°æ•°æ®
    y_counts = (X_counts.sum(axis=1) > X_counts.mean()).astype(int)
    datasets['è®¡æ•°ç‰¹å¾'] = (X_counts, y_counts)
    
    # 3. äºŒå€¼ç‰¹å¾æ•°æ®ï¼ˆé€‚åˆä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼‰
    X_binary = np.random.binomial(1, 0.3, size=(1000, 4))  # äºŒé¡¹åˆ†å¸ƒç”ŸæˆäºŒå€¼æ•°æ®
    y_binary = (X_binary.sum(axis=1) > 2).astype(int)
    datasets['äºŒå€¼ç‰¹å¾'] = (X_binary, y_binary)
    
    # æœ´ç´ è´å¶æ–¯å˜ä½“
    models = {
        'é«˜æ–¯æœ´ç´ è´å¶æ–¯': GaussianNB(),
        'å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯': MultinomialNB(),
        'ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯': BernoulliNB()
    }
    
    results = []
    
    for data_name, (X, y) in datasets.items():
        print(f"\nğŸ“Š æ•°æ®é›†: {data_name}")
        print(f"æ•°æ®å½¢çŠ¶: {X.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # åˆ’åˆ†æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        print(f"\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
        print("-" * 40)
        
        for model_name, model in models.items():
            try:
                # è®­ç»ƒå’Œé¢„æµ‹
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                
                print(f"{model_name:<15}: {accuracy:.4f}")
                
                results.append({
                    'æ•°æ®ç±»å‹': data_name,
                    'æ¨¡å‹': model_name,
                    'å‡†ç¡®ç‡': accuracy
                })
                
            except Exception as e:
                print(f"{model_name:<15}: é”™è¯¯ - {str(e)[:30]}...")
                results.append({
                    'æ•°æ®ç±»å‹': data_name,
                    'æ¨¡å‹': model_name,
                    'å‡†ç¡®ç‡': 0.0
                })
    
    # ç»“æœå¯è§†åŒ–
    results_df = pd.DataFrame(results)
    
    plt.figure(figsize=(12, 8))
    
    # çƒ­åŠ›å›¾
    pivot_table = results_df.pivot(index='æ¨¡å‹', columns='æ•°æ®ç±»å‹', values='å‡†ç¡®ç‡')
    
    plt.subplot(2, 1, 1)
    sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='YlOrRd', 
                cbar_kws={'label': 'å‡†ç¡®ç‡'})
    plt.title('æœ´ç´ è´å¶æ–¯å˜ä½“åœ¨ä¸åŒæ•°æ®ç±»å‹ä¸Šçš„æ€§èƒ½')
    
    # æŸ±çŠ¶å›¾
    plt.subplot(2, 1, 2)
    x_pos = np.arange(len(datasets))
    width = 0.25
    
    for i, model_name in enumerate(models.keys()):
        model_results = results_df[results_df['æ¨¡å‹'] == model_name]['å‡†ç¡®ç‡'].values
        plt.bar(x_pos + i * width, model_results, width, label=model_name)
    
    plt.xlabel('æ•°æ®ç±»å‹')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('æœ´ç´ è´å¶æ–¯å˜ä½“æ€§èƒ½å¯¹æ¯”')
    plt.xticks(x_pos + width, list(datasets.keys()))
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ€»ç»“å»ºè®®
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("=" * 30)
    print("ğŸ“ˆ é«˜æ–¯æœ´ç´ è´å¶æ–¯: é€‚ç”¨äºè¿ç»­ç‰¹å¾ï¼Œå‡è®¾ç‰¹å¾æœä»é«˜æ–¯åˆ†å¸ƒ")
    print("ğŸ“Š å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯: é€‚ç”¨äºè®¡æ•°ç‰¹å¾ï¼Œå¦‚æ–‡æœ¬ä¸­çš„è¯é¢‘")
    print("ğŸ”¢ ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯: é€‚ç”¨äºäºŒå€¼ç‰¹å¾ï¼Œå¦‚æ–‡æ¡£ä¸­è¯æ±‡çš„å‡ºç°ä¸å¦")
    
    return results_df

comparison_results = compare_naive_bayes_variants()
```

## Traeå®è·µç¯èŠ‚

### ä½¿ç”¨Traeæ„å»ºæœ´ç´ è´å¶æ–¯

```python
class TraeNaiveBayes:
    """Traeé£æ ¼çš„æœ´ç´ è´å¶æ–¯å®ç°"""
    
    def __init__(self, variant='gaussian', alpha=1.0):
        """
        å‚æ•°:
        variant: 'gaussian', 'multinomial', 'bernoulli'
        alpha: æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•°
        """
        self.variant = variant
        self.alpha = alpha
        self.class_priors = {}
        self.feature_params = {}  # å­˜å‚¨ç‰¹å¾å‚æ•°
        self.classes = None
        self.n_features = None
        
    def trae_fit(self, X, y):
        """Traeé£æ ¼çš„è®­ç»ƒæ–¹æ³•"""
        print(f"ğŸš€ Traeæœ´ç´ è´å¶æ–¯å¼€å§‹è®­ç»ƒ...")
        print(f"ğŸ“Š å˜ä½“ç±»å‹: {self.variant.upper()}")
        
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        self.n_features = X.shape[1]
        n_samples = len(y)
        
        print(f"ğŸ“ˆ æ•°æ®ä¿¡æ¯:")
        print(f"  â€¢ æ ·æœ¬æ•°é‡: {n_samples}")
        print(f"  â€¢ ç‰¹å¾ç»´åº¦: {self.n_features}")
        print(f"  â€¢ ç±»åˆ«æ•°é‡: {len(self.classes)}")
        print(f"  â€¢ ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # è®¡ç®—å…ˆéªŒæ¦‚ç‡
        print("ğŸ”§ è®¡ç®—å…ˆéªŒæ¦‚ç‡...")
        for class_label in self.classes:
            class_count = np.sum(y == class_label)
            self.class_priors[class_label] = class_count / n_samples
        
        # æ ¹æ®å˜ä½“ç±»å‹è®¡ç®—ç‰¹å¾å‚æ•°
        print(f"âš™ï¸ è®¡ç®—{self.variant}ç‰¹å¾å‚æ•°...")
        
        if self.variant == 'gaussian':
            self._fit_gaussian(X, y)
        elif self.variant == 'multinomial':
            self._fit_multinomial(X, y)
        elif self.variant == 'bernoulli':
            self._fit_bernoulli(X, y)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å˜ä½“: {self.variant}")
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        
        # æ˜¾ç¤ºå­¦ä¹ åˆ°çš„å‚æ•°
        self._display_learned_parameters()
        
        return self
    
    def _fit_gaussian(self, X, y):
        """æ‹Ÿåˆé«˜æ–¯æœ´ç´ è´å¶æ–¯"""
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            # è®¡ç®—å‡å€¼å’Œæ–¹å·®
            means = np.mean(class_samples, axis=0)
            variances = np.var(class_samples, axis=0)
            
            # é¿å…æ–¹å·®ä¸º0
            variances = np.maximum(variances, 1e-9)
            
            self.feature_params[class_label] = {
                'means': means,
                'variances': variances
            }
    
    def _fit_multinomial(self, X, y):
        """æ‹Ÿåˆå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯"""
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            # è®¡ç®—ç‰¹å¾è®¡æ•°
            feature_counts = np.sum(class_samples, axis=0)
            total_count = np.sum(feature_counts)
            
            # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
            smoothed_counts = feature_counts + self.alpha
            smoothed_total = total_count + self.alpha * self.n_features
            
            # è®¡ç®—æ¦‚ç‡
            probabilities = smoothed_counts / smoothed_total
            
            self.feature_params[class_label] = {
                'probabilities': probabilities,
                'counts': feature_counts
            }
    
    def _fit_bernoulli(self, X, y):
        """æ‹Ÿåˆä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯"""
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            # è®¡ç®—ç‰¹å¾å‡ºç°æ¦‚ç‡
            feature_counts = np.sum(class_samples, axis=0)
            n_class_samples = len(class_samples)
            
            # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
            smoothed_probs = (feature_counts + self.alpha) / (n_class_samples + 2 * self.alpha)
            
            self.feature_params[class_label] = {
                'probabilities': smoothed_probs,
                'counts': feature_counts
            }
    
    def _display_learned_parameters(self):
        """æ˜¾ç¤ºå­¦ä¹ åˆ°çš„å‚æ•°"""
        print(f"\nğŸ“‹ å­¦ä¹ åˆ°çš„å‚æ•°:")
        print("-" * 40)
        
        # å…ˆéªŒæ¦‚ç‡
        print("ğŸ¯ å…ˆéªŒæ¦‚ç‡:")
        for class_label, prior in self.class_priors.items():
            print(f"  P(ç±»åˆ«={class_label}) = {prior:.4f}")
        
        # ç‰¹å¾å‚æ•°ï¼ˆåªæ˜¾ç¤ºå‰å‡ ä¸ªç‰¹å¾ï¼‰
        print(f"\nğŸ” ç‰¹å¾å‚æ•°ç¤ºä¾‹ (å‰3ä¸ªç‰¹å¾):")
        
        if self.variant == 'gaussian':
            for class_label in self.classes:
                print(f"  ç±»åˆ« {class_label}:")
                means = self.feature_params[class_label]['means'][:3]
                variances = self.feature_params[class_label]['variances'][:3]
                for i, (mean, var) in enumerate(zip(means, variances)):
                    print(f"    ç‰¹å¾{i}: Î¼={mean:.4f}, ÏƒÂ²={var:.4f}")
        
        elif self.variant in ['multinomial', 'bernoulli']:
            for class_label in self.classes:
                print(f"  ç±»åˆ« {class_label}:")
                probs = self.feature_params[class_label]['probabilities'][:3]
                for i, prob in enumerate(probs):
                    print(f"    P(ç‰¹å¾{i}=1|ç±»åˆ«={class_label}) = {prob:.4f}")
    
    def trae_predict_proba(self, X):
        """Traeé£æ ¼çš„æ¦‚ç‡é¢„æµ‹"""
        print(f"ğŸ”® é¢„æµ‹ {len(X)} ä¸ªæ ·æœ¬çš„æ¦‚ç‡...")
        
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # å…ˆéªŒæ¦‚ç‡
                log_prob = np.log(self.class_priors[class_label])
                
                # ä¼¼ç„¶æ¦‚ç‡
                if self.variant == 'gaussian':
                    log_prob += self._gaussian_log_likelihood(sample, class_label)
                elif self.variant == 'multinomial':
                    log_prob += self._multinomial_log_likelihood(sample, class_label)
                elif self.variant == 'bernoulli':
                    log_prob += self._bernoulli_log_likelihood(sample, class_label)
                
                probabilities[i, j] = log_prob
        
        # è½¬æ¢å›æ¦‚ç‡ç©ºé—´å¹¶å½’ä¸€åŒ–
        probabilities = np.exp(probabilities)
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / row_sums
        
        print("âœ… æ¦‚ç‡é¢„æµ‹å®Œæˆ!")
        return probabilities
    
    def _gaussian_log_likelihood(self, sample, class_label):
        """è®¡ç®—é«˜æ–¯å¯¹æ•°ä¼¼ç„¶"""
        means = self.feature_params[class_label]['means']
        variances = self.feature_params[class_label]['variances']
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡å¯†åº¦
        log_prob = -0.5 * np.sum(np.log(2 * np.pi * variances))
        log_prob -= 0.5 * np.sum((sample - means) ** 2 / variances)
        
        return log_prob
    
    def _multinomial_log_likelihood(self, sample, class_label):
        """è®¡ç®—å¤šé¡¹å¼å¯¹æ•°ä¼¼ç„¶"""
        probs = self.feature_params[class_label]['probabilities']
        
        # é¿å…log(0)
        probs = np.maximum(probs, 1e-10)
        
        # è®¡ç®—å¯¹æ•°ä¼¼ç„¶
        log_prob = np.sum(sample * np.log(probs))
        
        return log_prob
    
    def _bernoulli_log_likelihood(self, sample, class_label):
        """è®¡ç®—ä¼¯åŠªåˆ©å¯¹æ•°ä¼¼ç„¶"""
        probs = self.feature_params[class_label]['probabilities']
        
        # é¿å…log(0)
        probs = np.maximum(probs, 1e-10)
        probs = np.minimum(probs, 1 - 1e-10)
        
        # è®¡ç®—å¯¹æ•°ä¼¼ç„¶
        log_prob = np.sum(sample * np.log(probs) + (1 - sample) * np.log(1 - probs))
        
        return log_prob
    
    def trae_predict(self, X):
        """Traeé£æ ¼çš„ç±»åˆ«é¢„æµ‹"""
        probabilities = self.trae_predict_proba(X)
        predictions = self.classes[np.argmax(probabilities, axis=1)]
        
        print(f"ğŸ¯ é¢„æµ‹å®Œæˆ! é¢„æµ‹äº† {len(predictions)} ä¸ªæ ·æœ¬")
        return predictions
    
    def trae_evaluate(self, X, y):
        """Traeé£æ ¼çš„æ¨¡å‹è¯„ä¼°"""
        print("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        predictions = self.trae_predict(X)
        accuracy = accuracy_score(y, predictions)
        
        print(f"ğŸ¯ è¯„ä¼°ç»“æœ:")
        print(f"  â€¢ å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        unique_classes = np.unique(y)
        if len(unique_classes) <= 5:  # åªåœ¨ç±»åˆ«ä¸å¤ªå¤šæ—¶æ˜¾ç¤ºè¯¦ç»†æŠ¥å‘Š
            print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y, predictions))
        
        return accuracy
    
    def trae_analyze_sample(self, sample, top_features=5):
        """Traeé£æ ¼çš„æ ·æœ¬åˆ†æ"""
        print(f"\nğŸ” æ ·æœ¬åˆ†æ: {sample[:min(5, len(sample))]}...")
        
        probabilities = self.trae_predict_proba([sample])[0]
        prediction = self.classes[np.argmax(probabilities)]
        
        print(f"ğŸ¯ é¢„æµ‹ç±»åˆ«: {prediction}")
        print(f"ğŸ“Š å„ç±»åˆ«æ¦‚ç‡:")
        
        for class_label, prob in zip(self.classes, probabilities):
            star = "â˜…" if class_label == prediction else " "
            print(f"  {class_label}: {prob:.4f} {star}")
        
        # åˆ†ææœ€é‡è¦çš„ç‰¹å¾
        if self.variant == 'gaussian':
            self._analyze_gaussian_features(sample, prediction, top_features)
        elif self.variant in ['multinomial', 'bernoulli']:
            self._analyze_discrete_features(sample, prediction, top_features)
        
        return prediction, probabilities
    
    def _analyze_gaussian_features(self, sample, prediction, top_features):
        """åˆ†æé«˜æ–¯ç‰¹å¾è´¡çŒ®"""
        print(f"\nğŸ”¬ ç‰¹å¾è´¡çŒ®åˆ†æ (Top {top_features}):")
        
        means = self.feature_params[prediction]['means']
        variances = self.feature_params[prediction]['variances']
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†åŒ–è·ç¦»
        distances = np.abs(sample - means) / np.sqrt(variances)
        
        # è·å–æœ€é‡è¦çš„ç‰¹å¾
        important_indices = np.argsort(distances)[::-1][:top_features]
        
        print("  ç‰¹å¾   å€¼      å‡å€¼    æ–¹å·®    æ ‡å‡†åŒ–è·ç¦»")
        print("  " + "-" * 45)
        
        for idx in important_indices:
            value = sample[idx]
            mean = means[idx]
            var = variances[idx]
            dist = distances[idx]
            
            print(f"  {idx:<5} {value:<7.3f} {mean:<7.3f} {var:<7.3f} {dist:<7.3f}")
    
    def _analyze_discrete_features(self, sample, prediction, top_features):
        """åˆ†æç¦»æ•£ç‰¹å¾è´¡çŒ®"""
        print(f"\nğŸ”¬ ç‰¹å¾è´¡çŒ®åˆ†æ (Top {top_features}):")
        
        probs = self.feature_params[prediction]['probabilities']
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºæ¦‚ç‡å’Œç‰¹å¾å€¼ï¼‰
        if self.variant == 'multinomial':
            importance = sample * np.log(probs + 1e-10)
        else:  # bernoulli
            importance = sample * np.log(probs + 1e-10) + (1 - sample) * np.log(1 - probs + 1e-10)
        
        # è·å–æœ€é‡è¦çš„ç‰¹å¾
        important_indices = np.argsort(np.abs(importance))[::-1][:top_features]
        
        print("  ç‰¹å¾   å€¼    æ¦‚ç‡    é‡è¦æ€§")
        print("  " + "-" * 30)
        
        for idx in important_indices:
            value = sample[idx]
            prob = probs[idx]
            imp = importance[idx]
            
            print(f"  {idx:<5} {value:<5.0f} {prob:<7.3f} {imp:<7.3f}")

# Traeæœ´ç´ è´å¶æ–¯æ¼”ç¤º
def demonstrate_trae_naive_bayes():
    """æ¼”ç¤ºTraeæœ´ç´ è´å¶æ–¯"""
    
    print("\nğŸŒŸ === Traeæœ´ç´ è´å¶æ–¯æ¼”ç¤º === ğŸŒŸ")
    
    # æ¼”ç¤º1: é«˜æ–¯æœ´ç´ è´å¶æ–¯
    print("\nğŸ“Š æ¼”ç¤º1: é«˜æ–¯æœ´ç´ è´å¶æ–¯")
    print("=" * 40)
    
    X_gaussian, y_gaussian = make_classification(
        n_samples=800, n_features=4, n_redundant=0, 
        n_informative=4, n_clusters_per_class=1, random_state=42
    )
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_gaussian, y_gaussian, test_size=0.3, random_state=42
    )
    
    # åˆ›å»ºå’Œè®­ç»ƒTraeé«˜æ–¯æœ´ç´ è´å¶æ–¯
    trae_gaussian = TraeNaiveBayes(variant='gaussian')
    trae_gaussian.trae_fit(X_train, y_train)
    
    # è¯„ä¼°
    accuracy_gaussian = trae_gaussian.trae_evaluate(X_test, y_test)
    
    # åˆ†ææ ·æœ¬
    sample_idx = 0
    trae_gaussian.trae_analyze_sample(X_test[sample_idx])
    
    # æ¼”ç¤º2: å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯
    print("\n\nğŸ“Š æ¼”ç¤º2: å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯")
    print("=" * 40)
    
    # ç”Ÿæˆè®¡æ•°æ•°æ®
    np.random.seed(42)
    X_counts = np.random.poisson(2, size=(800, 6))
    y_counts = (X_counts.sum(axis=1) > X_counts.mean()).astype(int)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_counts, y_counts, test_size=0.3, random_state=42
    )
    
    # åˆ›å»ºå’Œè®­ç»ƒTraeå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯
    trae_multinomial = TraeNaiveBayes(variant='multinomial', alpha=1.0)
    trae_multinomial.trae_fit(X_train, y_train)
    
    # è¯„ä¼°
    accuracy_multinomial = trae_multinomial.trae_evaluate(X_test, y_test)
    
    # åˆ†ææ ·æœ¬
    trae_multinomial.trae_analyze_sample(X_test[0])
    
    # æ¼”ç¤º3: ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯
    print("\n\nğŸ“Š æ¼”ç¤º3: ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯")
    print("=" * 40)
    
    # ç”ŸæˆäºŒå€¼æ•°æ®
    np.random.seed(42)
    X_binary = np.random.binomial(1, 0.4, size=(800, 6))
    y_binary = (X_binary.sum(axis=1) > 3).astype(int)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_binary, y_binary, test_size=0.3, random_state=42
    )
    
    # åˆ›å»ºå’Œè®­ç»ƒTraeä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯
    trae_bernoulli = TraeNaiveBayes(variant='bernoulli', alpha=1.0)
    trae_bernoulli.trae_fit(X_train, y_train)
    
    # è¯„ä¼°
    accuracy_bernoulli = trae_bernoulli.trae_evaluate(X_test, y_test)
    
    # åˆ†ææ ·æœ¬
    trae_bernoulli.trae_analyze_sample(X_test[0])
    
    # æ€§èƒ½æ€»ç»“
    print(f"\n\nğŸ† Traeæœ´ç´ è´å¶æ–¯æ€§èƒ½æ€»ç»“:")
    print("=" * 50)
    print(f"ğŸ“ˆ é«˜æ–¯æœ´ç´ è´å¶æ–¯å‡†ç¡®ç‡:     {accuracy_gaussian:.4f}")
    print(f"ğŸ“Š å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯å‡†ç¡®ç‡:   {accuracy_multinomial:.4f}")
    print(f"ğŸ”¢ ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯å‡†ç¡®ç‡:   {accuracy_bernoulli:.4f}")
    
    return trae_gaussian, trae_multinomial, trae_bernoulli

trae_models = demonstrate_trae_naive_bayes()
```

## å®é™…åº”ç”¨æŠ€å·§

### å¤„ç†æ•°æ®ä¸å¹³è¡¡

```python
def handle_imbalanced_data():
    """å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„æœ´ç´ è´å¶æ–¯æŠ€å·§"""
    
    print("âš–ï¸ å¤„ç†ä¸å¹³è¡¡æ•°æ®æ¼”ç¤º")
    print("=" * 30)
    
    # ç”Ÿæˆä¸å¹³è¡¡æ•°æ®
    from sklearn.datasets import make_classification
    
    X, y = make_classification(
        n_samples=1000, n_features=4, n_redundant=0,
        n_informative=4, n_clusters_per_class=1,
        weights=[0.9, 0.1], random_state=42  # 90% vs 10%
    )
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # æ–¹æ³•1: è°ƒæ•´ç±»åˆ«æƒé‡
    from sklearn.utils.class_weight import compute_class_weight
    
    class_weights = compute_class_weight(
        'balanced', classes=np.unique(y_train), y=y_train
    )
    
    print(f"\nğŸ¯ è®¡ç®—çš„ç±»åˆ«æƒé‡: {dict(zip(np.unique(y_train), class_weights))}")
    
    # æ–¹æ³•2: ä½¿ç”¨ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡
    from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
    
    # æ ‡å‡†æœ´ç´ è´å¶æ–¯
    nb_standard = GaussianNB()
    nb_standard.fit(X_train, y_train)
    
    y_pred_standard = nb_standard.predict(X_test)
    y_proba_standard = nb_standard.predict_proba(X_test)[:, 1]
    
    # è°ƒæ•´å†³ç­–é˜ˆå€¼çš„æœ´ç´ è´å¶æ–¯
    def predict_with_threshold(model, X, threshold=0.5):
        probabilities = model.predict_proba(X)[:, 1]
        return (probabilities >= threshold).astype(int)
    
    # å¯»æ‰¾æœ€ä½³é˜ˆå€¼
    thresholds = np.arange(0.1, 0.9, 0.1)
    best_threshold = 0.5
    best_f1 = 0
    
    for threshold in thresholds:
        y_pred_thresh = predict_with_threshold(nb_standard, X_test, threshold)
        _, _, f1, _ = precision_recall_fscore_support(y_test, y_pred_thresh, average='binary')
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"\nğŸ¯ æœ€ä½³å†³ç­–é˜ˆå€¼: {best_threshold:.2f} (F1åˆ†æ•°: {best_f1:.4f})")
    
    # ä½¿ç”¨æœ€ä½³é˜ˆå€¼é¢„æµ‹
    y_pred_optimized = predict_with_threshold(nb_standard, X_test, best_threshold)
    
    # è¯„ä¼°ç»“æœå¯¹æ¯”
    from sklearn.metrics import classification_report
    
    print(f"\nğŸ“Š æ ‡å‡†é˜ˆå€¼(0.5)ç»“æœ:")
    print(classification_report(y_test, y_pred_standard))
    
    print(f"\nğŸ“Š ä¼˜åŒ–é˜ˆå€¼({best_threshold:.2f})ç»“æœ:")
    print(classification_report(y_test, y_pred_optimized))
    
    # ROC-AUCå¯¹æ¯”
    auc_standard = roc_auc_score(y_test, y_proba_standard)
    print(f"\nğŸ¯ ROC-AUCåˆ†æ•°: {auc_standard:.4f}")
    
    return nb_standard, best_threshold

nb_imbalanced, optimal_threshold = handle_imbalanced_data()
```

### ç‰¹å¾é€‰æ‹©å’Œé™ç»´

```python
def feature_selection_naive_bayes():
    """æœ´ç´ è´å¶æ–¯çš„ç‰¹å¾é€‰æ‹©æŠ€å·§"""
    
    print("\nğŸ” æœ´ç´ è´å¶æ–¯ç‰¹å¾é€‰æ‹©æ¼”ç¤º")
    print("=" * 35)
    
    # ç”Ÿæˆé«˜ç»´æ•°æ®
    X, y = make_classification(
        n_samples=1000, n_features=50, n_informative=10,
        n_redundant=10, n_clusters_per_class=1, random_state=42
    )
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # åŸºå‡†æ¨¡å‹ï¼ˆä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼‰
    nb_baseline = GaussianNB()
    nb_baseline.fit(X_train, y_train)
    baseline_accuracy = nb_baseline.score(X_test, y_test)
    
    print(f"\nğŸ¯ åŸºå‡†å‡†ç¡®ç‡ (æ‰€æœ‰ç‰¹å¾): {baseline_accuracy:.4f}")
    
    # æ–¹æ³•1: åŸºäºæ–¹å·®çš„ç‰¹å¾é€‰æ‹©
    from sklearn.feature_selection import VarianceThreshold
    
    variance_selector = VarianceThreshold(threshold=0.1)
    X_train_var = variance_selector.fit_transform(X_train)
    X_test_var = variance_selector.transform(X_test)
    
    nb_variance = GaussianNB()
    nb_variance.fit(X_train_var, y_train)
    variance_accuracy = nb_variance.score(X_test_var, y_test)
    
    print(f"ğŸ“‰ æ–¹å·®é€‰æ‹©å: {X_train_var.shape[1]} ç‰¹å¾, å‡†ç¡®ç‡: {variance_accuracy:.4f}")
    
    # æ–¹æ³•2: åŸºäºå¡æ–¹æ£€éªŒçš„ç‰¹å¾é€‰æ‹©
    from sklearn.feature_selection import SelectKBest, chi2
    from sklearn.preprocessing import MinMaxScaler
    
    # æ ‡å‡†åŒ–åˆ°éè´Ÿå€¼ï¼ˆå¡æ–¹æ£€éªŒè¦æ±‚ï¼‰
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    chi2_selector = SelectKBest(chi2, k=20)
    X_train_chi2 = chi2_selector.fit_transform(X_train_scaled, y_train)
    X_test_chi2 = chi2_selector.transform(X_test_scaled)
    
    nb_chi2 = GaussianNB()
    nb_chi2.fit(X_train_chi2, y_train)
    chi2_accuracy = nb_chi2.score(X_test_chi2, y_test)
    
    print(f"ğŸ”¬ å¡æ–¹é€‰æ‹©å: {X_train_chi2.shape[1]} ç‰¹å¾, å‡†ç¡®ç‡: {chi2_accuracy:.4f}")
    
    # æ–¹æ³•3: åŸºäºäº’ä¿¡æ¯çš„ç‰¹å¾é€‰æ‹©
    from sklearn.feature_selection import mutual_info_classif
    
    mi_scores = mutual_info_classif(X_train, y_train, random_state=42)
    
    # é€‰æ‹©äº’ä¿¡æ¯æœ€é«˜çš„ç‰¹å¾
    k_best_mi = 15
    mi_indices = np.argsort(mi_scores)[::-1][:k_best_mi]
    
    X_train_mi = X_train[:, mi_indices]
    X_test_mi = X_test[:, mi_indices]
    
    nb_mi = GaussianNB()
    nb_mi.fit(X_train_mi, y_train)
    mi_accuracy = nb_mi.score(X_test_mi, y_test)
    
    print(f"ğŸ§  äº’ä¿¡æ¯é€‰æ‹©å: {X_train_mi.shape[1]} ç‰¹å¾, å‡†ç¡®ç‡: {mi_accuracy:.4f}")
    
    # æ–¹æ³•4: é€’å½’ç‰¹å¾æ¶ˆé™¤
    from sklearn.feature_selection import RFE
    
    rfe_selector = RFE(estimator=GaussianNB(), n_features_to_select=12)
    X_train_rfe = rfe_selector.fit_transform(X_train, y_train)
    X_test_rfe = rfe_selector.transform(X_test)
    
    nb_rfe = GaussianNB()
    nb_rfe.fit(X_train_rfe, y_train)
    rfe_accuracy = nb_rfe.score(X_test_rfe, y_test)
    
    print(f"ğŸ”„ RFEé€‰æ‹©å: {X_train_rfe.shape[1]} ç‰¹å¾, å‡†ç¡®ç‡: {rfe_accuracy:.4f}")
    
    # ç»“æœå¯è§†åŒ–
    methods = ['åŸºå‡†', 'æ–¹å·®é€‰æ‹©', 'å¡æ–¹é€‰æ‹©', 'äº’ä¿¡æ¯é€‰æ‹©', 'RFEé€‰æ‹©']
    accuracies = [baseline_accuracy, variance_accuracy, chi2_accuracy, mi_accuracy, rfe_accuracy]
    feature_counts = [X.shape[1], X_train_var.shape[1], X_train_chi2.shape[1], 
                     X_train_mi.shape[1], X_train_rfe.shape[1]]
    
    plt.figure(figsize=(12, 5))
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    plt.subplot(1, 2, 1)
    bars = plt.bar(methods, accuracies, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('ä¸åŒç‰¹å¾é€‰æ‹©æ–¹æ³•çš„å‡†ç¡®ç‡å¯¹æ¯”')
    plt.xticks(rotation=45)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, acc in zip(bars, accuracies):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                f'{acc:.3f}', ha='center', va='bottom')
    
    # ç‰¹å¾æ•°é‡å¯¹æ¯”
    plt.subplot(1, 2, 2)
    bars2 = plt.bar(methods, feature_counts, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
    plt.ylabel('ç‰¹å¾æ•°é‡')
    plt.title('ä¸åŒæ–¹æ³•é€‰æ‹©çš„ç‰¹å¾æ•°é‡')
    plt.xticks(rotation=45)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, count in zip(bars2, feature_counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                f'{count}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # æ€»ç»“
    print(f"\nğŸ’¡ ç‰¹å¾é€‰æ‹©æ€»ç»“:")
    print("=" * 25)
    best_method_idx = np.argmax(accuracies)
    best_method = methods[best_method_idx]
    best_accuracy = accuracies[best_method_idx]
    best_features = feature_counts[best_method_idx]
    
    print(f"ğŸ† æœ€ä½³æ–¹æ³•: {best_method}")
    print(f"ğŸ“ˆ æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
    print(f"ğŸ”¢ ä½¿ç”¨ç‰¹å¾æ•°: {best_features}")
    print(f"ğŸ“‰ ç‰¹å¾å‡å°‘: {(X.shape[1] - best_features) / X.shape[1] * 100:.1f}%")
    
    return nb_baseline, (nb_variance, nb_chi2, nb_mi, nb_rfe)

baseline_model, selected_models = feature_selection_naive_bayes()
```

## æ€è€ƒé¢˜

1. **æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼šæœ´ç´ è´å¶æ–¯çš„"æœ´ç´ "å‡è®¾åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä¼šä¸¥é‡å½±å“æ€§èƒ½ï¼Ÿå¦‚ä½•æ£€éªŒè¿™ä¸ªå‡è®¾ï¼Ÿ

2. **æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘**ï¼šä¸ºä»€ä¹ˆéœ€è¦æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Ÿå¹³æ»‘å‚æ•°Î±å¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ï¼Ÿ

3. **å˜ä½“é€‰æ‹©**ï¼šåœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©é«˜æ–¯ã€å¤šé¡¹å¼æˆ–ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼Ÿ

4. **é›¶æ¦‚ç‡é—®é¢˜**ï¼šé™¤äº†æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Œè¿˜æœ‰å“ªäº›æ–¹æ³•å¯ä»¥å¤„ç†é›¶æ¦‚ç‡é—®é¢˜ï¼Ÿ

5. **æœ´ç´ è´å¶æ–¯ vs å…¶ä»–ç®—æ³•**ï¼šæœ´ç´ è´å¶æ–¯ç›¸æ¯”é€»è¾‘å›å½’å’ŒSVMæœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ

## æœ¬èŠ‚å°ç»“

æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§åŸºäºæ¦‚ç‡è®ºçš„ç®€å•è€Œå¼ºå¤§çš„åˆ†ç±»ç®—æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### æ ¸å¿ƒä¼˜åŠ¿
- **ç®€å•é«˜æ•ˆ**ï¼šç®—æ³•ç®€å•ï¼Œè®­ç»ƒå’Œé¢„æµ‹é€Ÿåº¦å¿«
- **ç†è®ºåŸºç¡€**ï¼šåŸºäºè´å¶æ–¯å®šç†ï¼Œæœ‰åšå®çš„æ¦‚ç‡è®ºåŸºç¡€
- **å¤„ç†å¤šåˆ†ç±»**ï¼šå¤©ç„¶æ”¯æŒå¤šåˆ†ç±»é—®é¢˜
- **å°æ ·æœ¬å‹å¥½**ï¼šåœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½
- **ç‰¹å¾ç‹¬ç«‹**ï¼šä¸éœ€è¦ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§å‡è®¾

### å…³é”®æŠ€æœ¯
- **æ¡ä»¶ç‹¬ç«‹å‡è®¾**ï¼šç®€åŒ–è®¡ç®—ä½†å¯èƒ½å½±å“æ€§èƒ½
- **æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘**ï¼šè§£å†³é›¶æ¦‚ç‡é—®é¢˜
- **ä¸‰ç§å˜ä½“**ï¼šé«˜æ–¯ã€å¤šé¡¹å¼ã€ä¼¯åŠªåˆ©é€‚ç”¨äºä¸åŒæ•°æ®ç±»å‹

### å®é™…åº”ç”¨
- **æ–‡æœ¬åˆ†ç±»**ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹ã€æƒ…æ„Ÿåˆ†æã€æ–°é—»åˆ†ç±»
- **æ¨èç³»ç»Ÿ**ï¼šåŸºäºç”¨æˆ·è¡Œä¸ºçš„æ¨è
- **åŒ»ç–—è¯Šæ–­**ï¼šåŸºäºç—‡çŠ¶çš„ç–¾ç—…é¢„æµ‹
- **å®æ—¶åˆ†ç±»**ï¼šéœ€è¦å¿«é€Ÿå“åº”çš„åœ¨çº¿åˆ†ç±»ä»»åŠ¡

### ä½¿ç”¨å»ºè®®
- **æ•°æ®é¢„å¤„ç†**ï¼šæ ¹æ®å˜ä½“ç±»å‹å‡†å¤‡åˆé€‚çš„æ•°æ®æ ¼å¼
- **ç‰¹å¾é€‰æ‹©**ï¼šç§»é™¤å†—ä½™å’Œä¸ç›¸å…³ç‰¹å¾
- **å‚æ•°è°ƒä¼˜**ï¼šè°ƒæ•´å¹³æ»‘å‚æ•°å’Œå†³ç­–é˜ˆå€¼
- **æ€§èƒ½è¯„ä¼°**ï¼šä½¿ç”¨é€‚åˆçš„è¯„ä¼°æŒ‡æ ‡ï¼Œç‰¹åˆ«æ˜¯ä¸å¹³è¡¡æ•°æ®

### å±€é™æ€§
- **ç‹¬ç«‹æ€§å‡è®¾**ï¼šç°å®ä¸­ç‰¹å¾å¾€å¾€ç›¸å…³
- **æ•°å€¼ç¨³å®šæ€§**ï¼šéœ€è¦å¤„ç†æ¦‚ç‡è®¡ç®—ä¸­çš„æ•°å€¼é—®é¢˜
- **è¿ç»­ç‰¹å¾**ï¼šé«˜æ–¯å‡è®¾å¯èƒ½ä¸é€‚åˆæ‰€æœ‰è¿ç»­ç‰¹å¾

### ä¸‹ä¸€æ­¥å­¦ä¹ 
- **è´å¶æ–¯ç½‘ç»œ**ï¼šæ”¾æ¾ç‹¬ç«‹æ€§å‡è®¾çš„è´å¶æ–¯æ–¹æ³•
- **é›†æˆæ–¹æ³•**ï¼šå°†æœ´ç´ è´å¶æ–¯ä¸å…¶ä»–ç®—æ³•ç»“åˆ
- **æ·±åº¦å­¦ä¹ **ï¼šäº†è§£ç¥ç»ç½‘ç»œåœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨

æœ´ç´ è´å¶æ–¯è™½ç„¶"æœ´ç´ "ï¼Œä½†åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ†ç±»å’Œéœ€è¦å¿«é€Ÿè®­ç»ƒçš„åœºæ™¯ä¸­ã€‚å®ƒä¸ºæˆ‘ä»¬å±•ç¤ºäº†æ¦‚ç‡è®ºåœ¨æœºå™¨å­¦ä¹ ä¸­çš„å¼ºå¤§åº”ç”¨ã€‚
```