# 1.2.5 朴素贝叶斯算法

## 学习目标

通过本节学习，你将掌握：
- 朴素贝叶斯算法的数学原理和贝叶斯定理
- 三种主要的朴素贝叶斯变体：高斯、多项式和伯努利
- 朴素贝叶斯在文本分类和垃圾邮件检测中的应用
- 条件独立假设的含义和影响
- 拉普拉斯平滑技术处理零概率问题

## 算法原理

### 贝叶斯定理基础

朴素贝叶斯算法基于贝叶斯定理，这是概率论中的一个基本定理：

$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

在分类问题中，我们要计算给定特征 $X$ 的情况下，样本属于类别 $C_k$ 的概率：

$$P(C_k|X) = \frac{P(X|C_k) \cdot P(C_k)}{P(X)}$$

其中：
- $P(C_k|X)$：后验概率（我们要求的）
- $P(X|C_k)$：似然概率
- $P(C_k)$：先验概率
- $P(X)$：边缘概率（归一化常数）

### 朴素假设

"朴素"假设指的是**条件独立假设**：给定类别的情况下，各个特征之间相互独立。

对于特征向量 $X = (x_1, x_2, ..., x_n)$：

$$P(X|C_k) = P(x_1, x_2, ..., x_n|C_k) = \prod_{i=1}^{n} P(x_i|C_k)$$

因此，分类决策变为：

$$\hat{y} = \arg\max_{k} P(C_k) \prod_{i=1}^{n} P(x_i|C_k)$$

## 算法实现

### 基础朴素贝叶斯实现

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification, fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from collections import defaultdict
import pandas as pd

class SimpleNaiveBayes:
    """简单的朴素贝叶斯分类器实现"""
    
    def __init__(self, alpha=1.0):
        """
        参数:
        alpha: 拉普拉斯平滑参数
        """
        self.alpha = alpha
        self.class_priors = {}  # 先验概率 P(C_k)
        self.feature_probs = {}  # 条件概率 P(x_i|C_k)
        self.classes = None
        self.n_features = None
        
    def fit(self, X, y):
        """训练朴素贝叶斯分类器"""
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        self.n_features = X.shape[1]
        n_samples = len(y)
        
        print(f"🤖 训练朴素贝叶斯分类器...")
        print(f"样本数: {n_samples}, 特征数: {self.n_features}, 类别数: {len(self.classes)}")
        
        # 计算先验概率 P(C_k)
        for class_label in self.classes:
            class_count = np.sum(y == class_label)
            self.class_priors[class_label] = class_count / n_samples
            
        # 计算条件概率 P(x_i|C_k)
        self.feature_probs = {}
        
        for class_label in self.classes:
            class_mask = (y == class_label)
            class_samples = X[class_mask]
            
            self.feature_probs[class_label] = {}
            
            for feature_idx in range(self.n_features):
                feature_values = class_samples[:, feature_idx]
                unique_values = np.unique(X[:, feature_idx])  # 所有可能的特征值
                
                # 计算每个特征值的条件概率（带拉普拉斯平滑）
                feature_prob_dict = {}
                for value in unique_values:
                    count = np.sum(feature_values == value)
                    # 拉普拉斯平滑
                    prob = (count + self.alpha) / (len(class_samples) + self.alpha * len(unique_values))
                    feature_prob_dict[value] = prob
                    
                self.feature_probs[class_label][feature_idx] = feature_prob_dict
        
        print("✅ 训练完成!")
        return self
    
    def predict_proba(self, X):
        """预测概率"""
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # 计算 P(C_k) * ∏P(x_i|C_k)
                prob = self.class_priors[class_label]
                
                for feature_idx, feature_value in enumerate(sample):
                    if feature_value in self.feature_probs[class_label][feature_idx]:
                        prob *= self.feature_probs[class_label][feature_idx][feature_value]
                    else:
                        # 未见过的特征值，使用平滑概率
                        prob *= self.alpha / (sum(self.class_priors.values()) + self.alpha * len(self.classes))
                
                probabilities[i, j] = prob
        
        # 归一化
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / row_sums
        
        return probabilities
    
    def predict(self, X):
        """预测类别"""
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
    
    def score(self, X, y):
        """计算准确率"""
        predictions = self.predict(X)
        return accuracy_score(y, predictions)

# 演示基础朴素贝叶斯
def demonstrate_basic_naive_bayes():
    """演示基础朴素贝叶斯分类器"""
    
    # 生成离散特征数据
    np.random.seed(42)
    n_samples = 1000
    
    # 创建离散特征数据
    X = np.random.randint(0, 3, size=(n_samples, 4))  # 4个特征，每个特征有0,1,2三个值
    
    # 创建标签（基于特征的简单规则）
    y = ((X[:, 0] + X[:, 1]) > (X[:, 2] + X[:, 3])).astype(int)
    
    print("📊 离散特征朴素贝叶斯演示")
    print(f"数据形状: {X.shape}")
    print(f"类别分布: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 训练自实现的朴素贝叶斯
    nb_custom = SimpleNaiveBayes(alpha=1.0)
    nb_custom.fit(X_train, y_train)
    
    # 预测
    y_pred_custom = nb_custom.predict(X_test)
    accuracy_custom = nb_custom.score(X_test, y_test)
    
    print(f"\n🎯 自实现朴素贝叶斯结果:")
    print(f"测试准确率: {accuracy_custom:.4f}")
    
    # 显示先验概率
    print(f"\n📈 先验概率:")
    for class_label, prior in nb_custom.class_priors.items():
        print(f"  P(类别={class_label}) = {prior:.4f}")
    
    # 显示部分条件概率
    print(f"\n🔍 条件概率示例 (特征0):")
    for class_label in nb_custom.classes:
        print(f"  类别 {class_label}:")
        for value, prob in nb_custom.feature_probs[class_label][0].items():
            print(f"    P(特征0={value}|类别={class_label}) = {prob:.4f}")
    
    return nb_custom, X_test, y_test, y_pred_custom

nb_model, X_test, y_test, y_pred = demonstrate_basic_naive_bayes()
```

### 高斯朴素贝叶斯

```python
class GaussianNaiveBayes:
    """高斯朴素贝叶斯分类器（适用于连续特征）"""
    
    def __init__(self):
        self.class_priors = {}
        self.feature_means = {}  # 每个类别每个特征的均值
        self.feature_vars = {}   # 每个类别每个特征的方差
        self.classes = None
        
    def fit(self, X, y):
        """训练高斯朴素贝叶斯"""
        X = np.array(X)
        y = np.array(y)
        
        self.classes = np.unique(y)
        n_samples = len(y)
        
        print(f"🤖 训练高斯朴素贝叶斯...")
        
        for class_label in self.classes:
            # 先验概率
            class_mask = (y == class_label)
            self.class_priors[class_label] = np.sum(class_mask) / n_samples
            
            # 该类别的样本
            class_samples = X[class_mask]
            
            # 计算每个特征的均值和方差
            self.feature_means[class_label] = np.mean(class_samples, axis=0)
            self.feature_vars[class_label] = np.var(class_samples, axis=0)
            
        print("✅ 训练完成!")
        return self
    
    def _gaussian_pdf(self, x, mean, var):
        """计算高斯概率密度函数"""
        # 避免除零
        var = np.maximum(var, 1e-9)
        coeff = 1.0 / np.sqrt(2 * np.pi * var)
        exponent = -0.5 * ((x - mean) ** 2) / var
        return coeff * np.exp(exponent)
    
    def predict_proba(self, X):
        """预测概率"""
        X = np.array(X)
        n_samples = X.shape[0]
        probabilities = np.zeros((n_samples, len(self.classes)))
        
        for i, sample in enumerate(X):
            for j, class_label in enumerate(self.classes):
                # 先验概率
                prob = self.class_priors[class_label]
                
                # 似然概率（各特征概率的乘积）
                mean = self.feature_means[class_label]
                var = self.feature_vars[class_label]
                
                likelihood = np.prod(self._gaussian_pdf(sample, mean, var))
                prob *= likelihood
                
                probabilities[i, j] = prob
        
        # 归一化
        row_sums = probabilities.sum(axis=1, keepdims=True)
        probabilities = probabilities / (row_sums + 1e-10)
        
        return probabilities
    
    def predict(self, X):
        """预测类别"""
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
    
    def score(self, X, y):
        """计算准确率"""
        predictions = self.predict(X)
        return accuracy_score(y, predictions)

# 演示高斯朴素贝叶斯
def demonstrate_gaussian_naive_bayes():
    """演示高斯朴素贝叶斯"""
    
    # 生成连续特征数据
    X, y = make_classification(
        n_samples=1000, n_features=2, n_redundant=0, 
        n_informative=2, n_clusters_per_class=1, random_state=42
    )
    
    print("\n📊 高斯朴素贝叶斯演示")
    print(f"数据形状: {X.shape}")
    print(f"类别分布: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 自实现的高斯朴素贝叶斯
    gnb_custom = GaussianNaiveBayes()
    gnb_custom.fit(X_train, y_train)
    
    # sklearn的高斯朴素贝叶斯
    gnb_sklearn = GaussianNB()
    gnb_sklearn.fit(X_train, y_train)
    
    # 预测和评估
    y_pred_custom = gnb_custom.predict(X_test)
    y_pred_sklearn = gnb_sklearn.predict(X_test)
    
    accuracy_custom = gnb_custom.score(X_test, y_test)
    accuracy_sklearn = gnb_sklearn.score(X_test, y_test)
    
    print(f"\n🎯 结果对比:")
    print(f"自实现准确率: {accuracy_custom:.4f}")
    print(f"sklearn准确率: {accuracy_sklearn:.4f}")
    
    # 可视化决策边界
    plt.figure(figsize=(15, 5))
    
    # 原始数据
    plt.subplot(1, 3, 1)
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
    plt.xlabel('特征 1')
    plt.ylabel('特征 2')
    plt.title('原始数据分布')
    plt.colorbar(scatter)
    plt.grid(True, alpha=0.3)
    
    # 自实现模型决策边界
    plt.subplot(1, 3, 2)
    plot_decision_boundary(gnb_custom, X, y, "自实现高斯朴素贝叶斯")
    
    # sklearn模型决策边界
    plt.subplot(1, 3, 3)
    plot_decision_boundary(gnb_sklearn, X, y, "sklearn高斯朴素贝叶斯")
    
    plt.tight_layout()
    plt.show()
    
    # 显示学习到的参数
    print(f"\n📈 学习到的参数:")
    for class_label in gnb_custom.classes:
        print(f"类别 {class_label}:")
        print(f"  先验概率: {gnb_custom.class_priors[class_label]:.4f}")
        print(f"  特征均值: {gnb_custom.feature_means[class_label]}")
        print(f"  特征方差: {gnb_custom.feature_vars[class_label]}")
    
    return gnb_custom, gnb_sklearn

def plot_decision_boundary(model, X, y, title):
    """绘制决策边界"""
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='viridis')
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black')
    plt.xlabel('特征 1')
    plt.ylabel('特征 2')
    plt.title(title)
    plt.grid(True, alpha=0.3)

gnb_custom, gnb_sklearn = demonstrate_gaussian_naive_bayes()
```

## 文本分类应用

### 多项式朴素贝叶斯用于文本分类

```python
class TextClassificationNB:
    """基于朴素贝叶斯的文本分类系统"""
    
    def __init__(self, alpha=1.0, use_tfidf=False):
        self.alpha = alpha
        self.use_tfidf = use_tfidf
        self.vectorizer = None
        self.nb_model = None
        self.classes = None
        
    def generate_text_data(self, n_samples=1000):
        """生成模拟文本数据"""
        np.random.seed(42)
        
        # 定义不同类别的关键词
        categories = {
            'sports': ['football', 'basketball', 'soccer', 'game', 'team', 'player', 'score', 'match', 'win', 'championship'],
            'technology': ['computer', 'software', 'internet', 'data', 'algorithm', 'programming', 'AI', 'machine learning', 'code', 'digital'],
            'politics': ['government', 'election', 'policy', 'president', 'congress', 'vote', 'democracy', 'law', 'political', 'campaign']
        }
        
        texts = []
        labels = []
        
        for category, keywords in categories.items():
            for _ in range(n_samples // len(categories)):
                # 随机选择3-8个关键词
                n_words = np.random.randint(3, 9)
                selected_words = np.random.choice(keywords, n_words, replace=True)
                
                # 添加一些噪声词
                noise_words = ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of']
                n_noise = np.random.randint(1, 4)
                noise = np.random.choice(noise_words, n_noise, replace=True)
                
                # 组合成文本
                all_words = np.concatenate([selected_words, noise])
                np.random.shuffle(all_words)
                text = ' '.join(all_words)
                
                texts.append(text)
                labels.append(category)
        
        return texts, labels
    
    def fit(self, texts, labels):
        """训练文本分类模型"""
        print(f"📚 开始训练文本分类模型...")
        print(f"文本数量: {len(texts)}")
        print(f"类别分布: {dict(pd.Series(labels).value_counts())}")
        
        # 文本向量化
        if self.use_tfidf:
            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        else:
            self.vectorizer = CountVectorizer(max_features=1000, stop_words='english')
        
        X = self.vectorizer.fit_transform(texts)
        
        print(f"特征维度: {X.shape[1]}")
        
        # 训练朴素贝叶斯模型
        self.nb_model = MultinomialNB(alpha=self.alpha)
        self.nb_model.fit(X, labels)
        
        self.classes = self.nb_model.classes_
        
        print("✅ 训练完成!")
        return self
    
    def predict(self, texts):
        """预测文本类别"""
        X = self.vectorizer.transform(texts)
        return self.nb_model.predict(X)
    
    def predict_proba(self, texts):
        """预测文本类别概率"""
        X = self.vectorizer.transform(texts)
        return self.nb_model.predict_proba(X)
    
    def analyze_features(self, top_n=10):
        """分析重要特征"""
        feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"\n🔍 各类别最重要的{top_n}个特征:")
        print("=" * 60)
        
        for i, class_name in enumerate(self.classes):
            # 获取该类别的特征权重
            feature_log_probs = self.nb_model.feature_log_prob_[i]
            
            # 获取top特征
            top_indices = np.argsort(feature_log_probs)[::-1][:top_n]
            
            print(f"\n📂 {class_name.upper()}:")
            print("-" * 30)
            
            for j, idx in enumerate(top_indices):
                feature_name = feature_names[idx]
                log_prob = feature_log_probs[idx]
                prob = np.exp(log_prob)
                
                print(f"{j+1:2d}. {feature_name:<15} (概率: {prob:.6f})")
    
    def classify_text(self, text):
        """分类单个文本并显示详细信息"""
        prediction = self.predict([text])[0]
        probabilities = self.predict_proba([text])[0]
        
        print(f"\n📄 文本: '{text}'")
        print(f"🎯 预测类别: {prediction}")
        print(f"📊 各类别概率:")
        
        for class_name, prob in zip(self.classes, probabilities):
            print(f"  {class_name:<12}: {prob:.4f} {'★' if class_name == prediction else ''}")
        
        return prediction, probabilities

# 演示文本分类
def demonstrate_text_classification():
    """演示文本分类"""
    
    # 创建文本分类器
    text_classifier = TextClassificationNB(alpha=1.0, use_tfidf=True)
    
    # 生成数据
    texts, labels = text_classifier.generate_text_data(900)
    
    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.3, random_state=42, stratify=labels
    )
    
    # 训练模型
    text_classifier.fit(X_train, y_train)
    
    # 预测和评估
    y_pred = text_classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\n🎯 文本分类结果:")
    print(f"测试准确率: {accuracy:.4f}")
    
    # 详细分类报告
    print(f"\n📊 详细分类报告:")
    print(classification_report(y_test, y_pred))
    
    # 混淆矩阵
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=text_classifier.classes, 
                yticklabels=text_classifier.classes)
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.title(f'文本分类混淆矩阵\n准确率: {accuracy:.4f}')
    plt.show()
    
    # 分析重要特征
    text_classifier.analyze_features()
    
    # 测试新文本
    test_texts = [
        "football team won the championship game",
        "new AI algorithm improves machine learning performance",
        "president announced new government policy",
        "basketball player scored in the match",
        "computer software programming code"
    ]
    
    print(f"\n🧪 测试新文本分类:")
    for text in test_texts:
        text_classifier.classify_text(text)
    
    return text_classifier

text_model = demonstrate_text_classification()
```

### 垃圾邮件检测实战

```python
class SpamDetectorNB:
    """基于朴素贝叶斯的垃圾邮件检测器"""
    
    def __init__(self):
        self.vectorizer = None
        self.nb_model = None
        self.feature_names = None
        
    def generate_email_dataset(self, n_samples=2000):
        """生成更真实的邮件数据集"""
        np.random.seed(42)
        
        # 垃圾邮件模式
        spam_patterns = {
            'promotional': ['free', 'win', 'prize', 'offer', 'discount', 'sale', 'deal', 'limited time'],
            'urgent': ['urgent', 'act now', 'hurry', 'expires', 'deadline', 'immediate'],
            'money': ['money', 'cash', 'earn', 'income', 'profit', 'rich', 'wealthy'],
            'suspicious': ['click here', 'guarantee', 'no risk', 'amazing', 'incredible']
        }
        
        # 正常邮件模式
        normal_patterns = {
            'work': ['meeting', 'project', 'deadline', 'report', 'presentation', 'team', 'colleague'],
            'personal': ['family', 'friend', 'birthday', 'vacation', 'dinner', 'weekend'],
            'formal': ['regards', 'sincerely', 'thank you', 'please', 'kindly', 'appreciate'],
            'business': ['company', 'client', 'contract', 'proposal', 'budget', 'schedule']
        }
        
        emails = []
        labels = []
        
        # 生成垃圾邮件
        for _ in range(n_samples // 2):
            email_parts = []
            
            # 随机选择1-3个垃圾邮件模式
            n_patterns = np.random.randint(1, 4)
            selected_patterns = np.random.choice(list(spam_patterns.keys()), n_patterns, replace=False)
            
            for pattern in selected_patterns:
                n_words = np.random.randint(2, 5)
                words = np.random.choice(spam_patterns[pattern], n_words, replace=True)
                email_parts.extend(words)
            
            # 添加一些正常词汇作为噪声
            if np.random.random() < 0.3:
                normal_pattern = np.random.choice(list(normal_patterns.keys()))
                noise_words = np.random.choice(normal_patterns[normal_pattern], 2, replace=True)
                email_parts.extend(noise_words)
            
            email = ' '.join(email_parts)
            emails.append(email)
            labels.append('spam')
        
        # 生成正常邮件
        for _ in range(n_samples // 2):
            email_parts = []
            
            # 随机选择1-2个正常邮件模式
            n_patterns = np.random.randint(1, 3)
            selected_patterns = np.random.choice(list(normal_patterns.keys()), n_patterns, replace=False)
            
            for pattern in selected_patterns:
                n_words = np.random.randint(3, 7)
                words = np.random.choice(normal_patterns[pattern], n_words, replace=True)
                email_parts.extend(words)
            
            # 添加一些垃圾词汇作为噪声
            if np.random.random() < 0.1:
                spam_pattern = np.random.choice(list(spam_patterns.keys()))
                noise_words = np.random.choice(spam_patterns[spam_pattern], 1, replace=True)
                email_parts.extend(noise_words)
            
            email = ' '.join(email_parts)
            emails.append(email)
            labels.append('ham')
        
        return emails, labels
    
    def train(self, emails, labels):
        """训练垃圾邮件检测模型"""
        print(f"📧 训练垃圾邮件检测器...")
        print(f"邮件总数: {len(emails)}")
        print(f"标签分布: {dict(pd.Series(labels).value_counts())}")
        
        # 特征提取：使用TF-IDF
        self.vectorizer = TfidfVectorizer(
            max_features=2000,
            stop_words='english',
            ngram_range=(1, 2),  # 1-2gram
            min_df=2,  # 至少出现2次
            max_df=0.95  # 最多出现在95%的文档中
        )
        
        X = self.vectorizer.fit_transform(emails)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"特征维度: {X.shape[1]}")
        
        # 训练多项式朴素贝叶斯
        self.nb_model = MultinomialNB(alpha=1.0)
        self.nb_model.fit(X, labels)
        
        print("✅ 训练完成!")
        return self
    
    def predict_email(self, email_text):
        """预测单封邮件"""
        X = self.vectorizer.transform([email_text])
        prediction = self.nb_model.predict(X)[0]
        probabilities = self.nb_model.predict_proba(X)[0]
        
        return prediction, probabilities
    
    def analyze_spam_indicators(self, top_n=15):
        """分析垃圾邮件指示词"""
        # 获取类别索引
        classes = self.nb_model.classes_
        spam_idx = list(classes).index('spam')
        ham_idx = list(classes).index('ham')
        
        # 计算特征的对数概率比
        spam_log_probs = self.nb_model.feature_log_prob_[spam_idx]
        ham_log_probs = self.nb_model.feature_log_prob_[ham_idx]
        
        # 计算对数概率差（spam - ham）
        log_prob_diff = spam_log_probs - ham_log_probs
        
        # 获取最强的垃圾邮件指示词
        spam_indicators = np.argsort(log_prob_diff)[::-1][:top_n]
        
        # 获取最强的正常邮件指示词
        ham_indicators = np.argsort(log_prob_diff)[:top_n]
        
        print(f"\n🚨 最强垃圾邮件指示词 (Top {top_n}):")
        print("-" * 50)
        for i, idx in enumerate(spam_indicators):
            word = self.feature_names[idx]
            score = log_prob_diff[idx]
            print(f"{i+1:2d}. {word:<20} (分数: {score:.4f})")
        
        print(f"\n✅ 最强正常邮件指示词 (Top {top_n}):")
        print("-" * 50)
        for i, idx in enumerate(ham_indicators):
            word = self.feature_names[idx]
            score = log_prob_diff[idx]
            print(f"{i+1:2d}. {word:<20} (分数: {score:.4f})")
    
    def detailed_prediction(self, email_text):
        """详细预测分析"""
        prediction, probabilities = self.predict_email(email_text)
        
        print(f"\n📧 邮件内容: '{email_text}'")
        print(f"🎯 预测结果: {'垃圾邮件' if prediction == 'spam' else '正常邮件'}")
        print(f"📊 概率分布:")
        
        classes = self.nb_model.classes_
        for class_name, prob in zip(classes, probabilities):
            emoji = "🚨" if class_name == 'spam' else "✅"
            star = "★" if class_name == prediction else " "
            print(f"  {emoji} {class_name:<8}: {prob:.4f} {star}")
        
        # 分析关键词贡献
        X = self.vectorizer.transform([email_text])
        feature_indices = X.nonzero()[1]
        
        if len(feature_indices) > 0:
            print(f"\n🔍 关键词分析:")
            
            spam_idx = list(classes).index('spam')
            ham_idx = list(classes).index('ham')
            
            word_contributions = []
            
            for idx in feature_indices:
                word = self.feature_names[idx]
                tfidf_score = X[0, idx]
                
                spam_log_prob = self.nb_model.feature_log_prob_[spam_idx, idx]
                ham_log_prob = self.nb_model.feature_log_prob_[ham_idx, idx]
                
                contribution = (spam_log_prob - ham_log_prob) * tfidf_score
                word_contributions.append((word, contribution, tfidf_score))
            
            # 按贡献度排序
            word_contributions.sort(key=lambda x: abs(x[1]), reverse=True)
            
            print("  词汇           贡献度    TF-IDF   类型")
            print("  " + "-" * 45)
            
            for word, contrib, tfidf in word_contributions[:8]:
                contrib_type = "垃圾" if contrib > 0 else "正常"
                print(f"  {word:<12} {contrib:>8.4f} {tfidf:>8.4f}   {contrib_type}")
        
        return prediction, probabilities

# 演示垃圾邮件检测
def demonstrate_spam_detection():
    """演示垃圾邮件检测"""
    
    # 创建垃圾邮件检测器
    spam_detector = SpamDetectorNB()
    
    # 生成数据
    emails, labels = spam_detector.generate_email_dataset(1600)
    
    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(
        emails, labels, test_size=0.25, random_state=42, stratify=labels
    )
    
    # 训练模型
    spam_detector.train(X_train, y_train)
    
    # 评估模型
    y_pred = []
    y_proba = []
    
    for email in X_test:
        pred, prob = spam_detector.predict_email(email)
        y_pred.append(pred)
        y_proba.append(prob[1] if spam_detector.nb_model.classes_[1] == 'spam' else prob[0])
    
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\n🎯 垃圾邮件检测结果:")
    print(f"测试准确率: {accuracy:.4f}")
    
    # 详细分类报告
    print(f"\n📊 详细分类报告:")
    print(classification_report(y_test, y_pred))
    
    # ROC曲线
    from sklearn.metrics import roc_curve, auc
    
    # 将标签转换为数值
    y_test_binary = [1 if label == 'spam' else 0 for label in y_test]
    
    fpr, tpr, _ = roc_curve(y_test_binary, y_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(12, 5))
    
    # ROC曲线
    plt.subplot(1, 2, 1)
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC曲线 (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('假正率 (FPR)')
    plt.ylabel('真正率 (TPR)')
    plt.title('ROC曲线')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # 混淆矩阵
    plt.subplot(1, 2, 2)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['正常', '垃圾'], yticklabels=['正常', '垃圾'])
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.title(f'混淆矩阵\n准确率: {accuracy:.4f}')
    
    plt.tight_layout()
    plt.show()
    
    # 分析垃圾邮件指示词
    spam_detector.analyze_spam_indicators()
    
    # 测试具体邮件
    test_emails = [
        "free money win prize click here urgent offer",
        "meeting tomorrow project deadline report",
        "amazing deal limited time act now guarantee",
        "thank you for the presentation regards team",
        "earn cash incredible profit no risk",
        "family dinner weekend birthday celebration"
    ]
    
    print(f"\n🧪 测试邮件分类:")
    for email in test_emails:
        spam_detector.detailed_prediction(email)
    
    return spam_detector

spam_model = demonstrate_spam_detection()
```

## 朴素贝叶斯变体对比

```python
def compare_naive_bayes_variants():
    """对比不同朴素贝叶斯变体"""
    
    print("\n🔬 朴素贝叶斯变体对比实验")
    print("=" * 50)
    
    # 准备不同类型的数据
    datasets = {}
    
    # 1. 连续特征数据（适合高斯朴素贝叶斯）
    X_continuous, y_continuous = make_classification(
        n_samples=1000, n_features=4, n_redundant=0, 
        n_informative=4, n_clusters_per_class=1, random_state=42
    )
    datasets['连续特征'] = (X_continuous, y_continuous)
    
    # 2. 离散计数数据（适合多项式朴素贝叶斯）
    np.random.seed(42)
    X_counts = np.random.poisson(3, size=(1000, 4))  # 泊松分布生成计数数据
    y_counts = (X_counts.sum(axis=1) > X_counts.mean()).astype(int)
    datasets['计数特征'] = (X_counts, y_counts)
    
    # 3. 二值特征数据（适合伯努利朴素贝叶斯）
    X_binary = np.random.binomial(1, 0.3, size=(1000, 4))  # 二项分布生成二值数据
    y_binary = (X_binary.sum(axis=1) > 2).astype(int)
    datasets['二值特征'] = (X_binary, y_binary)
    
    # 朴素贝叶斯变体
    models = {
        '高斯朴素贝叶斯': GaussianNB(),
        '多项式朴素贝叶斯': MultinomialNB(),
        '伯努利朴素贝叶斯': BernoulliNB()
    }
    
    results = []
    
    for data_name, (X, y) in datasets.items():
        print(f"\n📊 数据集: {data_name}")
        print(f"数据形状: {X.shape}, 类别分布: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # 划分数据
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        print(f"\n模型性能对比:")
        print("-" * 40)
        
        for model_name, model in models.items():
            try:
                # 训练和预测
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                
                print(f"{model_name:<15}: {accuracy:.4f}")
                
                results.append({
                    '数据类型': data_name,
                    '模型': model_name,
                    '准确率': accuracy
                })
                
            except Exception as e:
                print(f"{model_name:<15}: 错误 - {str(e)[:30]}...")
                results.append({
                    '数据类型': data_name,
                    '模型': model_name,
                    '准确率': 0.0
                })
    
    # 结果可视化
    results_df = pd.DataFrame(results)
    
    plt.figure(figsize=(12, 8))
    
    # 热力图
    pivot_table = results_df.pivot(index='模型', columns='数据类型', values='准确率')
    
    plt.subplot(2, 1, 1)
    sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='YlOrRd', 
                cbar_kws={'label': '准确率'})
    plt.title('朴素贝叶斯变体在不同数据类型上的性能')
    
    # 柱状图
    plt.subplot(2, 1, 2)
    x_pos = np.arange(len(datasets))
    width = 0.25
    
    for i, model_name in enumerate(models.keys()):
        model_results = results_df[results_df['模型'] == model_name]['准确率'].values
        plt.bar(x_pos + i * width, model_results, width, label=model_name)
    
    plt.xlabel('数据类型')
    plt.ylabel('准确率')
    plt.title('朴素贝叶斯变体性能对比')
    plt.xticks(x_pos + width, list(datasets.keys()))
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 总结建议
    print(f"\n💡 使用建议:")
    print("=" * 30)
    print("📈 高斯朴素贝叶斯: 适用于连续特征，假设特征服从高斯分布")
    print("📊 多项式朴素贝叶斯: 适用于计数特征，如文本中的词频")
    print("🔢 伯努利朴素贝叶斯: 适用于二值特征，如文档中词汇的出现与否")
    
    return results_df

comparison_results = compare_naive_bayes_variants()
```