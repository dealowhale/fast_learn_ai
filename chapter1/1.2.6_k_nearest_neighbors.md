# 1.2.6 Kè¿‘é‚»ç®—æ³• (K-Nearest Neighbors, KNN)

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å°†æŒæ¡ï¼š
- Kè¿‘é‚»ç®—æ³•çš„åŸºæœ¬åŸç†å’Œå‡ ä½•ç›´è§‰
- è·ç¦»åº¦é‡æ–¹æ³•å’ŒKå€¼é€‰æ‹©ç­–ç•¥
- KNNåœ¨åˆ†ç±»å’Œå›å½’ä»»åŠ¡ä¸­çš„åº”ç”¨
- ç®—æ³•ä¼˜åŒ–æŠ€æœ¯å’Œå®é™…åº”ç”¨åœºæ™¯
- ä½¿ç”¨Pythonå’ŒTraeå®ç°å®Œæ•´çš„KNNç³»ç»Ÿ

## ç®—æ³•æ¦‚è¿°

Kè¿‘é‚»ç®—æ³•æ˜¯ä¸€ç§åŸºäºå®ä¾‹çš„å­¦ä¹ æ–¹æ³•ï¼Œä¹Ÿç§°ä¸º"æ‡’æƒ°å­¦ä¹ "ç®—æ³•ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³éå¸¸ç›´è§‚ï¼š**ç›¸ä¼¼çš„æ ·æœ¬åº”è¯¥æœ‰ç›¸ä¼¼çš„æ ‡ç­¾**ã€‚

### åŸºæœ¬æ€æƒ³

```mermaid
graph TD
    A[æ–°æ ·æœ¬] --> B[è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»]
    B --> C[æ‰¾åˆ°Kä¸ªæœ€è¿‘é‚»]
    C --> D{ä»»åŠ¡ç±»å‹}
    D -->|åˆ†ç±»| E[å¤šæ•°æŠ•ç¥¨å†³å®šç±»åˆ«]
    D -->|å›å½’| F[å¹³å‡å€¼ä½œä¸ºé¢„æµ‹ç»“æœ]
    E --> G[è¾“å‡ºé¢„æµ‹ç±»åˆ«]
    F --> G[è¾“å‡ºé¢„æµ‹æ•°å€¼]
```

### ç®—æ³•ç‰¹ç‚¹

- **éå‚æ•°æ–¹æ³•**ï¼šä¸å¯¹æ•°æ®åˆ†å¸ƒåšå‡è®¾
- **æ‡’æƒ°å­¦ä¹ **ï¼šè®­ç»ƒé˜¶æ®µåªå­˜å‚¨æ•°æ®ï¼Œé¢„æµ‹æ—¶æ‰è®¡ç®—
- **å±€éƒ¨æ€§**ï¼šé¢„æµ‹åŸºäºå±€éƒ¨é‚»åŸŸä¿¡æ¯
- **ç®€å•ç›´è§‚**ï¼šç®—æ³•é€»è¾‘å®¹æ˜“ç†è§£å’Œå®ç°

## æ•°å­¦åŸç†

### è·ç¦»åº¦é‡

KNNçš„æ ¸å¿ƒæ˜¯è®¡ç®—æ ·æœ¬é—´çš„è·ç¦»ã€‚å¸¸ç”¨çš„è·ç¦»åº¦é‡åŒ…æ‹¬ï¼š

#### 1. æ¬§å‡ é‡Œå¾—è·ç¦» (Euclidean Distance)

$$d(x_i, x_j) = \sqrt{\sum_{k=1}^{n} (x_{ik} - x_{jk})^2}$$

#### 2. æ›¼å“ˆé¡¿è·ç¦» (Manhattan Distance)

$$d(x_i, x_j) = \sum_{k=1}^{n} |x_{ik} - x_{jk}|$$

#### 3. é—µå¯å¤«æ–¯åŸºè·ç¦» (Minkowski Distance)

$$d(x_i, x_j) = \left(\sum_{k=1}^{n} |x_{ik} - x_{jk}|^p\right)^{1/p}$$

å½“p=1æ—¶ä¸ºæ›¼å“ˆé¡¿è·ç¦»ï¼Œp=2æ—¶ä¸ºæ¬§å‡ é‡Œå¾—è·ç¦»ã€‚

#### 4. ä½™å¼¦è·ç¦» (Cosine Distance)

$$d(x_i, x_j) = 1 - \frac{x_i \cdot x_j}{||x_i|| \cdot ||x_j||}$$

### é¢„æµ‹è§„åˆ™

#### åˆ†ç±»ä»»åŠ¡

å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨å¤šæ•°æŠ•ç¥¨ï¼š

$$\hat{y} = \arg\max_{c} \sum_{i \in N_k(x)} I(y_i = c)$$

å…¶ä¸­$N_k(x)$è¡¨ç¤ºæ ·æœ¬$x$çš„Kä¸ªæœ€è¿‘é‚»ï¼Œ$I(\cdot)$æ˜¯æŒ‡ç¤ºå‡½æ•°ã€‚

#### å›å½’ä»»åŠ¡

å¯¹äºå›å½’ä»»åŠ¡ï¼Œä½¿ç”¨å¹³å‡å€¼ï¼š

$$\hat{y} = \frac{1}{k} \sum_{i \in N_k(x)} y_i$$

æˆ–è€…ä½¿ç”¨è·ç¦»åŠ æƒå¹³å‡ï¼š

$$\hat{y} = \frac{\sum_{i \in N_k(x)} w_i y_i}{\sum_{i \in N_k(x)} w_i}$$

å…¶ä¸­$w_i = \frac{1}{d(x, x_i) + \epsilon}$ï¼Œ$\epsilon$æ˜¯å°çš„æ­£æ•°é˜²æ­¢é™¤é›¶ã€‚

## åŸºç¡€å®ç°

### ä»é›¶å®ç°KNNåˆ†ç±»å™¨

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
from collections import Counter
import seaborn as sns

class SimpleKNN:
    """ç®€å•çš„KNNå®ç°"""
    
    def __init__(self, k=3, distance_metric='euclidean', task='classification'):
        """
        å‚æ•°:
        k: è¿‘é‚»æ•°é‡
        distance_metric: è·ç¦»åº¦é‡æ–¹æ³•
        task: 'classification' æˆ– 'regression'
        """
        self.k = k
        self.distance_metric = distance_metric
        self.task = task
        self.X_train = None
        self.y_train = None
    
    def fit(self, X, y):
        """è®­ç»ƒKNNï¼ˆå®é™…ä¸Šåªæ˜¯å­˜å‚¨æ•°æ®ï¼‰"""
        self.X_train = np.array(X)
        self.y_train = np.array(y)
        print(f"ğŸ“š KNNè®­ç»ƒå®Œæˆ: å­˜å‚¨äº† {len(X)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return self
    
    def _calculate_distance(self, x1, x2):
        """è®¡ç®—ä¸¤ä¸ªæ ·æœ¬é—´çš„è·ç¦»"""
        if self.distance_metric == 'euclidean':
            return np.sqrt(np.sum((x1 - x2) ** 2))
        elif self.distance_metric == 'manhattan':
            return np.sum(np.abs(x1 - x2))
        elif self.distance_metric == 'minkowski':
            p = 3  # å¯ä»¥ä½œä¸ºå‚æ•°
            return np.sum(np.abs(x1 - x2) ** p) ** (1/p)
        elif self.distance_metric == 'cosine':
            dot_product = np.dot(x1, x2)
            norm_x1 = np.linalg.norm(x1)
            norm_x2 = np.linalg.norm(x2)
            return 1 - dot_product / (norm_x1 * norm_x2 + 1e-8)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è·ç¦»åº¦é‡: {self.distance_metric}")
    
    def _get_neighbors(self, x):
        """è·å–Kä¸ªæœ€è¿‘é‚»"""
        distances = []
        
        # è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
        for i, x_train in enumerate(self.X_train):
            dist = self._calculate_distance(x, x_train)
            distances.append((dist, i))
        
        # æŒ‰è·ç¦»æ’åºå¹¶å–å‰Kä¸ª
        distances.sort(key=lambda x: x[0])
        neighbors = distances[:self.k]
        
        return neighbors
    
    def predict_single(self, x):
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        neighbors = self._get_neighbors(x)
        
        if self.task == 'classification':
            # å¤šæ•°æŠ•ç¥¨
            neighbor_labels = [self.y_train[idx] for _, idx in neighbors]
            prediction = Counter(neighbor_labels).most_common(1)[0][0]
        else:  # regression
            # å¹³å‡å€¼æˆ–è·ç¦»åŠ æƒå¹³å‡
            if all(dist > 0 for dist, _ in neighbors):
                # è·ç¦»åŠ æƒå¹³å‡
                weights = [1 / (dist + 1e-8) for dist, _ in neighbors]
                weighted_sum = sum(w * self.y_train[idx] for w, (_, idx) in zip(weights, neighbors))
                prediction = weighted_sum / sum(weights)
            else:
                # ç®€å•å¹³å‡
                neighbor_values = [self.y_train[idx] for _, idx in neighbors]
                prediction = np.mean(neighbor_values)
        
        return prediction
    
    def predict(self, X):
        """é¢„æµ‹å¤šä¸ªæ ·æœ¬"""
        X = np.array(X)
        predictions = []
        
        for x in X:
            pred = self.predict_single(x)
            predictions.append(pred)
        
        return np.array(predictions)
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡ï¼ˆä»…ç”¨äºåˆ†ç±»ï¼‰"""
        if self.task != 'classification':
            raise ValueError("predict_probaåªé€‚ç”¨äºåˆ†ç±»ä»»åŠ¡")
        
        X = np.array(X)
        probabilities = []
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„ç±»åˆ«
        unique_classes = np.unique(self.y_train)
        
        for x in X:
            neighbors = self._get_neighbors(x)
            neighbor_labels = [self.y_train[idx] for _, idx in neighbors]
            
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡
            class_probs = []
            for class_label in unique_classes:
                count = neighbor_labels.count(class_label)
                prob = count / self.k
                class_probs.append(prob)
            
            probabilities.append(class_probs)
        
        return np.array(probabilities), unique_classes

# æ¼”ç¤ºKNNåˆ†ç±»
def demonstrate_knn_classification():
    """æ¼”ç¤ºKNNåˆ†ç±»"""
    print("\nğŸ¯ === KNNåˆ†ç±»æ¼”ç¤º === ğŸ¯")
    
    # ç”Ÿæˆåˆ†ç±»æ•°æ®
    X, y = make_classification(
        n_samples=300, n_features=2, n_redundant=0, 
        n_informative=2, n_clusters_per_class=1, random_state=42
    )
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42
    )
    
    print(f"ğŸ“Š æ•°æ®ä¿¡æ¯:")
    print(f"  â€¢ è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  â€¢ æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    print(f"  â€¢ ç‰¹å¾ç»´åº¦: {X.shape[1]}")
    print(f"  â€¢ ç±»åˆ«æ•°é‡: {len(np.unique(y))}")
    
    # æµ‹è¯•ä¸åŒçš„Kå€¼
    k_values = [1, 3, 5, 7, 9, 15]
    accuracies = []
    
    plt.figure(figsize=(15, 10))
    
    for i, k in enumerate(k_values):
        # è®­ç»ƒKNN
        knn = SimpleKNN(k=k, task='classification')
        knn.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = knn.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        accuracies.append(accuracy)
        
        print(f"\nğŸ” K={k}: å‡†ç¡®ç‡ = {accuracy:.4f}")
        
        # å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
        plt.subplot(2, 3, i+1)
        
        # åˆ›å»ºç½‘æ ¼
        h = 0.02
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                            np.arange(y_min, y_max, h))
        
        # é¢„æµ‹ç½‘æ ¼ç‚¹
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        Z = knn.predict(grid_points)
        Z = Z.reshape(xx.shape)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)
        
        # ç»˜åˆ¶æ•°æ®ç‚¹
        scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, 
                            cmap=plt.cm.RdYlBu, edgecolors='black')
        plt.title(f'K={k}, å‡†ç¡®ç‡={accuracy:.3f}')
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
    
    plt.tight_layout()
    plt.show()
    
    # Kå€¼é€‰æ‹©åˆ†æ
    plt.figure(figsize=(10, 6))
    plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)
    plt.xlabel('Kå€¼')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('ä¸åŒKå€¼çš„åˆ†ç±»å‡†ç¡®ç‡')
    plt.grid(True, alpha=0.3)
    
    # æ ‡æ³¨æœ€ä½³Kå€¼
    best_k_idx = np.argmax(accuracies)
    best_k = k_values[best_k_idx]
    best_accuracy = accuracies[best_k_idx]
    
    plt.annotate(f'æœ€ä½³K={best_k}\nå‡†ç¡®ç‡={best_accuracy:.3f}', 
                xy=(best_k, best_accuracy), xytext=(best_k+2, best_accuracy-0.02),
                arrowprops=dict(arrowstyle='->', color='red'),
                fontsize=12, color='red')
    
    plt.show()
    
    print(f"\nğŸ† æœ€ä½³Kå€¼: {best_k}, æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
    
    return knn, best_k

knn_classifier, optimal_k = demonstrate_knn_classification()
```

### è·ç¦»åº¦é‡å¯¹æ¯”

```python
def compare_distance_metrics():
    """æ¯”è¾ƒä¸åŒè·ç¦»åº¦é‡çš„æ•ˆæœ"""
    print("\nğŸ“ === è·ç¦»åº¦é‡å¯¹æ¯” === ğŸ“")
    
    # ç”Ÿæˆæ•°æ®
    X, y = make_classification(
        n_samples=400, n_features=2, n_redundant=0,
        n_informative=2, n_clusters_per_class=1, random_state=42
    )
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42
    )
    
    # æµ‹è¯•ä¸åŒè·ç¦»åº¦é‡
    distance_metrics = ['euclidean', 'manhattan', 'minkowski', 'cosine']
    metric_names = ['æ¬§å‡ é‡Œå¾—è·ç¦»', 'æ›¼å“ˆé¡¿è·ç¦»', 'é—µå¯å¤«æ–¯åŸºè·ç¦»', 'ä½™å¼¦è·ç¦»']
    
    results = []
    
    plt.figure(figsize=(16, 4))
    
    for i, (metric, name) in enumerate(zip(distance_metrics, metric_names)):
        # è®­ç»ƒKNN
        knn = SimpleKNN(k=5, distance_metric=metric, task='classification')
        knn.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = knn.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        results.append((name, accuracy))
        
        print(f"ğŸ“Š {name}: å‡†ç¡®ç‡ = {accuracy:.4f}")
        
        # å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
        plt.subplot(1, 4, i+1)
        
        # åˆ›å»ºç½‘æ ¼
        h = 0.02
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                            np.arange(y_min, y_max, h))
        
        # é¢„æµ‹ç½‘æ ¼ç‚¹
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        Z = knn.predict(grid_points)
        Z = Z.reshape(xx.shape)
        
        # ç»˜åˆ¶
        plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)
        plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, 
                   cmap=plt.cm.RdYlBu, edgecolors='black')
        plt.title(f'{name}\nå‡†ç¡®ç‡={accuracy:.3f}')
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
    
    plt.tight_layout()
    plt.show()
    
    # ç»“æœæ€»ç»“
    results.sort(key=lambda x: x[1], reverse=True)
    print(f"\nğŸ† è·ç¦»åº¦é‡æ’å:")
    for i, (name, acc) in enumerate(results, 1):
        print(f"  {i}. {name}: {acc:.4f}")
    
    return results

distance_comparison = compare_distance_metrics()
```

## KNNå›å½’å®ç°

```python
def demonstrate_knn_regression():
    """æ¼”ç¤ºKNNå›å½’"""
    print("\nğŸ“ˆ === KNNå›å½’æ¼”ç¤º === ğŸ“ˆ")
    
    # ç”Ÿæˆå›å½’æ•°æ®
    X, y = make_regression(
        n_samples=300, n_features=1, noise=10, random_state=42
    )
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    print(f"ğŸ“Š å›å½’æ•°æ®ä¿¡æ¯:")
    print(f"  â€¢ è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  â€¢ æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    print(f"  â€¢ ç‰¹å¾ç»´åº¦: {X.shape[1]}")
    
    # æµ‹è¯•ä¸åŒKå€¼çš„å›å½’æ•ˆæœ
    k_values = [1, 3, 5, 10, 20]
    mse_scores = []
    
    plt.figure(figsize=(15, 10))
    
    for i, k in enumerate(k_values):
        # è®­ç»ƒKNNå›å½’
        knn_reg = SimpleKNN(k=k, task='regression')
        knn_reg.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = knn_reg.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        mse_scores.append(mse)
        
        print(f"\nğŸ” K={k}: MSE = {mse:.2f}")
        
        # å¯è§†åŒ–å›å½’ç»“æœ
        plt.subplot(2, 3, i+1)
        
        # åˆ›å»ºå¹³æ»‘çš„é¢„æµ‹çº¿
        X_plot = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
        y_plot = knn_reg.predict(X_plot)
        
        # ç»˜åˆ¶æ•°æ®ç‚¹å’Œé¢„æµ‹çº¿
        plt.scatter(X_train, y_train, alpha=0.6, label='è®­ç»ƒæ•°æ®', color='blue')
        plt.scatter(X_test, y_test, alpha=0.8, label='æµ‹è¯•æ•°æ®', color='red')
        plt.plot(X_plot, y_plot, color='green', linewidth=2, label=f'KNNé¢„æµ‹(K={k})')
        
        plt.title(f'K={k}, MSE={mse:.1f}')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    # æ·»åŠ æœ€åä¸€ä¸ªå­å›¾æ˜¾ç¤ºMSEå¯¹æ¯”
    plt.subplot(2, 3, 6)
    plt.plot(k_values, mse_scores, 'ro-', linewidth=2, markersize=8)
    plt.xlabel('Kå€¼')
    plt.ylabel('MSE')
    plt.title('ä¸åŒKå€¼çš„MSEå¯¹æ¯”')
    plt.grid(True, alpha=0.3)
    
    # æ ‡æ³¨æœ€ä½³Kå€¼
    best_k_idx = np.argmin(mse_scores)
    best_k = k_values[best_k_idx]
    best_mse = mse_scores[best_k_idx]
    
    plt.annotate(f'æœ€ä½³K={best_k}\nMSE={best_mse:.1f}', 
                xy=(best_k, best_mse), xytext=(best_k+2, best_mse+50),
                arrowprops=dict(arrowstyle='->', color='red'),
                fontsize=10, color='red')
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nğŸ† æœ€ä½³Kå€¼: {best_k}, æœ€ä½³MSE: {best_mse:.2f}")
    
    return knn_reg, best_k

knn_regressor, optimal_k_reg = demonstrate_knn_regression()
```

## é«˜çº§KNNå®ç°

### åŠ æƒKNNå’Œä¼˜åŒ–æŠ€æœ¯

```python
class AdvancedKNN:
    """é«˜çº§KNNå®ç°ï¼ŒåŒ…å«å¤šç§ä¼˜åŒ–æŠ€æœ¯"""
    
    def __init__(self, k=3, distance_metric='euclidean', 
                 weights='uniform', algorithm='brute', task='classification'):
        """
        å‚æ•°:
        k: è¿‘é‚»æ•°é‡
        distance_metric: è·ç¦»åº¦é‡
        weights: 'uniform' æˆ– 'distance'
        algorithm: 'brute', 'kd_tree', 'ball_tree'
        task: 'classification' æˆ– 'regression'
        """
        self.k = k
        self.distance_metric = distance_metric
        self.weights = weights
        self.algorithm = algorithm
        self.task = task
        self.X_train = None
        self.y_train = None
        self.tree = None
    
    def fit(self, X, y):
        """è®­ç»ƒKNN"""
        self.X_train = np.array(X)
        self.y_train = np.array(y)
        
        # æ„å»ºç©ºé—´ç´¢å¼•ç»“æ„ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if self.algorithm == 'kd_tree':
            print("ğŸŒ³ æ„å»ºKDæ ‘ç´¢å¼•...")
            # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨sklearnçš„KDTree
            self.tree = 'kd_tree_placeholder'
        elif self.algorithm == 'ball_tree':
            print("ğŸ€ æ„å»ºBallæ ‘ç´¢å¼•...")
            self.tree = 'ball_tree_placeholder'
        else:
            print("ğŸ’ª ä½¿ç”¨æš´åŠ›æœç´¢...")
        
        print(f"ğŸ“š é«˜çº§KNNè®­ç»ƒå®Œæˆ: {len(X)} æ ·æœ¬, ç®—æ³•={self.algorithm}")
        return self
    
    def _calculate_distance(self, x1, x2):
        """è®¡ç®—è·ç¦»"""
        if self.distance_metric == 'euclidean':
            return np.sqrt(np.sum((x1 - x2) ** 2))
        elif self.distance_metric == 'manhattan':
            return np.sum(np.abs(x1 - x2))
        elif self.distance_metric == 'chebyshev':
            return np.max(np.abs(x1 - x2))
        else:
            return np.sqrt(np.sum((x1 - x2) ** 2))  # é»˜è®¤æ¬§å‡ é‡Œå¾—
    
    def _get_neighbors_with_weights(self, x):
        """è·å–Kä¸ªæœ€è¿‘é‚»åŠå…¶æƒé‡"""
        distances = []
        
        # è®¡ç®—è·ç¦»
        for i, x_train in enumerate(self.X_train):
            dist = self._calculate_distance(x, x_train)
            distances.append((dist, i))
        
        # æ’åºå¹¶å–å‰Kä¸ª
        distances.sort(key=lambda x: x[0])
        neighbors = distances[:self.k]
        
        # è®¡ç®—æƒé‡
        if self.weights == 'uniform':
            weights = [1.0] * len(neighbors)
        else:  # distance weighting
            weights = []
            for dist, _ in neighbors:
                if dist == 0:
                    weights.append(float('inf'))  # å®Œå…¨ç›¸åŒçš„ç‚¹
                else:
                    weights.append(1.0 / dist)
        
        return neighbors, weights
    
    def predict_single_advanced(self, x):
        """é«˜çº§å•æ ·æœ¬é¢„æµ‹"""
        neighbors, weights = self._get_neighbors_with_weights(x)
        
        if self.task == 'classification':
            # åŠ æƒæŠ•ç¥¨
            class_weights = {}
            
            for (_, idx), weight in zip(neighbors, weights):
                label = self.y_train[idx]
                if label not in class_weights:
                    class_weights[label] = 0
                class_weights[label] += weight
            
            # è¿”å›æƒé‡æœ€å¤§çš„ç±»åˆ«
            prediction = max(class_weights, key=class_weights.get)
            
        else:  # regression
            # åŠ æƒå¹³å‡
            if any(w == float('inf') for w in weights):
                # æœ‰å®Œå…¨ç›¸åŒçš„ç‚¹ï¼Œåªè€ƒè™‘è¿™äº›ç‚¹
                exact_matches = [self.y_train[idx] for (_, idx), w in zip(neighbors, weights) if w == float('inf')]
                prediction = np.mean(exact_matches)
            else:
                weighted_sum = sum(w * self.y_train[idx] for (_, idx), w in zip(neighbors, weights))
                weight_sum = sum(weights)
                prediction = weighted_sum / weight_sum if weight_sum > 0 else 0
        
        return prediction
    
    def predict(self, X):
        """é¢„æµ‹å¤šä¸ªæ ·æœ¬"""
        X = np.array(X)
        predictions = []
        
        for x in X:
            pred = self.predict_single_advanced(x)
            predictions.append(pred)
        
        return np.array(predictions)
    
    def predict_proba_advanced(self, X):
        """é«˜çº§æ¦‚ç‡é¢„æµ‹"""
        if self.task != 'classification':
            raise ValueError("predict_probaåªé€‚ç”¨äºåˆ†ç±»ä»»åŠ¡")
        
        X = np.array(X)
        unique_classes = np.unique(self.y_train)
        probabilities = []
        
        for x in X:
            neighbors, weights = self._get_neighbors_with_weights(x)
            
            # è®¡ç®—åŠ æƒæ¦‚ç‡
            class_weights = {cls: 0 for cls in unique_classes}
            
            for (_, idx), weight in zip(neighbors, weights):
                label = self.y_train[idx]
                class_weights[label] += weight
            
            # å½’ä¸€åŒ–
            total_weight = sum(class_weights.values())
            if total_weight > 0:
                class_probs = [class_weights[cls] / total_weight for cls in unique_classes]
            else:
                class_probs = [1.0 / len(unique_classes)] * len(unique_classes)
            
            probabilities.append(class_probs)
        
        return np.array(probabilities), unique_classes

# æ¼”ç¤ºé«˜çº§KNN
def demonstrate_advanced_knn():
    """æ¼”ç¤ºé«˜çº§KNNåŠŸèƒ½"""
    print("\nğŸš€ === é«˜çº§KNNæ¼”ç¤º === ğŸš€")
    
    # ç”Ÿæˆæ•°æ®
    X, y = make_classification(
        n_samples=500, n_features=2, n_redundant=0,
        n_informative=2, n_clusters_per_class=2, random_state=42
    )
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42
    )
    
    # æ¯”è¾ƒä¸åŒæƒé‡æ–¹æ³•
    weight_methods = ['uniform', 'distance']
    weight_names = ['å‡åŒ€æƒé‡', 'è·ç¦»æƒé‡']
    
    plt.figure(figsize=(12, 5))
    
    for i, (method, name) in enumerate(zip(weight_methods, weight_names)):
        # è®­ç»ƒé«˜çº§KNN
        adv_knn = AdvancedKNN(k=7, weights=method, task='classification')
        adv_knn.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = adv_knn.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"ğŸ“Š {name}: å‡†ç¡®ç‡ = {accuracy:.4f}")
        
        # å¯è§†åŒ–
        plt.subplot(1, 2, i+1)
        
        # åˆ›å»ºç½‘æ ¼
        h = 0.02
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                            np.arange(y_min, y_max, h))
        
        # é¢„æµ‹ç½‘æ ¼ç‚¹
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        Z = adv_knn.predict(grid_points)
        Z = Z.reshape(xx.shape)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)
        plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, 
                   cmap=plt.cm.RdYlBu, edgecolors='black')
        plt.title(f'{name}\nå‡†ç¡®ç‡={accuracy:.3f}')
        plt.xlabel('ç‰¹å¾1')
        plt.ylabel('ç‰¹å¾2')
    
    plt.tight_layout()
    plt.show()
    
    return adv_knn

advanced_knn = demonstrate_advanced_knn()
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæ¨èç³»ç»Ÿ

```python
class MovieRecommendationKNN:
    """åŸºäºKNNçš„ç”µå½±æ¨èç³»ç»Ÿ"""
    
    def __init__(self, k=5, similarity_metric='cosine'):
        self.k = k
        self.similarity_metric = similarity_metric
        self.user_ratings = None
        self.movie_names = None
        self.user_names = None
    
    def fit(self, user_ratings, movie_names=None, user_names=None):
        """è®­ç»ƒæ¨èç³»ç»Ÿ"""
        self.user_ratings = np.array(user_ratings)
        self.movie_names = movie_names or [f'ç”µå½±{i}' for i in range(user_ratings.shape[1])]
        self.user_names = user_names or [f'ç”¨æˆ·{i}' for i in range(user_ratings.shape[0])]
        
        print(f"ğŸ¬ æ¨èç³»ç»Ÿè®­ç»ƒå®Œæˆ:")
        print(f"  â€¢ ç”¨æˆ·æ•°é‡: {len(self.user_names)}")
        print(f"  â€¢ ç”µå½±æ•°é‡: {len(self.movie_names)}")
        print(f"  â€¢ ç›¸ä¼¼åº¦åº¦é‡: {self.similarity_metric}")
        
        return self
    
    def _calculate_similarity(self, user1_ratings, user2_ratings):
        """è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦"""
        # åªè€ƒè™‘ä¸¤ä¸ªç”¨æˆ·éƒ½è¯„è¿‡åˆ†çš„ç”µå½±
        mask = (user1_ratings > 0) & (user2_ratings > 0)
        
        if np.sum(mask) == 0:
            return 0  # æ²¡æœ‰å…±åŒè¯„åˆ†çš„ç”µå½±
        
        ratings1 = user1_ratings[mask]
        ratings2 = user2_ratings[mask]
        
        if self.similarity_metric == 'cosine':
            # ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(ratings1, ratings2)
            norm1 = np.linalg.norm(ratings1)
            norm2 = np.linalg.norm(ratings2)
            
            if norm1 == 0 or norm2 == 0:
                return 0
            
            return dot_product / (norm1 * norm2)
        
        elif self.similarity_metric == 'pearson':
            # çš®å°”é€Šç›¸å…³ç³»æ•°
            if len(ratings1) < 2:
                return 0
            
            mean1 = np.mean(ratings1)
            mean2 = np.mean(ratings2)
            
            numerator = np.sum((ratings1 - mean1) * (ratings2 - mean2))
            denominator = np.sqrt(np.sum((ratings1 - mean1)**2) * np.sum((ratings2 - mean2)**2))
            
            if denominator == 0:
                return 0
            
            return numerator / denominator
        
        else:  # euclidean
            # æ¬§å‡ é‡Œå¾—è·ç¦»è½¬ç›¸ä¼¼åº¦
            distance = np.sqrt(np.sum((ratings1 - ratings2)**2))
            return 1 / (1 + distance)
    
    def find_similar_users(self, target_user_idx):
        """æ‰¾åˆ°ç›¸ä¼¼ç”¨æˆ·"""
        target_ratings = self.user_ratings[target_user_idx]
        similarities = []
        
        for i, user_ratings in enumerate(self.user_ratings):
            if i != target_user_idx:
                similarity = self._calculate_similarity(target_ratings, user_ratings)
                similarities.append((similarity, i))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[0], reverse=True)
        
        return similarities[:self.k]
    
    def recommend_movies(self, target_user_idx, n_recommendations=5):
        """ä¸ºç”¨æˆ·æ¨èç”µå½±"""
        target_ratings = self.user_ratings[target_user_idx]
        similar_users = self.find_similar_users(target_user_idx)
        
        print(f"\nğŸ¯ ä¸º {self.user_names[target_user_idx]} æ¨èç”µå½±:")
        print(f"ğŸ“Š æ‰¾åˆ° {len(similar_users)} ä¸ªç›¸ä¼¼ç”¨æˆ·")
        
        # è®¡ç®—ç”µå½±æ¨èåˆ†æ•°
        movie_scores = {}
        
        for movie_idx in range(len(self.movie_names)):
            if target_ratings[movie_idx] > 0:
                continue  # è·³è¿‡å·²è¯„åˆ†çš„ç”µå½±
            
            weighted_sum = 0
            similarity_sum = 0
            
            for similarity, user_idx in similar_users:
                if self.user_ratings[user_idx][movie_idx] > 0:
                    weighted_sum += similarity * self.user_ratings[user_idx][movie_idx]
                    similarity_sum += abs(similarity)
            
            if similarity_sum > 0:
                predicted_rating = weighted_sum / similarity_sum
                movie_scores[movie_idx] = predicted_rating
        
        # æ’åºå¹¶è¿”å›æ¨è
        recommendations = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ¬ æ¨èç”µå½± (Top {n_recommendations}):")
        print("-" * 40)
        
        for i, (movie_idx, score) in enumerate(recommendations[:n_recommendations], 1):
            print(f"{i}. {self.movie_names[movie_idx]}: é¢„æµ‹è¯„åˆ† {score:.2f}")
        
        return recommendations[:n_recommendations]
    
    def analyze_user_preferences(self, user_idx):
        """åˆ†æç”¨æˆ·åå¥½"""
        user_ratings = self.user_ratings[user_idx]
        rated_movies = [(i, rating) for i, rating in enumerate(user_ratings) if rating > 0]
        
        if not rated_movies:
            print(f"ç”¨æˆ· {self.user_names[user_idx]} è¿˜æ²¡æœ‰è¯„åˆ†è®°å½•")
            return
        
        # æŒ‰è¯„åˆ†æ’åº
        rated_movies.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ‘¤ {self.user_names[user_idx]} çš„è¯„åˆ†è®°å½•:")
        print("-" * 30)
        
        for movie_idx, rating in rated_movies:
            print(f"{self.movie_names[movie_idx]}: {rating}åˆ†")
        
        # ç»Ÿè®¡ä¿¡æ¯
        ratings = [rating for _, rating in rated_movies]
        print(f"\nğŸ“Š è¯„åˆ†ç»Ÿè®¡:")
        print(f"  â€¢ è¯„åˆ†ç”µå½±æ•°: {len(rated_movies)}")
        print(f"  â€¢ å¹³å‡è¯„åˆ†: {np.mean(ratings):.2f}")
        print(f"  â€¢ æœ€é«˜è¯„åˆ†: {max(ratings)}")
        print(f"  â€¢ æœ€ä½è¯„åˆ†: {min(ratings)}")

# æ¼”ç¤ºæ¨èç³»ç»Ÿ
def demonstrate_movie_recommendation():
    """æ¼”ç¤ºç”µå½±æ¨èç³»ç»Ÿ"""
    print("\nğŸ¬ === ç”µå½±æ¨èç³»ç»Ÿæ¼”ç¤º === ğŸ¬")
    
    # æ¨¡æ‹Ÿç”¨æˆ·-ç”µå½±è¯„åˆ†çŸ©é˜µ (0è¡¨ç¤ºæœªè¯„åˆ†)
    np.random.seed(42)
    
    # ç”¨æˆ·å’Œç”µå½±åç§°
    users = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank']
    movies = ['å¤ä»‡è€…è”ç›Ÿ', 'æ³°å¦å°¼å…‹å·', 'é˜¿å‡¡è¾¾', 'æ˜Ÿçƒå¤§æˆ˜', 'å“ˆåˆ©æ³¢ç‰¹', 
             'æŒ‡ç¯ç‹', 'é»‘å®¢å¸å›½', 'è‚–ç”³å…‹çš„æ•‘èµ', 'æ•™çˆ¶', 'ç›—æ¢¦ç©ºé—´']
    
    # åˆ›å»ºè¯„åˆ†çŸ©é˜µ (ç”¨æˆ· x ç”µå½±)
    ratings_matrix = np.array([
        [5, 3, 4, 5, 0, 4, 5, 0, 3, 4],  # Alice: å–œæ¬¢ç§‘å¹»åŠ¨ä½œ
        [2, 5, 3, 2, 5, 3, 2, 5, 4, 3],  # Bob: å–œæ¬¢å‰§æƒ…ç‰‡
        [5, 2, 5, 4, 0, 3, 4, 0, 2, 5],  # Charlie: å–œæ¬¢ç§‘å¹»
        [3, 4, 3, 3, 4, 4, 3, 4, 5, 3],  # Diana: å£å‘³å‡è¡¡
        [4, 3, 4, 4, 0, 5, 4, 0, 3, 4],  # Eve: å–œæ¬¢å¥‡å¹»å†’é™©
        [2, 4, 2, 2, 4, 3, 2, 5, 5, 2],  # Frank: å–œæ¬¢ç»å…¸å‰§æƒ…
    ])
    
    print(f"ğŸ“Š è¯„åˆ†çŸ©é˜µä¿¡æ¯:")
    print(f"  â€¢ ç”¨æˆ·æ•°: {len(users)}")
    print(f"  â€¢ ç”µå½±æ•°: {len(movies)}")
    print(f"  â€¢ æ€»è¯„åˆ†æ•°: {np.sum(ratings_matrix > 0)}")
    print(f"  â€¢ ç¨€ç–åº¦: {(1 - np.sum(ratings_matrix > 0) / ratings_matrix.size) * 100:.1f}%")
    
    # åˆ›å»ºæ¨èç³»ç»Ÿ
    recommender = MovieRecommendationKNN(k=3, similarity_metric='cosine')
    recommender.fit(ratings_matrix, movies, users)
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ†æåå¥½å’Œæ¨èç”µå½±
    for user_idx in range(len(users)):
        print(f"\n{'='*50}")
        recommender.analyze_user_preferences(user_idx)
        
        # æ¨èç”µå½±
        recommendations = recommender.recommend_movies(user_idx, n_recommendations=3)
        
        # æ˜¾ç¤ºç›¸ä¼¼ç”¨æˆ·
        similar_users = recommender.find_similar_users(user_idx)
        print(f"\nğŸ‘¥ æœ€ç›¸ä¼¼çš„ç”¨æˆ·:")
        for similarity, similar_user_idx in similar_users:
            print(f"  {users[similar_user_idx]}: ç›¸ä¼¼åº¦ {similarity:.3f}")
    
    return recommender

recommender_system = demonstrate_movie_recommendation()
```

### æ¡ˆä¾‹2ï¼šå›¾åƒåˆ†ç±»

```python
class ImageClassificationKNN:
    """åŸºäºKNNçš„ç®€å•å›¾åƒåˆ†ç±»"""
    
    def __init__(self, k=3, distance_metric='euclidean'):
        self.k = k
        self.distance_metric = distance_metric
        self.X_train = None
        self.y_train = None
        self.feature_extractor = None
    
    def extract_features(self, images):
        """æå–å›¾åƒç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        features = []
        
        for img in images:
            # ç®€å•ç‰¹å¾ï¼šåƒç´ å‡å€¼ã€æ ‡å‡†å·®ã€è¾¹ç¼˜å¯†åº¦ç­‰
            feature_vector = [
                np.mean(img),                    # å¹³å‡äº®åº¦
                np.std(img),                     # äº®åº¦æ ‡å‡†å·®
                np.mean(np.abs(np.diff(img, axis=0))),  # å‚ç›´è¾¹ç¼˜
                np.mean(np.abs(np.diff(img, axis=1))),  # æ°´å¹³è¾¹ç¼˜
                np.sum(img > np.mean(img)) / img.size,   # é«˜äº®åƒç´ æ¯”ä¾‹
            ]
            features.append(feature_vector)
        
        return np.array(features)
    
    def fit(self, images, labels):
        """è®­ç»ƒå›¾åƒåˆ†ç±»å™¨"""
        print("ğŸ–¼ï¸ æå–å›¾åƒç‰¹å¾...")
        self.X_train = self.extract_features(images)
        self.y_train = np.array(labels)
        
        print(f"ğŸ“Š å›¾åƒåˆ†ç±»å™¨è®­ç»ƒå®Œæˆ:")
        print(f"  â€¢ è®­ç»ƒå›¾åƒæ•°: {len(images)}")
        print(f"  â€¢ ç‰¹å¾ç»´åº¦: {self.X_train.shape[1]}")
        print(f"  â€¢ ç±»åˆ«æ•°: {len(np.unique(labels))}")
        
        return self
    
    def _calculate_distance(self, x1, x2):
        """è®¡ç®—ç‰¹å¾è·ç¦»"""
        if self.distance_metric == 'euclidean':
            return np.sqrt(np.sum((x1 - x2) ** 2))
        elif self.distance_metric == 'manhattan':
            return np.sum(np.abs(x1 - x2))
        else:
            return np.sqrt(np.sum((x1 - x2) ** 2))
    
    def predict(self, images):
        """é¢„æµ‹å›¾åƒç±»åˆ«"""
        print(f"ğŸ” é¢„æµ‹ {len(images)} å¼ å›¾åƒ...")
        
        # æå–æµ‹è¯•å›¾åƒç‰¹å¾
        X_test = self.extract_features(images)
        predictions = []
        
        for test_features in X_test:
            # è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
            distances = []
            for train_features in self.X_train:
                dist = self._calculate_distance(test_features, train_features)
                distances.append(dist)
            
            # æ‰¾åˆ°Kä¸ªæœ€è¿‘é‚»
            nearest_indices = np.argsort(distances)[:self.k]
            nearest_labels = self.y_train[nearest_indices]
            
            # å¤šæ•°æŠ•ç¥¨
            prediction = Counter(nearest_labels).most_common(1)[0][0]
            predictions.append(prediction)
        
        return np.array(predictions)
    
    def predict_with_confidence(self, images):
        """é¢„æµ‹å›¾åƒç±»åˆ«å¹¶è¿”å›ç½®ä¿¡åº¦"""
        X_test = self.extract_features(images)
        predictions = []
        confidences = []
        
        for test_features in X_test:
            # è®¡ç®—è·ç¦»
            distances = []
            for train_features in self.X_train:
                dist = self._calculate_distance(test_features, train_features)
                distances.append(dist)
            
            # æ‰¾åˆ°Kä¸ªæœ€è¿‘é‚»
            nearest_indices = np.argsort(distances)[:self.k]
            nearest_labels = self.y_train[nearest_indices]
            nearest_distances = np.array(distances)[nearest_indices]
            
            # è®¡ç®—åŠ æƒæŠ•ç¥¨
            label_weights = {}
            for label, dist in zip(nearest_labels, nearest_distances):
                weight = 1 / (dist + 1e-8)  # è·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§
                if label not in label_weights:
                    label_weights[label] = 0
                label_weights[label] += weight
            
            # é¢„æµ‹å’Œç½®ä¿¡åº¦
            total_weight = sum(label_weights.values())
            prediction = max(label_weights, key=label_weights.get)
            confidence = label_weights[prediction] / total_weight
            
            predictions.append(prediction)
            confidences.append(confidence)
        
        return np.array(predictions), np.array(confidences)

# æ¼”ç¤ºå›¾åƒåˆ†ç±»
def demonstrate_image_classification():
    """æ¼”ç¤ºå›¾åƒåˆ†ç±»"""
    print("\nğŸ–¼ï¸ === å›¾åƒåˆ†ç±»æ¼”ç¤º === ğŸ–¼ï¸")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒæ•°æ®
    np.random.seed(42)
    
    def generate_synthetic_images(n_samples, image_size=(28, 28), pattern_type='circle'):
        """ç”Ÿæˆåˆæˆå›¾åƒ"""
        images = []
        
        for _ in range(n_samples):
            img = np.random.normal(0.1, 0.05, image_size)  # èƒŒæ™¯å™ªå£°
            
            if pattern_type == 'circle':
                # ç”Ÿæˆåœ†å½¢å›¾æ¡ˆ
                center_x, center_y = image_size[0]//2, image_size[1]//2
                radius = np.random.randint(5, 10)
                
                for i in range(image_size[0]):
                    for j in range(image_size[1]):
                        if (i - center_x)**2 + (j - center_y)**2 <= radius**2:
                            img[i, j] = np.random.normal(0.8, 0.1)
            
            elif pattern_type == 'square':
                # ç”Ÿæˆæ–¹å½¢å›¾æ¡ˆ
                size = np.random.randint(8, 16)
                start_x = (image_size[0] - size) // 2
                start_y = (image_size[1] - size) // 2
                
                img[start_x:start_x+size, start_y:start_y+size] = np.random.normal(0.8, 0.1, (size, size))
            
            elif pattern_type == 'line':
                # ç”Ÿæˆçº¿æ¡å›¾æ¡ˆ
                if np.random.random() > 0.5:  # å‚ç›´çº¿
                    col = image_size[1] // 2
                    img[:, col-1:col+2] = np.random.normal(0.8, 0.1, (image_size[0], 3))
                else:  # æ°´å¹³çº¿
                    row = image_size[0] // 2
                    img[row-1:row+2, :] = np.random.normal(0.8, 0.1, (3, image_size[1]))
            
            # ç¡®ä¿åƒç´ å€¼åœ¨åˆç†èŒƒå›´å†…
            img = np.clip(img, 0, 1)
            images.append(img)
        
        return images
    
    # ç”Ÿæˆä¸‰ç±»å›¾åƒæ•°æ®
    print("ğŸ¨ ç”Ÿæˆåˆæˆå›¾åƒæ•°æ®...")
    
    circles = generate_synthetic_images(50, pattern_type='circle')
    squares = generate_synthetic_images(50, pattern_type='square')
    lines = generate_synthetic_images(50, pattern_type='line')
    
    # åˆå¹¶æ•°æ®
    all_images = circles + squares + lines
    all_labels = ['circle'] * 50 + ['square'] * 50 + ['line'] * 50
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    from sklearn.model_selection import train_test_split
    
    train_images, test_images, train_labels, test_labels = train_test_split(
        all_images, all_labels, test_size=0.3, random_state=42, stratify=all_labels
    )
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  â€¢ è®­ç»ƒå›¾åƒ: {len(train_images)}")
    print(f"  â€¢ æµ‹è¯•å›¾åƒ: {len(test_images)}")
    print(f"  â€¢ å›¾åƒå°ºå¯¸: {train_images[0].shape}")
    print(f"  â€¢ ç±»åˆ«: {set(all_labels)}")
    
    # è®­ç»ƒKNNå›¾åƒåˆ†ç±»å™¨
    img_classifier = ImageClassificationKNN(k=5, distance_metric='euclidean')
    img_classifier.fit(train_images, train_labels)
    
    # é¢„æµ‹
    predictions, confidences = img_classifier.predict_with_confidence(test_images)
    
    # è¯„ä¼°ç»“æœ
    accuracy = accuracy_score(test_labels, predictions)
    print(f"\nğŸ¯ åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    from sklearn.metrics import classification_report, confusion_matrix
    
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(test_labels, predictions))
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(test_labels, predictions)
    
    plt.figure(figsize=(12, 5))
    
    # æ··æ·†çŸ©é˜µå¯è§†åŒ–
    plt.subplot(1, 2, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['circle', 'line', 'square'],
                yticklabels=['circle', 'line', 'square'])
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹ç±»åˆ«')
    plt.ylabel('çœŸå®ç±»åˆ«')
    
    # ç½®ä¿¡åº¦åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    correct_mask = np.array(predictions) == np.array(test_labels)
    
    plt.hist(confidences[correct_mask], alpha=0.7, label='æ­£ç¡®é¢„æµ‹', bins=20, color='green')
    plt.hist(confidences[~correct_mask], alpha=0.7, label='é”™è¯¯é¢„æµ‹', bins=20, color='red')
    plt.xlabel('é¢„æµ‹ç½®ä¿¡åº¦')
    plt.ylabel('é¢‘æ¬¡')
    plt.title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    # æ˜¾ç¤ºä¸€äº›é¢„æµ‹ç¤ºä¾‹
    print(f"\nğŸ” é¢„æµ‹ç¤ºä¾‹:")
    print("-" * 50)
    
    for i in range(min(10, len(test_images))):
        true_label = test_labels[i]
        pred_label = predictions[i]
        confidence = confidences[i]
        status = "âœ…" if true_label == pred_label else "âŒ"
        
        print(f"{status} çœŸå®: {true_label:<6} | é¢„æµ‹: {pred_label:<6} | ç½®ä¿¡åº¦: {confidence:.3f}")
    
    return img_classifier

image_classifier = demonstrate_image_classification()
```

## Traeå®è·µç¯èŠ‚

### ä½¿ç”¨Traeæ„å»ºKNNç³»ç»Ÿ

```python
class TraeKNN:
    """Traeé£æ ¼çš„KNNå®ç°"""
    
    def __init__(self, k=3, distance_metric='euclidean', 
                 weights='uniform', task='classification'):
        """
        å‚æ•°:
        k: è¿‘é‚»æ•°é‡
        distance_metric: è·ç¦»åº¦é‡æ–¹æ³•
        weights: æƒé‡æ–¹æ³• ('uniform', 'distance')
        task: ä»»åŠ¡ç±»å‹ ('classification', 'regression')
        """
        self.k = k
        self.distance_metric = distance_metric
        self.weights = weights
        self.task = task
        self.X_train = None
        self.y_train = None
        self.feature_names = None
        self.class_names = None
        
    def trae_fit(self, X, y, feature_names=None):
        """Traeé£æ ¼çš„è®­ç»ƒæ–¹æ³•"""
        print(f"ğŸš€ Trae KNNå¼€å§‹è®­ç»ƒ...")
        print(f"ğŸ“Š ç®—æ³•é…ç½®:")
        print(f"  â€¢ Kå€¼: {self.k}")
        print(f"  â€¢ è·ç¦»åº¦é‡: {self.distance_metric}")
        print(f"  â€¢ æƒé‡æ–¹æ³•: {self.weights}")
        print(f"  â€¢ ä»»åŠ¡ç±»å‹: {self.task}")
        
        self.X_train = np.array(X)
        self.y_train = np.array(y)
        self.feature_names = feature_names or [f'ç‰¹å¾{i}' for i in range(X.shape[1])]
        
        if self.task == 'classification':
            self.class_names = np.unique(y)
        
        print(f"\nğŸ“ˆ æ•°æ®ä¿¡æ¯:")
        print(f"  â€¢ è®­ç»ƒæ ·æœ¬æ•°: {len(X)}")
        print(f"  â€¢ ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        
        if self.task == 'classification':
            print(f"  â€¢ ç±»åˆ«æ•°é‡: {len(self.class_names)}")
            print(f"  â€¢ ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
        else:
            print(f"  â€¢ ç›®æ ‡å€¼èŒƒå›´: [{np.min(y):.3f}, {np.max(y):.3f}]")
            print(f"  â€¢ ç›®æ ‡å€¼å‡å€¼: {np.mean(y):.3f}")
        
        print("âœ… è®­ç»ƒå®Œæˆ! (KNNæ˜¯æ‡’æƒ°å­¦ä¹ ï¼Œå®é™…è®¡ç®—åœ¨é¢„æµ‹æ—¶è¿›è¡Œ)")
        
        return self
    
    def _trae_calculate_distance(self, x1, x2):
        """Traeé£æ ¼çš„è·ç¦»è®¡ç®—"""
        if self.distance_metric == 'euclidean':
            return np.sqrt(np.sum((x1 - x2) ** 2))
        elif self.distance_metric == 'manhattan':
            return np.sum(np.abs(x1 - x2))
        elif self.distance_metric == 'chebyshev':
            return np.max(np.abs(x1 - x2))
        elif self.distance_metric == 'cosine':
            dot_product = np.dot(x1, x2)
            norm1 = np.linalg.norm(x1)
            norm2 = np.linalg.norm(x2)
            if norm1 == 0 or norm2 == 0:
                return 1
            return 1 - dot_product / (norm1 * norm2)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è·ç¦»åº¦é‡: {self.distance_metric}")
    
    def _trae_get_neighbors(self, x, return_distances=False):
        """Traeé£æ ¼çš„é‚»å±…æŸ¥æ‰¾"""
        distances = []
        
        for i, x_train in enumerate(self.X_train):
            dist = self._trae_calculate_distance(x, x_train)
            distances.append((dist, i))
        
        # æŒ‰è·ç¦»æ’åº
        distances.sort(key=lambda x: x[0])
        neighbors = distances[:self.k]
        
        if return_distances:
            return neighbors
        else:
            return [idx for _, idx in neighbors]
    
    def trae_predict_single(self, x, verbose=False):
        """Traeé£æ ¼çš„å•æ ·æœ¬é¢„æµ‹"""
        neighbors_with_dist = self._trae_get_neighbors(x, return_distances=True)
        
        if verbose:
            print(f"\nğŸ” æ ·æœ¬åˆ†æ: {x[:min(3, len(x))]}...")
            print(f"ğŸ“Š æ‰¾åˆ° {len(neighbors_with_dist)} ä¸ªæœ€è¿‘é‚»:")
            
            for i, (dist, idx) in enumerate(neighbors_with_dist, 1):
                label = self.y_train[idx]
                print(f"  {i}. è·ç¦»={dist:.4f}, æ ‡ç­¾={label}")
        
        if self.task == 'classification':
            if self.weights == 'uniform':
                # å‡åŒ€æƒé‡æŠ•ç¥¨
                neighbor_labels = [self.y_train[idx] for _, idx in neighbors_with_dist]
                prediction = Counter(neighbor_labels).most_common(1)[0][0]
                
                if verbose:
                    label_counts = Counter(neighbor_labels)
                    print(f"\nğŸ—³ï¸ æŠ•ç¥¨ç»“æœ:")
                    for label, count in label_counts.most_common():
                        print(f"  {label}: {count} ç¥¨")
                    print(f"ğŸ¯ é¢„æµ‹ç»“æœ: {prediction}")
                
            else:  # distance weighting
                class_weights = {}
                total_weight = 0
                
                for dist, idx in neighbors_with_dist:
                    label = self.y_train[idx]
                    weight = 1 / (dist + 1e-8)
                    
                    if label not in class_weights:
                        class_weights[label] = 0
                    class_weights[label] += weight
                    total_weight += weight
                
                prediction = max(class_weights, key=class_weights.get)
                
                if verbose:
                    print(f"\nâš–ï¸ åŠ æƒæŠ•ç¥¨ç»“æœ:")
                    for label, weight in sorted(class_weights.items(), key=lambda x: x[1], reverse=True):
                        prob = weight / total_weight
                        print(f"  {label}: æƒé‡={weight:.4f}, æ¦‚ç‡={prob:.4f}")
                    print(f"ğŸ¯ é¢„æµ‹ç»“æœ: {prediction}")
        
        else:  # regression
            if self.weights == 'uniform':
                neighbor_values = [self.y_train[idx] for _, idx in neighbors_with_dist]
                prediction = np.mean(neighbor_values)
                
                if verbose:
                    print(f"\nğŸ“Š é‚»å±…å€¼: {neighbor_values}")
                    print(f"ğŸ¯ é¢„æµ‹ç»“æœ (å‡å€¼): {prediction:.4f}")
                
            else:  # distance weighting
                weighted_sum = 0
                weight_sum = 0
                
                for dist, idx in neighbors_with_dist:
                    value = self.y_train[idx]
                    weight = 1 / (dist + 1e-8)
                    weighted_sum += weight * value
                    weight_sum += weight
                
                prediction = weighted_sum / weight_sum if weight_sum > 0 else 0
                
                if verbose:
                    print(f"\nâš–ï¸ åŠ æƒå¹³å‡:")
                    for dist, idx in neighbors_with_dist:
                        value = self.y_train[idx]
                        weight = 1 / (dist + 1e-8)
                        print(f"  å€¼={value:.4f}, æƒé‡={weight:.4f}")
                    print(f"ğŸ¯ é¢„æµ‹ç»“æœ (åŠ æƒå‡å€¼): {prediction:.4f}")
        
        return prediction
    
    def trae_predict(self, X, verbose=False):
        """Traeé£æ ¼çš„æ‰¹é‡é¢„æµ‹"""
        print(f"ğŸ”® å¼€å§‹é¢„æµ‹ {len(X)} ä¸ªæ ·æœ¬...")
        
        X = np.array(X)
        predictions = []
        
        for i, x in enumerate(X):
            if verbose and i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                print(f"\n--- æ ·æœ¬ {i+1} ---")
                pred = self.trae_predict_single(x, verbose=True)
            else:
                pred = self.trae_predict_single(x, verbose=False)
            
            predictions.append(pred)
        
        print(f"âœ… é¢„æµ‹å®Œæˆ!")
        return np.array(predictions)
    
    def trae_evaluate(self, X, y, verbose=True):
        """Traeé£æ ¼çš„æ¨¡å‹è¯„ä¼°"""
        print(f"ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        predictions = self.trae_predict(X, verbose=False)
        
        if self.task == 'classification':
            accuracy = accuracy_score(y, predictions)
            
            print(f"\nğŸ¯ è¯„ä¼°ç»“æœ:")
            print(f"  â€¢ å‡†ç¡®ç‡: {accuracy:.4f}")
            
            if verbose:
                from sklearn.metrics import classification_report, confusion_matrix
                
                print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
                print(classification_report(y, predictions))
                
                # æ··æ·†çŸ©é˜µ
                cm = confusion_matrix(y, predictions)
                print(f"\nğŸ” æ··æ·†çŸ©é˜µ:")
                print(cm)
            
            return accuracy
        
        else:  # regression
            mse = mean_squared_error(y, predictions)
            rmse = np.sqrt(mse)
            mae = np.mean(np.abs(y - predictions))
            
            print(f"\nğŸ¯ è¯„ä¼°ç»“æœ:")
            print(f"  â€¢ MSE: {mse:.4f}")
            print(f"  â€¢ RMSE: {rmse:.4f}")
            print(f"  â€¢ MAE: {mae:.4f}")
            
            if verbose:
                # è®¡ç®—RÂ²
                ss_res = np.sum((y - predictions) ** 2)
                ss_tot = np.sum((y - np.mean(y)) ** 2)
                r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
                
                print(f"  â€¢ RÂ²: {r2:.4f}")
                
                # æ®‹å·®åˆ†æ
                residuals = y - predictions
                print(f"\nğŸ“Š æ®‹å·®åˆ†æ:")
                print(f"  â€¢ æ®‹å·®å‡å€¼: {np.mean(residuals):.4f}")
                print(f"  â€¢ æ®‹å·®æ ‡å‡†å·®: {np.std(residuals):.4f}")
            
            return mse
    
    def trae_analyze_feature_importance(self, X, y, n_samples=100):
        """Traeé£æ ¼çš„ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        print(f"\nğŸ”¬ ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        
        X = np.array(X)
        y = np.array(y)
        
        # éšæœºé€‰æ‹©æ ·æœ¬è¿›è¡Œåˆ†æ
        if len(X) > n_samples:
            indices = np.random.choice(len(X), n_samples, replace=False)
            X_sample = X[indices]
            y_sample = y[indices]
        else:
            X_sample = X
            y_sample = y
        
        feature_importance = np.zeros(X.shape[1])
        
        for i in range(X.shape[1]):
            # æ‰°åŠ¨ç¬¬iä¸ªç‰¹å¾
            X_perturbed = X_sample.copy()
            X_perturbed[:, i] = np.random.permutation(X_perturbed[:, i])
            
            # è®¡ç®—æ€§èƒ½ä¸‹é™
            original_pred = self.trae_predict(X_sample, verbose=False)
            perturbed_pred = self.trae_predict(X_perturbed, verbose=False)
            
            if self.task == 'classification':
                original_acc = accuracy_score(y_sample, original_pred)
                perturbed_acc = accuracy_score(y_sample, perturbed_pred)
                importance = original_acc - perturbed_acc
            else:
                original_mse = mean_squared_error(y_sample, original_pred)
                perturbed_mse = mean_squared_error(y_sample, perturbed_pred)
                importance = perturbed_mse - original_mse  # MSEå¢åŠ é‡
            
            feature_importance[i] = max(0, importance)  # ç¡®ä¿éè´Ÿ
        
        # å½’ä¸€åŒ–
        if np.sum(feature_importance) > 0:
            feature_importance = feature_importance / np.sum(feature_importance)
        
        # æ’åºå¹¶æ˜¾ç¤º
        importance_ranking = sorted(enumerate(feature_importance), key=lambda x: x[1], reverse=True)
        
        print(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§æ’å:")
        print("-" * 40)
        
        for rank, (feature_idx, importance) in enumerate(importance_ranking, 1):
            feature_name = self.feature_names[feature_idx]
            print(f"{rank:2d}. {feature_name:<15}: {importance:.4f}")
        
        return feature_importance
    
    def trae_cross_validate(self, X, y, cv=5):
        """Traeé£æ ¼çš„äº¤å‰éªŒè¯"""
        print(f"\nğŸ”„ {cv}æŠ˜äº¤å‰éªŒè¯...")
        
        X = np.array(X)
        y = np.array(y)
        
        from sklearn.model_selection import KFold, StratifiedKFold
        
        if self.task == 'classification':
            kfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)
        else:
            kfold = KFold(n_splits=cv, shuffle=True, random_state=42)
        
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y), 1):
            print(f"\nğŸ“ ç¬¬ {fold} æŠ˜:")
            
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            # åˆ›å»ºä¸´æ—¶æ¨¡å‹
            temp_knn = TraeKNN(k=self.k, distance_metric=self.distance_metric, 
                              weights=self.weights, task=self.task)
            temp_knn.trae_fit(X_train_fold, y_train_fold)
            
            # è¯„ä¼°
            score = temp_knn.trae_evaluate(X_val_fold, y_val_fold, verbose=False)
            scores.append(score)
            
            metric_name = "å‡†ç¡®ç‡" if self.task == 'classification' else "MSE"
            print(f"  {metric_name}: {score:.4f}")
        
        # æ€»ç»“
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        
        print(f"\nğŸ† äº¤å‰éªŒè¯ç»“æœ:")
        print(f"  â€¢ å¹³å‡{metric_name}: {mean_score:.4f} Â± {std_score:.4f}")
        print(f"  â€¢ æœ€ä½³{metric_name}: {max(scores) if self.task == 'classification' else min(scores):.4f}")
        print(f"  â€¢ æœ€å·®{metric_name}: {min(scores) if self.task == 'classification' else max(scores):.4f}")
        
        return scores

# Trae KNNæ¼”ç¤º
def demonstrate_trae_knn():
    """æ¼”ç¤ºTrae KNN"""
    
    print("\nğŸŒŸ === Trae KNNæ¼”ç¤º === ğŸŒŸ")
    
    # æ¼”ç¤º1: åˆ†ç±»ä»»åŠ¡
    print("\nğŸ“Š æ¼”ç¤º1: åˆ†ç±»ä»»åŠ¡")
    print("=" * 40)
    
    # ç”Ÿæˆåˆ†ç±»æ•°æ®
    X_cls, y_cls = make_classification(
        n_samples=300, n_features=4, n_redundant=0, 
        n_informative=4, n_clusters_per_class=1, random_state=42
    )
    
    # ç‰¹å¾åç§°
    feature_names = ['èº«é«˜', 'ä½“é‡', 'å¹´é¾„', 'æ”¶å…¥']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_cls, y_cls, test_size=0.3, random_state=42
    )
    
    # åˆ›å»ºå’Œè®­ç»ƒTrae KNNåˆ†ç±»å™¨
    trae_knn_cls = TraeKNN(k=5, distance_metric='euclidean', 
                          weights='distance', task='classification')
    trae_knn_cls.trae_fit(X_train, y_train, feature_names)
    
    # è¯„ä¼°
    accuracy = trae_knn_cls.trae_evaluate(X_test, y_test)
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    importance = trae_knn_cls.trae_analyze_feature_importance(X_train, y_train)
    
    # äº¤å‰éªŒè¯
    cv_scores = trae_knn_cls.trae_cross_validate(X_train, y_train)
    
    # æ¼”ç¤º2: å›å½’ä»»åŠ¡
    print("\n\nğŸ“ˆ æ¼”ç¤º2: å›å½’ä»»åŠ¡")
    print("=" * 40)
    
    # ç”Ÿæˆå›å½’æ•°æ®
    X_reg, y_reg = make_regression(
        n_samples=300, n_features=3, noise=10, random_state=42
    )
    
    feature_names_reg = ['ç‰¹å¾A', 'ç‰¹å¾B', 'ç‰¹å¾C']
    
    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
        X_reg, y_reg, test_size=0.3, random_state=42
    )
    
    # åˆ›å»ºå’Œè®­ç»ƒTrae KNNå›å½’å™¨
    trae_knn_reg = TraeKNN(k=7, distance_metric='euclidean', 
                          weights='distance', task='regression')
    trae_knn_reg.trae_fit(X_train_reg, y_train_reg, feature_names_reg)
    
    # è¯„ä¼°
    mse = trae_knn_reg.trae_evaluate(X_test_reg, y_test_reg)
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    importance_reg = trae_knn_reg.trae_analyze_feature_importance(X_train_reg, y_train_reg)
    
    # äº¤å‰éªŒè¯
    cv_scores_reg = trae_knn_reg.trae_cross_validate(X_train_reg, y_train_reg)
    
    print(f"\n\nğŸ† Trae KNNæ¼”ç¤ºæ€»ç»“:")
    print("=" * 50)
    print(f"ğŸ“ˆ åˆ†ç±»ä»»åŠ¡å‡†ç¡®ç‡:     {accuracy:.4f}")
    print(f"ğŸ“Š å›å½’ä»»åŠ¡MSE:        {mse:.4f}")
    print(f"ğŸ”„ åˆ†ç±»äº¤å‰éªŒè¯å‡å€¼:   {np.mean(cv_scores):.4f}")
    print(f"ğŸ”„ å›å½’äº¤å‰éªŒè¯å‡å€¼:   {np.mean(cv_scores_reg):.4f}")
    
    return trae_knn_cls, trae_knn_reg

trae_models = demonstrate_trae_knn()
```

## ç®—æ³•ä¼˜åŒ–æŠ€æœ¯

### Kå€¼é€‰æ‹©å’Œæ¨¡å‹è°ƒä¼˜

```python
def optimize_knn_parameters():
    """KNNå‚æ•°ä¼˜åŒ–"""
    print("\nâš™ï¸ === KNNå‚æ•°ä¼˜åŒ– === âš™ï¸")
    
    # ç”Ÿæˆæ•°æ®
    X, y = make_classification(
        n_samples=500, n_features=10, n_informative=5,
        n_redundant=2, n_clusters_per_class=2, random_state=42
    )
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42
    )
    
    # 1. Kå€¼ä¼˜åŒ–
    print("ğŸ” 1. Kå€¼ä¼˜åŒ–")
    print("-" * 20)
    
    k_range = range(1, 31)
    train_scores = []
    val_scores = []
    
    from sklearn.model_selection import cross_val_score
    from sklearn.neighbors import KNeighborsClassifier
    
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        
        # è®­ç»ƒé›†æ€§èƒ½
        knn.fit(X_train, y_train)
        train_score = knn.score(X_train, y_train)
        train_scores.append(train_score)
        
        # äº¤å‰éªŒè¯æ€§èƒ½
        cv_scores = cross_val_score(knn, X_train, y_train, cv=5)
        val_scores.append(cv_scores.mean())
    
    # å¯è§†åŒ–Kå€¼é€‰æ‹©
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(k_range, train_scores, 'bo-', label='è®­ç»ƒé›†å‡†ç¡®ç‡', alpha=0.8)
    plt.plot(k_range, val_scores, 'ro-', label='éªŒè¯é›†å‡†ç¡®ç‡', alpha=0.8)
    plt.xlabel('Kå€¼')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('Kå€¼ vs å‡†ç¡®ç‡')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ‰¾åˆ°æœ€ä½³Kå€¼
    best_k = k_range[np.argmax(val_scores)]
    best_val_score = max(val_scores)
    
    plt.axvline(x=best_k, color='green', linestyle='--', alpha=0.7)
    plt.text(best_k+1, best_val_score-0.02, f'æœ€ä½³K={best_k}', 
             bbox=dict(boxstyle='round', facecolor='lightgreen'))
    
    print(f"æœ€ä½³Kå€¼: {best_k}, éªŒè¯å‡†ç¡®ç‡: {best_val_score:.4f}")
    
    # 2. è·ç¦»åº¦é‡ä¼˜åŒ–
    print("\nğŸ“ 2. è·ç¦»åº¦é‡ä¼˜åŒ–")
    print("-" * 25)
    
    distance_metrics = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']
    distance_scores = []
    
    for metric in distance_metrics:
        if metric == 'minkowski':
            knn = KNeighborsClassifier(n_neighbors=best_k, metric=metric, p=3)
        else:
            knn = KNeighborsClassifier(n_neighbors=best_k, metric=metric)
        
        cv_scores = cross_val_score(knn, X_train, y_train, cv=5)
        score = cv_scores.mean()
        distance_scores.append(score)
        
        print(f"{metric:<12}: {score:.4f}")
    
    # å¯è§†åŒ–è·ç¦»åº¦é‡å¯¹æ¯”
    plt.subplot(1, 3, 2)
    bars = plt.bar(distance_metrics, distance_scores, 
                   color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])
    plt.ylabel('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
    plt.title('ä¸åŒè·ç¦»åº¦é‡çš„æ€§èƒ½')
    plt.xticks(rotation=45)
    
    # æ ‡æ³¨æœ€ä½³è·ç¦»åº¦é‡
    best_metric_idx = np.argmax(distance_scores)
    best_metric = distance_metrics[best_metric_idx]
    best_metric_score = distance_scores[best_metric_idx]
    
    bars[best_metric_idx].set_color('gold')
    plt.text(best_metric_idx, best_metric_score + 0.005, 
             f'{best_metric_score:.3f}', ha='center', va='bottom', fontweight='bold')
    
    print(f"\næœ€ä½³è·ç¦»åº¦é‡: {best_metric}, å‡†ç¡®ç‡: {best_metric_score:.4f}")
    
    # 3. æƒé‡æ–¹æ³•ä¼˜åŒ–
    print("\nâš–ï¸ 3. æƒé‡æ–¹æ³•ä¼˜åŒ–")
    print("-" * 22)
    
    weight_methods = ['uniform', 'distance']
    weight_scores = []
    
    for weights in weight_methods:
        knn = KNeighborsClassifier(n_neighbors=best_k, metric=best_metric, weights=weights)
        cv_scores = cross_val_score(knn, X_train, y_train, cv=5)
        score = cv_scores.mean()
        weight_scores.append(score)
        
        print(f"{weights:<10}: {score:.4f}")
    
    # å¯è§†åŒ–æƒé‡æ–¹æ³•å¯¹æ¯”
    plt.subplot(1, 3, 3)
    bars = plt.bar(weight_methods, weight_scores, color=['lightblue', 'lightpink'])
    plt.ylabel('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
    plt.title('ä¸åŒæƒé‡æ–¹æ³•çš„æ€§èƒ½')
    
    best_weight_idx = np.argmax(weight_scores)
    best_weights = weight_methods[best_weight_idx]
    best_weight_score = weight_scores[best_weight_idx]
    
    bars[best_weight_idx].set_color('gold')
    plt.text(best_weight_idx, best_weight_score + 0.005, 
             f'{best_weight_score:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    print(f"\næœ€ä½³æƒé‡æ–¹æ³•: {best_weights}, å‡†ç¡®ç‡: {best_weight_score:.4f}")
    
    # 4. æœ€ç»ˆæ¨¡å‹æµ‹è¯•
    print("\nğŸ† 4. æœ€ç»ˆä¼˜åŒ–æ¨¡å‹æµ‹è¯•")
    print("-" * 30)
    
    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_knn = KNeighborsClassifier(
        n_neighbors=best_k, 
        metric=best_metric, 
        weights=best_weights
    )
    
    final_knn.fit(X_train, y_train)
    final_accuracy = final_knn.score(X_test, y_test)
    
    # ä¸é»˜è®¤å‚æ•°å¯¹æ¯”
    default_knn = KNeighborsClassifier()
    default_knn.fit(X_train, y_train)
    default_accuracy = default_knn.score(X_test, y_test)
    
    print(f"é»˜è®¤å‚æ•°å‡†ç¡®ç‡: {default_accuracy:.4f}")
    print(f"ä¼˜åŒ–å‚æ•°å‡†ç¡®ç‡: {final_accuracy:.4f}")
    print(f"æ€§èƒ½æå‡: {(final_accuracy - default_accuracy) * 100:.2f}%")
    
    # è¿”å›æœ€ä½³å‚æ•°
    best_params = {
        'k': best_k,
        'metric': best_metric,
        'weights': best_weights,
        'accuracy': final_accuracy
    }
    
    return best_params, final_knn

best_params, optimized_knn = optimize_knn_parameters()
```

## æ€è€ƒé¢˜

1. **Kå€¼é€‰æ‹©**ï¼šKå€¼è¿‡å¤§æˆ–è¿‡å°ä¼šå¯¼è‡´ä»€ä¹ˆé—®é¢˜ï¼Ÿå¦‚ä½•é€‰æ‹©åˆé€‚çš„Kå€¼ï¼Ÿ

2. **è·ç¦»åº¦é‡**ï¼šåœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©æ›¼å“ˆé¡¿è·ç¦»è€Œä¸æ˜¯æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Ÿ

3. **ç»´åº¦è¯…å’’**ï¼šé«˜ç»´æ•°æ®å¯¹KNNç®—æ³•æœ‰ä»€ä¹ˆå½±å“ï¼Ÿå¦‚ä½•ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Ÿ

4. **è®¡ç®—å¤æ‚åº¦**ï¼šKNNçš„æ—¶é—´å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿå¦‚ä½•ä¼˜åŒ–é¢„æµ‹é€Ÿåº¦ï¼Ÿ

5. **æ•°æ®é¢„å¤„ç†**ï¼šä¸ºä»€ä¹ˆKNNç®—æ³•é€šå¸¸éœ€è¦ç‰¹å¾æ ‡å‡†åŒ–ï¼Ÿ

## æœ¬èŠ‚å°ç»“

Kè¿‘é‚»ç®—æ³•æ˜¯ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„éå‚æ•°å­¦ä¹ æ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### æ ¸å¿ƒä¼˜åŠ¿
- **ç®€å•ç›´è§‚**ï¼šç®—æ³•é€»è¾‘å®¹æ˜“ç†è§£å’Œå®ç°
- **éå‚æ•°æ–¹æ³•**ï¼šä¸å¯¹æ•°æ®åˆ†å¸ƒåšå‡è®¾
- **å¤šä»»åŠ¡é€‚ç”¨**ï¼šåŒæ—¶æ”¯æŒåˆ†ç±»å’Œå›å½’ä»»åŠ¡
- **å±€éƒ¨é€‚åº”æ€§**ï¼šèƒ½å¤Ÿæ•æ‰æ•°æ®çš„å±€éƒ¨æ¨¡å¼
- **ç†è®ºåŸºç¡€**ï¼šæœ‰åšå®çš„ç»Ÿè®¡å­¦ç†è®ºæ”¯æ’‘

### å…³é”®æŠ€æœ¯
- **è·ç¦»åº¦é‡**ï¼šæ¬§å‡ é‡Œå¾—ã€æ›¼å“ˆé¡¿ã€ä½™å¼¦ç­‰å¤šç§é€‰æ‹©
- **Kå€¼é€‰æ‹©**ï¼šå¹³è¡¡åå·®å’Œæ–¹å·®çš„é‡è¦å‚æ•°
- **æƒé‡æ–¹æ³•**ï¼šå‡åŒ€æƒé‡vsè·ç¦»åŠ æƒ
- **ä¼˜åŒ–æŠ€æœ¯**ï¼šKDæ ‘ã€Ballæ ‘ç­‰ç©ºé—´ç´¢å¼•ç»“æ„

### å®é™…åº”ç”¨
- **æ¨èç³»ç»Ÿ**ï¼šåŸºäºç”¨æˆ·ç›¸ä¼¼åº¦çš„ååŒè¿‡æ»¤
- **å›¾åƒè¯†åˆ«**ï¼šåŸºäºç‰¹å¾ç›¸ä¼¼åº¦çš„å›¾åƒåˆ†ç±»
- **å¼‚å¸¸æ£€æµ‹**ï¼šè¯†åˆ«ä¸æ­£å¸¸æ ·æœ¬å·®å¼‚è¾ƒå¤§çš„å¼‚å¸¸ç‚¹
- **æ–‡æœ¬åˆ†ç±»**ï¼šåŸºäºæ–‡æ¡£ç›¸ä¼¼åº¦çš„åˆ†ç±»ä»»åŠ¡

### ä½¿ç”¨å»ºè®®
- **æ•°æ®é¢„å¤„ç†**ï¼šè¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–å’Œé™ç»´
- **å‚æ•°è°ƒä¼˜**ï¼šé€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³Kå€¼å’Œè·ç¦»åº¦é‡
- **æ€§èƒ½ä¼˜åŒ–**ï¼šä½¿ç”¨ç©ºé—´ç´¢å¼•ç»“æ„åŠ é€Ÿè¿‘é‚»æœç´¢
- **ç‰¹å¾å·¥ç¨‹**ï¼šé€‰æ‹©ç›¸å…³ç‰¹å¾ï¼Œé¿å…ç»´åº¦è¯…å’’

### å±€é™æ€§
- **è®¡ç®—å¤æ‚åº¦**ï¼šé¢„æµ‹æ—¶éœ€è¦è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
- **å­˜å‚¨éœ€æ±‚**ï¼šéœ€è¦å­˜å‚¨æ‰€æœ‰è®­ç»ƒæ•°æ®
- **ç»´åº¦è¯…å’’**ï¼šé«˜ç»´ç©ºé—´ä¸­è·ç¦»åº¦é‡å¤±æ•ˆ
- **ä¸å¹³è¡¡æ•°æ®**ï¼šå¤šæ•°ç±»åˆ«å®¹æ˜“ä¸»å¯¼é¢„æµ‹ç»“æœ

### ä¸‹ä¸€æ­¥å­¦ä¹ 
- **é›†æˆæ–¹æ³•**ï¼šäº†è§£å¦‚ä½•å°†KNNä¸å…¶ä»–ç®—æ³•ç»“åˆ
- **æ·±åº¦å­¦ä¹ **ï¼šå­¦ä¹ ç¥ç»ç½‘ç»œåœ¨ç›¸ä¼¼åº¦å­¦ä¹ ä¸­çš„åº”ç”¨
- **è¿‘ä¼¼ç®—æ³•**ï¼šç ”ç©¶å¤§è§„æ¨¡æ•°æ®çš„è¿‘ä¼¼è¿‘é‚»æœç´¢æ–¹æ³•

KNNç®—æ³•è™½ç„¶ç®€å•ï¼Œä½†åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®é‡é€‚ä¸­ã€ç‰¹å¾ç»´åº¦ä¸å¤ªé«˜çš„åœºæ™¯ä¸­ã€‚å®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç†è§£æœºå™¨å­¦ä¹ åŸºæœ¬åŸç†çš„è‰¯å¥½èµ·ç‚¹ã€‚
```