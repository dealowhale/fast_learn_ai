# 1.4.6 ä¼˜åŒ–ç®—æ³•

## æ¦‚è¿°

ä¼˜åŒ–ç®—æ³•æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œè´Ÿè´£è°ƒæ•´ç¥ç»ç½‘ç»œçš„å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç®—æ³•å¯¹æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½éƒ½æœ‰é‡è¦å½±å“ã€‚æœ¬èŠ‚å°†æ·±å…¥ä»‹ç»å„ç§ä¼˜åŒ–ç®—æ³•çš„åŸç†ã€å®ç°å’Œåº”ç”¨åœºæ™¯ã€‚

```mermaid
graph TD
    A[ä¼˜åŒ–ç®—æ³•] --> B[ä¸€é˜¶æ–¹æ³•]
    A --> C[äºŒé˜¶æ–¹æ³•]
    A --> D[è‡ªé€‚åº”æ–¹æ³•]
    
    B --> B1[æ¢¯åº¦ä¸‹é™]
    B --> B2[éšæœºæ¢¯åº¦ä¸‹é™]
    B --> B3[å°æ‰¹é‡æ¢¯åº¦ä¸‹é™]
    B --> B4[åŠ¨é‡æ³•]
    
    C --> C1[ç‰›é¡¿æ³•]
    C --> C2[æ‹Ÿç‰›é¡¿æ³•]
    
    D --> D1[AdaGrad]
    D --> D2[RMSprop]
    D --> D3[Adam]
    D --> D4[AdamW]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
```

## 1. æ¢¯åº¦ä¸‹é™åŸºç¡€

### 1.1 åŸºæœ¬æ¢¯åº¦ä¸‹é™

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.animation import FuncAnimation
import seaborn as sns

class GradientDescentOptimizer:
    """åŸºæœ¬æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate
        self.history = []
    
    def step(self, params, gradients):
        """æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–"""
        updated_params = []
        for param, grad in zip(params, gradients):
            new_param = param - self.learning_rate * grad
            updated_params.append(new_param)
        
        # è®°å½•å†å²
        self.history.append([p.copy() for p in updated_params])
        return updated_params
    
    def reset_history(self):
        """é‡ç½®å†å²è®°å½•"""
        self.history = []

class OptimizationVisualizer:
    """ä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–å·¥å…·"""
    
    def __init__(self):
        self.colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']
    
    def create_test_function(self, function_type='quadratic'):
        """åˆ›å»ºæµ‹è¯•å‡½æ•°"""
        if function_type == 'quadratic':
            # ç®€å•äºŒæ¬¡å‡½æ•°
            def f(x, y):
                return x**2 + y**2
            
            def grad_f(x, y):
                return np.array([2*x, 2*y])
        
        elif function_type == 'rosenbrock':
            # Rosenbrockå‡½æ•°
            def f(x, y):
                return (1 - x)**2 + 100 * (y - x**2)**2
            
            def grad_f(x, y):
                dx = -2 * (1 - x) - 400 * x * (y - x**2)
                dy = 200 * (y - x**2)
                return np.array([dx, dy])
        
        elif function_type == 'beale':
            # Bealeå‡½æ•°
            def f(x, y):
                term1 = (1.5 - x + x*y)**2
                term2 = (2.25 - x + x*y**2)**2
                term3 = (2.625 - x + x*y**3)**2
                return term1 + term2 + term3
            
            def grad_f(x, y):
                dx = (2*(1.5 - x + x*y)*(y - 1) + 
                      2*(2.25 - x + x*y**2)*(y**2 - 1) + 
                      2*(2.625 - x + x*y**3)*(y**3 - 1))
                dy = (2*(1.5 - x + x*y)*x + 
                      2*(2.25 - x + x*y**2)*2*x*y + 
                      2*(2.625 - x + x*y**3)*3*x*y**2)
                return np.array([dx, dy])
        
        return f, grad_f
    
    def plot_optimization_path(self, optimizers, function_type='quadratic', 
                             start_point=None, num_iterations=100):
        """å¯è§†åŒ–ä¼˜åŒ–è·¯å¾„"""
        f, grad_f = self.create_test_function(function_type)
        
        if start_point is None:
            if function_type == 'quadratic':
                start_point = [3.0, 2.0]
            elif function_type == 'rosenbrock':
                start_point = [-1.0, 1.0]
            else:
                start_point = [1.0, 1.0]
        
        # åˆ›å»ºç½‘æ ¼ç”¨äºç­‰é«˜çº¿å›¾
        if function_type == 'quadratic':
            x_range = np.linspace(-4, 4, 100)
            y_range = np.linspace(-3, 3, 100)
        elif function_type == 'rosenbrock':
            x_range = np.linspace(-2, 2, 100)
            y_range = np.linspace(-1, 3, 100)
        else:
            x_range = np.linspace(-1, 4, 100)
            y_range = np.linspace(-1, 4, 100)
        
        X, Y = np.meshgrid(x_range, y_range)
        Z = f(X, Y)
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # å·¦å›¾ï¼šç­‰é«˜çº¿å›¾ + ä¼˜åŒ–è·¯å¾„
        ax1 = axes[0]
        contour = ax1.contour(X, Y, Z, levels=20, alpha=0.6)
        ax1.clabel(contour, inline=True, fontsize=8)
        
        # è¿è¡Œä¸åŒä¼˜åŒ–å™¨
        for i, (name, optimizer) in enumerate(optimizers.items()):
            optimizer.reset_history()
            
            # åˆå§‹åŒ–å‚æ•°
            params = [np.array(start_point)]
            
            # ä¼˜åŒ–è¿‡ç¨‹
            for iteration in range(num_iterations):
                current_params = params[-1]
                gradients = [grad_f(current_params[0], current_params[1])]
                
                new_params = optimizer.step(params, gradients)
                params = new_params
                
                # æ£€æŸ¥æ”¶æ•›
                if np.linalg.norm(gradients[0]) < 1e-6:
                    break
            
            # ç»˜åˆ¶è·¯å¾„
            path = np.array([p[0] for p in optimizer.history])
            ax1.plot(path[:, 0], path[:, 1], 'o-', 
                    color=self.colors[i % len(self.colors)], 
                    label=f'{name} ({len(path)} steps)',
                    markersize=4, linewidth=2, alpha=0.8)
            
            # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
            ax1.plot(start_point[0], start_point[1], 's', 
                    color=self.colors[i % len(self.colors)], 
                    markersize=8, alpha=0.7)
            ax1.plot(path[-1, 0], path[-1, 1], '*', 
                    color=self.colors[i % len(self.colors)], 
                    markersize=12, alpha=0.9)
        
        ax1.set_xlabel('x')
        ax1.set_ylabel('y')
        ax1.set_title(f'{function_type.title()} Function Optimization Paths')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³å›¾ï¼šæŸå¤±å‡½æ•°æ”¶æ•›æ›²çº¿
        ax2 = axes[1]
        for i, (name, optimizer) in enumerate(optimizers.items()):
            if optimizer.history:
                losses = [f(p[0][0], p[0][1]) for p in optimizer.history]
                ax2.semilogy(losses, 'o-', 
                           color=self.colors[i % len(self.colors)], 
                           label=name, linewidth=2, markersize=3)
        
        ax2.set_xlabel('Iteration')
        ax2.set_ylabel('Loss (log scale)')
        ax2.set_title('Convergence Curves')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°æ”¶æ•›ä¿¡æ¯
        print(f"\nğŸ“Š {function_type.title()} Function Optimization Results:")
        print(f"{'Optimizer':<15} {'Steps':<8} {'Final Loss':<12} {'Final Point':<20}")
        print("-" * 60)
        
        for name, optimizer in optimizers.items():
            if optimizer.history:
                final_point = optimizer.history[-1][0]
                final_loss = f(final_point[0], final_point[1])
                steps = len(optimizer.history)
                print(f"{name:<15} {steps:<8} {final_loss:<12.6f} [{final_point[0]:.4f}, {final_point[1]:.4f}]")
    
    def compare_learning_rates(self, learning_rates, function_type='quadratic'):
        """æ¯”è¾ƒä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ"""
        f, grad_f = self.create_test_function(function_type)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        start_point = [2.0, 1.5] if function_type == 'quadratic' else [-1.0, 1.0]
        
        for idx, lr in enumerate(learning_rates):
            ax = axes[idx]
            
            # åˆ›å»ºç½‘æ ¼
            if function_type == 'quadratic':
                x_range = np.linspace(-3, 3, 50)
                y_range = np.linspace(-2, 2, 50)
            else:
                x_range = np.linspace(-2, 2, 50)
                y_range = np.linspace(-1, 3, 50)
            
            X, Y = np.meshgrid(x_range, y_range)
            Z = f(X, Y)
            
            # ç»˜åˆ¶ç­‰é«˜çº¿
            contour = ax.contour(X, Y, Z, levels=15, alpha=0.6)
            
            # ä¼˜åŒ–è¿‡ç¨‹
            optimizer = GradientDescentOptimizer(learning_rate=lr)
            params = [np.array(start_point)]
            
            for iteration in range(100):
                current_params = params[-1]
                gradients = [grad_f(current_params[0], current_params[1])]
                
                new_params = optimizer.step(params, gradients)
                params = new_params
                
                # æ£€æŸ¥æ”¶æ•›æˆ–å‘æ•£
                if (np.linalg.norm(gradients[0]) < 1e-6 or 
                    np.any(np.abs(params[-1]) > 10)):
                    break
            
            # ç»˜åˆ¶è·¯å¾„
            if optimizer.history:
                path = np.array([p[0] for p in optimizer.history])
                ax.plot(path[:, 0], path[:, 1], 'ro-', 
                       markersize=3, linewidth=1.5, alpha=0.8)
                ax.plot(start_point[0], start_point[1], 'gs', markersize=8)
                
                if len(path) > 0:
                    ax.plot(path[-1, 0], path[-1, 1], 'b*', markersize=12)
            
            ax.set_title(f'Learning Rate = {lr}')
            ax.set_xlabel('x')
            ax.set_ylabel('y')
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ æ”¶æ•›ä¿¡æ¯
            if optimizer.history:
                final_loss = f(path[-1, 0], path[-1, 1])
                ax.text(0.02, 0.98, f'Steps: {len(path)}\nFinal Loss: {final_loss:.4f}', 
                       transform=ax.transAxes, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
        
        return learning_rates

# åŸºç¡€æ¢¯åº¦ä¸‹é™æ¼”ç¤º
print("=" * 80)
print("ğŸ“ˆ åŸºç¡€æ¢¯åº¦ä¸‹é™ç®—æ³•")
print("=" * 80)

# åˆ›å»ºå¯è§†åŒ–å·¥å…·
visualizer = OptimizationVisualizer()

# æ¯”è¾ƒä¸åŒå­¦ä¹ ç‡
print("\nğŸ¯ å­¦ä¹ ç‡å¯¹ä¼˜åŒ–æ•ˆæœçš„å½±å“:")
learning_rates = [0.001, 0.01, 0.1, 0.5]
visualizer.compare_learning_rates(learning_rates, 'quadratic')

# åˆ›å»ºä¸åŒçš„ä¼˜åŒ–å™¨è¿›è¡Œæ¯”è¾ƒ
optimizers = {
    'GD (lr=0.01)': GradientDescentOptimizer(0.01),
    'GD (lr=0.05)': GradientDescentOptimizer(0.05),
    'GD (lr=0.1)': GradientDescentOptimizer(0.1)
}

# åœ¨ä¸åŒå‡½æ•°ä¸Šæµ‹è¯•
print("\nğŸ¯ åœ¨ä¸åŒå‡½æ•°ä¸Šçš„ä¼˜åŒ–è¡¨ç°:")
for func_type in ['quadratic', 'rosenbrock']:
    print(f"\n--- {func_type.title()} Function ---")
    visualizer.plot_optimization_path(optimizers, func_type)

print("\nâœ… åŸºç¡€æ¢¯åº¦ä¸‹é™æ¼”ç¤ºå®Œæˆ!")
```

## 2. éšæœºæ¢¯åº¦ä¸‹é™å˜ä½“

### 2.1 SGDä¸å°æ‰¹é‡æ¢¯åº¦ä¸‹é™

```python
class SGDOptimizer:
    """éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate=0.01, batch_size=32):
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.history = []
    
    def step(self, params, gradients):
        """æ‰§è¡Œä¸€æ­¥SGDä¼˜åŒ–"""
        updated_params = []
        for param, grad in zip(params, gradients):
            new_param = param - self.learning_rate * grad
            updated_params.append(new_param)
        
        self.history.append([p.copy() for p in updated_params])
        return updated_params
    
    def reset_history(self):
        self.history = []

class MomentumOptimizer:
    """åŠ¨é‡ä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.velocity = None
        self.history = []
    
    def step(self, params, gradients):
        """æ‰§è¡Œä¸€æ­¥åŠ¨é‡ä¼˜åŒ–"""
        if self.velocity is None:
            self.velocity = [np.zeros_like(grad) for grad in gradients]
        
        updated_params = []
        for i, (param, grad) in enumerate(zip(params, gradients)):
            # æ›´æ–°é€Ÿåº¦
            self.velocity[i] = self.momentum * self.velocity[i] - self.learning_rate * grad
            # æ›´æ–°å‚æ•°
            new_param = param + self.velocity[i]
            updated_params.append(new_param)
        
        self.history.append([p.copy() for p in updated_params])
        return updated_params
    
    def reset_history(self):
        self.history = []
        self.velocity = None

class NesterovOptimizer:
    """NesterovåŠ é€Ÿæ¢¯åº¦ä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.velocity = None
        self.history = []
    
    def step(self, params, gradients):
        """æ‰§è¡Œä¸€æ­¥Nesterovä¼˜åŒ–"""
        if self.velocity is None:
            self.velocity = [np.zeros_like(grad) for grad in gradients]
        
        updated_params = []
        for i, (param, grad) in enumerate(zip(params, gradients)):
            # NesterovåŠ¨é‡æ›´æ–°
            prev_velocity = self.velocity[i].copy()
            self.velocity[i] = self.momentum * self.velocity[i] - self.learning_rate * grad
            new_param = param - self.momentum * prev_velocity + (1 + self.momentum) * self.velocity[i]
            updated_params.append(new_param)
        
        self.history.append([p.copy() for p in updated_params])
        return updated_params
    
    def reset_history(self):
        self.history = []
        self.velocity = None

class SGDComparison:
    """SGDå˜ä½“å¯¹æ¯”å·¥å…·"""
    
    def __init__(self):
        self.colors = ['red', 'blue', 'green', 'orange', 'purple']
    
    def simulate_noisy_gradients(self, true_grad, noise_level=0.1):
        """æ¨¡æ‹Ÿå¸¦å™ªå£°çš„æ¢¯åº¦"""
        noise = np.random.normal(0, noise_level, true_grad.shape)
        return true_grad + noise
    
    def compare_sgd_variants(self, function_type='quadratic', noise_level=0.1, num_iterations=200):
        """æ¯”è¾ƒSGDå˜ä½“"""
        print(f"\nğŸ”„ SGDå˜ä½“å¯¹æ¯” (å™ªå£°æ°´å¹³: {noise_level})")
        
        # åˆ›å»ºæµ‹è¯•å‡½æ•°
        visualizer = OptimizationVisualizer()
        f, grad_f = visualizer.create_test_function(function_type)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizers = {
            'SGD': SGDOptimizer(learning_rate=0.01),
            'Momentum': MomentumOptimizer(learning_rate=0.01, momentum=0.9),
            'Nesterov': NesterovOptimizer(learning_rate=0.01, momentum=0.9)
        }
        
        start_point = [2.0, 1.5] if function_type == 'quadratic' else [-1.0, 1.0]
        
        # åˆ›å»ºç½‘æ ¼
        if function_type == 'quadratic':
            x_range = np.linspace(-3, 3, 100)
            y_range = np.linspace(-2, 2, 100)
        else:
            x_range = np.linspace(-2, 2, 100)
            y_range = np.linspace(-1, 3, 100)
        
        X, Y = np.meshgrid(x_range, y_range)
        Z = f(X, Y)
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # è¿è¡Œä¼˜åŒ–
        results = {}
        for name, optimizer in optimizers.items():
            optimizer.reset_history()
            params = [np.array(start_point)]
            losses = []
            
            for iteration in range(num_iterations):
                current_params = params[-1]
                true_grad = grad_f(current_params[0], current_params[1])
                
                # æ·»åŠ å™ªå£°
                noisy_grad = self.simulate_noisy_gradients(true_grad, noise_level)
                gradients = [noisy_grad]
                
                new_params = optimizer.step(params, gradients)
                params = new_params
                
                # è®°å½•æŸå¤±
                current_loss = f(params[-1][0], params[-1][1])
                losses.append(current_loss)
                
                # æ£€æŸ¥æ”¶æ•›
                if current_loss < 1e-6:
                    break
            
            results[name] = {
                'path': np.array([p[0] for p in optimizer.history]),
                'losses': losses,
                'final_loss': losses[-1] if losses else float('inf'),
                'steps': len(optimizer.history)
            }
        
        # ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„
        ax1 = axes[0]
        contour = ax1.contour(X, Y, Z, levels=20, alpha=0.6)
        
        for i, (name, result) in enumerate(results.items()):
            path = result['path']
            ax1.plot(path[:, 0], path[:, 1], 'o-', 
                    color=self.colors[i], label=name,
                    markersize=2, linewidth=1.5, alpha=0.8)
            ax1.plot(path[-1, 0], path[-1, 1], '*', 
                    color=self.colors[i], markersize=10)
        
        ax1.plot(start_point[0], start_point[1], 'ks', markersize=8, label='Start')
        ax1.set_xlabel('x')
        ax1.set_ylabel('y')
        ax1.set_title('Optimization Paths')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        ax2 = axes[1]
        for i, (name, result) in enumerate(results.items()):
            losses = result['losses']
            ax2.semilogy(losses, color=self.colors[i], label=name, linewidth=2)
        
        ax2.set_xlabel('Iteration')
        ax2.set_ylabel('Loss (log scale)')
        ax2.set_title('Loss Convergence')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æ¢¯åº¦æ–¹å·®åˆ†æ
        ax3 = axes[2]
        
        # è®¡ç®—æ¯ä¸ªä¼˜åŒ–å™¨çš„æ¢¯åº¦æ–¹å·®
        gradient_vars = []
        for name in optimizers.keys():
            # æ¨¡æ‹Ÿæ¢¯åº¦æ–¹å·®
            vars_over_time = []
            for i in range(0, min(100, num_iterations), 5):
                grad_samples = []
                for _ in range(10):
                    true_grad = grad_f(start_point[0], start_point[1])
                    noisy_grad = self.simulate_noisy_gradients(true_grad, noise_level)
                    grad_samples.append(np.linalg.norm(noisy_grad))
                vars_over_time.append(np.var(grad_samples))
            gradient_vars.append(vars_over_time)
        
        for i, (name, var_data) in enumerate(zip(optimizers.keys(), gradient_vars)):
            ax3.plot(range(0, len(var_data)*5, 5), var_data, 
                    color=self.colors[i], label=name, linewidth=2)
        
        ax3.set_xlabel('Iteration')
        ax3.set_ylabel('Gradient Variance')
        ax3.set_title('Gradient Noise Analysis')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š ä¼˜åŒ–ç»“æœå¯¹æ¯”:")
        print(f"{'Method':<12} {'Steps':<8} {'Final Loss':<12} {'Convergence':<12}")
        print("-" * 50)
        
        for name, result in results.items():
            convergence = "Good" if result['final_loss'] < 0.01 else "Poor"
            print(f"{name:<12} {result['steps']:<8} {result['final_loss']:<12.6f} {convergence:<12}")
        
        return results
    
    def analyze_momentum_effect(self):
        """åˆ†æåŠ¨é‡æ•ˆåº”"""
        print(f"\nğŸš€ åŠ¨é‡æ•ˆåº”åˆ†æ")
        
        # åˆ›å»ºä¸€ä¸ªæœ‰å±€éƒ¨æœ€å°å€¼çš„å‡½æ•°
        def complex_function(x, y):
            return (x**2 + y**2) + 0.3 * np.sin(5*x) * np.sin(5*y)
        
        def complex_grad(x, y):
            dx = 2*x + 0.3 * 5 * np.cos(5*x) * np.sin(5*y)
            dy = 2*y + 0.3 * 5 * np.sin(5*x) * np.cos(5*y)
            return np.array([dx, dy])
        
        # æµ‹è¯•ä¸åŒåŠ¨é‡å€¼
        momentum_values = [0.0, 0.5, 0.9, 0.99]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        # åˆ›å»ºç½‘æ ¼
        x_range = np.linspace(-2, 2, 100)
        y_range = np.linspace(-2, 2, 100)
        X, Y = np.meshgrid(x_range, y_range)
        Z = complex_function(X, Y)
        
        start_point = [1.5, 1.0]
        
        for idx, momentum in enumerate(momentum_values):
            ax = axes[idx]
            
            # ç»˜åˆ¶ç­‰é«˜çº¿
            contour = ax.contour(X, Y, Z, levels=20, alpha=0.6)
            
            # åˆ›å»ºä¼˜åŒ–å™¨
            if momentum == 0.0:
                optimizer = SGDOptimizer(learning_rate=0.01)
                title = 'No Momentum (SGD)'
            else:
                optimizer = MomentumOptimizer(learning_rate=0.01, momentum=momentum)
                title = f'Momentum = {momentum}'
            
            # ä¼˜åŒ–è¿‡ç¨‹
            params = [np.array(start_point)]
            
            for iteration in range(200):
                current_params = params[-1]
                gradients = [complex_grad(current_params[0], current_params[1])]
                
                new_params = optimizer.step(params, gradients)
                params = new_params
                
                # æ£€æŸ¥æ”¶æ•›
                if np.linalg.norm(gradients[0]) < 1e-4:
                    break
            
            # ç»˜åˆ¶è·¯å¾„
            if optimizer.history:
                path = np.array([p[0] for p in optimizer.history])
                ax.plot(path[:, 0], path[:, 1], 'ro-', 
                       markersize=2, linewidth=1.5, alpha=0.8)
                ax.plot(start_point[0], start_point[1], 'gs', markersize=8)
                ax.plot(path[-1, 0], path[-1, 1], 'b*', markersize=12)
                
                # æ·»åŠ ä¿¡æ¯
                final_loss = complex_function(path[-1, 0], path[-1, 1])
                ax.text(0.02, 0.98, f'Steps: {len(path)}\nFinal Loss: {final_loss:.4f}', 
                       transform=ax.transAxes, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            ax.set_title(title)
            ax.set_xlabel('x')
            ax.set_ylabel('y')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print(f"\nğŸ’¡ åŠ¨é‡æ•ˆåº”è§‚å¯Ÿ:")
        print(f"   â€¢ æ— åŠ¨é‡: å®¹æ˜“é™·å…¥å±€éƒ¨éœ‡è¡")
        print(f"   â€¢ å°åŠ¨é‡(0.5): å‡å°‘éœ‡è¡ï¼ŒåŠ é€Ÿæ”¶æ•›")
        print(f"   â€¢ å¤§åŠ¨é‡(0.9): æ›´å¥½åœ°è·¨è¶Šå±€éƒ¨æœ€å°å€¼")
        print(f"   â€¢ è¿‡å¤§åŠ¨é‡(0.99): å¯èƒ½å¯¼è‡´è¿‡å†²å’Œä¸ç¨³å®š")

# SGDå˜ä½“æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ”„ éšæœºæ¢¯åº¦ä¸‹é™å˜ä½“")
print("=" * 80)

# åˆ›å»ºå¯¹æ¯”å·¥å…·
sgd_comparison = SGDComparison()

# æ¯”è¾ƒä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„è¡¨ç°
for noise in [0.05, 0.2]:
    print(f"\n--- å™ªå£°æ°´å¹³: {noise} ---")
    results = sgd_comparison.compare_sgd_variants('quadratic', noise_level=noise, num_iterations=150)

# åˆ†æåŠ¨é‡æ•ˆåº”
sgd_comparison.analyze_momentum_effect()

print("\nâœ… SGDå˜ä½“æ¼”ç¤ºå®Œæˆ!")
```

## 3. è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³•

### 3.1 AdaGradç®—æ³•

```python
class AdaGradOptimizer:
    """AdaGradä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate=0.01, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.epsilon = epsilon
        self.accumulated_gradients = None
        self.history = []
    
    def step(self, params, gradients):
        """æ‰§è¡Œä¸€æ­¥AdaGradä¼˜åŒ–"""
        if self.accumulated_gradients is None:
            self.accumulated_gradients = [np.zeros_like(grad) for grad in gradients]
        
        updated_params = []
        for i, (param, grad) in enumerate(zip(params, gradients)):
            # ç´¯ç§¯æ¢¯åº¦å¹³æ–¹
            self.accumulated_gradients[i] += grad ** 2
            
            # è‡ªé€‚åº”å­¦ä¹ ç‡
            adapted_lr = self.learning_rate / (np.sqrt(self.accumulated_gradients[i]) + self.epsilon)
            
            # æ›´æ–°å‚æ•°
            new_param = param - adapted_lr * grad
            updated_params.append(new_param)
        
        self.history.append([p.copy() for p in updated_params])
        return updated_params
    
    def reset_history(self):
        self.history = []
        self.accumulated_gradients = None

class RMSpropOptimizer:
    """RMSpropä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate=0.01, decay_rate=0.9, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.decay_rate = decay_rate
        self.epsilon = epsilon
        self.moving_avg_gradients = None
        self.history = []
    
    def step(self, params, gradients):
        """æ‰§è¡Œä¸€æ­¥RMSpropä¼˜åŒ–"""
        if self.moving_avg_gradients is None:
            self.moving_avg_gradients = [np.zeros_like(grad) for grad in gradients]
        
        updated_params = []
        for i, (param, grad) in enumerate(zip(params, gradients)):
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            self.moving_avg_gradients[i] = (self.decay_rate * self.moving_avg_gradients[i] + 
                                          (1 - self.decay_rate) * grad ** 2)
            
            # è‡ªé€‚åº”å­¦ä¹ ç‡
            adapted_lr = self.learning_rate / (np.sqrt(self.moving_avg_gradients[i]) + self.epsilon)
            
            # æ›´æ–°å‚æ•°
            new_param = param - adapted_lr * grad
            updated_params.append(new_param)
        
        self.history.append([p.copy() for p in updated_params])
        return updated_params
    
    def reset_history(self):
        self.history = []
        self.moving_avg_gradients = None

class AdamOptimizer:
    """Adamä¼˜åŒ–å™¨"""
    
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None  # ä¸€é˜¶çŸ©ä¼°è®¡
        self.v = None  # äºŒé˜¶çŸ©ä¼°è®¡
        self.t = 0     # æ—¶é—´æ­¥
        self.history = []
    
    def step(self, params, gradients):
        """æ‰§è¡Œä¸€æ­¥Adamä¼˜åŒ–"""
        if self.m is None:
            self.m = [np.zeros_like(grad) for grad in gradients]
            self.v = [np.zeros_like(grad) for grad in gradients]
        
        self.t += 1
        updated_params = []
        
        for i, (param, grad) in enumerate(zip(params, gradients)):
            # æ›´æ–°ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2
            
            # åå·®ä¿®æ­£
            m_corrected = self.m[i] / (1 - self.beta1 ** self.t)
            v_corrected = self.v[i] / (1 - self.beta2 ** self.t)
            
            # æ›´æ–°å‚æ•°
            new_param = param - self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)
            updated_params.append(new_param)
        
        self.history.append([p.copy() for p in updated_params])
        return updated_params
    
    def reset_history(self):
        self.history = []
        self.m = None
        self.v = None
        self.t = 0

class AdamWOptimizer:
    """AdamWä¼˜åŒ–å™¨ï¼ˆå¸¦æƒé‡è¡°å‡çš„Adamï¼‰"""
    
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, 
                 epsilon=1e-8, weight_decay=0.01):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.weight_decay = weight_decay
        self.m = None
        self.v = None
        self.t = 0
        self.history = []
    
    def step(self, params, gradients):
        """æ‰§è¡Œä¸€æ­¥AdamWä¼˜åŒ–"""
        if self.m is None:
            self.m = [np.zeros_like(grad) for grad in gradients]
            self.v = [np.zeros_like(grad) for grad in gradients]
        
        self.t += 1
        updated_params = []
        
        for i, (param, grad) in enumerate(zip(params, gradients)):
            # æƒé‡è¡°å‡
            param_with_decay = param * (1 - self.learning_rate * self.weight_decay)
            
            # æ›´æ–°ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2
            
            # åå·®ä¿®æ­£
            m_corrected = self.m[i] / (1 - self.beta1 ** self.t)
            v_corrected = self.v[i] / (1 - self.beta2 ** self.t)
            
            # æ›´æ–°å‚æ•°
            new_param = param_with_decay - self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)
            updated_params.append(new_param)
        
        self.history.append([p.copy() for p in updated_params])
        return updated_params
    
    def reset_history(self):
        self.history = []
        self.m = None
        self.v = None
        self.t = 0

class AdaptiveOptimizerComparison:
    """è‡ªé€‚åº”ä¼˜åŒ–å™¨å¯¹æ¯”å·¥å…·"""
    
    def __init__(self):
        self.colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']
    
    def compare_adaptive_optimizers(self, function_type='rosenbrock', num_iterations=500):
        """æ¯”è¾ƒè‡ªé€‚åº”ä¼˜åŒ–å™¨"""
        print(f"\nğŸ¯ è‡ªé€‚åº”ä¼˜åŒ–å™¨å¯¹æ¯” ({function_type} function)")
        
        # åˆ›å»ºæµ‹è¯•å‡½æ•°
        visualizer = OptimizationVisualizer()
        f, grad_f = visualizer.create_test_function(function_type)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizers = {
            'SGD': SGDOptimizer(learning_rate=0.01),
            'Momentum': MomentumOptimizer(learning_rate=0.01, momentum=0.9),
            'AdaGrad': AdaGradOptimizer(learning_rate=0.1),
            'RMSprop': RMSpropOptimizer(learning_rate=0.01, decay_rate=0.9),
            'Adam': AdamOptimizer(learning_rate=0.01),
            'AdamW': AdamWOptimizer(learning_rate=0.01, weight_decay=0.01)
        }
        
        start_point = [-1.0, 1.0] if function_type == 'rosenbrock' else [2.0, 1.5]
        
        # åˆ›å»ºç½‘æ ¼
        if function_type == 'rosenbrock':
            x_range = np.linspace(-2, 2, 100)
            y_range = np.linspace(-1, 3, 100)
        else:
            x_range = np.linspace(-3, 3, 100)
            y_range = np.linspace(-2, 2, 100)
        
        X, Y = np.meshgrid(x_range, y_range)
        Z = f(X, Y)
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # è¿è¡Œä¼˜åŒ–
        results = {}
        for name, optimizer in optimizers.items():
            optimizer.reset_history()
            params = [np.array(start_point)]
            losses = []
            
            for iteration in range(num_iterations):
                current_params = params[-1]
                gradients = [grad_f(current_params[0], current_params[1])]
                
                new_params = optimizer.step(params, gradients)
                params = new_params
                
                # è®°å½•æŸå¤±
                current_loss = f(params[-1][0], params[-1][1])
                losses.append(current_loss)
                
                # æ£€æŸ¥æ”¶æ•›
                if current_loss < 1e-8:
                    break
            
            results[name] = {
                'path': np.array([p[0] for p in optimizer.history]),
                'losses': losses,
                'final_loss': losses[-1] if losses else float('inf'),
                'steps': len(optimizer.history)
            }
        
        # ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„
        ax1 = axes[0, 0]
        contour = ax1.contour(X, Y, Z, levels=30, alpha=0.6)
        
        for i, (name, result) in enumerate(results.items()):
            path = result['path']
            if len(path) > 0:
                ax1.plot(path[:, 0], path[:, 1], 'o-', 
                        color=self.colors[i % len(self.colors)], 
                        label=f'{name} ({len(path)})',
                        markersize=2, linewidth=1.5, alpha=0.8)
                ax1.plot(path[-1, 0], path[-1, 1], '*', 
                        color=self.colors[i % len(self.colors)], markersize=8)
        
        ax1.plot(start_point[0], start_point[1], 'ks', markersize=8, label='Start')
        ax1.set_xlabel('x')
        ax1.set_ylabel('y')
        ax1.set_title('Optimization Paths')
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        ax2 = axes[0, 1]
        for i, (name, result) in enumerate(results.items()):
            losses = result['losses']
            if losses:
                ax2.semilogy(losses, color=self.colors[i % len(self.colors)], 
                           label=name, linewidth=2)
        
        ax2.set_xlabel('Iteration')
        ax2.set_ylabel('Loss (log scale)')
        ax2.set_title('Loss Convergence')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
        ax3 = axes[1, 0]
        convergence_thresholds = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]
        convergence_steps = {name: [] for name in optimizers.keys()}
        
        for threshold in convergence_thresholds:
            for name, result in results.items():
                losses = result['losses']
                steps_to_converge = len(losses)
                for i, loss in enumerate(losses):
                    if loss < threshold:
                        steps_to_converge = i
                        break
                convergence_steps[name].append(steps_to_converge)
        
        x_pos = np.arange(len(convergence_thresholds))
        width = 0.12
        
        for i, (name, steps) in enumerate(convergence_steps.items()):
            ax3.bar(x_pos + i * width, steps, width, 
                   color=self.colors[i % len(self.colors)], 
                   label=name, alpha=0.8)
        
        ax3.set_xlabel('Convergence Threshold')
        ax3.set_ylabel('Steps to Converge')
        ax3.set_title('Convergence Speed Comparison')
        ax3.set_xticks(x_pos + width * 2.5)
        ax3.set_xticklabels([f'1e-{i+1}' for i in range(len(convergence_thresholds))])
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
        ax4 = axes[1, 1]
        names = list(results.keys())
        final_losses = [results[name]['final_loss'] for name in names]
        steps_taken = [results[name]['steps'] for name in names]
        
        scatter = ax4.scatter(steps_taken, final_losses, 
                            c=range(len(names)), 
                            s=100, alpha=0.7, cmap='tab10')
        
        for i, name in enumerate(names):
            ax4.annotate(name, (steps_taken[i], final_losses[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax4.set_xlabel('Steps Taken')
        ax4.set_ylabel('Final Loss')
        ax4.set_title('Efficiency vs Accuracy')
        ax4.set_yscale('log')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†ä¼˜åŒ–ç»“æœ:")
        print(f"{'Optimizer':<12} {'Steps':<8} {'Final Loss':<15} {'Convergence':<12} {'Efficiency':<12}")
        print("-" * 75)
        
        for name, result in results.items():
            convergence = "Excellent" if result['final_loss'] < 1e-6 else \
                         "Good" if result['final_loss'] < 1e-3 else \
                         "Fair" if result['final_loss'] < 1e-1 else "Poor"
            
            efficiency = "High" if result['steps'] < 100 else \
                        "Medium" if result['steps'] < 300 else "Low"
            
            print(f"{name:<12} {result['steps']:<8} {result['final_loss']:<15.8f} {convergence:<12} {efficiency:<12}")
        
        return results
    
    def analyze_learning_rate_adaptation(self):
        """åˆ†æå­¦ä¹ ç‡è‡ªé€‚åº”æ•ˆæœ"""
        print(f"\nğŸ“ˆ å­¦ä¹ ç‡è‡ªé€‚åº”æ•ˆæœåˆ†æ")
        
        # åˆ›å»ºä¸€ä¸ªå…·æœ‰ä¸åŒå°ºåº¦çš„å‡½æ•°
        def scaled_function(x, y):
            return 100 * x**2 + y**2  # xæ–¹å‘æ¢¯åº¦æ¯”yæ–¹å‘å¤§100å€
        
        def scaled_grad(x, y):
            return np.array([200 * x, 2 * y])
        
        # æµ‹è¯•ä¼˜åŒ–å™¨
        optimizers = {
            'SGD': SGDOptimizer(learning_rate=0.001),
            'AdaGrad': AdaGradOptimizer(learning_rate=0.1),
            'RMSprop': RMSpropOptimizer(learning_rate=0.01),
            'Adam': AdamOptimizer(learning_rate=0.01)
        }
        
        start_point = [1.0, 1.0]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        # åˆ›å»ºç½‘æ ¼
        x_range = np.linspace(-1.5, 1.5, 100)
        y_range = np.linspace(-3, 3, 100)
        X, Y = np.meshgrid(x_range, y_range)
        Z = scaled_function(X, Y)
        
        results = {}
        for idx, (name, optimizer) in enumerate(optimizers.items()):
            ax = axes[idx]
            
            # ç»˜åˆ¶ç­‰é«˜çº¿
            contour = ax.contour(X, Y, Z, levels=20, alpha=0.6)
            
            # ä¼˜åŒ–è¿‡ç¨‹
            optimizer.reset_history()
            params = [np.array(start_point)]
            
            for iteration in range(200):
                current_params = params[-1]
                gradients = [scaled_grad(current_params[0], current_params[1])]
                
                new_params = optimizer.step(params, gradients)
                params = new_params
                
                # æ£€æŸ¥æ”¶æ•›
                if np.linalg.norm(gradients[0]) < 1e-6:
                    break
            
            # ç»˜åˆ¶è·¯å¾„
            if optimizer.history:
                path = np.array([p[0] for p in optimizer.history])
                ax.plot(path[:, 0], path[:, 1], 'ro-', 
                       markersize=3, linewidth=1.5, alpha=0.8)
                ax.plot(start_point[0], start_point[1], 'gs', markersize=8)
                ax.plot(path[-1, 0], path[-1, 1], 'b*', markersize=12)
                
                # è®°å½•ç»“æœ
                final_loss = scaled_function(path[-1, 0], path[-1, 1])
                results[name] = {
                    'steps': len(path),
                    'final_loss': final_loss,
                    'path': path
                }
                
                # æ·»åŠ ä¿¡æ¯
                ax.text(0.02, 0.98, f'Steps: {len(path)}\nFinal Loss: {final_loss:.4f}', 
                       transform=ax.transAxes, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            ax.set_title(f'{name} Optimizer')
            ax.set_xlabel('x (high curvature)')
            ax.set_ylabel('y (low curvature)')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print(f"\nğŸ” è‡ªé€‚åº”æ•ˆæœè§‚å¯Ÿ:")
        for name, result in results.items():
            print(f"   {name}: {result['steps']} steps, final loss: {result['final_loss']:.6f}")
        
        print(f"\nğŸ’¡ å…³é”®è§‚å¯Ÿ:")
        print(f"   â€¢ SGD: åœ¨é«˜æ›²ç‡æ–¹å‘æ”¶æ•›ç¼“æ…¢")
        print(f"   â€¢ AdaGrad: è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼Œä½†å¯èƒ½è¿‡æ—©åœæ­¢")
        print(f"   â€¢ RMSprop: è§£å†³AdaGradçš„å­¦ä¹ ç‡è¡°å‡é—®é¢˜")
        print(f"   â€¢ Adam: ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€šå¸¸è¡¨ç°æœ€ä½³")
        
        return results

# è‡ªé€‚åº”ä¼˜åŒ–å™¨æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ¯ è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³•")
print("=" * 80)

# åˆ›å»ºå¯¹æ¯”å·¥å…·
adaptive_comparison = AdaptiveOptimizerComparison()

# åœ¨ä¸åŒå‡½æ•°ä¸Šæ¯”è¾ƒä¼˜åŒ–å™¨
for func_type in ['quadratic', 'rosenbrock']:
    print(f"\n--- {func_type.title()} Function ---")
    results = adaptive_comparison.compare_adaptive_optimizers(func_type, num_iterations=300)

# åˆ†æå­¦ä¹ ç‡è‡ªé€‚åº”æ•ˆæœ
adaptive_comparison.analyze_learning_rate_adaptation()

print("\nâœ… è‡ªé€‚åº”ä¼˜åŒ–å™¨æ¼”ç¤ºå®Œæˆ!")

## 4. é«˜çº§ä¼˜åŒ–æŠ€æœ¯

### 4.1 å­¦ä¹ ç‡è°ƒåº¦

```python
class LearningRateScheduler:
    """å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, initial_lr=0.01, schedule_type='step'):
        self.initial_lr = initial_lr
        self.schedule_type = schedule_type
        self.current_lr = initial_lr
        self.step_count = 0
    
    def step(self, epoch=None, loss=None):
        """æ›´æ–°å­¦ä¹ ç‡"""
        self.step_count += 1
        
        if self.schedule_type == 'step':
            # é˜¶æ¢¯å¼è¡°å‡
            if self.step_count % 100 == 0:
                self.current_lr *= 0.5
        
        elif self.schedule_type == 'exponential':
            # æŒ‡æ•°è¡°å‡
            decay_rate = 0.95
            self.current_lr = self.initial_lr * (decay_rate ** (self.step_count / 100))
        
        elif self.schedule_type == 'cosine':
            # ä½™å¼¦é€€ç«
            import math
            T_max = 500  # æœ€å¤§æ­¥æ•°
            self.current_lr = self.initial_lr * 0.5 * (1 + math.cos(math.pi * self.step_count / T_max))
        
        elif self.schedule_type == 'plateau':
            # åŸºäºæŸå¤±çš„è‡ªé€‚åº”è°ƒæ•´ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if loss is not None and hasattr(self, 'best_loss'):
                if loss >= self.best_loss:
                    self.patience_count += 1
                    if self.patience_count >= 10:
                        self.current_lr *= 0.5
                        self.patience_count = 0
                else:
                    self.best_loss = loss
                    self.patience_count = 0
            elif loss is not None:
                self.best_loss = loss
                self.patience_count = 0
        
        return self.current_lr
    
    def get_lr(self):
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.current_lr

class ScheduledOptimizer:
    """å¸¦å­¦ä¹ ç‡è°ƒåº¦çš„ä¼˜åŒ–å™¨"""
    
    def __init__(self, base_optimizer, scheduler):
        self.base_optimizer = base_optimizer
        self.scheduler = scheduler
        self.history = []
    
    def step(self, params, gradients, epoch=None, loss=None):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        # æ›´æ–°å­¦ä¹ ç‡
        new_lr = self.scheduler.step(epoch, loss)
        
        # æ›´æ–°åŸºç¡€ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        if hasattr(self.base_optimizer, 'learning_rate'):
            self.base_optimizer.learning_rate = new_lr
        
        # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
        updated_params = self.base_optimizer.step(params, gradients)
        
        # è®°å½•å†å²
        self.history.append({
            'params': [p.copy() for p in updated_params],
            'lr': new_lr,
            'loss': loss
        })
        
        return updated_params
    
    def reset_history(self):
        self.history = []
        self.base_optimizer.reset_history()
        self.scheduler.step_count = 0
        self.scheduler.current_lr = self.scheduler.initial_lr

class OptimizationTechniques:
    """é«˜çº§ä¼˜åŒ–æŠ€æœ¯æ¼”ç¤º"""
    
    def __init__(self):
        self.colors = ['red', 'blue', 'green', 'orange', 'purple']
    
    def demonstrate_lr_scheduling(self):
        """æ¼”ç¤ºå­¦ä¹ ç‡è°ƒåº¦æ•ˆæœ"""
        print(f"\nğŸ“… å­¦ä¹ ç‡è°ƒåº¦æ•ˆæœæ¼”ç¤º")
        
        # åˆ›å»ºæµ‹è¯•å‡½æ•°
        visualizer = OptimizationVisualizer()
        f, grad_f = visualizer.create_test_function('rosenbrock')
        
        # åˆ›å»ºä¸åŒè°ƒåº¦ç­–ç•¥çš„ä¼˜åŒ–å™¨
        schedulers = {
            'Constant': LearningRateScheduler(0.01, 'constant'),
            'Step Decay': LearningRateScheduler(0.05, 'step'),
            'Exponential': LearningRateScheduler(0.05, 'exponential'),
            'Cosine': LearningRateScheduler(0.05, 'cosine')
        }
        
        optimizers = {}
        for name, scheduler in schedulers.items():
            base_opt = SGDOptimizer(learning_rate=0.01)
            optimizers[name] = ScheduledOptimizer(base_opt, scheduler)
        
        start_point = [-1.0, 1.0]
        num_iterations = 400
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # è¿è¡Œä¼˜åŒ–
        results = {}
        for name, optimizer in optimizers.items():
            optimizer.reset_history()
            params = [np.array(start_point)]
            losses = []
            learning_rates = []
            
            for iteration in range(num_iterations):
                current_params = params[-1]
                gradients = [grad_f(current_params[0], current_params[1])]
                current_loss = f(current_params[0], current_params[1])
                
                new_params = optimizer.step(params, gradients, 
                                          epoch=iteration, loss=current_loss)
                params = new_params
                
                losses.append(current_loss)
                learning_rates.append(optimizer.scheduler.get_lr())
                
                if current_loss < 1e-8:
                    break
            
            results[name] = {
                'losses': losses,
                'learning_rates': learning_rates,
                'path': np.array([h['params'][0] for h in optimizer.history]),
                'final_loss': losses[-1] if losses else float('inf')
            }
        
        # ç»˜åˆ¶å­¦ä¹ ç‡å˜åŒ–
        ax1 = axes[0, 0]
        for i, (name, result) in enumerate(results.items()):
            ax1.plot(result['learning_rates'], 
                    color=self.colors[i], label=name, linewidth=2)
        
        ax1.set_xlabel('Iteration')
        ax1.set_ylabel('Learning Rate')
        ax1.set_title('Learning Rate Schedules')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        ax2 = axes[0, 1]
        for i, (name, result) in enumerate(results.items()):
            ax2.semilogy(result['losses'], 
                        color=self.colors[i], label=name, linewidth=2)
        
        ax2.set_xlabel('Iteration')
        ax2.set_ylabel('Loss (log scale)')
        ax2.set_title('Loss Convergence with Different Schedules')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„
        ax3 = axes[1, 0]
        x_range = np.linspace(-2, 2, 100)
        y_range = np.linspace(-1, 3, 100)
        X, Y = np.meshgrid(x_range, y_range)
        Z = f(X, Y)
        
        contour = ax3.contour(X, Y, Z, levels=20, alpha=0.6)
        
        for i, (name, result) in enumerate(results.items()):
            path = result['path']
            if len(path) > 0:
                ax3.plot(path[:, 0], path[:, 1], 'o-', 
                        color=self.colors[i], label=name,
                        markersize=2, linewidth=1.5, alpha=0.8)
        
        ax3.plot(start_point[0], start_point[1], 'ks', markersize=8, label='Start')
        ax3.set_xlabel('x')
        ax3.set_ylabel('y')
        ax3.set_title('Optimization Paths')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
        ax4 = axes[1, 1]
        convergence_times = []
        names = []
        
        for name, result in results.items():
            losses = result['losses']
            convergence_time = len(losses)
            for i, loss in enumerate(losses):
                if loss < 1e-3:
                    convergence_time = i
                    break
            convergence_times.append(convergence_time)
            names.append(name)
        
        bars = ax4.bar(names, convergence_times, 
                      color=self.colors[:len(names)], alpha=0.7)
        ax4.set_ylabel('Steps to Converge (loss < 1e-3)')
        ax4.set_title('Convergence Speed Comparison')
        ax4.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time in zip(bars, convergence_times):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 5,
                    f'{time}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š å­¦ä¹ ç‡è°ƒåº¦æ•ˆæœå¯¹æ¯”:")
        print(f"{'Schedule':<15} {'Final Loss':<15} {'Convergence Steps':<18} {'Efficiency':<12}")
        print("-" * 65)
        
        for name, result in results.items():
            conv_steps = len(result['losses'])
            for i, loss in enumerate(result['losses']):
                if loss < 1e-3:
                    conv_steps = i
                    break
            
            efficiency = "High" if conv_steps < 100 else \
                        "Medium" if conv_steps < 200 else "Low"
            
            print(f"{name:<15} {result['final_loss']:<15.8f} {conv_steps:<18} {efficiency:<12}")
        
        return results
    
    def demonstrate_gradient_clipping(self):
        """æ¼”ç¤ºæ¢¯åº¦è£å‰ªæ•ˆæœ"""
        print(f"\nâœ‚ï¸ æ¢¯åº¦è£å‰ªæ•ˆæœæ¼”ç¤º")
        
        class GradientClippingOptimizer:
            """å¸¦æ¢¯åº¦è£å‰ªçš„ä¼˜åŒ–å™¨"""
            
            def __init__(self, base_optimizer, clip_norm=1.0):
                self.base_optimizer = base_optimizer
                self.clip_norm = clip_norm
                self.history = []
            
            def step(self, params, gradients):
                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                total_norm = 0
                for grad in gradients:
                    total_norm += np.sum(grad ** 2)
                total_norm = np.sqrt(total_norm)
                
                # æ¢¯åº¦è£å‰ª
                clipped_gradients = []
                if total_norm > self.clip_norm:
                    clip_coef = self.clip_norm / total_norm
                    for grad in gradients:
                        clipped_gradients.append(grad * clip_coef)
                else:
                    clipped_gradients = gradients
                
                # è®°å½•æ¢¯åº¦ä¿¡æ¯
                self.history.append({
                    'original_norm': total_norm,
                    'clipped_norm': min(total_norm, self.clip_norm),
                    'clipped': total_norm > self.clip_norm
                })
                
                return self.base_optimizer.step(params, clipped_gradients)
            
            def reset_history(self):
                self.history = []
                self.base_optimizer.reset_history()
        
        # åˆ›å»ºä¸€ä¸ªå®¹æ˜“äº§ç”Ÿå¤§æ¢¯åº¦çš„å‡½æ•°
        def steep_function(x, y):
            return np.exp(x**2 + y**2) - 1
        
        def steep_grad(x, y):
            exp_val = np.exp(x**2 + y**2)
            return np.array([2*x*exp_val, 2*y*exp_val])
        
        # æµ‹è¯•ä¸åŒçš„æ¢¯åº¦è£å‰ªç­–ç•¥
        clip_norms = [None, 10.0, 1.0, 0.1]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        start_point = [0.8, 0.6]
        
        results = {}
        for idx, clip_norm in enumerate(clip_norms):
            ax = axes[idx]
            
            # åˆ›å»ºä¼˜åŒ–å™¨
            base_opt = SGDOptimizer(learning_rate=0.01)
            if clip_norm is None:
                optimizer = base_opt
                title = 'No Clipping'
            else:
                optimizer = GradientClippingOptimizer(base_opt, clip_norm)
                title = f'Clip Norm = {clip_norm}'
            
            # ä¼˜åŒ–è¿‡ç¨‹
            optimizer.reset_history()
            params = [np.array(start_point)]
            losses = []
            gradient_norms = []
            
            for iteration in range(100):
                current_params = params[-1]
                
                # æ£€æŸ¥å‚æ•°æ˜¯å¦è¿‡å¤§ï¼ˆé˜²æ­¢æ•°å€¼æº¢å‡ºï¼‰
                if np.any(np.abs(current_params) > 2):
                    break
                
                gradients = [steep_grad(current_params[0], current_params[1])]
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦è¿‡å¤§
                grad_norm = np.linalg.norm(gradients[0])
                if grad_norm > 1e6:  # é˜²æ­¢æ•°å€¼æº¢å‡º
                    break
                
                gradient_norms.append(grad_norm)
                
                new_params = optimizer.step(params, gradients)
                params = new_params
                
                current_loss = steep_function(params[-1][0], params[-1][1])
                losses.append(current_loss)
                
                if current_loss < 1e-6 or current_loss > 1e6:
                    break
            
            # ç»˜åˆ¶ä¼˜åŒ–è·¯å¾„
            if hasattr(optimizer, 'base_optimizer'):
                path = np.array([p[0] for p in optimizer.base_optimizer.history])
            else:
                path = np.array([p[0] for p in optimizer.history])
            
            if len(path) > 0:
                ax.plot(path[:, 0], path[:, 1], 'ro-', 
                       markersize=3, linewidth=1.5, alpha=0.8)
                ax.plot(start_point[0], start_point[1], 'gs', markersize=8)
                ax.plot(path[-1, 0], path[-1, 1], 'b*', markersize=12)
                
                # æ·»åŠ ä¿¡æ¯
                final_loss = losses[-1] if losses else float('inf')
                ax.text(0.02, 0.98, f'Steps: {len(path)}\nFinal Loss: {final_loss:.4f}', 
                       transform=ax.transAxes, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            ax.set_title(title)
            ax.set_xlabel('x')
            ax.set_ylabel('y')
            ax.grid(True, alpha=0.3)
            ax.set_xlim(-1.5, 1.5)
            ax.set_ylim(-1.5, 1.5)
            
            results[title] = {
                'losses': losses,
                'gradient_norms': gradient_norms,
                'path': path if len(path) > 0 else np.array([]),
                'converged': len(losses) > 0 and losses[-1] < 1e-3
            }
        
        plt.tight_layout()
        plt.show()
        
        # ç»˜åˆ¶æ¢¯åº¦èŒƒæ•°å˜åŒ–
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ¢¯åº¦èŒƒæ•°å˜åŒ–
        for i, (title, result) in enumerate(results.items()):
            if result['gradient_norms']:
                ax1.semilogy(result['gradient_norms'], 
                           color=self.colors[i], label=title, linewidth=2)
        
        ax1.set_xlabel('Iteration')
        ax1.set_ylabel('Gradient Norm (log scale)')
        ax1.set_title('Gradient Norm Evolution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æŸå¤±å˜åŒ–
        for i, (title, result) in enumerate(results.items()):
            if result['losses']:
                ax2.semilogy(result['losses'], 
                           color=self.colors[i], label=title, linewidth=2)
        
        ax2.set_xlabel('Iteration')
        ax2.set_ylabel('Loss (log scale)')
        ax2.set_title('Loss Evolution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        print(f"\nğŸ“Š æ¢¯åº¦è£å‰ªæ•ˆæœåˆ†æ:")
        for title, result in results.items():
            status = "æ”¶æ•›" if result['converged'] else "å‘æ•£/ä¸ç¨³å®š"
            max_grad = max(result['gradient_norms']) if result['gradient_norms'] else 0
            print(f"   {title}: {status}, æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max_grad:.2f}")
        
        print(f"\nğŸ’¡ å…³é”®è§‚å¯Ÿ:")
        print(f"   â€¢ æ— è£å‰ª: å¯èƒ½å› æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´è®­ç»ƒä¸ç¨³å®š")
        print(f"   â€¢ é€‚åº¦è£å‰ª: ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œä¿æŒæ”¶æ•›æ€§")
        print(f"   â€¢ è¿‡åº¦è£å‰ª: å¯èƒ½é˜»ç¢æ”¶æ•›ï¼Œé™ä½è®­ç»ƒæ•ˆç‡")
        
        return results

### 4.2 Traeé£æ ¼çš„ä¼˜åŒ–å™¨å®ç°

```python
class TraeOptimizer:
    """Traeé£æ ¼çš„ç»¼åˆä¼˜åŒ–å™¨"""
    
    def __init__(self, optimizer_type='adam', learning_rate=0.001, 
                 schedule_type='cosine', clip_norm=1.0, **kwargs):
        self.optimizer_type = optimizer_type
        self.learning_rate = learning_rate
        self.schedule_type = schedule_type
        self.clip_norm = clip_norm
        
        # åˆ›å»ºåŸºç¡€ä¼˜åŒ–å™¨
        if optimizer_type == 'sgd':
            self.base_optimizer = SGDOptimizer(learning_rate)
        elif optimizer_type == 'momentum':
            momentum = kwargs.get('momentum', 0.9)
            self.base_optimizer = MomentumOptimizer(learning_rate, momentum)
        elif optimizer_type == 'adam':
            beta1 = kwargs.get('beta1', 0.9)
            beta2 = kwargs.get('beta2', 0.999)
            self.base_optimizer = AdamOptimizer(learning_rate, beta1, beta2)
        elif optimizer_type == 'adamw':
            beta1 = kwargs.get('beta1', 0.9)
            beta2 = kwargs.get('beta2', 0.999)
            weight_decay = kwargs.get('weight_decay', 0.01)
            self.base_optimizer = AdamWOptimizer(learning_rate, beta1, beta2, weight_decay=weight_decay)
        else:
            raise ValueError(f"Unsupported optimizer type: {optimizer_type}")
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = LearningRateScheduler(learning_rate, schedule_type)
        
        # è®­ç»ƒå†å²
        self.trae_history = {
            'losses': [],
            'learning_rates': [],
            'gradient_norms': [],
            'parameters': [],
            'optimization_metrics': []
        }
        
        self.step_count = 0
    
    def trae_step(self, params, gradients, loss=None):
        """Traeé£æ ¼çš„ä¼˜åŒ–æ­¥éª¤"""
        self.step_count += 1
        
        # 1. è®¡ç®—åŸå§‹æ¢¯åº¦èŒƒæ•°
        original_grad_norm = 0
        for grad in gradients:
            original_grad_norm += np.sum(grad ** 2)
        original_grad_norm = np.sqrt(original_grad_norm)
        
        # 2. æ¢¯åº¦è£å‰ª
        clipped_gradients = []
        if self.clip_norm is not None and original_grad_norm > self.clip_norm:
            clip_coef = self.clip_norm / original_grad_norm
            for grad in gradients:
                clipped_gradients.append(grad * clip_coef)
            clipped_grad_norm = self.clip_norm
        else:
            clipped_gradients = gradients
            clipped_grad_norm = original_grad_norm
        
        # 3. æ›´æ–°å­¦ä¹ ç‡
        current_lr = self.scheduler.step(self.step_count, loss)
        if hasattr(self.base_optimizer, 'learning_rate'):
            self.base_optimizer.learning_rate = current_lr
        
        # 4. æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
        updated_params = self.base_optimizer.step(params, clipped_gradients)
        
        # 5. è®°å½•è¯¦ç»†å†å²
        self.trae_history['losses'].append(loss if loss is not None else 0)
        self.trae_history['learning_rates'].append(current_lr)
        self.trae_history['gradient_norms'].append(original_grad_norm)
        self.trae_history['parameters'].append([p.copy() for p in updated_params])
        
        # è®¡ç®—ä¼˜åŒ–æŒ‡æ ‡
        metrics = self._compute_optimization_metrics(params, updated_params, 
                                                   gradients, clipped_gradients)
        self.trae_history['optimization_metrics'].append(metrics)
        
        return updated_params
    
    def _compute_optimization_metrics(self, old_params, new_params, 
                                    original_grads, clipped_grads):
        """è®¡ç®—ä¼˜åŒ–æŒ‡æ ‡"""
        # å‚æ•°å˜åŒ–é‡
        param_change_norm = 0
        for old_p, new_p in zip(old_params, new_params):
            param_change_norm += np.sum((new_p - old_p) ** 2)
        param_change_norm = np.sqrt(param_change_norm)
        
        # æ¢¯åº¦è£å‰ªæ¯”ä¾‹
        original_norm = np.sqrt(sum(np.sum(g**2) for g in original_grads))
        clipped_norm = np.sqrt(sum(np.sum(g**2) for g in clipped_grads))
        clip_ratio = clipped_norm / original_norm if original_norm > 0 else 1.0
        
        return {
            'param_change_norm': param_change_norm,
            'gradient_clip_ratio': clip_ratio,
            'learning_rate': self.scheduler.get_lr()
        }
    
    def trae_analyze_optimization(self):
        """Traeé£æ ¼çš„ä¼˜åŒ–åˆ†æ"""
        print(f"\n{'='*80}")
        print(f"ğŸ” Traeä¼˜åŒ–åˆ†ææŠ¥å‘Š")
        print(f"{'='*80}")
        
        if not self.trae_history['losses']:
            print("âš ï¸ æ²¡æœ‰ä¼˜åŒ–å†å²æ•°æ®")
            return
        
        # åŸºæœ¬ç»Ÿè®¡
        total_steps = len(self.trae_history['losses'])
        final_loss = self.trae_history['losses'][-1]
        initial_loss = self.trae_history['losses'][0]
        loss_reduction = (initial_loss - final_loss) / initial_loss * 100
        
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»ä¼˜åŒ–æ­¥æ•°: {total_steps}")
        print(f"   åˆå§‹æŸå¤±: {initial_loss:.6f}")
        print(f"   æœ€ç»ˆæŸå¤±: {final_loss:.6f}")
        print(f"   æŸå¤±é™ä½: {loss_reduction:.2f}%")
        
        # æ”¶æ•›åˆ†æ
        convergence_threshold = initial_loss * 0.01  # 1%çš„åˆå§‹æŸå¤±
        convergence_step = total_steps
        for i, loss in enumerate(self.trae_history['losses']):
            if loss < convergence_threshold:
                convergence_step = i
                break
        
        print(f"\nğŸ¯ æ”¶æ•›åˆ†æ:")
        print(f"   æ”¶æ•›é˜ˆå€¼: {convergence_threshold:.6f}")
        print(f"   æ”¶æ•›æ­¥æ•°: {convergence_step}")
        print(f"   æ”¶æ•›æ•ˆç‡: {convergence_step/total_steps*100:.1f}%")
        
        # æ¢¯åº¦åˆ†æ
        grad_norms = self.trae_history['gradient_norms']
        avg_grad_norm = np.mean(grad_norms)
        max_grad_norm = np.max(grad_norms)
        min_grad_norm = np.min(grad_norms)
        
        print(f"\nğŸ“ˆ æ¢¯åº¦åˆ†æ:")
        print(f"   å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.6f}")
        print(f"   æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max_grad_norm:.6f}")
        print(f"   æœ€å°æ¢¯åº¦èŒƒæ•°: {min_grad_norm:.6f}")
        
        # å­¦ä¹ ç‡åˆ†æ
        learning_rates = self.trae_history['learning_rates']
        initial_lr = learning_rates[0]
        final_lr = learning_rates[-1]
        lr_decay_ratio = final_lr / initial_lr
        
        print(f"\nğŸ“‰ å­¦ä¹ ç‡åˆ†æ:")
        print(f"   åˆå§‹å­¦ä¹ ç‡: {initial_lr:.6f}")
        print(f"   æœ€ç»ˆå­¦ä¹ ç‡: {final_lr:.6f}")
        print(f"   è¡°å‡æ¯”ä¾‹: {lr_decay_ratio:.4f}")
        
        # ä¼˜åŒ–ç¨³å®šæ€§åˆ†æ
        metrics = self.trae_history['optimization_metrics']
        clip_ratios = [m['gradient_clip_ratio'] for m in metrics]
        param_changes = [m['param_change_norm'] for m in metrics]
        
        clipped_steps = sum(1 for ratio in clip_ratios if ratio < 0.99)
        clip_frequency = clipped_steps / total_steps * 100
        
        print(f"\nğŸ›¡ï¸ ç¨³å®šæ€§åˆ†æ:")
        print(f"   æ¢¯åº¦è£å‰ªé¢‘ç‡: {clip_frequency:.1f}%")
        print(f"   å¹³å‡å‚æ•°å˜åŒ–: {np.mean(param_changes):.6f}")
        print(f"   å‚æ•°å˜åŒ–æ ‡å‡†å·®: {np.std(param_changes):.6f}")
        
        return {
            'total_steps': total_steps,
            'loss_reduction': loss_reduction,
            'convergence_step': convergence_step,
            'gradient_stats': {
                'mean': avg_grad_norm,
                'max': max_grad_norm,
                'min': min_grad_norm
            },
            'lr_decay_ratio': lr_decay_ratio,
            'clip_frequency': clip_frequency
        }
    
    def trae_visualize_optimization(self):
        """Traeé£æ ¼çš„ä¼˜åŒ–å¯è§†åŒ–"""
        if not self.trae_history['losses']:
            print("âš ï¸ æ²¡æœ‰ä¼˜åŒ–å†å²æ•°æ®å¯è§†åŒ–")
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. æŸå¤±æ›²çº¿
        ax1 = axes[0, 0]
        ax1.semilogy(self.trae_history['losses'], 'b-', linewidth=2)
        ax1.set_xlabel('Iteration')
        ax1.set_ylabel('Loss (log scale)')
        ax1.set_title('Loss Convergence')
        ax1.grid(True, alpha=0.3)
        
        # 2. å­¦ä¹ ç‡å˜åŒ–
        ax2 = axes[0, 1]
        ax2.plot(self.trae_history['learning_rates'], 'g-', linewidth=2)
        ax2.set_xlabel('Iteration')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('Learning Rate Schedule')
        ax2.grid(True, alpha=0.3)
        
        # 3. æ¢¯åº¦èŒƒæ•°
        ax3 = axes[0, 2]
        ax3.semilogy(self.trae_history['gradient_norms'], 'r-', linewidth=2)
        ax3.set_xlabel('Iteration')
        ax3.set_ylabel('Gradient Norm (log scale)')
        ax3.set_title('Gradient Norm Evolution')
        ax3.grid(True, alpha=0.3)
        
        # 4. å‚æ•°å˜åŒ–
        ax4 = axes[1, 0]
        metrics = self.trae_history['optimization_metrics']
        param_changes = [m['param_change_norm'] for m in metrics]
        ax4.plot(param_changes, 'purple', linewidth=2)
        ax4.set_xlabel('Iteration')
        ax4.set_ylabel('Parameter Change Norm')
        ax4.set_title('Parameter Update Magnitude')
        ax4.grid(True, alpha=0.3)
        
        # 5. æ¢¯åº¦è£å‰ªæ¯”ä¾‹
        ax5 = axes[1, 1]
        clip_ratios = [m['gradient_clip_ratio'] for m in metrics]
        ax5.plot(clip_ratios, 'orange', linewidth=2)
        ax5.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='No Clipping')
        ax5.set_xlabel('Iteration')
        ax5.set_ylabel('Gradient Clip Ratio')
        ax5.set_title('Gradient Clipping Activity')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # 6. ä¼˜åŒ–æ•ˆç‡åˆ†æ
        ax6 = axes[1, 2]
        # è®¡ç®—æ»‘åŠ¨å¹³å‡æŸå¤±æ”¹å–„
        window_size = max(10, len(self.trae_history['losses']) // 20)
        loss_improvements = []
        for i in range(window_size, len(self.trae_history['losses'])):
            current_avg = np.mean(self.trae_history['losses'][i-window_size:i])
            prev_avg = np.mean(self.trae_history['losses'][i-window_size-1:i-1])
            improvement = (prev_avg - current_avg) / prev_avg if prev_avg > 0 else 0
            loss_improvements.append(improvement)
        
        if loss_improvements:
            ax6.plot(range(window_size, len(self.trae_history['losses'])), 
                    loss_improvements, 'brown', linewidth=2)
            ax6.axhline(y=0, color='black', linestyle='-', alpha=0.5)
            ax6.set_xlabel('Iteration')
            ax6.set_ylabel('Loss Improvement Rate')
            ax6.set_title('Optimization Efficiency')
            ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def trae_export_results(self, filename=None):
        """å¯¼å‡ºä¼˜åŒ–ç»“æœ"""
        results = {
            'optimizer_config': {
                'type': self.optimizer_type,
                'learning_rate': self.learning_rate,
                'schedule_type': self.schedule_type,
                'clip_norm': self.clip_norm
            },
            'optimization_history': self.trae_history,
            'analysis': self.trae_analyze_optimization() if self.trae_history['losses'] else None
        }
        
        if filename:
            import json
            with open(filename, 'w') as f:
                # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
                serializable_results = self._make_serializable(results)
                json.dump(serializable_results, f, indent=2)
            print(f"\nğŸ’¾ ä¼˜åŒ–ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
        
        return results
    
    def _make_serializable(self, obj):
        """å°†numpyæ•°ç»„è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        else:
            return obj
    
    def reset_history(self):
        """é‡ç½®å†å²è®°å½•"""
        self.trae_history = {
            'losses': [],
            'learning_rates': [],
            'gradient_norms': [],
            'parameters': [],
            'optimization_metrics': []
        }
        self.step_count = 0
        self.base_optimizer.reset_history()
        self.scheduler.step_count = 0
        self.scheduler.current_lr = self.scheduler.initial_lr

# é«˜çº§ä¼˜åŒ–æŠ€æœ¯æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸš€ é«˜çº§ä¼˜åŒ–æŠ€æœ¯")
print("=" * 80)

# åˆ›å»ºæ¼”ç¤ºå·¥å…·
opt_techniques = OptimizationTechniques()

# æ¼”ç¤ºå­¦ä¹ ç‡è°ƒåº¦
print("\n--- å­¦ä¹ ç‡è°ƒåº¦æ•ˆæœ ---")
lr_results = opt_techniques.demonstrate_lr_scheduling()

# æ¼”ç¤ºæ¢¯åº¦è£å‰ª
print("\n--- æ¢¯åº¦è£å‰ªæ•ˆæœ ---")
clip_results = opt_techniques.demonstrate_gradient_clipping()

print("\nâœ… é«˜çº§ä¼˜åŒ–æŠ€æœ¯æ¼”ç¤ºå®Œæˆ!")

## 5. Traeä¼˜åŒ–å™¨ç»¼åˆæ¼”ç¤º

print("\n" + "=" * 80)
print("ğŸ¯ Traeä¼˜åŒ–å™¨ç»¼åˆæ¼”ç¤º")
print("=" * 80)

# åˆ›å»ºæµ‹è¯•å‡½æ•°
visualizer = OptimizationVisualizer()
f, grad_f = visualizer.create_test_function('rosenbrock')

# åˆ›å»ºTraeä¼˜åŒ–å™¨
trae_optimizer = TraeOptimizer(
    optimizer_type='adam',
    learning_rate=0.01,
    schedule_type='cosine',
    clip_norm=1.0,
    beta1=0.9,
    beta2=0.999
)

print("\nğŸ¯ ä½¿ç”¨Traeä¼˜åŒ–å™¨ä¼˜åŒ–Rosenbrockå‡½æ•°...")

# ä¼˜åŒ–è¿‡ç¨‹
start_point = [-1.0, 1.0]
params = [np.array(start_point)]

for iteration in range(500):
    current_params = params[-1]
    gradients = [grad_f(current_params[0], current_params[1])]
    current_loss = f(current_params[0], current_params[1])
    
    new_params = trae_optimizer.trae_step(params, gradients, current_loss)
    params = new_params
    
    # æ¯100æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦
    if iteration % 100 == 0:
        print(f"   Step {iteration:3d}: Loss = {current_loss:.6f}, LR = {trae_optimizer.scheduler.get_lr():.6f}")
    
    # æ£€æŸ¥æ”¶æ•›
    if current_loss < 1e-8:
        print(f"   æ”¶æ•›äºç¬¬ {iteration} æ­¥")
        break

# åˆ†æä¼˜åŒ–è¿‡ç¨‹
analysis_results = trae_optimizer.trae_analyze_optimization()

# å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
print("\nğŸ“Š ç”Ÿæˆä¼˜åŒ–å¯è§†åŒ–...")
trae_optimizer.trae_visualize_optimization()

# å¯¼å‡ºç»“æœ
results = trae_optimizer.trae_export_results('trae_optimization_results.json')

print(f"\nâœ… Traeä¼˜åŒ–å™¨æ¼”ç¤ºå®Œæˆ!")
print(f"   æœ€ç»ˆæŸå¤±: {trae_optimizer.trae_history['losses'][-1]:.8f}")
print(f"   æ€»æ­¥æ•°: {len(trae_optimizer.trae_history['losses'])}")
print(f"   æ”¶æ•›æ•ˆç‡: {analysis_results['convergence_step']}/{analysis_results['total_steps']} = {analysis_results['convergence_step']/analysis_results['total_steps']*100:.1f}%")

print("\nâœ… ä¼˜åŒ–ç®—æ³•ç« èŠ‚æ¼”ç¤ºå®Œæˆ!")

## 6. å®é™…åº”ç”¨æ¡ˆä¾‹

### 6.1 ç¥ç»ç½‘ç»œè®­ç»ƒä¼˜åŒ–

```python
class NeuralNetworkOptimizationCase:
    """ç¥ç»ç½‘ç»œè®­ç»ƒä¼˜åŒ–æ¡ˆä¾‹"""
    
    def __init__(self):
        self.colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']
    
    def create_neural_network_problem(self):
        """åˆ›å»ºç¥ç»ç½‘ç»œè®­ç»ƒé—®é¢˜"""
        # ç”Ÿæˆåˆ†ç±»æ•°æ®
        np.random.seed(42)
        n_samples = 200
        n_features = 2
        n_classes = 3
        
        # ç”Ÿæˆèºæ—‹å½¢æ•°æ®
        X = np.zeros((n_samples * n_classes, n_features))
        y = np.zeros(n_samples * n_classes, dtype=int)
        
        for class_idx in range(n_classes):
            ix = range(n_samples * class_idx, n_samples * (class_idx + 1))
            r = np.linspace(0.0, 1, n_samples)
            t = np.linspace(class_idx * 4, (class_idx + 1) * 4, n_samples) + \
                np.random.randn(n_samples) * 0.2
            X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]
            y[ix] = class_idx
        
        return X, y
    
    def simple_neural_network(self, X, y, optimizer_type='adam', 
                            learning_rate=0.01, epochs=1000):
        """ç®€å•ç¥ç»ç½‘ç»œè®­ç»ƒ"""
        n_samples, n_features = X.shape
        n_classes = len(np.unique(y))
        
        # ç½‘ç»œå‚æ•°
        hidden_size = 10
        
        # åˆå§‹åŒ–æƒé‡
        np.random.seed(42)
        W1 = np.random.randn(n_features, hidden_size) * 0.1
        b1 = np.zeros((1, hidden_size))
        W2 = np.random.randn(hidden_size, n_classes) * 0.1
        b2 = np.zeros((1, n_classes))
        
        params = [W1, b1, W2, b2]
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if optimizer_type == 'sgd':
            optimizer = SGDOptimizer(learning_rate)
        elif optimizer_type == 'momentum':
            optimizer = MomentumOptimizer(learning_rate, momentum=0.9)
        elif optimizer_type == 'adam':
            optimizer = AdamOptimizer(learning_rate)
        elif optimizer_type == 'trae':
            optimizer = TraeOptimizer(
                optimizer_type='adam',
                learning_rate=learning_rate,
                schedule_type='cosine',
                clip_norm=1.0
            )
        else:
            raise ValueError(f"Unknown optimizer: {optimizer_type}")
        
        # è®­ç»ƒå†å²
        history = {
            'losses': [],
            'accuracies': [],
            'params': []
        }
        
        def softmax(x):
            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
            return exp_x / np.sum(exp_x, axis=1, keepdims=True)
        
        def relu(x):
            return np.maximum(0, x)
        
        def relu_derivative(x):
            return (x > 0).astype(float)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            z1 = np.dot(X, W1) + b1
            a1 = relu(z1)
            z2 = np.dot(a1, W2) + b2
            a2 = softmax(z2)
            
            # è®¡ç®—æŸå¤±
            y_one_hot = np.eye(n_classes)[y]
            loss = -np.mean(np.sum(y_one_hot * np.log(a2 + 1e-8), axis=1))
            
            # è®¡ç®—å‡†ç¡®ç‡
            predictions = np.argmax(a2, axis=1)
            accuracy = np.mean(predictions == y)
            
            # åå‘ä¼ æ’­
            dz2 = a2 - y_one_hot
            dW2 = np.dot(a1.T, dz2) / n_samples
            db2 = np.mean(dz2, axis=0, keepdims=True)
            
            da1 = np.dot(dz2, W2.T)
            dz1 = da1 * relu_derivative(z1)
            dW1 = np.dot(X.T, dz1) / n_samples
            db1 = np.mean(dz1, axis=0, keepdims=True)
            
            gradients = [dW1, db1, dW2, db2]
            
            # ä¼˜åŒ–æ­¥éª¤
            if optimizer_type == 'trae':
                params = optimizer.trae_step(params, gradients, loss)
            else:
                params = optimizer.step(params, gradients)
            
            W1, b1, W2, b2 = params
            
            # è®°å½•å†å²
            history['losses'].append(loss)
            history['accuracies'].append(accuracy)
            history['params'].append([p.copy() for p in params])
            
            # æ—©åœ
            if loss < 1e-6:
                break
        
        return history, params, optimizer
    
    def compare_optimizers_on_neural_network(self):
        """æ¯”è¾ƒä¸åŒä¼˜åŒ–å™¨åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„è¡¨ç°"""
        print(f"\nğŸ§  ç¥ç»ç½‘ç»œè®­ç»ƒä¼˜åŒ–å™¨å¯¹æ¯”")
        
        # åˆ›å»ºæ•°æ®
        X, y = self.create_neural_network_problem()
        
        # æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨
        optimizers = {
            'SGD': 'sgd',
            'Momentum': 'momentum', 
            'Adam': 'adam',
            'Trae': 'trae'
        }
        
        results = {}
        
        for name, opt_type in optimizers.items():
            print(f"\n   è®­ç»ƒä½¿ç”¨ {name} ä¼˜åŒ–å™¨...")
            history, final_params, optimizer = self.simple_neural_network(
                X, y, optimizer_type=opt_type, learning_rate=0.01, epochs=1000
            )
            
            results[name] = {
                'history': history,
                'final_params': final_params,
                'optimizer': optimizer,
                'final_loss': history['losses'][-1],
                'final_accuracy': history['accuracies'][-1],
                'epochs_trained': len(history['losses'])
            }
        
        # å¯è§†åŒ–ç»“æœ
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. æŸå¤±æ›²çº¿å¯¹æ¯”
        ax1 = axes[0, 0]
        for i, (name, result) in enumerate(results.items()):
            ax1.semilogy(result['history']['losses'], 
                        color=self.colors[i], label=name, linewidth=2)
        
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss (log scale)')
        ax1.set_title('Training Loss Comparison')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å‡†ç¡®ç‡æ›²çº¿å¯¹æ¯”
        ax2 = axes[0, 1]
        for i, (name, result) in enumerate(results.items()):
            ax2.plot(result['history']['accuracies'], 
                    color=self.colors[i], label=name, linewidth=2)
        
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.set_title('Training Accuracy Comparison')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
        ax3 = axes[0, 2]
        convergence_epochs = []
        names = []
        
        for name, result in results.items():
            # æ‰¾åˆ°è¾¾åˆ°90%æœ€ç»ˆå‡†ç¡®ç‡çš„epoch
            target_acc = result['final_accuracy'] * 0.9
            conv_epoch = len(result['history']['accuracies'])
            
            for i, acc in enumerate(result['history']['accuracies']):
                if acc >= target_acc:
                    conv_epoch = i
                    break
            
            convergence_epochs.append(conv_epoch)
            names.append(name)
        
        bars = ax3.bar(names, convergence_epochs, 
                      color=self.colors[:len(names)], alpha=0.7)
        ax3.set_ylabel('Epochs to 90% Final Accuracy')
        ax3.set_title('Convergence Speed')
        
        for bar, epochs in zip(bars, convergence_epochs):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 5,
                    f'{epochs}', ha='center', va='bottom')
        
        # 4. æ•°æ®åˆ†å¸ƒå’Œå†³ç­–è¾¹ç•Œ
        ax4 = axes[1, 0]
        
        # ç»˜åˆ¶æ•°æ®ç‚¹
        scatter = ax4.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
        ax4.set_xlabel('Feature 1')
        ax4.set_ylabel('Feature 2')
        ax4.set_title('Training Data Distribution')
        plt.colorbar(scatter, ax=ax4)
        
        # 5. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”
        ax5 = axes[1, 1]
        
        final_losses = [result['final_loss'] for result in results.values()]
        final_accuracies = [result['final_accuracy'] for result in results.values()]
        
        x_pos = np.arange(len(names))
        width = 0.35
        
        # å½’ä¸€åŒ–æŸå¤±ç”¨äºæ˜¾ç¤º
        normalized_losses = np.array(final_losses) / max(final_losses)
        
        bars1 = ax5.bar(x_pos - width/2, normalized_losses, width, 
                       label='Normalized Loss', alpha=0.7, color='red')
        bars2 = ax5.bar(x_pos + width/2, final_accuracies, width,
                       label='Accuracy', alpha=0.7, color='blue')
        
        ax5.set_xlabel('Optimizer')
        ax5.set_ylabel('Performance')
        ax5.set_title('Final Performance Comparison')
        ax5.set_xticks(x_pos)
        ax5.set_xticklabels(names)
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # 6. è®­ç»ƒæ•ˆç‡åˆ†æ
        ax6 = axes[1, 2]
        
        # è®¡ç®—è®­ç»ƒæ•ˆç‡ï¼ˆå‡†ç¡®ç‡/epochï¼‰
        efficiency = [result['final_accuracy'] / result['epochs_trained'] 
                     for result in results.values()]
        
        bars = ax6.bar(names, efficiency, color=self.colors[:len(names)], alpha=0.7)
        ax6.set_ylabel('Training Efficiency (Accuracy/Epoch)')
        ax6.set_title('Training Efficiency Comparison')
        
        for bar, eff in zip(bars, efficiency):
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{eff:.4f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š ç¥ç»ç½‘ç»œè®­ç»ƒç»“æœå¯¹æ¯”:")
        print(f"{'Optimizer':<12} {'Final Loss':<12} {'Final Acc':<12} {'Epochs':<8} {'Efficiency':<12}")
        print("-" * 70)
        
        for name, result in results.items():
            eff = result['final_accuracy'] / result['epochs_trained']
            print(f"{name:<12} {result['final_loss']:<12.6f} "
                  f"{result['final_accuracy']:<12.4f} {result['epochs_trained']:<8} "
                  f"{eff:<12.6f}")
        
        return results

### 6.2 è¶…å‚æ•°ä¼˜åŒ–æ¡ˆä¾‹

```python
class HyperparameterOptimizationCase:
    """è¶…å‚æ•°ä¼˜åŒ–æ¡ˆä¾‹"""
    
    def __init__(self):
        self.colors = ['red', 'blue', 'green', 'orange', 'purple']
    
    def objective_function(self, params):
        """ç›®æ ‡å‡½æ•°ï¼šæ¨¡æ‹Ÿæ¨¡å‹éªŒè¯æŸå¤±"""
        lr, momentum, weight_decay = params
        
        # æ¨¡æ‹Ÿå¤æ‚çš„è¶…å‚æ•°-æ€§èƒ½å…³ç³»
        # æœ€ä¼˜ç‚¹å¤§çº¦åœ¨ lr=0.01, momentum=0.9, weight_decay=0.001
        optimal_lr = 0.01
        optimal_momentum = 0.9
        optimal_wd = 0.001
        
        # è®¡ç®—è·ç¦»æœ€ä¼˜ç‚¹çš„åå·®
        lr_penalty = ((np.log10(lr) - np.log10(optimal_lr)) ** 2) * 2
        momentum_penalty = ((momentum - optimal_momentum) ** 2) * 10
        wd_penalty = ((np.log10(weight_decay + 1e-6) - np.log10(optimal_wd)) ** 2) * 1
        
        # æ·»åŠ å™ªå£°æ¨¡æ‹Ÿå®éªŒä¸ç¡®å®šæ€§
        noise = np.random.normal(0, 0.01)
        
        loss = lr_penalty + momentum_penalty + wd_penalty + 0.1 + noise
        
        return loss
    
    def gradient_objective(self, params):
        """ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦ï¼ˆæ•°å€¼è®¡ç®—ï¼‰"""
        eps = 1e-6
        gradients = []
        
        base_loss = self.objective_function(params)
        
        for i in range(len(params)):
            params_plus = params.copy()
            params_plus[i] += eps
            
            loss_plus = self.objective_function(params_plus)
            grad = (loss_plus - base_loss) / eps
            gradients.append(grad)
        
        return np.array(gradients)
    
    def optimize_hyperparameters(self):
        """ä½¿ç”¨ä¸åŒä¼˜åŒ–å™¨è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        print(f"\nğŸ›ï¸ è¶…å‚æ•°ä¼˜åŒ–æ¡ˆä¾‹")
        
        # åˆå§‹è¶…å‚æ•° [learning_rate, momentum, weight_decay]
        initial_params = np.array([0.1, 0.5, 0.01])
        
        # å‚æ•°è¾¹ç•Œ
        bounds = {
            'lr': (1e-4, 1e-1),
            'momentum': (0.0, 0.99),
            'weight_decay': (1e-5, 1e-1)
        }
        
        def clip_params(params):
            """å°†å‚æ•°é™åˆ¶åœ¨åˆç†èŒƒå›´å†…"""
            clipped = params.copy()
            clipped[0] = np.clip(clipped[0], bounds['lr'][0], bounds['lr'][1])
            clipped[1] = np.clip(clipped[1], bounds['momentum'][0], bounds['momentum'][1])
            clipped[2] = np.clip(clipped[2], bounds['weight_decay'][0], bounds['weight_decay'][1])
            return clipped
        
        # æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨
        optimizers = {
            'SGD': SGDOptimizer(learning_rate=0.01),
            'Momentum': MomentumOptimizer(learning_rate=0.01, momentum=0.9),
            'Adam': AdamOptimizer(learning_rate=0.01),
            'Trae': TraeOptimizer(
                optimizer_type='adam',
                learning_rate=0.01,
                schedule_type='exponential',
                clip_norm=0.1
            )
        }
        
        results = {}
        num_iterations = 200
        
        for name, optimizer in optimizers.items():
            print(f"\n   ä½¿ç”¨ {name} ä¼˜åŒ–è¶…å‚æ•°...")
            
            optimizer.reset_history()
            params = [initial_params.copy()]
            losses = []
            
            for iteration in range(num_iterations):
                current_params = params[-1]
                
                # è®¡ç®—ç›®æ ‡å‡½æ•°å€¼å’Œæ¢¯åº¦
                current_loss = self.objective_function(current_params)
                gradients = [self.gradient_objective(current_params)]
                
                # ä¼˜åŒ–æ­¥éª¤
                if name == 'Trae':
                    new_params = optimizer.trae_step(params, gradients, current_loss)
                else:
                    new_params = optimizer.step(params, gradients)
                
                # é™åˆ¶å‚æ•°èŒƒå›´
                new_params[0] = clip_params(new_params[0])
                params = new_params
                
                losses.append(current_loss)
                
                # æ—©åœ
                if current_loss < 0.11:  # æ¥è¿‘æœ€ä¼˜å€¼
                    break
            
            results[name] = {
                'params_history': np.array([p[0] for p in (optimizer.trae_history['parameters'] 
                                                         if name == 'Trae' 
                                                         else optimizer.history)]),
                'losses': losses,
                'final_params': params[0],
                'final_loss': losses[-1],
                'iterations': len(losses)
            }
        
        # å¯è§†åŒ–ç»“æœ
        self.visualize_hyperparameter_optimization(results)
        
        return results
    
    def visualize_hyperparameter_optimization(self, results):
        """å¯è§†åŒ–è¶…å‚æ•°ä¼˜åŒ–ç»“æœ"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. æŸå¤±æ”¶æ•›æ›²çº¿
        ax1 = axes[0, 0]
        for i, (name, result) in enumerate(results.items()):
            ax1.semilogy(result['losses'], 
                        color=self.colors[i], label=name, linewidth=2)
        
        ax1.set_xlabel('Iteration')
        ax1.set_ylabel('Validation Loss (log scale)')
        ax1.set_title('Hyperparameter Optimization Convergence')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å­¦ä¹ ç‡ä¼˜åŒ–è½¨è¿¹
        ax2 = axes[0, 1]
        for i, (name, result) in enumerate(results.items()):
            if len(result['params_history']) > 0:
                lr_trajectory = result['params_history'][:, 0]
                ax2.semilogx(lr_trajectory, 
                           color=self.colors[i], label=name, linewidth=2, marker='o', markersize=3)
        
        ax2.axvline(x=0.01, color='red', linestyle='--', alpha=0.7, label='Optimal LR')
        ax2.set_xlabel('Iteration')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('Learning Rate Optimization Trajectory')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. åŠ¨é‡ä¼˜åŒ–è½¨è¿¹
        ax3 = axes[0, 2]
        for i, (name, result) in enumerate(results.items()):
            if len(result['params_history']) > 0:
                momentum_trajectory = result['params_history'][:, 1]
                ax3.plot(momentum_trajectory, 
                        color=self.colors[i], label=name, linewidth=2, marker='o', markersize=3)
        
        ax3.axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='Optimal Momentum')
        ax3.set_xlabel('Iteration')
        ax3.set_ylabel('Momentum')
        ax3.set_title('Momentum Optimization Trajectory')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. æƒé‡è¡°å‡ä¼˜åŒ–è½¨è¿¹
        ax4 = axes[1, 0]
        for i, (name, result) in enumerate(results.items()):
            if len(result['params_history']) > 0:
                wd_trajectory = result['params_history'][:, 2]
                ax4.semilogy(wd_trajectory, 
                           color=self.colors[i], label=name, linewidth=2, marker='o', markersize=3)
        
        ax4.axhline(y=0.001, color='red', linestyle='--', alpha=0.7, label='Optimal WD')
        ax4.set_xlabel('Iteration')
        ax4.set_ylabel('Weight Decay (log scale)')
        ax4.set_title('Weight Decay Optimization Trajectory')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. æœ€ç»ˆè¶…å‚æ•°å¯¹æ¯”
        ax5 = axes[1, 1]
        
        param_names = ['Learning Rate', 'Momentum', 'Weight Decay']
        optimal_values = [0.01, 0.9, 0.001]
        
        x_pos = np.arange(len(param_names))
        width = 0.15
        
        for i, (name, result) in enumerate(results.items()):
            final_params = result['final_params']
            offset = (i - len(results)/2 + 0.5) * width
            
            bars = ax5.bar(x_pos + offset, final_params, width, 
                          label=name, alpha=0.7, color=self.colors[i])
        
        # æ·»åŠ æœ€ä¼˜å€¼å‚è€ƒçº¿
        for i, opt_val in enumerate(optimal_values):
            ax5.axhline(y=opt_val, xmin=(i-0.4)/len(param_names), 
                       xmax=(i+0.4)/len(param_names), 
                       color='red', linestyle='--', alpha=0.7)
        
        ax5.set_xlabel('Hyperparameter')
        ax5.set_ylabel('Value')
        ax5.set_title('Final Hyperparameter Values')
        ax5.set_xticks(x_pos)
        ax5.set_xticklabels(param_names)
        ax5.legend()
        ax5.set_yscale('log')
        
        # 6. ä¼˜åŒ–æ•ˆç‡å¯¹æ¯”
        ax6 = axes[1, 2]
        
        optimizer_names = list(results.keys())
        final_losses = [result['final_loss'] for result in results.values()]
        iterations = [result['iterations'] for result in results.values()]
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        efficiency = [(1/loss) / iters for loss, iters in zip(final_losses, iterations)]
        
        bars = ax6.bar(optimizer_names, efficiency, 
                      color=self.colors[:len(optimizer_names)], alpha=0.7)
        ax6.set_ylabel('Optimization Efficiency')
        ax6.set_title('Hyperparameter Optimization Efficiency')
        
        for bar, eff in zip(bars, efficiency):
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{eff:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š è¶…å‚æ•°ä¼˜åŒ–ç»“æœ:")
        print(f"{'Optimizer':<10} {'Final Loss':<12} {'LR':<10} {'Momentum':<10} {'Weight Decay':<12} {'Iterations':<12}")
        print("-" * 80)
        
        for name, result in results.items():
            params = result['final_params']
            print(f"{name:<10} {result['final_loss']:<12.6f} "
                  f"{params[0]:<10.6f} {params[1]:<10.4f} "
                  f"{params[2]:<12.6f} {result['iterations']:<12}")

# å®é™…åº”ç”¨æ¡ˆä¾‹æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ¯ å®é™…åº”ç”¨æ¡ˆä¾‹")
print("=" * 80)

# ç¥ç»ç½‘ç»œè®­ç»ƒä¼˜åŒ–æ¡ˆä¾‹
print("\n--- ç¥ç»ç½‘ç»œè®­ç»ƒä¼˜åŒ– ---")
nn_case = NeuralNetworkOptimizationCase()
nn_results = nn_case.compare_optimizers_on_neural_network()

# è¶…å‚æ•°ä¼˜åŒ–æ¡ˆä¾‹
print("\n--- è¶…å‚æ•°ä¼˜åŒ– ---")
hp_case = HyperparameterOptimizationCase()
hp_results = hp_case.optimize_hyperparameters()

print("\nâœ… å®é™…åº”ç”¨æ¡ˆä¾‹æ¼”ç¤ºå®Œæˆ!")

## 7. æ€è€ƒé¢˜

### ğŸ¤” æ·±å…¥æ€è€ƒ

1. **ä¼˜åŒ–å™¨é€‰æ‹©ç­–ç•¥**
   - åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©SGDè€Œä¸æ˜¯Adamï¼Ÿ
   - å¦‚ä½•æ ¹æ®é—®é¢˜ç‰¹æ€§é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨ï¼Ÿ
   - ä¸åŒä¼˜åŒ–å™¨çš„è®¡ç®—å¼€é”€å¦‚ä½•æƒè¡¡ï¼Ÿ

2. **å­¦ä¹ ç‡è°ƒåº¦è®¾è®¡**
   - å¦‚ä½•è®¾è®¡é€‚åˆç‰¹å®šé—®é¢˜çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼Ÿ
   - ä½™å¼¦é€€ç«å’ŒæŒ‡æ•°è¡°å‡åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æ›´æœ‰æ•ˆï¼Ÿ
   - å¦‚ä½•å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Ÿ

3. **æ¢¯åº¦å¤„ç†æŠ€æœ¯**
   - æ¢¯åº¦è£å‰ªçš„é˜ˆå€¼åº”è¯¥å¦‚ä½•è®¾ç½®ï¼Ÿ
   - åœ¨ä»€ä¹ˆæƒ…å†µä¸‹æ¢¯åº¦å™ªå£°æ˜¯æœ‰ç›Šçš„ï¼Ÿ
   - å¦‚ä½•æ£€æµ‹å’Œå¤„ç†æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ï¼Ÿ

4. **ä¼˜åŒ–å™¨è¶…å‚æ•°**
   - Adamçš„Î²1å’ŒÎ²2å‚æ•°å¦‚ä½•å½±å“æ”¶æ•›æ€§ï¼Ÿ
   - åŠ¨é‡ç³»æ•°çš„é€‰æ‹©æœ‰ä»€ä¹ˆç»éªŒæ³•åˆ™ï¼Ÿ
   - æƒé‡è¡°å‡ç³»æ•°å¦‚ä½•ä¸å­¦ä¹ ç‡åè°ƒï¼Ÿ

5. **å®é™…åº”ç”¨è€ƒè™‘**
   - åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å¦‚ä½•é€‰æ‹©å’Œè°ƒæ•´ä¼˜åŒ–å™¨ï¼Ÿ
   - å¦‚ä½•å¤„ç†ä¸åŒå±‚éœ€è¦ä¸åŒå­¦ä¹ ç‡çš„æƒ…å†µï¼Ÿ
   - è¿ç§»å­¦ä¹ ä¸­çš„ä¼˜åŒ–ç­–ç•¥æœ‰ä»€ä¹ˆç‰¹æ®Šè€ƒè™‘ï¼Ÿ

## 8. ç« èŠ‚å°ç»“

### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µå›é¡¾

**ä¼˜åŒ–ç®—æ³•åŸºç¡€**
- **æ¢¯åº¦ä¸‹é™**: æ²¿è´Ÿæ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°çš„åŸºæœ¬ä¼˜åŒ–æ–¹æ³•
- **å­¦ä¹ ç‡**: æ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿çš„å…³é”®è¶…å‚æ•°
- **æ”¶æ•›æ€§**: ç®—æ³•è¾¾åˆ°æœ€ä¼˜è§£çš„èƒ½åŠ›å’Œé€Ÿåº¦
- **æ•°å€¼ç¨³å®šæ€§**: ç®—æ³•åœ¨è®¡ç®—è¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œé²æ£’æ€§

**ç»å…¸ä¼˜åŒ–æ–¹æ³•**
- **SGD**: ç®€å•æœ‰æ•ˆï¼Œé€‚åˆå¤§è§„æ¨¡é—®é¢˜ï¼Œä½†æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢
- **åŠ¨é‡æ³•**: åŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘éœ‡è¡ï¼Œé€‚åˆæœ‰å™ªå£°çš„æ¢¯åº¦
- **Nesterov**: é¢„æµ‹æ€§æ›´æ–°ï¼Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦

**è‡ªé€‚åº”ä¼˜åŒ–å™¨**
- **AdaGrad**: è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆç¨€ç–æ¢¯åº¦
- **RMSprop**: è§£å†³AdaGradå­¦ä¹ ç‡è¡°å‡è¿‡å¿«çš„é—®é¢˜
- **Adam**: ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå¹¿æ³›é€‚ç”¨
- **AdamW**: æ”¹è¿›çš„æƒé‡è¡°å‡ï¼Œæ›´å¥½çš„æ³›åŒ–æ€§èƒ½

### ğŸ”§ å…³é”®æŠ€æœ¯è¦ç‚¹

**å­¦ä¹ ç‡è°ƒåº¦**
- **é˜¶æ¢¯è¡°å‡**: åœ¨ç‰¹å®šepoché™ä½å­¦ä¹ ç‡
- **æŒ‡æ•°è¡°å‡**: å¹³æ»‘çš„å­¦ä¹ ç‡è¡°å‡
- **ä½™å¼¦é€€ç«**: å‘¨æœŸæ€§çš„å­¦ä¹ ç‡å˜åŒ–
- **è‡ªé€‚åº”è°ƒåº¦**: åŸºäºéªŒè¯æŸå¤±çš„åŠ¨æ€è°ƒæ•´

**æ¢¯åº¦å¤„ç†**
- **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç¨³å®šè®­ç»ƒ
- **æ¢¯åº¦ç´¯ç§¯**: æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒ
- **æ¢¯åº¦å™ªå£°**: å¸®åŠ©é€ƒç¦»å±€éƒ¨æœ€ä¼˜

**ä¼˜åŒ–æŠ€å·§**
- **æƒé‡åˆå§‹åŒ–**: å½±å“ä¼˜åŒ–çš„èµ·å§‹æ¡ä»¶
- **æ‰¹æ¬¡å¤§å°**: å½±å“æ¢¯åº¦ä¼°è®¡çš„è´¨é‡
- **æ­£åˆ™åŒ–**: é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ”¹å–„æ³›åŒ–

### ğŸ¯ å®é™…åº”ç”¨æŒ‡å—

**ä¼˜åŒ–å™¨é€‰æ‹©å»ºè®®**
- **è®¡ç®—æœºè§†è§‰**: Adamæˆ–AdamWï¼Œé…åˆä½™å¼¦é€€ç«
- **è‡ªç„¶è¯­è¨€å¤„ç†**: AdamWï¼Œè¾ƒå°çš„å­¦ä¹ ç‡
- **å¼ºåŒ–å­¦ä¹ **: Adamæˆ–RMSprop
- **å¤§è§„æ¨¡è®­ç»ƒ**: SGD with momentumï¼Œæ›´å¥½çš„æ³›åŒ–

**è¶…å‚æ•°è®¾ç½®ç»éªŒ**
- **å­¦ä¹ ç‡**: ä»0.001å¼€å§‹ï¼Œæ ¹æ®æŸå¤±æ›²çº¿è°ƒæ•´
- **æ‰¹æ¬¡å¤§å°**: 32-256ï¼Œæ ¹æ®å†…å­˜å’Œæ”¶æ•›æ€§å¹³è¡¡
- **æƒé‡è¡°å‡**: 1e-4åˆ°1e-2ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

**è°ƒè¯•å’Œç›‘æ§**
- **æŸå¤±æ›²çº¿**: ç›‘æ§è®­ç»ƒå’ŒéªŒè¯æŸå¤±
- **æ¢¯åº¦èŒƒæ•°**: æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- **å­¦ä¹ ç‡**: è§‚å¯Ÿå­¦ä¹ ç‡å¯¹æ”¶æ•›çš„å½±å“
- **å‚æ•°æ›´æ–°**: ç›‘æ§å‚æ•°å˜åŒ–çš„å¹…åº¦

### ğŸš€ è¿›é˜¶å­¦ä¹ æ–¹å‘

**ç†è®ºæ·±å…¥**
- ä¼˜åŒ–ç†è®ºï¼šå‡¸ä¼˜åŒ–ã€éå‡¸ä¼˜åŒ–ç†è®º
- æ”¶æ•›æ€§åˆ†æï¼šæ”¶æ•›é€Ÿåº¦ã€æ”¶æ•›æ¡ä»¶
- æ³›åŒ–ç†è®ºï¼šä¼˜åŒ–ä¸æ³›åŒ–çš„å…³ç³»

**å‰æ²¿æŠ€æœ¯**
- äºŒé˜¶ä¼˜åŒ–æ–¹æ³•ï¼šL-BFGSã€è‡ªç„¶æ¢¯åº¦
- åˆ†å¸ƒå¼ä¼˜åŒ–ï¼šåŒæ­¥/å¼‚æ­¥SGDã€è”é‚¦å­¦ä¹ 
- å…ƒå­¦ä¹ ä¼˜åŒ–ï¼šå­¦ä¹ ä¼˜åŒ–ç®—æ³•æœ¬èº«

**å·¥ç¨‹å®è·µ**
- å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–
- æ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–
- æ¨¡å‹å‹ç¼©ä¸­çš„ä¼˜åŒ–æŠ€æœ¯

### ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“

1. **æ²¡æœ‰ä¸‡èƒ½çš„ä¼˜åŒ–å™¨**ï¼šé€‰æ‹©éœ€è¦æ ¹æ®å…·ä½“é—®é¢˜å’Œæ•°æ®ç‰¹æ€§
2. **å­¦ä¹ ç‡æ˜¯å…³é”®**ï¼šåˆé€‚çš„å­¦ä¹ ç‡è°ƒåº¦æ¯”ä¼˜åŒ–å™¨é€‰æ‹©æ›´é‡è¦
3. **ç›‘æ§æ˜¯å¿…è¦çš„**ï¼šå®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼ŒåŠæ—¶è°ƒæ•´ç­–ç•¥
4. **å®éªŒæ˜¯ç‹é“**ï¼šç†è®ºæŒ‡å¯¼å®è·µï¼Œä½†æœ€ç»ˆéœ€è¦å®éªŒéªŒè¯
5. **å·¥ç¨‹è€ƒè™‘**ï¼šè®¡ç®—æ•ˆç‡ã€å†…å­˜ä½¿ç”¨ã€å®ç°å¤æ‚åº¦éƒ½å¾ˆé‡è¦

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- ç†è§£å„ç§ä¼˜åŒ–ç®—æ³•çš„åŸç†å’Œç‰¹ç‚¹
- æ ¹æ®é—®é¢˜é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥
- å®ç°å’Œè°ƒè¯•ä¼˜åŒ–ç®—æ³•
- åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨ä¼˜åŒ–æŠ€æœ¯
- åˆ†æå’Œè§£å†³ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„é—®é¢˜

ä¼˜åŒ–ç®—æ³•æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼ŒæŒæ¡è¿™äº›çŸ¥è¯†å°†ä¸ºä½ çš„AIå¼€å‘ä¹‹è·¯å¥ å®šåšå®çš„åŸºç¡€ï¼

print("\nâœ… ä¼˜åŒ–ç®—æ³•ç« èŠ‚å†…å®¹å®Œæˆ!")
print("ğŸ“š æ­å–œå®Œæˆæ·±åº¦å­¦ä¹ å…¥é—¨éƒ¨åˆ†çš„å­¦ä¹ !")
print("ğŸ¯ æ¥ä¸‹æ¥å¯ä»¥ç»§ç»­å­¦ä¹ æ›´é«˜çº§çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯!")
```