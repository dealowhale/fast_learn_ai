# 1.4.5 å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)

## 1. RNNæ¦‚è¿°ä¸æ ¸å¿ƒæ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯å¾ªç¯ç¥ç»ç½‘ç»œ

å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Network, RNNï¼‰æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œå…·æœ‰è®°å¿†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥åºåˆ—ã€‚

```mermaid
graph LR
    subgraph "RNNåŸºæœ¬ç»“æ„"
        A[xâ‚] --> B[RNN Cell]
        B --> C[hâ‚]
        B --> D[yâ‚]
        
        E[xâ‚‚] --> F[RNN Cell]
        F --> G[hâ‚‚]
        F --> H[yâ‚‚]
        
        I[xâ‚ƒ] --> J[RNN Cell]
        J --> K[hâ‚ƒ]
        J --> L[yâ‚ƒ]
        
        C --> F
        G --> J
    end
    
    subgraph "å±•å¼€çš„RNN"
        M["hâ‚€ (åˆå§‹çŠ¶æ€)"] --> N[RNN]
        O[xâ‚] --> N
        N --> P[hâ‚]
        N --> Q[yâ‚]
        
        P --> R[RNN]
        S[xâ‚‚] --> R
        R --> T[hâ‚‚]
        R --> U[yâ‚‚]
        
        T --> V[RNN]
        W[xâ‚ƒ] --> V
        V --> X[hâ‚ƒ]
        V --> Y[yâ‚ƒ]
    end
```

**RNNçš„æ ¸å¿ƒç‰¹ç‚¹ï¼š**
- **è®°å¿†èƒ½åŠ›**: é€šè¿‡éšè—çŠ¶æ€ä¿å­˜å†å²ä¿¡æ¯
- **å‚æ•°å…±äº«**: æ‰€æœ‰æ—¶é—´æ­¥å…±äº«ç›¸åŒçš„å‚æ•°
- **å¯å˜é•¿åº¦**: èƒ½å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—
- **æ—¶åºå»ºæ¨¡**: æ•æ‰åºåˆ—ä¸­çš„æ—¶é—´ä¾èµ–å…³ç³»

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

class SimpleRNN:
    """ç®€å•RNNå®ç°"""
    
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # åˆå§‹åŒ–å‚æ•°
        self.initialize_parameters()
        
        # è®­ç»ƒå†å²
        self.training_history = {'losses': [], 'hidden_states': []}
    
    def initialize_parameters(self):
        """åˆå§‹åŒ–ç½‘ç»œå‚æ•°"""
        np.random.seed(42)
        
        # Xavieråˆå§‹åŒ–
        # è¾“å…¥åˆ°éšè—å±‚çš„æƒé‡
        self.Wxh = np.random.randn(self.hidden_size, self.input_size) * np.sqrt(2.0 / self.input_size)
        # éšè—å±‚åˆ°éšè—å±‚çš„æƒé‡ï¼ˆå¾ªç¯è¿æ¥ï¼‰
        self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * np.sqrt(2.0 / self.hidden_size)
        # éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡
        self.Why = np.random.randn(self.output_size, self.hidden_size) * np.sqrt(2.0 / self.hidden_size)
        
        # åç½®
        self.bh = np.zeros((self.hidden_size, 1))
        self.by = np.zeros((self.output_size, 1))
    
    def tanh(self, x):
        """Tanhæ¿€æ´»å‡½æ•°"""
        return np.tanh(x)
    
    def tanh_derivative(self, x):
        """Tanhå¯¼æ•°"""
        return 1 - np.tanh(x) ** 2
    
    def softmax(self, x):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))
        return exp_x / np.sum(exp_x, axis=0, keepdims=True)
    
    def forward_propagation(self, inputs, h_prev=None):
        """å‰å‘ä¼ æ’­"""
        seq_len = len(inputs)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        if h_prev is None:
            h_prev = np.zeros((self.hidden_size, 1))
        
        # å­˜å‚¨ä¸­é—´ç»“æœ
        xs, hs, ys, ps = {}, {}, {}, {}
        hs[-1] = h_prev.copy()
        
        # å‰å‘ä¼ æ’­æ¯ä¸ªæ—¶é—´æ­¥
        for t in range(seq_len):
            xs[t] = inputs[t].reshape(-1, 1)  # è¾“å…¥å‘é‡
            
            # è®¡ç®—éšè—çŠ¶æ€
            hs[t] = self.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)
            
            # è®¡ç®—è¾“å‡º
            ys[t] = np.dot(self.Why, hs[t]) + self.by
            ps[t] = self.softmax(ys[t])  # æ¦‚ç‡åˆ†å¸ƒ
        
        return xs, hs, ys, ps
    
    def backward_propagation(self, xs, hs, ps, targets):
        """åå‘ä¼ æ’­"""
        seq_len = len(xs)
        
        # åˆå§‹åŒ–æ¢¯åº¦
        dWxh = np.zeros_like(self.Wxh)
        dWhh = np.zeros_like(self.Whh)
        dWhy = np.zeros_like(self.Why)
        dbh = np.zeros_like(self.bh)
        dby = np.zeros_like(self.by)
        dh_next = np.zeros_like(hs[0])
        
        # åå‘ä¼ æ’­æ¯ä¸ªæ—¶é—´æ­¥
        for t in reversed(range(seq_len)):
            # è¾“å‡ºå±‚æ¢¯åº¦
            dy = ps[t].copy()
            dy[targets[t]] -= 1  # äº¤å‰ç†µæŸå¤±çš„æ¢¯åº¦
            
            # è¾“å‡ºå±‚å‚æ•°æ¢¯åº¦
            dWhy += np.dot(dy, hs[t].T)
            dby += dy
            
            # éšè—å±‚æ¢¯åº¦
            dh = np.dot(self.Why.T, dy) + dh_next
            dh_raw = self.tanh_derivative(hs[t]) * dh  # é€šè¿‡tanhçš„æ¢¯åº¦
            
            # éšè—å±‚å‚æ•°æ¢¯åº¦
            dbh += dh_raw
            dWxh += np.dot(dh_raw, xs[t].T)
            dWhh += np.dot(dh_raw, hs[t-1].T)
            
            # ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æ¢¯åº¦
            dh_next = np.dot(self.Whh.T, dh_raw)
        
        # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
            np.clip(dparam, -5, 5, out=dparam)
        
        return dWxh, dWhh, dWhy, dbh, dby
    
    def update_parameters(self, dWxh, dWhh, dWhy, dbh, dby):
        """æ›´æ–°å‚æ•°"""
        self.Wxh -= self.learning_rate * dWxh
        self.Whh -= self.learning_rate * dWhh
        self.Why -= self.learning_rate * dWhy
        self.bh -= self.learning_rate * dbh
        self.by -= self.learning_rate * dby
    
    def compute_loss(self, ps, targets):
        """è®¡ç®—æŸå¤±"""
        loss = 0
        for t in range(len(targets)):
            loss += -np.log(ps[t][targets[t], 0])
        return loss
    
    def train_step(self, inputs, targets, h_prev=None):
        """å•æ­¥è®­ç»ƒ"""
        # å‰å‘ä¼ æ’­
        xs, hs, ys, ps = self.forward_propagation(inputs, h_prev)
        
        # è®¡ç®—æŸå¤±
        loss = self.compute_loss(ps, targets)
        
        # åå‘ä¼ æ’­
        dWxh, dWhh, dWhy, dbh, dby = self.backward_propagation(xs, hs, ps, targets)
        
        # æ›´æ–°å‚æ•°
        self.update_parameters(dWxh, dWhh, dWhy, dbh, dby)
        
        return loss, hs[len(inputs)-1]
    
    def predict(self, inputs, h_prev=None):
        """é¢„æµ‹"""
        xs, hs, ys, ps = self.forward_propagation(inputs, h_prev)
        predictions = [np.argmax(p) for p in ps.values()]
        return predictions, hs[len(inputs)-1]

class RNNVisualizer:
    """RNNå¯è§†åŒ–å·¥å…·"""
    
    def __init__(self, rnn_model):
        self.model = rnn_model
    
    def visualize_rnn_architecture(self):
        """å¯è§†åŒ–RNNæ¶æ„"""
        print(f"\n{'='*80}")
        print(f"ğŸ—ï¸ RNNç½‘ç»œæ¶æ„å¯è§†åŒ–")
        print(f"{'='*80}")
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # å·¦å›¾ï¼šRNNå±•å¼€ç»“æ„
        ax1.set_xlim(0, 10)
        ax1.set_ylim(0, 8)
        
        # ç»˜åˆ¶æ—¶é—´æ­¥
        time_steps = 4
        for t in range(time_steps):
            x_pos = 2 + t * 2
            
            # è¾“å…¥èŠ‚ç‚¹
            ax1.add_patch(plt.Circle((x_pos, 2), 0.3, color='lightblue', ec='black'))
            ax1.text(x_pos, 2, f'x{t+1}', ha='center', va='center', fontweight='bold')
            
            # éšè—çŠ¶æ€èŠ‚ç‚¹
            ax1.add_patch(plt.Rectangle((x_pos-0.4, 4-0.3), 0.8, 0.6, 
                                      color='lightgreen', ec='black'))
            ax1.text(x_pos, 4, f'h{t+1}', ha='center', va='center', fontweight='bold')
            
            # è¾“å‡ºèŠ‚ç‚¹
            ax1.add_patch(plt.Circle((x_pos, 6), 0.3, color='lightcoral', ec='black'))
            ax1.text(x_pos, 6, f'y{t+1}', ha='center', va='center', fontweight='bold')
            
            # è¿æ¥çº¿
            ax1.arrow(x_pos, 2.3, 0, 1.4, head_width=0.1, head_length=0.1, fc='black', ec='black')
            ax1.arrow(x_pos, 4.3, 0, 1.4, head_width=0.1, head_length=0.1, fc='black', ec='black')
            
            # å¾ªç¯è¿æ¥
            if t < time_steps - 1:
                ax1.arrow(x_pos + 0.4, 4, 1.2, 0, head_width=0.1, head_length=0.1, 
                         fc='red', ec='red', linestyle='--')
        
        ax1.set_title('RNNå±•å¼€ç»“æ„', fontsize=14, fontweight='bold')
        ax1.axis('off')
        
        # å³å›¾ï¼šå‚æ•°çŸ©é˜µå¯è§†åŒ–
        matrices = {
            'Wxh': self.model.Wxh,
            'Whh': self.model.Whh,
            'Why': self.model.Why
        }
        
        y_positions = [0.7, 0.4, 0.1]
        colors = ['Blues', 'Greens', 'Reds']
        
        for i, (name, matrix) in enumerate(matrices.items()):
            # åˆ›å»ºå­å›¾æ˜¾ç¤ºçŸ©é˜µ
            im = ax2.imshow(matrix, cmap=colors[i], aspect='auto', 
                          extent=[0, matrix.shape[1], y_positions[i], y_positions[i] + 0.2])
            ax2.text(-0.5, y_positions[i] + 0.1, name, ha='right', va='center', 
                    fontweight='bold', fontsize=12)
            
            # æ·»åŠ å°ºå¯¸æ ‡æ³¨
            ax2.text(matrix.shape[1]/2, y_positions[i] + 0.25, 
                    f'{matrix.shape[0]}Ã—{matrix.shape[1]}', 
                    ha='center', va='bottom', fontsize=10)
        
        ax2.set_xlim(-1, max([m.shape[1] for m in matrices.values()]) + 1)
        ax2.set_ylim(0, 1)
        ax2.set_title('RNNå‚æ•°çŸ©é˜µ', fontsize=14, fontweight='bold')
        ax2.axis('off')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°å‚æ•°ç»Ÿè®¡
        total_params = sum(np.prod(matrix.shape) for matrix in matrices.values())
        total_params += np.prod(self.model.bh.shape) + np.prod(self.model.by.shape)
        
        print(f"\nğŸ“Š ç½‘ç»œå‚æ•°ç»Ÿè®¡:")
        print(f"   è¾“å…¥åˆ°éšè— (Wxh): {np.prod(self.model.Wxh.shape):,}")
        print(f"   éšè—åˆ°éšè— (Whh): {np.prod(self.model.Whh.shape):,}")
        print(f"   éšè—åˆ°è¾“å‡º (Why): {np.prod(self.model.Why.shape):,}")
        print(f"   åç½®å‚æ•°: {np.prod(self.model.bh.shape) + np.prod(self.model.by.shape):,}")
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        
        return fig
    
    def visualize_hidden_states(self, sequence, targets=None):
        """å¯è§†åŒ–éšè—çŠ¶æ€æ¼”åŒ–"""
        print(f"\n{'='*80}")
        print(f"ğŸ§  éšè—çŠ¶æ€æ¼”åŒ–å¯è§†åŒ–")
        print(f"{'='*80}")
        
        # å‰å‘ä¼ æ’­è·å–éšè—çŠ¶æ€
        xs, hs, ys, ps = self.model.forward_propagation(sequence)
        
        # æå–éšè—çŠ¶æ€
        hidden_states = np.array([hs[t].flatten() for t in range(len(sequence))])
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # éšè—çŠ¶æ€çƒ­å›¾
        im1 = axes[0, 0].imshow(hidden_states.T, cmap='RdBu', aspect='auto')
        axes[0, 0].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 0].set_ylabel('éšè—å•å…ƒ')
        axes[0, 0].set_title('éšè—çŠ¶æ€æ¼”åŒ–çƒ­å›¾', fontweight='bold')
        plt.colorbar(im1, ax=axes[0, 0])
        
        # é€‰æ‹©å‡ ä¸ªéšè—å•å…ƒçš„æ—¶é—´åºåˆ—
        selected_units = min(5, self.model.hidden_size)
        for i in range(selected_units):
            axes[0, 1].plot(hidden_states[:, i], label=f'å•å…ƒ {i+1}', marker='o')
        
        axes[0, 1].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 1].set_ylabel('æ¿€æ´»å€¼')
        axes[0, 1].set_title('é€‰å®šéšè—å•å…ƒçš„æ—¶é—´åºåˆ—', fontweight='bold')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
        if len(ps) > 0:
            output_probs = np.array([ps[t].flatten() for t in range(len(sequence))])
            im2 = axes[1, 0].imshow(output_probs.T, cmap='viridis', aspect='auto')
            axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
            axes[1, 0].set_ylabel('è¾“å‡ºç±»åˆ«')
            axes[1, 0].set_title('è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ', fontweight='bold')
            plt.colorbar(im2, ax=axes[1, 0])
        
        # é¢„æµ‹vsçœŸå®ï¼ˆå¦‚æœæä¾›äº†ç›®æ ‡ï¼‰
        if targets is not None:
            predictions = [np.argmax(ps[t]) for t in range(len(sequence))]
            
            x_pos = np.arange(len(sequence))
            width = 0.35
            
            axes[1, 1].bar(x_pos - width/2, targets, width, label='çœŸå®', alpha=0.7)
            axes[1, 1].bar(x_pos + width/2, predictions, width, label='é¢„æµ‹', alpha=0.7)
            
            axes[1, 1].set_xlabel('æ—¶é—´æ­¥')
            axes[1, 1].set_ylabel('ç±»åˆ«')
            axes[1, 1].set_title('é¢„æµ‹vsçœŸå®', fontweight='bold')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = np.mean(np.array(predictions) == np.array(targets))
            axes[1, 1].text(0.02, 0.98, f'å‡†ç¡®ç‡: {accuracy:.2%}', 
                           transform=axes[1, 1].transAxes, 
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow"),
                           verticalalignment='top')
        else:
            axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()
        
        return hidden_states, ps
    
    def analyze_gradient_flow(self, sequence, targets):
        """åˆ†ææ¢¯åº¦æµåŠ¨"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ æ¢¯åº¦æµåŠ¨åˆ†æ")
        print(f"{'='*80}")
        
        # å‰å‘ä¼ æ’­
        xs, hs, ys, ps = self.model.forward_propagation(sequence)
        
        # åå‘ä¼ æ’­è·å–æ¢¯åº¦
        dWxh, dWhh, dWhy, dbh, dby = self.model.backward_propagation(xs, hs, ps, targets)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æƒé‡æ¢¯åº¦å¯è§†åŒ–
        gradients = {'dWxh': dWxh, 'dWhh': dWhh, 'dWhy': dWhy}
        
        for i, (name, grad) in enumerate(gradients.items()):
            row, col = i // 2, i % 2
            if i < 3:
                im = axes[row, col].imshow(grad, cmap='RdBu', aspect='auto')
                axes[row, col].set_title(f'{name} æ¢¯åº¦', fontweight='bold')
                plt.colorbar(im, ax=axes[row, col])
                
                # æ·»åŠ æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯
                grad_norm = np.linalg.norm(grad)
                grad_mean = np.mean(np.abs(grad))
                axes[row, col].text(0.02, 0.98, 
                                  f'èŒƒæ•°: {grad_norm:.4f}\nå¹³å‡: {grad_mean:.4f}', 
                                  transform=axes[row, col].transAxes,
                                  bbox=dict(boxstyle="round,pad=0.3", facecolor="white"),
                                  verticalalignment='top')
        
        # æ¢¯åº¦èŒƒæ•°éšæ—¶é—´çš„å˜åŒ–
        seq_len = len(sequence)
        gradient_norms = []
        
        # é€æ­¥è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„æ¢¯åº¦èŒƒæ•°
        for t in range(1, seq_len + 1):
            partial_seq = sequence[:t]
            partial_targets = targets[:t]
            
            xs_partial, hs_partial, ys_partial, ps_partial = self.model.forward_propagation(partial_seq)
            dWxh_partial, dWhh_partial, dWhy_partial, _, _ = self.model.backward_propagation(
                xs_partial, hs_partial, ps_partial, partial_targets)
            
            total_grad_norm = (np.linalg.norm(dWxh_partial) + 
                             np.linalg.norm(dWhh_partial) + 
                             np.linalg.norm(dWhy_partial))
            gradient_norms.append(total_grad_norm)
        
        axes[1, 1].plot(range(1, seq_len + 1), gradient_norms, 'b-o', linewidth=2, markersize=6)
        axes[1, 1].set_xlabel('åºåˆ—é•¿åº¦')
        axes[1, 1].set_ylabel('æ¢¯åº¦èŒƒæ•°')
        axes[1, 1].set_title('æ¢¯åº¦èŒƒæ•°éšåºåˆ—é•¿åº¦å˜åŒ–', fontweight='bold')
        axes[1, 1].grid(True, alpha=0.3)
        
        # æ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
        if len(gradient_norms) > 1:
            grad_ratio = gradient_norms[-1] / gradient_norms[0]
            if grad_ratio < 0.1:
                status = "æ¢¯åº¦æ¶ˆå¤±"
                color = "red"
            elif grad_ratio > 10:
                status = "æ¢¯åº¦çˆ†ç‚¸"
                color = "orange"
            else:
                status = "æ¢¯åº¦æ­£å¸¸"
                color = "green"
            
            axes[1, 1].text(0.02, 0.98, f'çŠ¶æ€: {status}\næ¯”ç‡: {grad_ratio:.3f}', 
                          transform=axes[1, 1].transAxes,
                          bbox=dict(boxstyle="round,pad=0.3", facecolor=color, alpha=0.3),
                          verticalalignment='top')
        
        plt.tight_layout()
        plt.show()
        
        return gradients, gradient_norms

# RNNåŸºç¡€æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ”„ å¾ªç¯ç¥ç»ç½‘ç»œ - åŸºç¡€æ¦‚å¿µæ¼”ç¤º")
print("=" * 80)

# åˆ›å»ºä¸€ä¸ªç®€å•çš„åºåˆ—åˆ†ç±»ä»»åŠ¡
# ä»»åŠ¡ï¼šè¯†åˆ«åºåˆ—ä¸­çš„æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼šè¿ç»­çš„1æˆ–è¿ç»­çš„0ï¼‰

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)

def generate_sequence_data(num_sequences=100, seq_length=10):
    """ç”Ÿæˆåºåˆ—åˆ†ç±»æ•°æ®"""
    sequences = []
    labels = []
    
    for _ in range(num_sequences):
        # ç”ŸæˆéšæœºäºŒè¿›åˆ¶åºåˆ—
        seq = np.random.randint(0, 2, seq_length)
        
        # æ ‡ç­¾ï¼šå¦‚æœåºåˆ—ä¸­æœ‰è¿ç»­3ä¸ªæˆ–æ›´å¤šç›¸åŒçš„æ•°å­—ï¼Œæ ‡è®°ä¸º1ï¼Œå¦åˆ™ä¸º0
        label = 0
        for i in range(len(seq) - 2):
            if seq[i] == seq[i+1] == seq[i+2]:
                label = 1
                break
        
        sequences.append(seq)
        labels.append(label)
    
    return sequences, labels

# ç”Ÿæˆè®­ç»ƒæ•°æ®
train_sequences, train_labels = generate_sequence_data(200, 8)
test_sequences, test_labels = generate_sequence_data(50, 8)

print(f"\nğŸ“Š åºåˆ—åˆ†ç±»ä»»åŠ¡æ•°æ®:")
print(f"   è®­ç»ƒåºåˆ—æ•°é‡: {len(train_sequences)}")
print(f"   æµ‹è¯•åºåˆ—æ•°é‡: {len(test_sequences)}")
print(f"   åºåˆ—é•¿åº¦: {len(train_sequences[0])}")
print(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(train_labels):.2%}")

# æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
print(f"\nğŸ” æ•°æ®ç¤ºä¾‹:")
for i in range(5):
    seq_str = ''.join(map(str, train_sequences[i]))
    print(f"   åºåˆ—: {seq_str} -> æ ‡ç­¾: {train_labels[i]}")

# åˆ›å»ºRNNæ¨¡å‹
rnn = SimpleRNN(input_size=1, hidden_size=10, output_size=2, learning_rate=0.01)

# åˆ›å»ºå¯è§†åŒ–å·¥å…·
visualizer = RNNVisualizer(rnn)

# å¯è§†åŒ–RNNæ¶æ„
arch_fig = visualizer.visualize_rnn_architecture()

# æµ‹è¯•å‰å‘ä¼ æ’­
print(f"\nğŸ”„ æµ‹è¯•RNNå‰å‘ä¼ æ’­:")
test_sequence = [np.array([x]) for x in train_sequences[0]]  # è½¬æ¢ä¸ºRNNè¾“å…¥æ ¼å¼
test_target = [train_labels[0]] * len(test_sequence)  # æ¯ä¸ªæ—¶é—´æ­¥éƒ½æœ‰æ ‡ç­¾

# å¯è§†åŒ–éšè—çŠ¶æ€
hidden_states, output_probs = visualizer.visualize_hidden_states(test_sequence, test_target)

# åˆ†ææ¢¯åº¦æµåŠ¨
gradients, grad_norms = visualizer.analyze_gradient_flow(test_sequence, test_target)

print(f"\nâœ… RNNåŸºç¡€æ¼”ç¤ºå®Œæˆ!")
```

## 2. LSTM (é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ)

### 2.1 LSTMçš„åŠ¨æœºä¸ç»“æ„

LSTMï¼ˆLong Short-Term Memoryï¼‰æ˜¯ä¸ºäº†è§£å†³ä¼ ç»ŸRNNçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜è€Œè®¾è®¡çš„ï¼Œé€šè¿‡é—¨æ§æœºåˆ¶æ¥æ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ã€‚

```python
class LSTM:
    """LSTMå®ç°"""
    
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # åˆå§‹åŒ–å‚æ•°
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """åˆå§‹åŒ–LSTMå‚æ•°"""
        np.random.seed(42)
        
        # é—å¿˜é—¨å‚æ•°
        self.Wf = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.1
        self.bf = np.zeros((self.hidden_size, 1))
        
        # è¾“å…¥é—¨å‚æ•°
        self.Wi = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.1
        self.bi = np.zeros((self.hidden_size, 1))
        
        # å€™é€‰å€¼å‚æ•°
        self.Wc = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.1
        self.bc = np.zeros((self.hidden_size, 1))
        
        # è¾“å‡ºé—¨å‚æ•°
        self.Wo = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.1
        self.bo = np.zeros((self.hidden_size, 1))
        
        # è¾“å‡ºå±‚å‚æ•°
        self.Wy = np.random.randn(self.output_size, self.hidden_size) * 0.1
        self.by = np.zeros((self.output_size, 1))
    
    def sigmoid(self, x):
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def tanh(self, x):
        """Tanhæ¿€æ´»å‡½æ•°"""
        return np.tanh(x)
    
    def softmax(self, x):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))
        return exp_x / np.sum(exp_x, axis=0, keepdims=True)
    
    def forward_step(self, x, h_prev, c_prev):
        """LSTMå•æ­¥å‰å‘ä¼ æ’­"""
        # æ‹¼æ¥è¾“å…¥å’Œå‰ä¸€æ—¶åˆ»çš„éšè—çŠ¶æ€
        concat = np.vstack((x, h_prev))
        
        # é—å¿˜é—¨
        f = self.sigmoid(np.dot(self.Wf, concat) + self.bf)
        
        # è¾“å…¥é—¨
        i = self.sigmoid(np.dot(self.Wi, concat) + self.bi)
        
        # å€™é€‰å€¼
        c_tilde = self.tanh(np.dot(self.Wc, concat) + self.bc)
        
        # æ›´æ–°ç»†èƒçŠ¶æ€
        c = f * c_prev + i * c_tilde
        
        # è¾“å‡ºé—¨
        o = self.sigmoid(np.dot(self.Wo, concat) + self.bo)
        
        # éšè—çŠ¶æ€
        h = o * self.tanh(c)
        
        # è¾“å‡º
        y = np.dot(self.Wy, h) + self.by
        p = self.softmax(y)
        
        # ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­
        cache = {
            'x': x, 'h_prev': h_prev, 'c_prev': c_prev,
            'concat': concat, 'f': f, 'i': i, 'c_tilde': c_tilde,
            'c': c, 'o': o, 'h': h, 'y': y, 'p': p
        }
        
        return h, c, p, cache
    
    def forward_propagation(self, inputs, h0=None, c0=None):
        """LSTMå‰å‘ä¼ æ’­"""
        seq_len = len(inputs)
        
        # åˆå§‹åŒ–çŠ¶æ€
        if h0 is None:
            h0 = np.zeros((self.hidden_size, 1))
        if c0 is None:
            c0 = np.zeros((self.hidden_size, 1))
        
        # å­˜å‚¨æ‰€æœ‰æ—¶é—´æ­¥çš„ç»“æœ
        hs, cs, ps, caches = {}, {}, {}, {}
        hs[-1], cs[-1] = h0, c0
        
        # é€æ­¥å‰å‘ä¼ æ’­
        for t in range(seq_len):
            x = inputs[t].reshape(-1, 1)
            hs[t], cs[t], ps[t], caches[t] = self.forward_step(x, hs[t-1], cs[t-1])
        
        return hs, cs, ps, caches
    
    def predict(self, inputs, h0=None, c0=None):
        """LSTMé¢„æµ‹"""
        hs, cs, ps, _ = self.forward_propagation(inputs, h0, c0)
        predictions = [np.argmax(ps[t]) for t in range(len(inputs))]
        return predictions, hs[len(inputs)-1], cs[len(inputs)-1]

class LSTMVisualizer:
    """LSTMå¯è§†åŒ–å·¥å…·"""
    
    def __init__(self, lstm_model):
        self.model = lstm_model
    
    def visualize_lstm_gates(self, sequence):
        """å¯è§†åŒ–LSTMé—¨æ§æœºåˆ¶"""
        print(f"\n{'='*80}")
        print(f"ğŸšª LSTMé—¨æ§æœºåˆ¶å¯è§†åŒ–")
        print(f"{'='*80}")
        
        # å‰å‘ä¼ æ’­è·å–é—¨æ§å€¼
        hs, cs, ps, caches = self.model.forward_propagation(sequence)
        
        # æå–é—¨æ§å€¼
        seq_len = len(sequence)
        forget_gates = np.array([caches[t]['f'].flatten() for t in range(seq_len)])
        input_gates = np.array([caches[t]['i'].flatten() for t in range(seq_len)])
        output_gates = np.array([caches[t]['o'].flatten() for t in range(seq_len)])
        cell_states = np.array([caches[t]['c'].flatten() for t in range(seq_len)])
        hidden_states = np.array([caches[t]['h'].flatten() for t in range(seq_len)])
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # é—å¿˜é—¨
        im1 = axes[0, 0].imshow(forget_gates.T, cmap='Reds', aspect='auto', vmin=0, vmax=1)
        axes[0, 0].set_title('é—å¿˜é—¨æ¿€æ´»', fontweight='bold')
        axes[0, 0].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 0].set_ylabel('éšè—å•å…ƒ')
        plt.colorbar(im1, ax=axes[0, 0])
        
        # è¾“å…¥é—¨
        im2 = axes[0, 1].imshow(input_gates.T, cmap='Blues', aspect='auto', vmin=0, vmax=1)
        axes[0, 1].set_title('è¾“å…¥é—¨æ¿€æ´»', fontweight='bold')
        axes[0, 1].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 1].set_ylabel('éšè—å•å…ƒ')
        plt.colorbar(im2, ax=axes[0, 1])
        
        # è¾“å‡ºé—¨
        im3 = axes[0, 2].imshow(output_gates.T, cmap='Greens', aspect='auto', vmin=0, vmax=1)
        axes[0, 2].set_title('è¾“å‡ºé—¨æ¿€æ´»', fontweight='bold')
        axes[0, 2].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 2].set_ylabel('éšè—å•å…ƒ')
        plt.colorbar(im3, ax=axes[0, 2])
        
        # ç»†èƒçŠ¶æ€
        im4 = axes[1, 0].imshow(cell_states.T, cmap='RdBu', aspect='auto')
        axes[1, 0].set_title('ç»†èƒçŠ¶æ€', fontweight='bold')
        axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
        axes[1, 0].set_ylabel('éšè—å•å…ƒ')
        plt.colorbar(im4, ax=axes[1, 0])
        
        # éšè—çŠ¶æ€
        im5 = axes[1, 1].imshow(hidden_states.T, cmap='viridis', aspect='auto')
        axes[1, 1].set_title('éšè—çŠ¶æ€', fontweight='bold')
        axes[1, 1].set_xlabel('æ—¶é—´æ­¥')
        axes[1, 1].set_ylabel('éšè—å•å…ƒ')
        plt.colorbar(im5, ax=axes[1, 1])
        
        # é—¨æ§ç»Ÿè®¡
        gate_means = {
            'é—å¿˜é—¨': np.mean(forget_gates, axis=1),
            'è¾“å…¥é—¨': np.mean(input_gates, axis=1),
            'è¾“å‡ºé—¨': np.mean(output_gates, axis=1)
        }
        
        for gate_name, gate_values in gate_means.items():
            axes[1, 2].plot(gate_values, label=gate_name, marker='o')
        
        axes[1, 2].set_xlabel('æ—¶é—´æ­¥')
        axes[1, 2].set_ylabel('å¹³å‡æ¿€æ´»å€¼')
        axes[1, 2].set_title('é—¨æ§å¹³å‡æ¿€æ´»å€¼', fontweight='bold')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
        axes[1, 2].set_ylim(0, 1)
        
        plt.tight_layout()
        plt.show()
        
        # åˆ†æé—¨æ§è¡Œä¸º
        print(f"\nğŸ“Š é—¨æ§è¡Œä¸ºåˆ†æ:")
        print(f"   é—å¿˜é—¨å¹³å‡æ¿€æ´»: {np.mean(forget_gates):.3f} (è¶Šé«˜è¶Šå®¹æ˜“é—å¿˜)")
        print(f"   è¾“å…¥é—¨å¹³å‡æ¿€æ´»: {np.mean(input_gates):.3f} (è¶Šé«˜è¶Šå®¹æ˜“æ¥å—æ–°ä¿¡æ¯)")
        print(f"   è¾“å‡ºé—¨å¹³å‡æ¿€æ´»: {np.mean(output_gates):.3f} (è¶Šé«˜è¶Šå®¹æ˜“è¾“å‡ºä¿¡æ¯)")
        
        return forget_gates, input_gates, output_gates, cell_states, hidden_states
    
    def compare_rnn_lstm_memory(self, long_sequence):
        """æ¯”è¾ƒRNNå’ŒLSTMçš„è®°å¿†èƒ½åŠ›"""
        print(f"\n{'='*80}")
        print(f"ğŸ§  RNN vs LSTM è®°å¿†èƒ½åŠ›å¯¹æ¯”")
        print(f"{'='*80}")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„RNNè¿›è¡Œå¯¹æ¯”
        rnn = SimpleRNN(input_size=self.model.input_size, 
                       hidden_size=self.model.hidden_size, 
                       output_size=self.model.output_size)
        
        # å‰å‘ä¼ æ’­
        lstm_hs, lstm_cs, lstm_ps, _ = self.model.forward_propagation(long_sequence)
        rnn_xs, rnn_hs, rnn_ys, rnn_ps = rnn.forward_propagation(long_sequence)
        
        # æå–éšè—çŠ¶æ€
        seq_len = len(long_sequence)
        lstm_hidden = np.array([lstm_hs[t].flatten() for t in range(seq_len)])
        rnn_hidden = np.array([rnn_hs[t].flatten() for t in range(seq_len)])
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # LSTMéšè—çŠ¶æ€
        im1 = axes[0, 0].imshow(lstm_hidden.T, cmap='viridis', aspect='auto')
        axes[0, 0].set_title('LSTMéšè—çŠ¶æ€', fontweight='bold')
        axes[0, 0].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 0].set_ylabel('éšè—å•å…ƒ')
        plt.colorbar(im1, ax=axes[0, 0])
        
        # RNNéšè—çŠ¶æ€
        im2 = axes[0, 1].imshow(rnn_hidden.T, cmap='viridis', aspect='auto')
        axes[0, 1].set_title('RNNéšè—çŠ¶æ€', fontweight='bold')
        axes[0, 1].set_xlabel('æ—¶é—´æ­¥')
        axes[0, 1].set_ylabel('éšè—å•å…ƒ')
        plt.colorbar(im2, ax=axes[0, 1])
        
        # éšè—çŠ¶æ€æ–¹å·®ï¼ˆè¡¡é‡ä¿¡æ¯ä¿æŒèƒ½åŠ›ï¼‰
        lstm_variance = np.var(lstm_hidden, axis=1)
        rnn_variance = np.var(rnn_hidden, axis=1)
        
        axes[1, 0].plot(lstm_variance, label='LSTM', color='blue', linewidth=2)
        axes[1, 0].plot(rnn_variance, label='RNN', color='red', linewidth=2)
        axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
        axes[1, 0].set_ylabel('éšè—çŠ¶æ€æ–¹å·®')
        axes[1, 0].set_title('éšè—çŠ¶æ€æ–¹å·®å¯¹æ¯”', fontweight='bold')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ¢¯åº¦èŒƒæ•°å¯¹æ¯”ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        lstm_grad_norms = []
        rnn_grad_norms = []
        
        for t in range(1, min(seq_len, 20) + 1):  # é™åˆ¶è®¡ç®—é‡
            # LSTMæ¢¯åº¦ï¼ˆç®€åŒ–ï¼‰
            lstm_grad_norm = np.linalg.norm(lstm_hidden[t-1] - lstm_hidden[0])
            lstm_grad_norms.append(lstm_grad_norm)
            
            # RNNæ¢¯åº¦ï¼ˆç®€åŒ–ï¼‰
            rnn_grad_norm = np.linalg.norm(rnn_hidden[t-1] - rnn_hidden[0])
            rnn_grad_norms.append(rnn_grad_norm)
        
        axes[1, 1].semilogy(lstm_grad_norms, label='LSTM', color='blue', linewidth=2, marker='o')
        axes[1, 1].semilogy(rnn_grad_norms, label='RNN', color='red', linewidth=2, marker='s')
        axes[1, 1].set_xlabel('æ—¶é—´æ­¥')
        axes[1, 1].set_ylabel('çŠ¶æ€å˜åŒ–èŒƒæ•° (log scale)')
        axes[1, 1].set_title('é•¿æœŸä¾èµ–ä¿æŒèƒ½åŠ›', fontweight='bold')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # è®°å¿†èƒ½åŠ›åˆ†æ
        lstm_memory_retention = np.corrcoef(lstm_hidden[0], lstm_hidden[-1])[0, 1]
        rnn_memory_retention = np.corrcoef(rnn_hidden[0], rnn_hidden[-1])[0, 1]
        
        print(f"\nğŸ” è®°å¿†èƒ½åŠ›åˆ†æ:")
        print(f"   LSTMé¦–æœ«çŠ¶æ€ç›¸å…³æ€§: {lstm_memory_retention:.3f}")
        print(f"   RNNé¦–æœ«çŠ¶æ€ç›¸å…³æ€§: {rnn_memory_retention:.3f}")
        print(f"   LSTMè®°å¿†ä¼˜åŠ¿: {(lstm_memory_retention - rnn_memory_retention):.3f}")
        
        return lstm_hidden, rnn_hidden

# LSTMæ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸ”’ é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM) æ¼”ç¤º")
print("=" * 80)

# åˆ›å»ºLSTMæ¨¡å‹
lstm = LSTM(input_size=1, hidden_size=8, output_size=2, learning_rate=0.01)

# åˆ›å»ºLSTMå¯è§†åŒ–å·¥å…·
lstm_visualizer = LSTMVisualizer(lstm)

# ç”Ÿæˆæµ‹è¯•åºåˆ—
test_sequence = [np.array([x]) for x in [1, 0, 1, 1, 0, 0, 1, 0, 1, 1]]

print(f"\nğŸ” æµ‹è¯•åºåˆ—: {''.join([str(int(x[0])) for x in test_sequence])}")

# å¯è§†åŒ–LSTMé—¨æ§æœºåˆ¶
forget_gates, input_gates, output_gates, cell_states, hidden_states = \
    lstm_visualizer.visualize_lstm_gates(test_sequence)

# ç”Ÿæˆé•¿åºåˆ—æµ‹è¯•è®°å¿†èƒ½åŠ›
long_test_sequence = [np.array([x]) for x in np.random.randint(0, 2, 30)]

# æ¯”è¾ƒRNNå’ŒLSTMçš„è®°å¿†èƒ½åŠ›
lstm_hidden, rnn_hidden = lstm_visualizer.compare_rnn_lstm_memory(long_test_sequence)

print(f"\nâœ… LSTMæ¼”ç¤ºå®Œæˆ!")
```

## 3. GRU (é—¨æ§å¾ªç¯å•å…ƒ)

### 3.1 GRUçš„è®¾è®¡ç†å¿µ

GRUï¼ˆGated Recurrent Unitï¼‰æ˜¯LSTMçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œé€šè¿‡å‡å°‘é—¨çš„æ•°é‡æ¥é™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚

```python
class GRU:
    """GRUå®ç°"""
    
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # åˆå§‹åŒ–å‚æ•°
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """åˆå§‹åŒ–GRUå‚æ•°"""
        np.random.seed(42)
        
        # é‡ç½®é—¨å‚æ•°
        self.Wr = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.1
        self.br = np.zeros((self.hidden_size, 1))
        
        # æ›´æ–°é—¨å‚æ•°
        self.Wz = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.1
        self.bz = np.zeros((self.hidden_size, 1))
        
        # å€™é€‰éšè—çŠ¶æ€å‚æ•°
        self.Wh = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.1
        self.bh = np.zeros((self.hidden_size, 1))
        
        # è¾“å‡ºå±‚å‚æ•°
        self.Wy = np.random.randn(self.output_size, self.hidden_size) * 0.1
        self.by = np.zeros((self.output_size, 1))
    
    def sigmoid(self, x):
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def tanh(self, x):
        """Tanhæ¿€æ´»å‡½æ•°"""
        return np.tanh(x)
    
    def softmax(self, x):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))
        return exp_x / np.sum(exp_x, axis=0, keepdims=True)
    
    def forward_step(self, x, h_prev):
        """GRUå•æ­¥å‰å‘ä¼ æ’­"""
        # æ‹¼æ¥è¾“å…¥å’Œå‰ä¸€æ—¶åˆ»çš„éšè—çŠ¶æ€
        concat = np.vstack((x, h_prev))
        
        # é‡ç½®é—¨
        r = self.sigmoid(np.dot(self.Wr, concat) + self.br)
        
        # æ›´æ–°é—¨
        z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)
        
        # å€™é€‰éšè—çŠ¶æ€
        concat_reset = np.vstack((x, r * h_prev))
        h_tilde = self.tanh(np.dot(self.Wh, concat_reset) + self.bh)
        
        # æ–°çš„éšè—çŠ¶æ€
        h = (1 - z) * h_prev + z * h_tilde
        
        # è¾“å‡º
        y = np.dot(self.Wy, h) + self.by
        p = self.softmax(y)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        cache = {
            'x': x, 'h_prev': h_prev, 'concat': concat,
            'r': r, 'z': z, 'h_tilde': h_tilde, 'h': h, 'y': y, 'p': p
        }
        
        return h, p, cache
    
    def forward_propagation(self, inputs, h0=None):
        """GRUå‰å‘ä¼ æ’­"""
        seq_len = len(inputs)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        if h0 is None:
            h0 = np.zeros((self.hidden_size, 1))
        
        # å­˜å‚¨æ‰€æœ‰æ—¶é—´æ­¥çš„ç»“æœ
        hs, ps, caches = {}, {}, {}
        hs[-1] = h0
        
        # é€æ­¥å‰å‘ä¼ æ’­
        for t in range(seq_len):
            x = inputs[t].reshape(-1, 1)
            hs[t], ps[t], caches[t] = self.forward_step(x, hs[t-1])
        
        return hs, ps, caches
    
    def predict(self, inputs, h0=None):
        """GRUé¢„æµ‹"""
        hs, ps, _ = self.forward_propagation(inputs, h0)
        predictions = [np.argmax(ps[t]) for t in range(len(inputs))]
        return predictions, hs[len(inputs)-1]

class RNNComparison:
    """RNNå˜ä½“å¯¹æ¯”å·¥å…·"""
    
    def __init__(self):
        pass
    
    def compare_architectures(self):
        """å¯¹æ¯”ä¸åŒRNNæ¶æ„"""
        print(f"\n{'='*80}")
        print(f"âš–ï¸ RNNæ¶æ„å¯¹æ¯”åˆ†æ")
        print(f"{'='*80}")
        
        # åˆ›å»ºä¸‰ç§æ¨¡å‹
        input_size, hidden_size, output_size = 1, 8, 2
        
        rnn = SimpleRNN(input_size, hidden_size, output_size)
        lstm = LSTM(input_size, hidden_size, output_size)
        gru = GRU(input_size, hidden_size, output_size)
        
        models = {'RNN': rnn, 'LSTM': lstm, 'GRU': gru}
        
        # è®¡ç®—å‚æ•°æ•°é‡
        def count_parameters(model):
            if isinstance(model, SimpleRNN):
                return (np.prod(model.Wxh.shape) + np.prod(model.Whh.shape) + 
                       np.prod(model.Why.shape) + np.prod(model.bh.shape) + 
                       np.prod(model.by.shape))
            elif isinstance(model, LSTM):
                return (np.prod(model.Wf.shape) + np.prod(model.Wi.shape) + 
                       np.prod(model.Wc.shape) + np.prod(model.Wo.shape) + 
                       np.prod(model.Wy.shape) + np.prod(model.bf.shape) + 
                       np.prod(model.bi.shape) + np.prod(model.bc.shape) + 
                       np.prod(model.bo.shape) + np.prod(model.by.shape))
            elif isinstance(model, GRU):
                return (np.prod(model.Wr.shape) + np.prod(model.Wz.shape) + 
                       np.prod(model.Wh.shape) + np.prod(model.Wy.shape) + 
                       np.prod(model.br.shape) + np.prod(model.bz.shape) + 
                       np.prod(model.bh.shape) + np.prod(model.by.shape))
        
        # ç»Ÿè®¡å‚æ•°
        param_counts = {name: count_parameters(model) for name, model in models.items()}
        
        # æµ‹è¯•åºåˆ—
        test_sequence = [np.array([x]) for x in [1, 0, 1, 1, 0, 0, 1, 0]]
        
        # æµ‹è¯•æ€§èƒ½
        results = {}
        for name, model in models.items():
            if isinstance(model, SimpleRNN):
                predictions, final_h = model.predict(test_sequence)
                results[name] = {'predictions': predictions, 'final_state': final_h}
            elif isinstance(model, LSTM):
                predictions, final_h, final_c = model.predict(test_sequence)
                results[name] = {'predictions': predictions, 'final_state': final_h}
            elif isinstance(model, GRU):
                predictions, final_h = model.predict(test_sequence)
                results[name] = {'predictions': predictions, 'final_state': final_h}
        
        # å¯è§†åŒ–å¯¹æ¯”
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # å‚æ•°æ•°é‡å¯¹æ¯”
        names = list(param_counts.keys())
        counts = list(param_counts.values())
        colors = ['skyblue', 'lightgreen', 'lightcoral']
        
        bars = axes[0, 0].bar(names, counts, color=colors)
        axes[0, 0].set_ylabel('å‚æ•°æ•°é‡')
        axes[0, 0].set_title('å‚æ•°æ•°é‡å¯¹æ¯”', fontweight='bold')
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar, count in zip(bars, counts):
            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                           str(count), ha='center', va='bottom', fontweight='bold')
        
        # è®¡ç®—å¤æ‚åº¦å¯¹æ¯”
        complexities = {
            'RNN': 'O(W)',
            'LSTM': 'O(4W)',
            'GRU': 'O(3W)'
        }
        
        complexity_values = [1, 4, 3]  # ç›¸å¯¹å¤æ‚åº¦
        bars2 = axes[0, 1].bar(names, complexity_values, color=colors)
        axes[0, 1].set_ylabel('ç›¸å¯¹è®¡ç®—å¤æ‚åº¦')
        axes[0, 1].set_title('è®¡ç®—å¤æ‚åº¦å¯¹æ¯”', fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)
        
        for bar, comp in zip(bars2, complexities.values()):
            axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                           comp, ha='center', va='bottom', fontweight='bold')
        
        # ç‰¹æ€§å¯¹æ¯”é›·è¾¾å›¾
        categories = ['å‚æ•°æ•ˆç‡', 'æ¢¯åº¦ç¨³å®šæ€§', 'é•¿æœŸè®°å¿†', 'è®¡ç®—é€Ÿåº¦', 'å®ç°ç®€å•æ€§']
        
        # è¯„åˆ† (1-5åˆ†)
        scores = {
            'RNN': [5, 2, 2, 5, 5],
            'LSTM': [2, 5, 5, 2, 2],
            'GRU': [3, 4, 4, 3, 3]
        }
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        ax_radar = plt.subplot(2, 2, 3, projection='polar')
        
        for i, (name, score) in enumerate(scores.items()):
            score += score[:1]  # é—­åˆå›¾å½¢
            ax_radar.plot(angles, score, 'o-', linewidth=2, label=name, color=colors[i])
            ax_radar.fill(angles, score, alpha=0.25, color=colors[i])
        
        ax_radar.set_xticks(angles[:-1])
        ax_radar.set_xticklabels(categories)
        ax_radar.set_ylim(0, 5)
        ax_radar.set_title('ç»¼åˆç‰¹æ€§å¯¹æ¯”', fontweight='bold', pad=20)
        ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        # åº”ç”¨åœºæ™¯æ¨è
        axes[1, 1].axis('off')
        
        recommendations = [
            "ğŸ“Š åº”ç”¨åœºæ™¯æ¨è:",
            "",
            "ğŸ”µ RNN:",
            "  â€¢ çŸ­åºåˆ—å¤„ç†",
            "  â€¢ è®¡ç®—èµ„æºå—é™",
            "  â€¢ ç®€å•æ—¶åºä»»åŠ¡",
            "",
            "ğŸŸ¢ LSTM:",
            "  â€¢ é•¿åºåˆ—å»ºæ¨¡",
            "  â€¢ å¤æ‚æ—¶åºä¾èµ–",
            "  â€¢ æ¢¯åº¦ç¨³å®šæ€§è¦æ±‚é«˜",
            "",
            "ğŸŸ  GRU:",
            "  â€¢ å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡",
            "  â€¢ ä¸­ç­‰é•¿åº¦åºåˆ—",
            "  â€¢ å¿«é€ŸåŸå‹å¼€å‘"
        ]
        
        for i, text in enumerate(recommendations):
            axes[1, 1].text(0.05, 0.95 - i*0.06, text, 
                           transform=axes[1, 1].transAxes,
                           fontsize=10, fontweight='bold' if 'ğŸ“Š' in text or 'ğŸ”µ' in text or 'ğŸŸ¢' in text or 'ğŸŸ ' in text else 'normal')
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°è¯¦ç»†å¯¹æ¯”
        print(f"\nğŸ“Š è¯¦ç»†å¯¹æ¯”åˆ†æ:")
        print(f"{'æ¨¡å‹':<8} {'å‚æ•°æ•°é‡':<10} {'ç›¸å¯¹å¤æ‚åº¦':<12} {'ä¸»è¦ä¼˜åŠ¿':<20} {'ä¸»è¦åŠ£åŠ¿'}")
        print(f"{'-'*70}")
        print(f"{'RNN':<8} {param_counts['RNN']:<10} {'1x':<12} {'ç®€å•å¿«é€Ÿ':<20} {'æ¢¯åº¦æ¶ˆå¤±'}")
        print(f"{'LSTM':<8} {param_counts['LSTM']:<10} {'4x':<12} {'é•¿æœŸè®°å¿†':<20} {'è®¡ç®—å¤æ‚'}")
        print(f"{'GRU':<8} {param_counts['GRU']:<10} {'3x':<12} {'å¹³è¡¡æ€§èƒ½':<20} {'è®°å¿†ç•¥å¼±'}")
        
        return param_counts, results
    
    def demonstrate_vanishing_gradient(self):
        """æ¼”ç¤ºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‰ æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ¼”ç¤º")
        print(f"{'='*80}")
        
        # åˆ›å»ºé•¿åºåˆ—
        seq_length = 50
        long_sequence = [np.array([np.sin(0.1 * t)]) for t in range(seq_length)]
        
        # åˆ›å»ºç®€å•RNN
        rnn = SimpleRNN(input_size=1, hidden_size=10, output_size=2)
        
        # æ¨¡æ‹Ÿæ¢¯åº¦è®¡ç®—
        gradient_norms = []
        
        for length in range(5, seq_length, 5):
            # æˆªå–ä¸åŒé•¿åº¦çš„åºåˆ—
            sub_sequence = long_sequence[:length]
            targets = [0] * length  # ç®€å•ç›®æ ‡
            
            # å‰å‘ä¼ æ’­
            xs, hs, ys, ps = rnn.forward_propagation(sub_sequence)
            
            # åå‘ä¼ æ’­
            dWxh, dWhh, dWhy, dbh, dby = rnn.backward_propagation(xs, hs, ps, targets)
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            total_grad_norm = np.linalg.norm(dWxh) + np.linalg.norm(dWhh) + np.linalg.norm(dWhy)
            gradient_norms.append(total_grad_norm)
        
        # å¯è§†åŒ–æ¢¯åº¦æ¶ˆå¤±
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ¢¯åº¦èŒƒæ•°éšåºåˆ—é•¿åº¦å˜åŒ–
        lengths = list(range(5, seq_length, 5))
        ax1.semilogy(lengths, gradient_norms, 'r-o', linewidth=2, markersize=6)
        ax1.set_xlabel('åºåˆ—é•¿åº¦')
        ax1.set_ylabel('æ¢¯åº¦èŒƒæ•° (log scale)')
        ax1.set_title('RNNæ¢¯åº¦æ¶ˆå¤±ç°è±¡', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ¢¯åº¦æ¶ˆå¤±é˜ˆå€¼çº¿
        ax1.axhline(y=1e-5, color='red', linestyle='--', alpha=0.7, label='æ¢¯åº¦æ¶ˆå¤±é˜ˆå€¼')
        ax1.legend()
        
        # æ¿€æ´»å‡½æ•°é¥±å’Œåˆ†æ
        x = np.linspace(-5, 5, 1000)
        tanh_vals = np.tanh(x)
        tanh_grads = 1 - tanh_vals**2
        
        ax2.plot(x, tanh_vals, 'b-', linewidth=2, label='tanh(x)')
        ax2.plot(x, tanh_grads, 'r-', linewidth=2, label="tanh'(x)")
        ax2.set_xlabel('x')
        ax2.set_ylabel('å€¼')
        ax2.set_title('Tanhæ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°', fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æ ‡æ³¨é¥±å’ŒåŒºåŸŸ
        ax2.axvspan(-5, -2, alpha=0.2, color='red', label='é¥±å’ŒåŒºåŸŸ')
        ax2.axvspan(2, 5, alpha=0.2, color='red')
        
        plt.tight_layout()
        plt.show()
        
        # åˆ†æç»“æœ
        initial_grad = gradient_norms[0]
        final_grad = gradient_norms[-1]
        decay_ratio = final_grad / initial_grad
        
        print(f"\nğŸ“‰ æ¢¯åº¦æ¶ˆå¤±åˆ†æ:")
        print(f"   åˆå§‹æ¢¯åº¦èŒƒæ•°: {initial_grad:.6f}")
        print(f"   æœ€ç»ˆæ¢¯åº¦èŒƒæ•°: {final_grad:.6f}")
        print(f"   è¡°å‡æ¯”ä¾‹: {decay_ratio:.6f}")
        print(f"   è¡°å‡ç¨‹åº¦: {(1-decay_ratio)*100:.2f}%")
        
        if decay_ratio < 0.01:
            print(f"   âš ï¸  ä¸¥é‡æ¢¯åº¦æ¶ˆå¤±ï¼")
        elif decay_ratio < 0.1:
            print(f"   âš ï¸  ä¸­åº¦æ¢¯åº¦æ¶ˆå¤±")
        else:
            print(f"   âœ… æ¢¯åº¦ç›¸å¯¹ç¨³å®š")
        
        return gradient_norms, lengths

# RNNå˜ä½“å¯¹æ¯”æ¼”ç¤º
print("\n" + "=" * 80)
print("âš–ï¸ RNNå˜ä½“å…¨é¢å¯¹æ¯”")
print("=" * 80)

# åˆ›å»ºå¯¹æ¯”å·¥å…·
comparison = RNNComparison()

# å¯¹æ¯”ä¸åŒæ¶æ„
param_counts, results = comparison.compare_architectures()

# æ¼”ç¤ºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜
gradient_norms, lengths = comparison.demonstrate_vanishing_gradient()

print(f"\nâœ… RNNå˜ä½“å¯¹æ¯”å®Œæˆ!")

## 5. å®é™…åº”ç”¨æ¡ˆä¾‹

### 5.1 æ–‡æœ¬ç”Ÿæˆ

```python
class TextGenerator:
    """åŸºäºRNNçš„æ–‡æœ¬ç”Ÿæˆå™¨"""
    
    def __init__(self, vocab_size, hidden_size=128):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        
        # ä½¿ç”¨LSTMä½œä¸ºæ ¸å¿ƒ
        self.lstm = LSTM(vocab_size, hidden_size, vocab_size)
        
        # å­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„
        self.char_to_idx = {}
        self.idx_to_char = {}
    
    def prepare_data(self, text):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # åˆ›å»ºå­—ç¬¦æ˜ å°„
        chars = sorted(list(set(text)))
        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}
        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}
        
        # è½¬æ¢æ–‡æœ¬ä¸ºæ•°å­—åºåˆ—
        data = [self.char_to_idx[ch] for ch in text]
        return data
    
    def create_sequences(self, data, seq_length=25):
        """åˆ›å»ºè®­ç»ƒåºåˆ—"""
        sequences = []
        targets = []
        
        for i in range(len(data) - seq_length):
            seq = data[i:i + seq_length]
            target = data[i + 1:i + seq_length + 1]
            sequences.append(seq)
            targets.append(target)
        
        return sequences, targets
    
    def train(self, text, epochs=100, seq_length=25, learning_rate=0.01):
        """è®­ç»ƒæ–‡æœ¬ç”Ÿæˆæ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒæ–‡æœ¬ç”Ÿæˆæ¨¡å‹...")
        
        # å‡†å¤‡æ•°æ®
        data = self.prepare_data(text)
        sequences, targets = self.create_sequences(data, seq_length)
        
        losses = []
        
        for epoch in range(epochs):
            total_loss = 0
            
            for seq, target in zip(sequences[:100], targets[:100]):  # é™åˆ¶è®­ç»ƒæ ·æœ¬
                # è½¬æ¢ä¸ºone-hotç¼–ç 
                input_seq = []
                for idx in seq:
                    one_hot = np.zeros(self.vocab_size)
                    one_hot[idx] = 1
                    input_seq.append(one_hot)
                
                # å‰å‘ä¼ æ’­
                outputs = self.lstm.forward(input_seq)
                
                # è®¡ç®—æŸå¤±ï¼ˆç®€åŒ–ç‰ˆäº¤å‰ç†µï¼‰
                loss = 0
                for i, target_idx in enumerate(target):
                    pred = outputs[i]
                    # Softmax
                    exp_pred = np.exp(pred - np.max(pred))
                    softmax_pred = exp_pred / np.sum(exp_pred)
                    loss -= np.log(softmax_pred[target_idx] + 1e-8)
                
                total_loss += loss / len(target)
            
            avg_loss = total_loss / min(100, len(sequences))
            losses.append(avg_loss)
            
            if epoch % 20 == 0:
                print(f"   Epoch {epoch:3d}: Loss = {avg_loss:.4f}")
        
        return losses
    
    def generate_text(self, seed_text, length=100, temperature=1.0):
        """ç”Ÿæˆæ–‡æœ¬"""
        if not self.char_to_idx:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨trainæ–¹æ³•")
        
        # åˆå§‹åŒ–
        generated = seed_text
        current_seq = [self.char_to_idx.get(ch, 0) for ch in seed_text[-25:]]
        
        for _ in range(length):
            # è½¬æ¢ä¸ºone-hot
            input_seq = []
            for idx in current_seq:
                one_hot = np.zeros(self.vocab_size)
                one_hot[idx] = 1
                input_seq.append(one_hot)
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦
            outputs = self.lstm.forward(input_seq)
            next_char_logits = outputs[-1]
            
            # åº”ç”¨æ¸©åº¦é‡‡æ ·
            next_char_logits = next_char_logits / temperature
            exp_logits = np.exp(next_char_logits - np.max(next_char_logits))
            probabilities = exp_logits / np.sum(exp_logits)
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦
            next_char_idx = np.random.choice(len(probabilities), p=probabilities)
            next_char = self.idx_to_char[next_char_idx]
            
            generated += next_char
            
            # æ›´æ–°åºåˆ—
            current_seq = current_seq[1:] + [next_char_idx]
        
        return generated

### 5.2 æƒ…æ„Ÿåˆ†æ

```python
class SentimentAnalyzer:
    """åŸºäºRNNçš„æƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self, vocab_size, hidden_size=64, num_classes=2):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        
        # ä½¿ç”¨GRUè¿›è¡Œæƒ…æ„Ÿåˆ†æ
        self.gru = GRU(vocab_size, hidden_size, num_classes)
        
        # è¯æ±‡è¡¨
        self.word_to_idx = {}
        self.idx_to_word = {}
    
    def prepare_vocabulary(self, texts):
        """æ„å»ºè¯æ±‡è¡¨"""
        all_words = set()
        for text in texts:
            words = text.lower().split()
            all_words.update(words)
        
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°
        vocab = ['<PAD>', '<UNK>'] + sorted(list(all_words))
        self.word_to_idx = {word: i for i, word in enumerate(vocab)}
        self.idx_to_word = {i: word for i, word in enumerate(vocab)}
        
        return len(vocab)
    
    def text_to_sequence(self, text, max_length=50):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—"""
        words = text.lower().split()
        sequence = []
        
        for word in words[:max_length]:
            idx = self.word_to_idx.get(word, self.word_to_idx['<UNK>'])
            sequence.append(idx)
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(sequence) < max_length:
            sequence.append(self.word_to_idx['<PAD>'])
        
        return sequence
    
    def train(self, texts, labels, epochs=50, learning_rate=0.01):
        """è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        
        # å‡†å¤‡è¯æ±‡è¡¨
        actual_vocab_size = self.prepare_vocabulary(texts)
        
        # è½¬æ¢æ•°æ®
        sequences = [self.text_to_sequence(text) for text in texts]
        
        losses = []
        accuracies = []
        
        for epoch in range(epochs):
            total_loss = 0
            correct_predictions = 0
            
            for seq, label in zip(sequences, labels):
                # è½¬æ¢ä¸ºone-hotç¼–ç 
                input_seq = []
                for idx in seq:
                    one_hot = np.zeros(actual_vocab_size)
                    if idx < actual_vocab_size:
                        one_hot[idx] = 1
                    input_seq.append(one_hot)
                
                # å‰å‘ä¼ æ’­
                outputs = self.gru.forward(input_seq)
                final_output = outputs[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªè¾“å‡º
                
                # Softmax
                exp_output = np.exp(final_output - np.max(final_output))
                probabilities = exp_output / np.sum(exp_output)
                
                # è®¡ç®—æŸå¤±
                loss = -np.log(probabilities[label] + 1e-8)
                total_loss += loss
                
                # è®¡ç®—å‡†ç¡®ç‡
                prediction = np.argmax(probabilities)
                if prediction == label:
                    correct_predictions += 1
            
            avg_loss = total_loss / len(sequences)
            accuracy = correct_predictions / len(sequences)
            
            losses.append(avg_loss)
            accuracies.append(accuracy)
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch:3d}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}")
        
        return losses, accuracies
    
    def predict_sentiment(self, text):
        """é¢„æµ‹æ–‡æœ¬æƒ…æ„Ÿ"""
        if not self.word_to_idx:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨trainæ–¹æ³•")
        
        # è½¬æ¢æ–‡æœ¬
        sequence = self.text_to_sequence(text)
        
        # è½¬æ¢ä¸ºone-hotç¼–ç 
        input_seq = []
        for idx in sequence:
            one_hot = np.zeros(len(self.word_to_idx))
            if idx < len(self.word_to_idx):
                one_hot[idx] = 1
            input_seq.append(one_hot)
        
        # å‰å‘ä¼ æ’­
        outputs = self.gru.forward(input_seq)
        final_output = outputs[-1]
        
        # Softmax
        exp_output = np.exp(final_output - np.max(final_output))
        probabilities = exp_output / np.sum(exp_output)
        
        # è¿”å›é¢„æµ‹ç»“æœ
        prediction = np.argmax(probabilities)
        confidence = probabilities[prediction]
        
        sentiment = "æ­£é¢" if prediction == 1 else "è´Ÿé¢"
        return sentiment, confidence, probabilities

# åº”ç”¨æ¡ˆä¾‹æ¼”ç¤º
print("\n" + "=" * 80)
print("ğŸš€ RNNå®é™…åº”ç”¨æ¡ˆä¾‹æ¼”ç¤º")
print("=" * 80)

# æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
print("\nğŸ“ æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹:")
sample_text = "hello world this is a simple text generation example using rnn lstm"
generator = TextGenerator(vocab_size=50)  # ç®€åŒ–çš„è¯æ±‡è¡¨å¤§å°

# è®­ç»ƒï¼ˆç®€åŒ–æ¼”ç¤ºï¼‰
print("   è®­ç»ƒæ–‡æœ¬ç”Ÿæˆæ¨¡å‹...")
losses = generator.train(sample_text, epochs=20, seq_length=10)
print(f"   è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")

# ç”Ÿæˆæ–‡æœ¬
generated = generator.generate_text("hello", length=30, temperature=0.8)
print(f"   ç”Ÿæˆçš„æ–‡æœ¬: {generated}")

# æƒ…æ„Ÿåˆ†æç¤ºä¾‹
print("\nğŸ˜Š æƒ…æ„Ÿåˆ†æç¤ºä¾‹:")
sample_texts = [
    "I love this movie it is amazing",
    "This is terrible I hate it",
    "Great product highly recommend",
    "Worst experience ever very bad"
]
sample_labels = [1, 0, 1, 0]  # 1: æ­£é¢, 0: è´Ÿé¢

analyzer = SentimentAnalyzer(vocab_size=100)

# è®­ç»ƒï¼ˆç®€åŒ–æ¼”ç¤ºï¼‰
print("   è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
losses, accuracies = analyzer.train(sample_texts, sample_labels, epochs=30)
print(f"   è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆå‡†ç¡®ç‡: {accuracies[-1]:.4f}")

# é¢„æµ‹æƒ…æ„Ÿ
test_text = "This is a wonderful product"
sentiment, confidence, probs = analyzer.predict_sentiment(test_text)
print(f"   æµ‹è¯•æ–‡æœ¬: '{test_text}'")
print(f"   é¢„æµ‹æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {confidence:.4f})")

print(f"\nâœ… åº”ç”¨æ¡ˆä¾‹æ¼”ç¤ºå®Œæˆ!")

## 6. æ€è€ƒé¢˜

1. **æ¢¯åº¦é—®é¢˜åˆ†æ**ï¼šä¸ºä»€ä¹ˆRNNå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŸLSTMå’ŒGRUæ˜¯å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼Ÿè¯·ä»æ•°å­¦è§’åº¦åˆ†ææ¢¯åº¦åœ¨æ—¶é—´æ­¥ä¸Šçš„ä¼ æ’­è¿‡ç¨‹ã€‚

2. **é—¨æ§æœºåˆ¶è®¾è®¡**ï¼šLSTMæœ‰ä¸‰ä¸ªé—¨ï¼ˆé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨ï¼‰ï¼Œè€ŒGRUåªæœ‰ä¸¤ä¸ªé—¨ï¼ˆé‡ç½®é—¨ã€æ›´æ–°é—¨ï¼‰ã€‚è¯·åˆ†æè¿™ç§è®¾è®¡å·®å¼‚å¯¹æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„å½±å“ã€‚

3. **åºåˆ—é•¿åº¦å½±å“**ï¼šåœ¨å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ï¼ŒRNNå˜ä½“çš„è¡¨ç°å¦‚ä½•ï¼Ÿè¯·è®¾è®¡å®éªŒæ¯”è¾ƒçŸ­åºåˆ—ï¼ˆ<10ï¼‰ã€ä¸­ç­‰åºåˆ—ï¼ˆ10-50ï¼‰å’Œé•¿åºåˆ—ï¼ˆ>50ï¼‰ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚

4. **åŒå‘RNNåº”ç”¨**ï¼šä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥ä½¿ç”¨åŒå‘RNNï¼Ÿè¯·åˆ†æåŒå‘RNNåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚

5. **æ³¨æ„åŠ›æœºåˆ¶ç»“åˆ**ï¼šå¦‚ä½•å°†æ³¨æ„åŠ›æœºåˆ¶ä¸RNNç»“åˆï¼Ÿè¿™ç§ç»“åˆå¯¹è§£å†³é•¿åºåˆ—ä¾èµ–é—®é¢˜æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ

## 7. æœ¬ç« å°ç»“

### 7.1 æ ¸å¿ƒæ¦‚å¿µå›é¡¾

**ğŸ”„ å¾ªç¯ç¥ç»ç½‘ç»œç‰¹ç‚¹**ï¼š
- **æ—¶åºå»ºæ¨¡**ï¼šèƒ½å¤Ÿå¤„ç†å˜é•¿åºåˆ—æ•°æ®
- **å‚æ•°å…±äº«**ï¼šæ‰€æœ‰æ—¶é—´æ­¥å…±äº«ç›¸åŒå‚æ•°
- **è®°å¿†æœºåˆ¶**ï¼šé€šè¿‡éšè—çŠ¶æ€ä¼ é€’å†å²ä¿¡æ¯
- **é€’å½’ç»“æ„**ï¼šè¾“å‡ºä¾èµ–äºå½“å‰è¾“å…¥å’Œå†å²çŠ¶æ€

**ğŸšª é—¨æ§æœºåˆ¶ä¼˜åŠ¿**ï¼š
- **é€‰æ‹©æ€§è®°å¿†**ï¼šå†³å®šä¿ç•™æˆ–é—å¿˜å“ªäº›ä¿¡æ¯
- **æ¢¯åº¦ç¨³å®š**ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸é—®é¢˜
- **é•¿æœŸä¾èµ–**ï¼šèƒ½å¤Ÿå­¦ä¹ é•¿è·ç¦»çš„æ—¶åºå…³ç³»
- **è‡ªé€‚åº”æ§åˆ¶**ï¼šæ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´ä¿¡æ¯æµ

### 7.2 å…³é”®æŠ€æœ¯è¦ç‚¹

**ğŸ”§ å®ç°æŠ€æœ¯**ï¼š
- **åå‘ä¼ æ’­**ï¼šé€šè¿‡æ—¶é—´çš„åå‘ä¼ æ’­ï¼ˆBPTTï¼‰
- **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸çš„é‡è¦æŠ€æœ¯
- **æƒé‡åˆå§‹åŒ–**ï¼šåˆé€‚çš„åˆå§‹åŒ–ç­–ç•¥å¾ˆå…³é”®
- **æ­£åˆ™åŒ–**ï¼šDropoutç­‰æŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆ
- **æ‰¹æ¬¡å¤„ç†**ï¼šæé«˜è®­ç»ƒæ•ˆç‡çš„é‡è¦æ–¹æ³•

**ğŸ“Š æ€§èƒ½ä¼˜åŒ–**ï¼š
- **æ¶æ„é€‰æ‹©**ï¼šæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹é€‰æ‹©RNNå˜ä½“
- **è¶…å‚æ•°è°ƒä¼˜**ï¼šéšè—å±‚å¤§å°ã€å­¦ä¹ ç‡ç­‰
- **æ•°æ®é¢„å¤„ç†**ï¼šåºåˆ—å¡«å……ã€æ ‡å‡†åŒ–ç­‰
- **è®­ç»ƒç­–ç•¥**ï¼šå­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœç­‰

### 7.3 å®é™…åº”ç”¨åœºæ™¯

**ğŸ“ è‡ªç„¶è¯­è¨€å¤„ç†**ï¼š
- æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘
- æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»
- å‘½åå®ä½“è¯†åˆ«ã€è¯æ€§æ ‡æ³¨

**ğŸµ åºåˆ—æ•°æ®åˆ†æ**ï¼š
- æ—¶é—´åºåˆ—é¢„æµ‹
- éŸ³é¢‘ä¿¡å·å¤„ç†
- è§†é¢‘åŠ¨ä½œè¯†åˆ«

**ğŸ¤– å¯¹è¯ç³»ç»Ÿ**ï¼š
- èŠå¤©æœºå™¨äºº
- é—®ç­”ç³»ç»Ÿ
- è¯­éŸ³åŠ©æ‰‹

### 7.4 å±€é™æ€§ä¸æ”¹è¿›æ–¹å‘

**âš ï¸ ä¸»è¦å±€é™**ï¼š
- **è®¡ç®—æ•ˆç‡**ï¼šåºåˆ—åŒ–è®¡ç®—éš¾ä»¥å¹¶è¡Œ
- **é•¿åºåˆ—å¤„ç†**ï¼šä»ç„¶å­˜åœ¨é•¿æœŸä¾èµ–é—®é¢˜
- **æ¢¯åº¦é—®é¢˜**ï¼šè™½æœ‰æ”¹å–„ä½†æœªå®Œå…¨è§£å†³
- **å†…å­˜éœ€æ±‚**ï¼šé•¿åºåˆ—éœ€è¦å¤§é‡å†…å­˜

**ğŸš€ å‘å±•æ–¹å‘**ï¼š
- **Transformeræ¶æ„**ï¼šå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶
- **å¹¶è¡Œè®¡ç®—**ï¼šæé«˜è®­ç»ƒå’Œæ¨ç†æ•ˆç‡
- **é¢„è®­ç»ƒæ¨¡å‹**ï¼šå¦‚BERTã€GPTç­‰
- **æ··åˆæ¶æ„**ï¼šç»“åˆCNNã€RNNã€Transformer

### 7.5 å­¦ä¹ å»ºè®®

**ğŸ“š æ·±å…¥å­¦ä¹ **ï¼š
1. **æ•°å­¦åŸºç¡€**ï¼šæ·±å…¥ç†è§£åå‘ä¼ æ’­å’Œæ¢¯åº¦è®¡ç®—
2. **å®è·µé¡¹ç›®**ï¼šå®Œæˆæ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰é¡¹ç›®
3. **æ¶æ„å¯¹æ¯”**ï¼šå®éªŒä¸åŒRNNå˜ä½“çš„æ€§èƒ½
4. **ä¼˜åŒ–æŠ€å·§**ï¼šæŒæ¡è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡ä¼˜åŒ–
5. **å‰æ²¿å‘å±•**ï¼šå…³æ³¨Transformerç­‰æ–°æ¶æ„

**ğŸ¯ ä¸‹ä¸€æ­¥æ–¹å‘**ï¼š
- å­¦ä¹ æ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer
- æ¢ç´¢é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
- ç ”ç©¶å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡
- äº†è§£å¼ºåŒ–å­¦ä¹ ä¸­çš„åºåˆ—å†³ç­–

---

**æœ¬ç« é‡ç‚¹**ï¼šRNNåŠå…¶å˜ä½“æ˜¯å¤„ç†åºåˆ—æ•°æ®çš„é‡è¦å·¥å…·ï¼Œè™½ç„¶åœ¨æŸäº›æ–¹é¢è¢«Transformerè¶…è¶Šï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³å’ŒæŠ€æœ¯ä»ç„¶å…·æœ‰é‡è¦ä»·å€¼ã€‚æŒæ¡RNNçš„åŸç†å’Œå®ç°ï¼Œæœ‰åŠ©äºç†è§£æ›´å¤æ‚çš„åºåˆ—å»ºæ¨¡æ–¹æ³•ã€‚

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šæˆ‘ä»¬å°†å­¦ä¹ ä¼˜åŒ–ç®—æ³•ï¼ŒåŒ…æ‹¬æ¢¯åº¦ä¸‹é™çš„å„ç§å˜ä½“ã€è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•ç­‰ï¼Œè¿™äº›æ˜¯è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæŠ€æœ¯ã€‚