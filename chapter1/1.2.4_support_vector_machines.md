# 1.2.4 æ”¯æŒå‘é‡æœº (SVM)

## å­¦ä¹ ç›®æ ‡
ç†è§£æ”¯æŒå‘é‡æœºçš„å‡ ä½•ç›´è§‰ï¼ŒæŒæ¡æ ¸æŠ€å·§å¤„ç†éçº¿æ€§é—®é¢˜ï¼Œå­¦ä¼šè°ƒä¼˜SVMå‚æ•°ã€‚

## å¼•è¨€ï¼šå¯»æ‰¾æœ€ä½³åˆ†ç•Œçº¿

æƒ³è±¡ä½ æ˜¯ä¸€ä¸ªè¾¹å¢ƒè­¦å¯Ÿï¼Œéœ€è¦åœ¨ä¸¤ä¸ªå›½å®¶ä¹‹é—´ç”»ä¸€æ¡è¾¹ç•Œçº¿ï¼š

```
å›½å®¶Açš„åŸå¸‚: â—â—â—â—â—
                    |
è¾¹ç•Œçº¿ --------------|---------------
                    |
å›½å®¶Bçš„åŸå¸‚:         â—‹â—‹â—‹â—‹â—‹
```

å¦‚ä½•ç”»è¿™æ¡çº¿æ‰èƒ½ï¼š
1. **æ­£ç¡®åˆ†å¼€**ä¸¤ä¸ªå›½å®¶çš„åŸå¸‚
2. **æœ€å¤§åŒ–å®‰å…¨è·ç¦»**ï¼Œè®©è¾¹ç•Œçº¿ç¦»ä¸¤è¾¹åŸå¸‚éƒ½å°½å¯èƒ½è¿œ
3. **å¤„ç†å¤æ‚åœ°å½¢**ï¼Œæ¯”å¦‚å±±è„‰ã€æ²³æµç­‰éç›´çº¿è¾¹ç•Œ

è¿™å°±æ˜¯**æ”¯æŒå‘é‡æœº (Support Vector Machine, SVM)** è¦è§£å†³çš„é—®é¢˜ï¼

## ä»€ä¹ˆæ˜¯æ”¯æŒå‘é‡æœºï¼Ÿ

**æ”¯æŒå‘é‡æœº** æ˜¯ä¸€ç§å¼ºå¤§çš„ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡å¯»æ‰¾**æœ€ä¼˜åˆ†ç¦»è¶…å¹³é¢**æ¥è¿›è¡Œåˆ†ç±»å’Œå›å½’ã€‚

### æ ¸å¿ƒæ€æƒ³

1. **æœ€å¤§é—´éš”**ï¼šå¯»æ‰¾èƒ½å¤Ÿæœ€å¤§åŒ–ç±»åˆ«é—´è·ç¦»çš„åˆ†ç•Œçº¿
2. **æ”¯æŒå‘é‡**ï¼šè·ç¦»åˆ†ç•Œçº¿æœ€è¿‘çš„å…³é”®æ ·æœ¬ç‚¹
3. **æ ¸æŠ€å·§**ï¼šå°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´å¤„ç†éçº¿æ€§é—®é¢˜

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC, SVR
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
from sklearn.datasets import make_classification, make_circles, make_moons, make_regression
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ç”Ÿæˆçº¿æ€§å¯åˆ†æ•°æ®
np.random.seed(42)
n_samples = 100

# ç®€å•çš„äºŒåˆ†ç±»æ•°æ®
X_linear = np.random.randn(n_samples, 2)
y_linear = (X_linear[:, 0] + X_linear[:, 1] > 0).astype(int)

# æ·»åŠ ä¸€äº›å™ªå£°ä½¿æ•°æ®æ›´çœŸå®
noise_indices = np.random.choice(n_samples, size=10, replace=False)
y_linear[noise_indices] = 1 - y_linear[noise_indices]

print(f"çº¿æ€§å¯åˆ†æ•°æ®:")
print(f"æ ·æœ¬æ•°é‡: {len(y_linear)}")
print(f"ç‰¹å¾ç»´åº¦: {X_linear.shape[1]}")
print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y_linear, return_counts=True)))}")  
```

## SVMçš„å‡ ä½•ç›´è§‰

### 1. çº¿æ€§SVMï¼šå¯»æ‰¾æœ€ä¼˜åˆ†ç¦»è¶…å¹³é¢

```python
def visualize_svm_concept():
    """å¯è§†åŒ–SVMçš„åŸºæœ¬æ¦‚å¿µ"""
    
    # åˆ›å»ºç®€å•çš„çº¿æ€§å¯åˆ†æ•°æ®
    np.random.seed(42)
    X_simple = np.array([
        [1, 2], [2, 3], [3, 3], [2, 1], [3, 2],  # ç±»åˆ«0
        [6, 6], [7, 7], [8, 6], [7, 5], [8, 8]   # ç±»åˆ«1
    ])
    y_simple = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])
    
    # è®­ç»ƒSVM
    svm_linear = SVC(kernel='linear', C=1000)  # å¤§Cå€¼ç¡®ä¿ç¡¬é—´éš”
    svm_linear.fit(X_simple, y_simple)
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # 1. åŸå§‹æ•°æ®
    axes[0].scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], 
                   c='red', marker='o', s=100, label='ç±»åˆ« 0')
    axes[0].scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], 
                   c='blue', marker='s', s=100, label='ç±»åˆ« 1')
    axes[0].set_xlabel('ç‰¹å¾ 1')
    axes[0].set_ylabel('ç‰¹å¾ 2')
    axes[0].set_title('åŸå§‹æ•°æ®')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # 2. å¤šç§å¯èƒ½çš„åˆ†ç•Œçº¿
    axes[1].scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], 
                   c='red', marker='o', s=100, label='ç±»åˆ« 0')
    axes[1].scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], 
                   c='blue', marker='s', s=100, label='ç±»åˆ« 1')
    
    # ç»˜åˆ¶å‡ æ¡å¯èƒ½çš„åˆ†ç•Œçº¿
    x_range = np.linspace(0, 10, 100)
    
    # åˆ†ç•Œçº¿1: y = 0.5x + 1
    y1 = 0.5 * x_range + 1
    axes[1].plot(x_range, y1, 'g--', alpha=0.7, label='åˆ†ç•Œçº¿1')
    
    # åˆ†ç•Œçº¿2: y = 0.8x - 0.5
    y2 = 0.8 * x_range - 0.5
    axes[1].plot(x_range, y2, 'm--', alpha=0.7, label='åˆ†ç•Œçº¿2')
    
    # åˆ†ç•Œçº¿3: y = x - 1
    y3 = x_range - 1
    axes[1].plot(x_range, y3, 'c--', alpha=0.7, label='åˆ†ç•Œçº¿3')
    
    axes[1].set_xlim(0, 10)
    axes[1].set_ylim(0, 10)
    axes[1].set_xlabel('ç‰¹å¾ 1')
    axes[1].set_ylabel('ç‰¹å¾ 2')
    axes[1].set_title('å¤šç§å¯èƒ½çš„åˆ†ç•Œçº¿')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # 3. SVMæœ€ä¼˜åˆ†ç•Œçº¿å’Œæ”¯æŒå‘é‡
    axes[2].scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], 
                   c='red', marker='o', s=100, label='ç±»åˆ« 0')
    axes[2].scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], 
                   c='blue', marker='s', s=100, label='ç±»åˆ« 1')
    
    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œå’Œé—´éš”
    ax = axes[2]
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    
    # åˆ›å»ºç½‘æ ¼
    xx = np.linspace(xlim[0], xlim[1], 30)
    yy = np.linspace(ylim[0], ylim[1], 30)
    YY, XX = np.meshgrid(yy, xx)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T
    Z = svm_linear.decision_function(xy).reshape(XX.shape)
    
    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œå’Œé—´éš”
    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, 
              linestyles=['--', '-', '--'])
    
    # æ ‡è®°æ”¯æŒå‘é‡
    support_vectors = svm_linear.support_vectors_
    ax.scatter(support_vectors[:, 0], support_vectors[:, 1], 
              s=300, linewidth=2, facecolors='none', edgecolors='black', 
              label='æ”¯æŒå‘é‡')
    
    axes[2].set_xlabel('ç‰¹å¾ 1')
    axes[2].set_ylabel('ç‰¹å¾ 2')
    axes[2].set_title('SVMæœ€ä¼˜åˆ†ç•Œçº¿\n(å®çº¿=å†³ç­–è¾¹ç•Œ, è™šçº¿=é—´éš”è¾¹ç•Œ)')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°æ”¯æŒå‘é‡ä¿¡æ¯
    print(f"\nSVMåˆ†æç»“æœ:")
    print(f"æ”¯æŒå‘é‡æ•°é‡: {len(svm_linear.support_vectors_)}")
    print(f"æ”¯æŒå‘é‡ç´¢å¼•: {svm_linear.support_}")
    print(f"æ”¯æŒå‘é‡:")
    for i, sv in enumerate(svm_linear.support_vectors_):
        print(f"  {i+1}: ({sv[0]:.2f}, {sv[1]:.2f})")
    
    # è®¡ç®—é—´éš”
    w = svm_linear.coef_[0]
    margin = 2 / np.sqrt(np.sum(w ** 2))
    print(f"é—´éš”å®½åº¦: {margin:.4f}")

visualize_svm_concept()
```

### 2. æ•°å­¦åŸç†ï¼šä¼˜åŒ–é—®é¢˜

#### ç¡¬é—´éš”SVM

å¯¹äºçº¿æ€§å¯åˆ†æ•°æ®ï¼ŒSVMè¦è§£å†³çš„ä¼˜åŒ–é—®é¢˜æ˜¯ï¼š

**ç›®æ ‡å‡½æ•°**ï¼š
```
min (1/2)||w||Â²
```

**çº¦æŸæ¡ä»¶**ï¼š
```
yáµ¢(wÂ·xáµ¢ + b) â‰¥ 1, âˆ€i
```

#### è½¯é—´éš”SVM

å¯¹äºçº¿æ€§ä¸å¯åˆ†æ•°æ®ï¼Œå¼•å…¥æ¾å¼›å˜é‡ Î¾áµ¢ï¼š

**ç›®æ ‡å‡½æ•°**ï¼š
```
min (1/2)||w||Â² + Câˆ‘Î¾áµ¢
```

**çº¦æŸæ¡ä»¶**ï¼š
```
yáµ¢(wÂ·xáµ¢ + b) â‰¥ 1 - Î¾áµ¢
Î¾áµ¢ â‰¥ 0, âˆ€i
```

```python
def demonstrate_soft_margin():
    """æ¼”ç¤ºè½¯é—´éš”SVM"""
    
    # ç”Ÿæˆçº¿æ€§ä¸å¯åˆ†æ•°æ®
    np.random.seed(42)
    X_overlap, y_overlap = make_classification(
        n_samples=100, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=1, class_sep=0.8, random_state=42
    )
    
    # ä¸åŒCå€¼çš„SVM
    C_values = [0.1, 1, 10, 100]
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    for i, C in enumerate(C_values):
        # è®­ç»ƒSVM
        svm = SVC(kernel='linear', C=C)
        svm.fit(X_overlap, y_overlap)
        
        # ç»˜åˆ¶ç»“æœ
        ax = axes[i]
        
        # ç»˜åˆ¶æ•°æ®ç‚¹
        scatter = ax.scatter(X_overlap[:, 0], X_overlap[:, 1], c=y_overlap, 
                           cmap='RdYlBu', s=50, alpha=0.8)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()
        
        xx = np.linspace(xlim[0], xlim[1], 30)
        yy = np.linspace(ylim[0], ylim[1], 30)
        YY, XX = np.meshgrid(yy, xx)
        xy = np.vstack([XX.ravel(), YY.ravel()]).T
        Z = svm.decision_function(xy).reshape(XX.shape)
        
        # å†³ç­–è¾¹ç•Œå’Œé—´éš”
        ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, 
                  linestyles=['--', '-', '--'])
        
        # æ”¯æŒå‘é‡
        support_vectors = svm.support_vectors_
        ax.scatter(support_vectors[:, 0], support_vectors[:, 1], 
                  s=200, linewidth=2, facecolors='none', edgecolors='black')
        
        # è®¡ç®—å‡†ç¡®ç‡å’Œæ”¯æŒå‘é‡æ•°é‡
        accuracy = svm.score(X_overlap, y_overlap)
        n_support = len(support_vectors)
        
        ax.set_title(f'C = {C}\nå‡†ç¡®ç‡: {accuracy:.3f}, æ”¯æŒå‘é‡: {n_support}')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†æCå€¼çš„å½±å“
    print("\nCå€¼å¯¹SVMçš„å½±å“:")
    print("-" * 50)
    print(f"{'Cå€¼':<8} {'å‡†ç¡®ç‡':<10} {'æ”¯æŒå‘é‡æ•°':<12} {'é—´éš”':<10}")
    print("-" * 50)
    
    for C in C_values:
        svm = SVC(kernel='linear', C=C)
        svm.fit(X_overlap, y_overlap)
        
        accuracy = svm.score(X_overlap, y_overlap)
        n_support = len(svm.support_vectors_)
        
        # è®¡ç®—é—´éš”
        w = svm.coef_[0]
        margin = 2 / np.sqrt(np.sum(w ** 2))
        
        print(f"{C:<8} {accuracy:<10.4f} {n_support:<12} {margin:<10.4f}")

demonstrate_soft_margin()
```

## æ ¸æŠ€å·§ï¼šå¤„ç†éçº¿æ€§é—®é¢˜

### 1. æ ¸å‡½æ•°çš„æ¦‚å¿µ

å½“æ•°æ®çº¿æ€§ä¸å¯åˆ†æ—¶ï¼ŒSVMä½¿ç”¨**æ ¸æŠ€å·§ (Kernel Trick)** å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼š

```
Ï†: â„â¿ â†’ â„áµ (m >> n)
K(xáµ¢, xâ±¼) = Ï†(xáµ¢) Â· Ï†(xâ±¼)
```

### 2. å¸¸ç”¨æ ¸å‡½æ•°

```python
def demonstrate_kernels():
    """æ¼”ç¤ºä¸åŒæ ¸å‡½æ•°çš„æ•ˆæœ"""
    
    # ç”Ÿæˆä¸åŒç±»å‹çš„éçº¿æ€§æ•°æ®
    datasets = {
        'åŒå¿ƒåœ†': make_circles(n_samples=200, noise=0.2, factor=0.5, random_state=42),
        'æœˆç‰™å½¢': make_moons(n_samples=200, noise=0.3, random_state=42),
        'å¤æ‚åˆ†å¸ƒ': make_classification(n_samples=200, n_features=2, n_redundant=0, 
                                    n_informative=2, n_clusters_per_class=2, 
                                    class_sep=0.5, random_state=42)
    }
    
    # ä¸åŒæ ¸å‡½æ•°
    kernels = {
        'çº¿æ€§æ ¸': 'linear',
        'å¤šé¡¹å¼æ ¸': 'poly', 
        'RBFæ ¸': 'rbf',
        'Sigmoidæ ¸': 'sigmoid'
    }
    
    fig, axes = plt.subplots(len(datasets), len(kernels), figsize=(20, 15))
    
    for i, (dataset_name, (X, y)) in enumerate(datasets.items()):
        for j, (kernel_name, kernel) in enumerate(kernels.items()):
            
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # è®­ç»ƒSVM
            if kernel == 'poly':
                svm = SVC(kernel=kernel, degree=3, C=1)
            else:
                svm = SVC(kernel=kernel, C=1)
            
            svm.fit(X_scaled, y)
            
            # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
            ax = axes[i, j]
            
            # åˆ›å»ºç½‘æ ¼
            h = 0.02
            x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
            y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                 np.arange(y_min, y_max, h))
            
            # é¢„æµ‹
            Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)
            
            # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
            ax.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
            
            # ç»˜åˆ¶æ•°æ®ç‚¹
            scatter = ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, 
                               cmap='RdYlBu', edgecolors='black')
            
            # ç»˜åˆ¶æ”¯æŒå‘é‡
            support_vectors = svm.support_vectors_
            ax.scatter(support_vectors[:, 0], support_vectors[:, 1], 
                      s=200, linewidth=2, facecolors='none', edgecolors='yellow')
            
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = svm.score(X_scaled, y)
            
            ax.set_title(f'{dataset_name} - {kernel_name}\nå‡†ç¡®ç‡: {accuracy:.3f}')
            ax.set_xlabel('ç‰¹å¾ 1')
            ax.set_ylabel('ç‰¹å¾ 2')
    
    plt.tight_layout()
    plt.show()
    
    # æ ¸å‡½æ•°æ€§èƒ½å¯¹æ¯”
    print("\næ ¸å‡½æ•°æ€§èƒ½å¯¹æ¯”:")
    print("=" * 80)
    
    for dataset_name, (X, y) in datasets.items():
        print(f"\n{dataset_name}:")
        print("-" * 50)
        print(f"{'æ ¸å‡½æ•°':<12} {'å‡†ç¡®ç‡':<10} {'æ”¯æŒå‘é‡æ•°':<12} {'è®­ç»ƒæ—¶é—´':<10}")
        print("-" * 50)
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        for kernel_name, kernel in kernels.items():
            import time
            
            start_time = time.time()
            
            if kernel == 'poly':
                svm = SVC(kernel=kernel, degree=3, C=1)
            else:
                svm = SVC(kernel=kernel, C=1)
            
            svm.fit(X_scaled, y)
            
            train_time = time.time() - start_time
            accuracy = svm.score(X_scaled, y)
            n_support = len(svm.support_vectors_)
            
            print(f"{kernel_name:<12} {accuracy:<10.4f} {n_support:<12} {train_time:<10.4f}")

demonstrate_kernels()
```

### 3. RBFæ ¸è¯¦è§£

**å¾„å‘åŸºå‡½æ•° (RBF) æ ¸** æ˜¯æœ€å¸¸ç”¨çš„æ ¸å‡½æ•°ï¼š

```
K(xáµ¢, xâ±¼) = exp(-Î³||xáµ¢ - xâ±¼||Â²)
```

```python
def demonstrate_rbf_kernel():
    """è¯¦ç»†æ¼”ç¤ºRBFæ ¸çš„ç‰¹æ€§"""
    
    # ç”ŸæˆåŒå¿ƒåœ†æ•°æ®
    X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)
    
    # ä¸åŒgammaå€¼
    gamma_values = [0.1, 1, 10, 100]
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    for i, gamma in enumerate(gamma_values):
        # è®­ç»ƒSVM
        svm_rbf = SVC(kernel='rbf', gamma=gamma, C=1)
        svm_rbf.fit(X_circles, y_circles)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        ax = axes[i]
        
        h = 0.02
        x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5
        y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        
        Z = svm_rbf.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        ax.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
        scatter = ax.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, 
                           cmap='RdYlBu', edgecolors='black')
        
        # æ”¯æŒå‘é‡
        support_vectors = svm_rbf.support_vectors_
        ax.scatter(support_vectors[:, 0], support_vectors[:, 1], 
                  s=200, linewidth=2, facecolors='none', edgecolors='yellow')
        
        accuracy = svm_rbf.score(X_circles, y_circles)
        n_support = len(support_vectors)
        
        ax.set_title(f'RBFæ ¸ (Î³={gamma})\nå‡†ç¡®ç‡: {accuracy:.3f}, æ”¯æŒå‘é‡: {n_support}')
        ax.set_xlabel('ç‰¹å¾ 1')
        ax.set_ylabel('ç‰¹å¾ 2')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Gammaå€¼çš„å½±å“åˆ†æ
    print("\nRBFæ ¸ä¸­Î³å€¼çš„å½±å“:")
    print("-" * 60)
    print(f"{'Î³å€¼':<8} {'å‡†ç¡®ç‡':<10} {'æ”¯æŒå‘é‡æ•°':<12} {'å†³ç­–è¾¹ç•Œå¤æ‚åº¦':<15}")
    print("-" * 60)
    
    for gamma in gamma_values:
        svm = SVC(kernel='rbf', gamma=gamma, C=1)
        svm.fit(X_circles, y_circles)
        
        accuracy = svm.score(X_circles, y_circles)
        n_support = len(svm.support_vectors_)
        
        if gamma <= 1:
            complexity = "ç®€å•"
        elif gamma <= 10:
            complexity = "ä¸­ç­‰"
        else:
            complexity = "å¤æ‚"
        
        print(f"{gamma:<8} {accuracy:<10.4f} {n_support:<12} {complexity:<15}")
    
    print("\nğŸ’¡ Î³å€¼é€‰æ‹©æŒ‡å—:")
    print("  â€¢ Î³å€¼å° â†’ å†³ç­–è¾¹ç•Œå¹³æ»‘ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ")
    print("  â€¢ Î³å€¼å¤§ â†’ å†³ç­–è¾¹ç•Œå¤æ‚ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
    print("  â€¢ å»ºè®®ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜Î³å€¼")

demonstrate_rbf_kernel()
```

## SVMå‚æ•°è°ƒä¼˜

### 1. ç½‘æ ¼æœç´¢è°ƒä¼˜

```python
def svm_parameter_tuning():
    """SVMå‚æ•°è°ƒä¼˜æ¼”ç¤º"""
    
    # ç”Ÿæˆå¤æ‚æ•°æ®é›†
    X_complex, y_complex = make_classification(
        n_samples=500, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=2, class_sep=0.8, random_state=42
    )
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_complex)
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_complex, test_size=0.3, random_state=42
    )
    
    # å‚æ•°ç½‘æ ¼
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'gamma': [0.001, 0.01, 0.1, 1, 10],
        'kernel': ['rbf']
    }
    
    # ç½‘æ ¼æœç´¢
    print("ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢...")
    grid_search = GridSearchCV(
        SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"\nâœ… ç½‘æ ¼æœç´¢å®Œæˆ!")
    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.4f}")
    
    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹
    best_svm = grid_search.best_estimator_
    y_pred = best_svm.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
    
    # å¯è§†åŒ–å‚æ•°æœç´¢ç»“æœ
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    pivot_table = results_df.pivot_table(
        values='mean_test_score', 
        index='param_gamma', 
        columns='param_C'
    )
    
    plt.figure(figsize=(12, 8))
    sns.heatmap(pivot_table, annot=True, cmap='viridis', fmt='.3f')
    plt.title('SVMå‚æ•°è°ƒä¼˜çƒ­åŠ›å›¾\n(é¢œè‰²è¶Šäº®è¡¨ç¤ºæ€§èƒ½è¶Šå¥½)')
    plt.xlabel('Cå€¼')
    plt.ylabel('Î³å€¼')
    plt.show()
    
    # ç»˜åˆ¶æœ€ä½³æ¨¡å‹çš„å†³ç­–è¾¹ç•Œ
    plt.figure(figsize=(12, 5))
    
    # åŸå§‹æ•°æ®
    plt.subplot(1, 2, 1)
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', alpha=0.7)
    plt.title('è®­ç»ƒæ•°æ®')
    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    plt.grid(True, alpha=0.3)
    
    # æœ€ä½³æ¨¡å‹å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 2, 2)
    
    h = 0.02
    x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
    y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = best_svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_complex, 
               cmap='RdYlBu', edgecolors='black')
    
    # æ”¯æŒå‘é‡
    support_vectors = best_svm.support_vectors_
    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], 
               s=200, linewidth=2, facecolors='none', edgecolors='yellow')
    
    plt.title(f'æœ€ä½³SVMæ¨¡å‹\nC={grid_search.best_params_["C"]}, Î³={grid_search.best_params_["gamma"]}')
    plt.xlabel('ç‰¹å¾ 1')
    plt.ylabel('ç‰¹å¾ 2')
    
    plt.tight_layout()
    plt.show()
    
    return grid_search

grid_result = svm_parameter_tuning()
```

### 2. å­¦ä¹ æ›²çº¿åˆ†æ

```python
def plot_svm_learning_curves():
    """ç»˜åˆ¶SVMå­¦ä¹ æ›²çº¿"""
    
    from sklearn.model_selection import learning_curve
    
    # ç”Ÿæˆæ•°æ®
    X_curve, y_curve = make_classification(
        n_samples=1000, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=2, class_sep=1.0, random_state=42
    )
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_curve)
    
    # ä¸åŒå¤æ‚åº¦çš„SVMæ¨¡å‹
    models = {
        'ç®€å•æ¨¡å‹ (C=0.1)': SVC(kernel='rbf', C=0.1, gamma=0.1),
        'å¹³è¡¡æ¨¡å‹ (C=1)': SVC(kernel='rbf', C=1, gamma=1),
        'å¤æ‚æ¨¡å‹ (C=100)': SVC(kernel='rbf', C=100, gamma=10)
    }
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    for i, (model_name, model) in enumerate(models.items()):
        # è®¡ç®—å­¦ä¹ æ›²çº¿
        train_sizes, train_scores, val_scores = learning_curve(
            model, X_scaled, y_curve, cv=5, n_jobs=-1,
            train_sizes=np.linspace(0.1, 1.0, 10),
            scoring='accuracy'
        )
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)
        
        # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
        ax = axes[i]
        
        ax.plot(train_sizes, train_mean, 'o-', color='blue', label='è®­ç»ƒåˆ†æ•°')
        ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, 
                       alpha=0.1, color='blue')
        
        ax.plot(train_sizes, val_mean, 'o-', color='red', label='éªŒè¯åˆ†æ•°')
        ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, 
                       alpha=0.1, color='red')
        
        ax.set_xlabel('è®­ç»ƒæ ·æœ¬æ•°')
        ax.set_ylabel('å‡†ç¡®ç‡')
        ax.set_title(f'{model_name}\nå­¦ä¹ æ›²çº¿')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # åˆ†æè¿‡æ‹Ÿåˆç¨‹åº¦
        final_train_score = train_mean[-1]
        final_val_score = val_mean[-1]
        overfitting = final_train_score - final_val_score
        
        ax.text(0.02, 0.98, f'è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting:.3f}', 
               transform=ax.transAxes, verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    plt.show()
    
    print("\nğŸ“Š å­¦ä¹ æ›²çº¿åˆ†æ:")
    print("â€¢ è®­ç»ƒåˆ†æ•°å’ŒéªŒè¯åˆ†æ•°éƒ½é«˜ä¸”æ¥è¿‘ â†’ æ¨¡å‹æ€§èƒ½è‰¯å¥½")
    print("â€¢ è®­ç»ƒåˆ†æ•°é«˜ä½†éªŒè¯åˆ†æ•°ä½ â†’ è¿‡æ‹Ÿåˆ")
    print("â€¢ è®­ç»ƒåˆ†æ•°å’ŒéªŒè¯åˆ†æ•°éƒ½ä½ â†’ æ¬ æ‹Ÿåˆ")
    print("â€¢ éªŒè¯åˆ†æ•°éšè®­ç»ƒæ ·æœ¬å¢åŠ è€Œæå‡ â†’ éœ€è¦æ›´å¤šæ•°æ®")

plot_svm_learning_curves()
```

## SVMç”¨äºå›å½’ (SVR)

```python
def demonstrate_svr():
    """æ¼”ç¤ºæ”¯æŒå‘é‡å›å½’"""
    
    # ç”Ÿæˆå›å½’æ•°æ®
    np.random.seed(42)
    X_reg = np.sort(5 * np.random.rand(100, 1), axis=0)
    y_reg = np.sin(X_reg).ravel() + np.random.normal(0, 0.1, X_reg.shape[0])
    
    # ä¸åŒçš„SVRæ¨¡å‹
    svr_models = {
        'çº¿æ€§SVR': SVR(kernel='linear', C=1),
        'å¤šé¡¹å¼SVR': SVR(kernel='poly', degree=3, C=1),
        'RBF SVR': SVR(kernel='rbf', gamma=0.1, C=1),
        'RBF SVR (é«˜C)': SVR(kernel='rbf', gamma=0.1, C=100)
    }
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    X_plot = np.linspace(0, 5, 300).reshape(-1, 1)
    
    for i, (model_name, svr) in enumerate(svr_models.items()):
        # è®­ç»ƒæ¨¡å‹
        svr.fit(X_reg, y_reg)
        
        # é¢„æµ‹
        y_pred = svr.predict(X_plot)
        
        # è®¡ç®—æ€§èƒ½
        y_train_pred = svr.predict(X_reg)
        mse = mean_squared_error(y_reg, y_train_pred)
        r2 = svr.score(X_reg, y_reg)
        
        # ç»˜å›¾
        ax = axes[i]
        
        # åŸå§‹æ•°æ®
        ax.scatter(X_reg, y_reg, color='red', alpha=0.6, label='è®­ç»ƒæ•°æ®')
        
        # é¢„æµ‹æ›²çº¿
        ax.plot(X_plot, y_pred, color='blue', linewidth=2, label='SVRé¢„æµ‹')
        
        # çœŸå®å‡½æ•°
        y_true = np.sin(X_plot).ravel()
        ax.plot(X_plot, y_true, color='green', linestyle='--', 
               linewidth=2, label='çœŸå®å‡½æ•°')
        
        # æ”¯æŒå‘é‡
        support_vectors = svr.support_vectors_
        ax.scatter(support_vectors, svr.predict(support_vectors), 
                  s=200, facecolors='none', edgecolors='black', 
                  linewidth=2, label='æ”¯æŒå‘é‡')
        
        ax.set_xlabel('X')
        ax.set_ylabel('y')
        ax.set_title(f'{model_name}\nMSE: {mse:.4f}, RÂ²: {r2:.4f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # SVRå‚æ•°å¯¹æ¯”
    print("\nSVRæ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print("-" * 60)
    print(f"{'æ¨¡å‹':<15} {'MSE':<10} {'RÂ²':<10} {'æ”¯æŒå‘é‡æ•°':<12}")
    print("-" * 60)
    
    for model_name, svr in svr_models.items():
        svr.fit(X_reg, y_reg)
        y_train_pred = svr.predict(X_reg)
        mse = mean_squared_error(y_reg, y_train_pred)
        r2 = svr.score(X_reg, y_reg)
        n_support = len(svr.support_vectors_)
        
        print(f"{model_name:<15} {mse:<10.4f} {r2:<10.4f} {n_support:<12}")

demonstrate_svr()
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### æ–‡æœ¬åˆ†ç±»ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹

```python
class SpamDetectionSVM:
    """åŸºäºSVMçš„åƒåœ¾é‚®ä»¶æ£€æµ‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.vectorizer = None
        self.scaler = None
        self.svm_model = None
        self.feature_names = None
        
    def generate_email_data(self, n_samples=1000):
        """ç”Ÿæˆæ¨¡æ‹Ÿé‚®ä»¶æ•°æ®"""
        np.random.seed(42)
        
        # åƒåœ¾é‚®ä»¶å…³é”®è¯
        spam_keywords = [
            'free', 'win', 'money', 'prize', 'offer', 'deal', 'discount',
            'urgent', 'limited', 'act now', 'click here', 'guarantee'
        ]
        
        # æ­£å¸¸é‚®ä»¶å…³é”®è¯
        normal_keywords = [
            'meeting', 'project', 'report', 'schedule', 'team', 'work',
            'family', 'friend', 'thank you', 'regards', 'sincerely'
        ]
        
        emails = []
        labels = []
        
        for i in range(n_samples):
            if i < n_samples // 2:  # åƒåœ¾é‚®ä»¶
                # éšæœºé€‰æ‹©åƒåœ¾é‚®ä»¶å…³é”®è¯
                n_words = np.random.randint(3, 8)
                words = np.random.choice(spam_keywords, n_words, replace=True)
                # æ·»åŠ ä¸€äº›æ­£å¸¸è¯æ±‡ä½œä¸ºå™ªå£°
                if np.random.random() < 0.3:
                    normal_words = np.random.choice(normal_keywords, 1, replace=True)
                    words = np.concatenate([words, normal_words])
                
                email = ' '.join(words)
                emails.append(email)
                labels.append(1)  # åƒåœ¾é‚®ä»¶
            else:  # æ­£å¸¸é‚®ä»¶
                n_words = np.random.randint(5, 12)
                words = np.random.choice(normal_keywords, n_words, replace=True)
                # æ·»åŠ ä¸€äº›åƒåœ¾è¯æ±‡ä½œä¸ºå™ªå£°
                if np.random.random() < 0.2:
                    spam_words = np.random.choice(spam_keywords, 1, replace=True)
                    words = np.concatenate([words, spam_words])
                
                email = ' '.join(words)
                emails.append(email)
                labels.append(0)  # æ­£å¸¸é‚®ä»¶
        
        return emails, np.array(labels)
    
    def extract_features(self, emails):
        """æå–é‚®ä»¶ç‰¹å¾"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        if self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(
                max_features=100,  # æœ€å¤š100ä¸ªç‰¹å¾
                stop_words='english',
                ngram_range=(1, 2)  # 1-2gram
            )
            X = self.vectorizer.fit_transform(emails)
            self.feature_names = self.vectorizer.get_feature_names_out()
        else:
            X = self.vectorizer.transform(emails)
        
        return X.toarray()
    
    def train(self, emails, labels):
        """è®­ç»ƒåƒåœ¾é‚®ä»¶æ£€æµ‹æ¨¡å‹"""
        print("ğŸ“§ å¼€å§‹è®­ç»ƒåƒåœ¾é‚®ä»¶æ£€æµ‹æ¨¡å‹...")
        
        # ç‰¹å¾æå–
        X = self.extract_features(emails)
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, labels, test_size=0.3, random_state=42, stratify=labels
        )
        
        # å‚æ•°è°ƒä¼˜
        param_grid = {
            'C': [0.1, 1, 10],
            'gamma': ['scale', 'auto', 0.1, 1],
            'kernel': ['rbf', 'linear']
        }
        
        grid_search = GridSearchCV(
            SVC(probability=True), param_grid, cv=3, scoring='f1'
        )
        
        grid_search.fit(X_train, y_train)
        
        self.svm_model = grid_search.best_estimator_
        
        # è¯„ä¼°æ€§èƒ½
        y_pred = self.svm_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['æ­£å¸¸é‚®ä»¶', 'åƒåœ¾é‚®ä»¶']))
        
        return X_test, y_test, y_pred
    
    def predict_email(self, email_text):
        """é¢„æµ‹å•å°é‚®ä»¶"""
        # ç‰¹å¾æå–
        X = self.extract_features([email_text])
        X_scaled = self.scaler.transform(X)
        
        # é¢„æµ‹
        prediction = self.svm_model.predict(X_scaled)[0]
        probability = self.svm_model.predict_proba(X_scaled)[0]
        
        print(f"\nğŸ“§ é‚®ä»¶å†…å®¹: '{email_text}'")
        print(f"ğŸ” é¢„æµ‹ç»“æœ: {'åƒåœ¾é‚®ä»¶' if prediction == 1 else 'æ­£å¸¸é‚®ä»¶'}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {probability[prediction]:.3f}")
        print(f"ğŸ“ˆ æ¦‚ç‡åˆ†å¸ƒ: æ­£å¸¸é‚®ä»¶ {probability[0]:.3f}, åƒåœ¾é‚®ä»¶ {probability[1]:.3f}")
        
        return prediction, probability
    
    def analyze_important_features(self, top_n=10):
        """åˆ†æé‡è¦ç‰¹å¾"""
        if self.svm_model.kernel == 'linear':
            # çº¿æ€§SVMå¯ä»¥ç›´æ¥è·å–ç‰¹å¾æƒé‡
            feature_weights = self.svm_model.coef_[0]
            
            # è·å–æœ€é‡è¦çš„ç‰¹å¾
            indices = np.argsort(np.abs(feature_weights))[::-1][:top_n]
            
            print(f"\nğŸ” æœ€é‡è¦çš„{top_n}ä¸ªç‰¹å¾:")
            print("-" * 50)
            print(f"{'ç‰¹å¾':<20} {'æƒé‡':<10} {'ç±»å‹':<10}")
            print("-" * 50)
            
            for i, idx in enumerate(indices):
                feature_name = self.feature_names[idx]
                weight = feature_weights[idx]
                feature_type = 'åƒåœ¾é‚®ä»¶æŒ‡ç¤º' if weight > 0 else 'æ­£å¸¸é‚®ä»¶æŒ‡ç¤º'
                
                print(f"{feature_name:<20} {weight:<10.4f} {feature_type:<10}")
        else:
            print("âš ï¸ éçº¿æ€§æ ¸å‡½æ•°æ— æ³•ç›´æ¥è§£é‡Šç‰¹å¾é‡è¦æ€§")

# æ¼”ç¤ºåƒåœ¾é‚®ä»¶æ£€æµ‹
spam_detector = SpamDetectionSVM()

# ç”Ÿæˆæ•°æ®
emails, labels = spam_detector.generate_email_data(800)
print(f"ç”Ÿæˆäº† {len(emails)} å°é‚®ä»¶")
print(f"åƒåœ¾é‚®ä»¶æ¯”ä¾‹: {labels.mean():.3f}")

# è®­ç»ƒæ¨¡å‹
X_test, y_test, y_pred = spam_detector.train(emails, labels)

# åˆ†æé‡è¦ç‰¹å¾
spam_detector.analyze_important_features()

# æµ‹è¯•å‡ å°é‚®ä»¶
test_emails = [
    "free money win prize click here urgent",
    "meeting schedule project report team work",
    "limited offer discount deal act now",
    "thank you for the meeting regards",
    "win free money guarantee click"
]

print("\nğŸ§ª æµ‹è¯•é‚®ä»¶é¢„æµ‹:")
for email in test_emails:
    spam_detector.predict_email(email)
```

### å›¾åƒåˆ†ç±»ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«

```python
def svm_digit_classification():
    """ä½¿ç”¨SVMè¿›è¡Œæ‰‹å†™æ•°å­—åˆ†ç±»"""
    
    from sklearn.datasets import load_digits
    from sklearn.decomposition import PCA
    
    # åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†
    digits = load_digits()
    X, y = digits.data, digits.target
    
    print(f"ğŸ“Š æ•°å­—è¯†åˆ«æ•°æ®é›†:")
    print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
    print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]} (8x8åƒç´ )")
    print(f"ç±»åˆ«æ•°é‡: {len(np.unique(y))} (0-9æ•°å­—)")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # ä½¿ç”¨PCAé™ç»´å¯è§†åŒ–
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    plt.figure(figsize=(12, 5))
    
    # åŸå§‹æ•°æ®æ ·æœ¬
    plt.subplot(1, 2, 1)
    for i in range(10):
        plt.subplot(2, 5, i+1)
        plt.imshow(digits.images[i], cmap='gray')
        plt.title(f'æ•°å­— {digits.target[i]}')
        plt.axis('off')
    
    plt.suptitle('æ‰‹å†™æ•°å­—æ ·æœ¬')
    plt.tight_layout()
    plt.show()
    
    # PCAå¯è§†åŒ–
    plt.figure(figsize=(10, 8))
    colors = plt.cm.tab10(np.linspace(0, 1, 10))
    
    for i in range(10):
        mask = y == i
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                   c=[colors[i]], label=f'æ•°å­— {i}', alpha=0.6)
    
    plt.xlabel(f'ç¬¬ä¸€ä¸»æˆåˆ† (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0]:.3f})')
    plt.ylabel(f'ç¬¬äºŒä¸»æˆåˆ† (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1]:.3f})')
    plt.title('æ‰‹å†™æ•°å­—PCAå¯è§†åŒ–')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # è®­ç»ƒSVMåˆ†ç±»å™¨
    print("ğŸ¤– è®­ç»ƒSVMæ•°å­—åˆ†ç±»å™¨...")
    
    # å¿«é€Ÿå‚æ•°æœç´¢ï¼ˆå‡å°‘è®¡ç®—æ—¶é—´ï¼‰
    param_grid = {
        'C': [1, 10],
        'gamma': ['scale', 0.1],
        'kernel': ['rbf']
    }
    
    svm_digits = GridSearchCV(
        SVC(), param_grid, cv=3, scoring='accuracy', n_jobs=-1
    )
    
    svm_digits.fit(X_train, y_train)
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred = svm_digits.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³å‚æ•°: {svm_digits.best_params_}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    from sklearn.metrics import confusion_matrix
    
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=range(10), yticklabels=range(10))
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title(f'æ‰‹å†™æ•°å­—è¯†åˆ«æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {accuracy:.4f}')
    plt.show()
    
    # åˆ†æé”™è¯¯åˆ†ç±»
    errors = X_test[y_test != y_pred]
    error_true = y_test[y_test != y_pred]
    error_pred = y_pred[y_test != y_pred]
    
    if len(errors) > 0:
        plt.figure(figsize=(15, 8))
        n_errors = min(20, len(errors))
        
        for i in range(n_errors):
            plt.subplot(4, 5, i+1)
            # é‡æ–°reshapeä¸º8x8å›¾åƒ
            image = errors[i].reshape(8, 8)
            plt.imshow(image, cmap='gray')
            plt.title(f'çœŸå®: {error_true[i]}, é¢„æµ‹: {error_pred[i]}')
            plt.axis('off')
        
        plt.suptitle('é”™è¯¯åˆ†ç±»æ ·æœ¬')
        plt.tight_layout()
        plt.show()
    
    return svm_digits

svm_digit_model = svm_digit_classification()
```

## Traeå®è·µç¯èŠ‚

### ä½¿ç”¨Traeæ„å»ºSVM

```python
class TraeSVM:
    """Traeé£æ ¼çš„SVMå®ç°"""
    
    def __init__(self, kernel='rbf', C=1.0, gamma='scale'):
        self.kernel = kernel
        self.C = C
        self.gamma = gamma
        self.model = None
        self.scaler = None
        
    def trae_fit(self, X, y):
        """Traeé£æ ¼çš„è®­ç»ƒæ–¹æ³•"""
        print("ğŸš€ Trae SVM å¼€å§‹è®­ç»ƒ...")
        
        # æ•°æ®é¢„å¤„ç†
        X = np.array(X)
        y = np.array(y)
        
        print(f"ğŸ“Š æ•°æ®ä¿¡æ¯:")
        print(f"  â€¢ æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"  â€¢ ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        print(f"  â€¢ ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # æ•°æ®æ ‡å‡†åŒ–
        print("ğŸ”§ æ•°æ®æ ‡å‡†åŒ–...")
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # åˆ›å»ºSVMæ¨¡å‹
        print(f"ğŸ¤– åˆ›å»ºSVMæ¨¡å‹ (kernel={self.kernel}, C={self.C}, gamma={self.gamma})...")
        
        if self.gamma == 'scale':
            gamma_value = 1 / (X.shape[1] * X.var())
        else:
            gamma_value = self.gamma
            
        self.model = SVC(
            kernel=self.kernel, 
            C=self.C, 
            gamma=gamma_value,
            probability=True
        )
        
        # è®­ç»ƒæ¨¡å‹
        print("âš¡ å¼€å§‹è®­ç»ƒ...")
        self.model.fit(X_scaled, y)
        
        # è®­ç»ƒç»“æœ
        train_accuracy = self.model.score(X_scaled, y)
        n_support = len(self.model.support_vectors_)
        
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"  â€¢ è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
        print(f"  â€¢ æ”¯æŒå‘é‡æ•°: {n_support}")
        print(f"  â€¢ æ”¯æŒå‘é‡æ¯”ä¾‹: {n_support/len(X):.3f}")
        
        return self
    
    def trae_predict(self, X):
        """Traeé£æ ¼çš„é¢„æµ‹"""
        print(f"ğŸ”® é¢„æµ‹ {len(X)} ä¸ªæ ·æœ¬...")
        
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        probabilities = self.model.predict_proba(X_scaled)
        
        print("âœ… é¢„æµ‹å®Œæˆ!")
        
        return predictions, probabilities
    
    def trae_evaluate(self, X, y):
        """Traeé£æ ¼çš„æ¨¡å‹è¯„ä¼°"""
        print("ğŸ“ˆ å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        predictions, probabilities = self.trae_predict(X)
        accuracy = accuracy_score(y, predictions)
        
        print(f"ğŸ¯ è¯„ä¼°ç»“æœ:")
        print(f"  â€¢ å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        unique_classes = np.unique(y)
        if len(unique_classes) == 2:
            print(f"\nğŸ“Š äºŒåˆ†ç±»è¯¦ç»†æŠ¥å‘Š:")
            print(classification_report(y, predictions))
        
        return accuracy
    
    def trae_visualize_2d(self, X, y, title="Trae SVM å†³ç­–è¾¹ç•Œ"):
        """Traeé£æ ¼çš„2Då¯è§†åŒ–"""
        if X.shape[1] != 2:
            print("âš ï¸ åªæ”¯æŒ2Dæ•°æ®å¯è§†åŒ–")
            return
        
        print("ğŸ¨ ç”Ÿæˆå†³ç­–è¾¹ç•Œå¯è§†åŒ–...")
        
        X_scaled = self.scaler.transform(X)
        
        plt.figure(figsize=(12, 8))
        
        # åˆ›å»ºç½‘æ ¼
        h = 0.02
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        
        # é¢„æµ‹ç½‘æ ¼ç‚¹
        Z = self.model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        plt.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
        
        # ç»˜åˆ¶æ•°æ®ç‚¹
        scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, 
                            cmap='RdYlBu', edgecolors='black')
        
        # ç»˜åˆ¶æ”¯æŒå‘é‡
        support_vectors = self.model.support_vectors_
        plt.scatter(support_vectors[:, 0], support_vectors[:, 1], 
                   s=200, linewidth=3, facecolors='none', 
                   edgecolors='yellow', label='æ”¯æŒå‘é‡')
        
        plt.xlabel('ç‰¹å¾ 1 (æ ‡å‡†åŒ–å)')
        plt.ylabel('ç‰¹å¾ 2 (æ ‡å‡†åŒ–å)')
        plt.title(f'{title}\næ ¸å‡½æ•°: {self.kernel}, C: {self.C}')
        plt.colorbar(scatter, label='ç±»åˆ«')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        print("âœ¨ å¯è§†åŒ–å®Œæˆ!")
    
    def trae_auto_tune(self, X, y, cv=3):
        """Traeé£æ ¼çš„è‡ªåŠ¨å‚æ•°è°ƒä¼˜"""
        print("ğŸ”§ Trae è‡ªåŠ¨å‚æ•°è°ƒä¼˜å¼€å§‹...")
        
        X_scaled = self.scaler.fit_transform(X)
        
        # å‚æ•°ç½‘æ ¼
        if self.kernel == 'rbf':
            param_grid = {
                'C': [0.1, 1, 10, 100],
                'gamma': [0.001, 0.01, 0.1, 1, 'scale']
            }
        elif self.kernel == 'linear':
            param_grid = {
                'C': [0.1, 1, 10, 100]
            }
        else:
            param_grid = {
                'C': [0.1, 1, 10, 100],
                'gamma': [0.001, 0.01, 0.1, 1]
            }
        
        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            SVC(kernel=self.kernel, probability=True),
            param_grid, cv=cv, scoring='accuracy', n_jobs=-1
        )
        
        grid_search.fit(X_scaled, y)
        
        # æ›´æ–°æœ€ä½³å‚æ•°
        self.model = grid_search.best_estimator_
        self.C = grid_search.best_params_['C']
        if 'gamma' in grid_search.best_params_:
            self.gamma = grid_search.best_params_['gamma']
        
        print(f"ğŸ¯ è°ƒä¼˜å®Œæˆ!")
        print(f"  â€¢ æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  â€¢ æœ€ä½³CVåˆ†æ•°: {grid_search.best_score_:.4f}")
        
        return grid_search.best_params_

# Trae SVMæ¼”ç¤º
print("\nğŸŒŸ === Trae SVM æ¼”ç¤º === ğŸŒŸ")

# ç”Ÿæˆæ¼”ç¤ºæ•°æ®
X_demo, y_demo = make_moons(n_samples=200, noise=0.3, random_state=42)

# åˆ›å»ºTrae SVM
trae_svm = TraeSVM(kernel='rbf', C=1, gamma='scale')

# è®­ç»ƒ
trae_svm.trae_fit(X_demo, y_demo)

# å¯è§†åŒ–
trae_svm.trae_visualize_2d(X_demo, y_demo)

# è‡ªåŠ¨è°ƒä¼˜
best_params = trae_svm.trae_auto_tune(X_demo, y_demo)

# é‡æ–°å¯è§†åŒ–è°ƒä¼˜åçš„æ¨¡å‹
trae_svm.trae_visualize_2d(X_demo, y_demo, "Trae SVM (è°ƒä¼˜å)")

# è¯„ä¼°
accuracy = trae_svm.trae_evaluate(X_demo, y_demo)
```

## æ€è€ƒé¢˜

1. **é—´éš”æœ€å¤§åŒ–**ï¼šä¸ºä»€ä¹ˆSVMè¦æœ€å¤§åŒ–é—´éš”ï¼Ÿè¿™ä¸å…¶ä»–åˆ†ç±»ç®—æ³•æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

2. **æ ¸æŠ€å·§**ï¼šæ ¸æŠ€å·§å¦‚ä½•è§£å†³éçº¿æ€§é—®é¢˜ï¼Ÿä¸ºä»€ä¹ˆä¸ç›´æ¥åœ¨é«˜ç»´ç©ºé—´è®¡ç®—ï¼Ÿ

3. **å‚æ•°é€‰æ‹©**ï¼šCå’ŒÎ³å‚æ•°å¦‚ä½•å½±å“SVMçš„æ€§èƒ½ï¼Ÿå¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­é€‰æ‹©åˆé€‚çš„å‚æ•°ï¼Ÿ

4. **æ”¯æŒå‘é‡**ï¼šä¸ºä»€ä¹ˆåªæœ‰æ”¯æŒå‘é‡å¯¹å†³ç­–è¾¹ç•Œæœ‰å½±å“ï¼Ÿè¿™æœ‰ä»€ä¹ˆå®é™…æ„ä¹‰ï¼Ÿ

5. **SVM vs å…¶ä»–ç®—æ³•**ï¼šåœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©SVMè€Œä¸æ˜¯å†³ç­–æ ‘æˆ–é€»è¾‘å›å½’ï¼Ÿ

## æœ¬èŠ‚å°ç»“

æ”¯æŒå‘é‡æœºæ˜¯ä¸€ç§å¼ºå¤§è€Œä¼˜é›…çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### æ ¸å¿ƒä¼˜åŠ¿
- **æœ€å¤§é—´éš”**ï¼šå¯»æ‰¾æœ€ä¼˜åˆ†ç¦»è¶…å¹³é¢ï¼Œæ³›åŒ–èƒ½åŠ›å¼º
- **æ ¸æŠ€å·§**ï¼šä¼˜é›…å¤„ç†éçº¿æ€§é—®é¢˜ï¼Œæ— éœ€æ˜¾å¼æ˜ å°„
- **ç¨€ç–è§£**ï¼šåªä¾èµ–æ”¯æŒå‘é‡ï¼Œå†…å­˜æ•ˆç‡é«˜
- **ç†è®ºåŸºç¡€**ï¼šåŸºäºç»Ÿè®¡å­¦ä¹ ç†è®ºï¼Œæœ‰åšå®çš„æ•°å­¦åŸºç¡€

### å…³é”®æŠ€æœ¯
- **è½¯é—´éš”**ï¼šé€šè¿‡æ¾å¼›å˜é‡å¤„ç†å™ªå£°å’Œé‡å 
- **æ ¸å‡½æ•°**ï¼šRBFã€å¤šé¡¹å¼ã€çº¿æ€§ç­‰ä¸åŒæ ¸å‡½æ•°
- **å‚æ•°è°ƒä¼˜**ï¼šCå’ŒÎ³å‚æ•°çš„ç½‘æ ¼æœç´¢ä¼˜åŒ–

### å®é™…åº”ç”¨
- **æ–‡æœ¬åˆ†ç±»**ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹ã€æƒ…æ„Ÿåˆ†æ
- **å›¾åƒè¯†åˆ«**ï¼šæ‰‹å†™æ•°å­—ã€äººè„¸è¯†åˆ«
- **ç”Ÿç‰©ä¿¡æ¯å­¦**ï¼šåŸºå› åˆ†ç±»ã€è›‹ç™½è´¨é¢„æµ‹
- **é‡‘èåˆ†æ**ï¼šä¿¡ç”¨è¯„ä¼°ã€é£é™©é¢„æµ‹

### ä½¿ç”¨å»ºè®®
- **æ•°æ®é¢„å¤„ç†**ï¼šç‰¹å¾æ ‡å‡†åŒ–å¾ˆé‡è¦
- **æ ¸å‡½æ•°é€‰æ‹©**ï¼šRBFæ ¸é€šå¸¸æ˜¯å¥½çš„èµ·ç‚¹
- **å‚æ•°è°ƒä¼˜**ï¼šä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜å‚æ•°
- **è®¡ç®—å¤æ‚åº¦**ï¼šå¤§æ•°æ®é›†å¯èƒ½éœ€è¦è€ƒè™‘å…¶ä»–ç®—æ³•

### ä¸‹ä¸€æ­¥å­¦ä¹ 
- **é›†æˆæ–¹æ³•**ï¼šå°†SVMä¸å…¶ä»–ç®—æ³•ç»“åˆ
- **æ·±åº¦å­¦ä¹ **ï¼šäº†è§£ç¥ç»ç½‘ç»œçš„ä¼˜åŠ¿
- **åœ¨çº¿å­¦ä¹ **ï¼šå¤„ç†æµå¼æ•°æ®çš„æ–¹æ³•

SVMä¸ºæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†å‡ ä½•ç›´è§‰ä¸æ•°å­¦ä¼˜åŒ–ç›¸ç»“åˆï¼Œåˆ›é€ å‡ºæ—¢ä¼˜é›…åˆå®ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚å®ƒçš„æ ¸æŠ€å·§æ€æƒ³ä¹Ÿä¸ºåç»­çš„æ ¸æ–¹æ³•å¥ å®šäº†åŸºç¡€ã€‚

demonstrate_svr()
```