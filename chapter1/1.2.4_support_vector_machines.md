# 1.2.4 支持向量机 (SVM)

## 学习目标
理解支持向量机的几何直觉，掌握核技巧处理非线性问题，学会调优SVM参数。

## 引言：寻找最佳分界线

想象你是一个边境警察，需要在两个国家之间画一条边界线：

```
国家A的城市: ●●●●●
                    |
边界线 --------------|---------------
                    |
国家B的城市:         ○○○○○
```

如何画这条线才能：
1. **正确分开**两个国家的城市
2. **最大化安全距离**，让边界线离两边城市都尽可能远
3. **处理复杂地形**，比如山脉、河流等非直线边界

这就是**支持向量机 (Support Vector Machine, SVM)** 要解决的问题！

## 什么是支持向量机？

**支持向量机** 是一种强大的监督学习算法，通过寻找**最优分离超平面**来进行分类和回归。

### 核心思想

1. **最大间隔**：寻找能够最大化类别间距离的分界线
2. **支持向量**：距离分界线最近的关键样本点
3. **核技巧**：将数据映射到高维空间处理非线性问题

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC, SVR
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
from sklearn.datasets import make_classification, make_circles, make_moons, make_regression
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# 生成线性可分数据
np.random.seed(42)
n_samples = 100

# 简单的二分类数据
X_linear = np.random.randn(n_samples, 2)
y_linear = (X_linear[:, 0] + X_linear[:, 1] > 0).astype(int)

# 添加一些噪声使数据更真实
noise_indices = np.random.choice(n_samples, size=10, replace=False)
y_linear[noise_indices] = 1 - y_linear[noise_indices]

print(f"线性可分数据:")
print(f"样本数量: {len(y_linear)}")
print(f"特征维度: {X_linear.shape[1]}")
print(f"类别分布: {dict(zip(*np.unique(y_linear, return_counts=True)))}")  
```

## SVM的几何直觉

### 1. 线性SVM：寻找最优分离超平面

```python
def visualize_svm_concept():
    """可视化SVM的基本概念"""
    
    # 创建简单的线性可分数据
    np.random.seed(42)
    X_simple = np.array([
        [1, 2], [2, 3], [3, 3], [2, 1], [3, 2],  # 类别0
        [6, 6], [7, 7], [8, 6], [7, 5], [8, 8]   # 类别1
    ])
    y_simple = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])
    
    # 训练SVM
    svm_linear = SVC(kernel='linear', C=1000)  # 大C值确保硬间隔
    svm_linear.fit(X_simple, y_simple)
    
    # 可视化
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # 1. 原始数据
    axes[0].scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], 
                   c='red', marker='o', s=100, label='类别 0')
    axes[0].scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], 
                   c='blue', marker='s', s=100, label='类别 1')
    axes[0].set_xlabel('特征 1')
    axes[0].set_ylabel('特征 2')
    axes[0].set_title('原始数据')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # 2. 多种可能的分界线
    axes[1].scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], 
                   c='red', marker='o', s=100, label='类别 0')
    axes[1].scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], 
                   c='blue', marker='s', s=100, label='类别 1')
    
    # 绘制几条可能的分界线
    x_range = np.linspace(0, 10, 100)
    
    # 分界线1: y = 0.5x + 1
    y1 = 0.5 * x_range + 1
    axes[1].plot(x_range, y1, 'g--', alpha=0.7, label='分界线1')
    
    # 分界线2: y = 0.8x - 0.5
    y2 = 0.8 * x_range - 0.5
    axes[1].plot(x_range, y2, 'm--', alpha=0.7, label='分界线2')
    
    # 分界线3: y = x - 1
    y3 = x_range - 1
    axes[1].plot(x_range, y3, 'c--', alpha=0.7, label='分界线3')
    
    axes[1].set_xlim(0, 10)
    axes[1].set_ylim(0, 10)
    axes[1].set_xlabel('特征 1')
    axes[1].set_ylabel('特征 2')
    axes[1].set_title('多种可能的分界线')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # 3. SVM最优分界线和支持向量
    axes[2].scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], 
                   c='red', marker='o', s=100, label='类别 0')
    axes[2].scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], 
                   c='blue', marker='s', s=100, label='类别 1')
    
    # 绘制决策边界和间隔
    ax = axes[2]
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    
    # 创建网格
    xx = np.linspace(xlim[0], xlim[1], 30)
    yy = np.linspace(ylim[0], ylim[1], 30)
    YY, XX = np.meshgrid(yy, xx)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T
    Z = svm_linear.decision_function(xy).reshape(XX.shape)
    
    # 绘制决策边界和间隔
    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, 
              linestyles=['--', '-', '--'])
    
    # 标记支持向量
    support_vectors = svm_linear.support_vectors_
    ax.scatter(support_vectors[:, 0], support_vectors[:, 1], 
              s=300, linewidth=2, facecolors='none', edgecolors='black', 
              label='支持向量')
    
    axes[2].set_xlabel('特征 1')
    axes[2].set_ylabel('特征 2')
    axes[2].set_title('SVM最优分界线\n(实线=决策边界, 虚线=间隔边界)')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 打印支持向量信息
    print(f"\nSVM分析结果:")
    print(f"支持向量数量: {len(svm_linear.support_vectors_)}")
    print(f"支持向量索引: {svm_linear.support_}")
    print(f"支持向量:")
    for i, sv in enumerate(svm_linear.support_vectors_):
        print(f"  {i+1}: ({sv[0]:.2f}, {sv[1]:.2f})")
    
    # 计算间隔
    w = svm_linear.coef_[0]
    margin = 2 / np.sqrt(np.sum(w ** 2))
    print(f"间隔宽度: {margin:.4f}")

visualize_svm_concept()
```

### 2. 数学原理：优化问题

#### 硬间隔SVM

对于线性可分数据，SVM要解决的优化问题是：

**目标函数**：
```
min (1/2)||w||²
```

**约束条件**：
```
yᵢ(w·xᵢ + b) ≥ 1, ∀i
```

#### 软间隔SVM

对于线性不可分数据，引入松弛变量 ξᵢ：

**目标函数**：
```
min (1/2)||w||² + C∑ξᵢ
```

**约束条件**：
```
yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ
ξᵢ ≥ 0, ∀i
```

```python
def demonstrate_soft_margin():
    """演示软间隔SVM"""
    
    # 生成线性不可分数据
    np.random.seed(42)
    X_overlap, y_overlap = make_classification(
        n_samples=100, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=1, class_sep=0.8, random_state=42
    )
    
    # 不同C值的SVM
    C_values = [0.1, 1, 10, 100]
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    for i, C in enumerate(C_values):
        # 训练SVM
        svm = SVC(kernel='linear', C=C)
        svm.fit(X_overlap, y_overlap)
        
        # 绘制结果
        ax = axes[i]
        
        # 绘制数据点
        scatter = ax.scatter(X_overlap[:, 0], X_overlap[:, 1], c=y_overlap, 
                           cmap='RdYlBu', s=50, alpha=0.8)
        
        # 绘制决策边界
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()
        
        xx = np.linspace(xlim[0], xlim[1], 30)
        yy = np.linspace(ylim[0], ylim[1], 30)
        YY, XX = np.meshgrid(yy, xx)
        xy = np.vstack([XX.ravel(), YY.ravel()]).T
        Z = svm.decision_function(xy).reshape(XX.shape)
        
        # 决策边界和间隔
        ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, 
                  linestyles=['--', '-', '--'])
        
        # 支持向量
        support_vectors = svm.support_vectors_
        ax.scatter(support_vectors[:, 0], support_vectors[:, 1], 
                  s=200, linewidth=2, facecolors='none', edgecolors='black')
        
        # 计算准确率和支持向量数量
        accuracy = svm.score(X_overlap, y_overlap)
        n_support = len(support_vectors)
        
        ax.set_title(f'C = {C}\n准确率: {accuracy:.3f}, 支持向量: {n_support}')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 分析C值的影响
    print("\nC值对SVM的影响:")
    print("-" * 50)
    print(f"{'C值':<8} {'准确率':<10} {'支持向量数':<12} {'间隔':<10}")
    print("-" * 50)
    
    for C in C_values:
        svm = SVC(kernel='linear', C=C)
        svm.fit(X_overlap, y_overlap)
        
        accuracy = svm.score(X_overlap, y_overlap)
        n_support = len(svm.support_vectors_)
        
        # 计算间隔
        w = svm.coef_[0]
        margin = 2 / np.sqrt(np.sum(w ** 2))
        
        print(f"{C:<8} {accuracy:<10.4f} {n_support:<12} {margin:<10.4f}")

demonstrate_soft_margin()
```

## 核技巧：处理非线性问题

### 1. 核函数的概念

当数据线性不可分时，SVM使用**核技巧 (Kernel Trick)** 将数据映射到高维空间：

```
φ: ℝⁿ → ℝᵐ (m >> n)
K(xᵢ, xⱼ) = φ(xᵢ) · φ(xⱼ)
```

### 2. 常用核函数

```python
def demonstrate_kernels():
    """演示不同核函数的效果"""
    
    # 生成不同类型的非线性数据
    datasets = {
        '同心圆': make_circles(n_samples=200, noise=0.2, factor=0.5, random_state=42),
        '月牙形': make_moons(n_samples=200, noise=0.3, random_state=42),
        '复杂分布': make_classification(n_samples=200, n_features=2, n_redundant=0, 
                                    n_informative=2, n_clusters_per_class=2, 
                                    class_sep=0.5, random_state=42)
    }
    
    # 不同核函数
    kernels = {
        '线性核': 'linear',
        '多项式核': 'poly', 
        'RBF核': 'rbf',
        'Sigmoid核': 'sigmoid'
    }
    
    fig, axes = plt.subplots(len(datasets), len(kernels), figsize=(20, 15))
    
    for i, (dataset_name, (X, y)) in enumerate(datasets.items()):
        for j, (kernel_name, kernel) in enumerate(kernels.items()):
            
            # 数据标准化
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # 训练SVM
            if kernel == 'poly':
                svm = SVC(kernel=kernel, degree=3, C=1)
            else:
                svm = SVC(kernel=kernel, C=1)
            
            svm.fit(X_scaled, y)
            
            # 绘制决策边界
            ax = axes[i, j]
            
            # 创建网格
            h = 0.02
            x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
            y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                 np.arange(y_min, y_max, h))
            
            # 预测
            Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)
            
            # 绘制决策边界
            ax.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
            
            # 绘制数据点
            scatter = ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, 
                               cmap='RdYlBu', edgecolors='black')
            
            # 绘制支持向量
            support_vectors = svm.support_vectors_
            ax.scatter(support_vectors[:, 0], support_vectors[:, 1], 
                      s=200, linewidth=2, facecolors='none', edgecolors='yellow')
            
            # 计算准确率
            accuracy = svm.score(X_scaled, y)
            
            ax.set_title(f'{dataset_name} - {kernel_name}\n准确率: {accuracy:.3f}')
            ax.set_xlabel('特征 1')
            ax.set_ylabel('特征 2')
    
    plt.tight_layout()
    plt.show()
    
    # 核函数性能对比
    print("\n核函数性能对比:")
    print("=" * 80)
    
    for dataset_name, (X, y) in datasets.items():
        print(f"\n{dataset_name}:")
        print("-" * 50)
        print(f"{'核函数':<12} {'准确率':<10} {'支持向量数':<12} {'训练时间':<10}")
        print("-" * 50)
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        for kernel_name, kernel in kernels.items():
            import time
            
            start_time = time.time()
            
            if kernel == 'poly':
                svm = SVC(kernel=kernel, degree=3, C=1)
            else:
                svm = SVC(kernel=kernel, C=1)
            
            svm.fit(X_scaled, y)
            
            train_time = time.time() - start_time
            accuracy = svm.score(X_scaled, y)
            n_support = len(svm.support_vectors_)
            
            print(f"{kernel_name:<12} {accuracy:<10.4f} {n_support:<12} {train_time:<10.4f}")

demonstrate_kernels()
```

### 3. RBF核详解

**径向基函数 (RBF) 核** 是最常用的核函数：

```
K(xᵢ, xⱼ) = exp(-γ||xᵢ - xⱼ||²)
```

```python
def demonstrate_rbf_kernel():
    """详细演示RBF核的特性"""
    
    # 生成同心圆数据
    X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)
    
    # 不同gamma值
    gamma_values = [0.1, 1, 10, 100]
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    for i, gamma in enumerate(gamma_values):
        # 训练SVM
        svm_rbf = SVC(kernel='rbf', gamma=gamma, C=1)
        svm_rbf.fit(X_circles, y_circles)
        
        # 绘制决策边界
        ax = axes[i]
        
        h = 0.02
        x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5
        y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        
        Z = svm_rbf.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        ax.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
        scatter = ax.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, 
                           cmap='RdYlBu', edgecolors='black')
        
        # 支持向量
        support_vectors = svm_rbf.support_vectors_
        ax.scatter(support_vectors[:, 0], support_vectors[:, 1], 
                  s=200, linewidth=2, facecolors='none', edgecolors='yellow')
        
        accuracy = svm_rbf.score(X_circles, y_circles)
        n_support = len(support_vectors)
        
        ax.set_title(f'RBF核 (γ={gamma})\n准确率: {accuracy:.3f}, 支持向量: {n_support}')
        ax.set_xlabel('特征 1')
        ax.set_ylabel('特征 2')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Gamma值的影响分析
    print("\nRBF核中γ值的影响:")
    print("-" * 60)
    print(f"{'γ值':<8} {'准确率':<10} {'支持向量数':<12} {'决策边界复杂度':<15}")
    print("-" * 60)
    
    for gamma in gamma_values:
        svm = SVC(kernel='rbf', gamma=gamma, C=1)
        svm.fit(X_circles, y_circles)
        
        accuracy = svm.score(X_circles, y_circles)
        n_support = len(svm.support_vectors_)
        
        if gamma <= 1:
            complexity = "简单"
        elif gamma <= 10:
            complexity = "中等"
        else:
            complexity = "复杂"
        
        print(f"{gamma:<8} {accuracy:<10.4f} {n_support:<12} {complexity:<15}")
    
    print("\n💡 γ值选择指南:")
    print("  • γ值小 → 决策边界平滑，可能欠拟合")
    print("  • γ值大 → 决策边界复杂，可能过拟合")
    print("  • 建议使用交叉验证选择最优γ值")

demonstrate_rbf_kernel()
```

## SVM参数调优

### 1. 网格搜索调优

```python
def svm_parameter_tuning():
    """SVM参数调优演示"""
    
    # 生成复杂数据集
    X_complex, y_complex = make_classification(
        n_samples=500, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=2, class_sep=0.8, random_state=42
    )
    
    # 数据标准化
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_complex)
    
    # 划分训练测试集
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_complex, test_size=0.3, random_state=42
    )
    
    # 参数网格
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'gamma': [0.001, 0.01, 0.1, 1, 10],
        'kernel': ['rbf']
    }
    
    # 网格搜索
    print("🔍 开始网格搜索...")
    grid_search = GridSearchCV(
        SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"\n✅ 网格搜索完成!")
    print(f"最佳参数: {grid_search.best_params_}")
    print(f"最佳交叉验证分数: {grid_search.best_score_:.4f}")
    
    # 使用最佳参数训练模型
    best_svm = grid_search.best_estimator_
    y_pred = best_svm.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    
    print(f"测试集准确率: {test_accuracy:.4f}")
    
    # 可视化参数搜索结果
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    # 创建热力图
    pivot_table = results_df.pivot_table(
        values='mean_test_score', 
        index='param_gamma', 
        columns='param_C'
    )
    
    plt.figure(figsize=(12, 8))
    sns.heatmap(pivot_table, annot=True, cmap='viridis', fmt='.3f')
    plt.title('SVM参数调优热力图\n(颜色越亮表示性能越好)')
    plt.xlabel('C值')
    plt.ylabel('γ值')
    plt.show()
    
    # 绘制最佳模型的决策边界
    plt.figure(figsize=(12, 5))
    
    # 原始数据
    plt.subplot(1, 2, 1)
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', alpha=0.7)
    plt.title('训练数据')
    plt.xlabel('特征 1')
    plt.ylabel('特征 2')
    plt.grid(True, alpha=0.3)
    
    # 最佳模型决策边界
    plt.subplot(1, 2, 2)
    
    h = 0.02
    x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
    y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = best_svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_complex, 
               cmap='RdYlBu', edgecolors='black')
    
    # 支持向量
    support_vectors = best_svm.support_vectors_
    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], 
               s=200, linewidth=2, facecolors='none', edgecolors='yellow')
    
    plt.title(f'最佳SVM模型\nC={grid_search.best_params_["C"]}, γ={grid_search.best_params_["gamma"]}')
    plt.xlabel('特征 1')
    plt.ylabel('特征 2')
    
    plt.tight_layout()
    plt.show()
    
    return grid_search

grid_result = svm_parameter_tuning()
```

### 2. 学习曲线分析

```python
def plot_svm_learning_curves():
    """绘制SVM学习曲线"""
    
    from sklearn.model_selection import learning_curve
    
    # 生成数据
    X_curve, y_curve = make_classification(
        n_samples=1000, n_features=2, n_redundant=0, n_informative=2,
        n_clusters_per_class=2, class_sep=1.0, random_state=42
    )
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_curve)
    
    # 不同复杂度的SVM模型
    models = {
        '简单模型 (C=0.1)': SVC(kernel='rbf', C=0.1, gamma=0.1),
        '平衡模型 (C=1)': SVC(kernel='rbf', C=1, gamma=1),
        '复杂模型 (C=100)': SVC(kernel='rbf', C=100, gamma=10)
    }
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    for i, (model_name, model) in enumerate(models.items()):
        # 计算学习曲线
        train_sizes, train_scores, val_scores = learning_curve(
            model, X_scaled, y_curve, cv=5, n_jobs=-1,
            train_sizes=np.linspace(0.1, 1.0, 10),
            scoring='accuracy'
        )
        
        # 计算均值和标准差
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)
        
        # 绘制学习曲线
        ax = axes[i]
        
        ax.plot(train_sizes, train_mean, 'o-', color='blue', label='训练分数')
        ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, 
                       alpha=0.1, color='blue')
        
        ax.plot(train_sizes, val_mean, 'o-', color='red', label='验证分数')
        ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, 
                       alpha=0.1, color='red')
        
        ax.set_xlabel('训练样本数')
        ax.set_ylabel('准确率')
        ax.set_title(f'{model_name}\n学习曲线')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 分析过拟合程度
        final_train_score = train_mean[-1]
        final_val_score = val_mean[-1]
        overfitting = final_train_score - final_val_score
        
        ax.text(0.02, 0.98, f'过拟合程度: {overfitting:.3f}', 
               transform=ax.transAxes, verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    plt.show()
    
    print("\n📊 学习曲线分析:")
    print("• 训练分数和验证分数都高且接近 → 模型性能良好")
    print("• 训练分数高但验证分数低 → 过拟合")
    print("• 训练分数和验证分数都低 → 欠拟合")
    print("• 验证分数随训练样本增加而提升 → 需要更多数据")

plot_svm_learning_curves()
```

## SVM用于回归 (SVR)

```python
def demonstrate_svr():
    """演示支持向量回归"""
    
    # 生成回归数据
    np.random.seed(42)
    X_reg = np.sort(5 * np.random.rand(100, 1), axis=0)
    y_reg = np.sin(X_reg).ravel() + np.random.normal(0, 0.1, X_reg.shape[0])
    
    # 不同的SVR模型
    svr_models = {
        '线性SVR': SVR(kernel='linear', C=1),
        '多项式SVR': SVR(kernel='poly', degree=3, C=1),
        'RBF SVR': SVR(kernel='rbf', gamma=0.1, C=1),
        'RBF SVR (高C)': SVR(kernel='rbf', gamma=0.1, C=100)
    }
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.ravel()
    
    X_plot = np.linspace(0, 5, 300).reshape(-1, 1)
    
    for i, (model_name, svr) in enumerate(svr_models.items()):
        # 训练模型
        svr.fit(X_reg, y_reg)
        
        # 预测
        y_pred = svr.predict(X_plot)
        
        # 计算性能
        y_train_pred = svr.predict(X_reg)
        mse = mean_squared_error(y_reg, y_train_pred)
        r2 = svr.score(X_reg, y_reg)
        
        # 绘图
        ax = axes[i]
        
        # 原始数据
        ax.scatter(X_reg, y_reg, color='red', alpha=0.6, label='训练数据')
        
        # 预测曲线
        ax.plot(X_plot, y_pred, color='blue', linewidth=2, label='SVR预测')
        
        # 真实函数
        y_true = np.sin(X_plot).ravel()
        ax.plot(X_plot, y_true, color='green', linestyle='--', 
               linewidth=2, label='真实函数')
        
        # 支持向量
        support_vectors = svr.support_vectors_
        ax.scatter(support_vectors, svr.predict(support_vectors), 
                  s=200, facecolors='none', edgecolors='black', 
                  linewidth=2, label='支持向量')
        
        ax.set_xlabel('X')
        ax.set_ylabel('y')
        ax.set_title(f'{model_name}\nMSE: {mse:.4f}, R²: {r2:.4f}')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # SVR参数对比
    print("\nSVR模型性能对比:")
    print("-" * 60)
    print(f"{'模型':<15} {'MSE':<10} {'R²':<10} {'支持向量数':<12}")
    print("-" * 60)
    
    for model_name, svr in svr_models.items():
        svr.fit(X_reg, y_reg)
        y_train_pred = svr.predict(X_reg)
        mse = mean_squared_error(y_reg, y_train_pred)
        r2 = svr.score(X_reg, y_reg)
        n_support = len(svr.support_vectors_)
        
        print(f"{model_name:<15} {mse:<10.4f} {r2:<10.4f} {n_support:<12}")

demonstrate_svr()
```

## 实际应用案例

### 文本分类：垃圾邮件检测

```python
class SpamDetectionSVM:
    """基于SVM的垃圾邮件检测系统"""
    
    def __init__(self):
        self.vectorizer = None
        self.scaler = None
        self.svm_model = None
        self.feature_names = None
        
    def generate_email_data(self, n_samples=1000):
        """生成模拟邮件数据"""
        np.random.seed(42)
        
        # 垃圾邮件关键词
        spam_keywords = [
            'free', 'win', 'money', 'prize', 'offer', 'deal', 'discount',
            'urgent', 'limited', 'act now', 'click here', 'guarantee'
        ]
        
        # 正常邮件关键词
        normal_keywords = [
            'meeting', 'project', 'report', 'schedule', 'team', 'work',
            'family', 'friend', 'thank you', 'regards', 'sincerely'
        ]
        
        emails = []
        labels = []
        
        for i in range(n_samples):
            if i < n_samples // 2:  # 垃圾邮件
                # 随机选择垃圾邮件关键词
                n_words = np.random.randint(3, 8)
                words = np.random.choice(spam_keywords, n_words, replace=True)
                # 添加一些正常词汇作为噪声
                if np.random.random() < 0.3:
                    normal_words = np.random.choice(normal_keywords, 1, replace=True)
                    words = np.concatenate([words, normal_words])
                
                email = ' '.join(words)
                emails.append(email)
                labels.append(1)  # 垃圾邮件
            else:  # 正常邮件
                n_words = np.random.randint(5, 12)
                words = np.random.choice(normal_keywords, n_words, replace=True)
                # 添加一些垃圾词汇作为噪声
                if np.random.random() < 0.2:
                    spam_words = np.random.choice(spam_keywords, 1, replace=True)
                    words = np.concatenate([words, spam_words])
                
                email = ' '.join(words)
                emails.append(email)
                labels.append(0)  # 正常邮件
        
        return emails, np.array(labels)
    
    def extract_features(self, emails):
        """提取邮件特征"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        if self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(
                max_features=100,  # 最多100个特征
                stop_words='english',
                ngram_range=(1, 2)  # 1-2gram
            )
            X = self.vectorizer.fit_transform(emails)
            self.feature_names = self.vectorizer.get_feature_names_out()
        else:
            X = self.vectorizer.transform(emails)
        
        return X.toarray()
    
    def train(self, emails, labels):
        """训练垃圾邮件检测模型"""
        print("📧 开始训练垃圾邮件检测模型...")
        
        # 特征提取
        X = self.extract_features(emails)
        
        # 特征标准化
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # 划分训练测试集
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, labels, test_size=0.3, random_state=42, stratify=labels
        )
        
        # 参数调优
        param_grid = {
            'C': [0.1, 1, 10],
            'gamma': ['scale', 'auto', 0.1, 1],
            'kernel': ['rbf', 'linear']
        }
        
        grid_search = GridSearchCV(
            SVC(probability=True), param_grid, cv=3, scoring='f1'
        )
        
        grid_search.fit(X_train, y_train)
        
        self.svm_model = grid_search.best_estimator_
        
        # 评估性能
        y_pred = self.svm_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"✅ 模型训练完成!")
        print(f"最佳参数: {grid_search.best_params_}")
        print(f"测试准确率: {accuracy:.4f}")
        print(f"\n详细分类报告:")
        print(classification_report(y_test, y_pred, target_names=['正常邮件', '垃圾邮件']))
        
        return X_test, y_test, y_pred
    
    def predict_email(self, email_text):
        """预测单封邮件"""
        # 特征提取
        X = self.extract_features([email_text])
        X_scaled = self.scaler.transform(X)
        
        # 预测
        prediction = self.svm_model.predict(X_scaled)[0]
        probability = self.svm_model.predict_proba(X_scaled)[0]
        
        print(f"\n📧 邮件内容: '{email_text}'")
        print(f"🔍 预测结果: {'垃圾邮件' if prediction == 1 else '正常邮件'}")
        print(f"📊 置信度: {probability[prediction]:.3f}")
        print(f"📈 概率分布: 正常邮件 {probability[0]:.3f}, 垃圾邮件 {probability[1]:.3f}")
        
        return prediction, probability
    
    def analyze_important_features(self, top_n=10):
        """分析重要特征"""
        if self.svm_model.kernel == 'linear':
            # 线性SVM可以直接获取特征权重
            feature_weights = self.svm_model.coef_[0]
            
            # 获取最重要的特征
            indices = np.argsort(np.abs(feature_weights))[::-1][:top_n]
            
            print(f"\n🔍 最重要的{top_n}个特征:")
            print("-" * 50)
            print(f"{'特征':<20} {'权重':<10} {'类型':<10}")
            print("-" * 50)
            
            for i, idx in enumerate(indices):
                feature_name = self.feature_names[idx]
                weight = feature_weights[idx]
                feature_type = '垃圾邮件指示' if weight > 0 else '正常邮件指示'
                
                print(f"{feature_name:<20} {weight:<10.4f} {feature_type:<10}")
        else:
            print("⚠️ 非线性核函数无法直接解释特征重要性")

# 演示垃圾邮件检测
spam_detector = SpamDetectionSVM()

# 生成数据
emails, labels = spam_detector.generate_email_data(800)
print(f"生成了 {len(emails)} 封邮件")
print(f"垃圾邮件比例: {labels.mean():.3f}")

# 训练模型
X_test, y_test, y_pred = spam_detector.train(emails, labels)

# 分析重要特征
spam_detector.analyze_important_features()

# 测试几封邮件
test_emails = [
    "free money win prize click here urgent",
    "meeting schedule project report team work",
    "limited offer discount deal act now",
    "thank you for the meeting regards",
    "win free money guarantee click"
]

print("\n🧪 测试邮件预测:")
for email in test_emails:
    spam_detector.predict_email(email)
```

### 图像分类：手写数字识别

```python
def svm_digit_classification():
    """使用SVM进行手写数字分类"""
    
    from sklearn.datasets import load_digits
    from sklearn.decomposition import PCA
    
    # 加载手写数字数据集
    digits = load_digits()
    X, y = digits.data, digits.target
    
    print(f"📊 数字识别数据集:")
    print(f"样本数量: {X.shape[0]}")
    print(f"特征维度: {X.shape[1]} (8x8像素)")
    print(f"类别数量: {len(np.unique(y))} (0-9数字)")
    
    # 数据标准化
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # 划分训练测试集
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 使用PCA降维可视化
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    plt.figure(figsize=(12, 5))
    
    # 原始数据样本
    plt.subplot(1, 2, 1)
    for i in range(10):
        plt.subplot(2, 5, i+1)
        plt.imshow(digits.images[i], cmap='gray')
        plt.title(f'数字 {digits.target[i]}')
        plt.axis('off')
    
    plt.suptitle('手写数字样本')
    plt.tight_layout()
    plt.show()
    
    # PCA可视化
    plt.figure(figsize=(10, 8))
    colors = plt.cm.tab10(np.linspace(0, 1, 10))
    
    for i in range(10):
        mask = y == i
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                   c=[colors[i]], label=f'数字 {i}', alpha=0.6)
    
    plt.xlabel(f'第一主成分 (解释方差: {pca.explained_variance_ratio_[0]:.3f})')
    plt.ylabel(f'第二主成分 (解释方差: {pca.explained_variance_ratio_[1]:.3f})')
    plt.title('手写数字PCA可视化')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # 训练SVM分类器
    print("🤖 训练SVM数字分类器...")
    
    # 快速参数搜索（减少计算时间）
    param_grid = {
        'C': [1, 10],
        'gamma': ['scale', 0.1],
        'kernel': ['rbf']
    }
    
    svm_digits = GridSearchCV(
        SVC(), param_grid, cv=3, scoring='accuracy', n_jobs=-1
    )
    
    svm_digits.fit(X_train, y_train)
    
    # 预测和评估
    y_pred = svm_digits.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"✅ 训练完成!")
    print(f"最佳参数: {svm_digits.best_params_}")
    print(f"测试准确率: {accuracy:.4f}")
    
    # 混淆矩阵
    from sklearn.metrics import confusion_matrix
    
    cm = confusion_matrix(y_test, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=range(10), yticklabels=range(10))
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.title(f'手写数字识别混淆矩阵\n准确率: {accuracy:.4f}')
    plt.show()
    
    # 分析错误分类
    errors = X_test[y_test != y_pred]
    error_true = y_test[y_test != y_pred]
    error_pred = y_pred[y_test != y_pred]
    
    if len(errors) > 0:
        plt.figure(figsize=(15, 8))
        n_errors = min(20, len(errors))
        
        for i in range(n_errors):
            plt.subplot(4, 5, i+1)
            # 重新reshape为8x8图像
            image = errors[i].reshape(8, 8)
            plt.imshow(image, cmap='gray')
            plt.title(f'真实: {error_true[i]}, 预测: {error_pred[i]}')
            plt.axis('off')
        
        plt.suptitle('错误分类样本')
        plt.tight_layout()
        plt.show()
    
    return svm_digits

svm_digit_model = svm_digit_classification()
```

## Trae实践环节

### 使用Trae构建SVM

```python
class TraeSVM:
    """Trae风格的SVM实现"""
    
    def __init__(self, kernel='rbf', C=1.0, gamma='scale'):
        self.kernel = kernel
        self.C = C
        self.gamma = gamma
        self.model = None
        self.scaler = None
        
    def trae_fit(self, X, y):
        """Trae风格的训练方法"""
        print("🚀 Trae SVM 开始训练...")
        
        # 数据预处理
        X = np.array(X)
        y = np.array(y)
        
        print(f"📊 数据信息:")
        print(f"  • 样本数量: {X.shape[0]}")
        print(f"  • 特征维度: {X.shape[1]}")
        print(f"  • 类别分布: {dict(zip(*np.unique(y, return_counts=True)))}")
        
        # 数据标准化
        print("🔧 数据标准化...")
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # 创建SVM模型
        print(f"🤖 创建SVM模型 (kernel={self.kernel}, C={self.C}, gamma={self.gamma})...")
        
        if self.gamma == 'scale':
            gamma_value = 1 / (X.shape[1] * X.var())
        else:
            gamma_value = self.gamma
            
        self.model = SVC(
            kernel=self.kernel, 
            C=self.C, 
            gamma=gamma_value,
            probability=True
        )
        
        # 训练模型
        print("⚡ 开始训练...")
        self.model.fit(X_scaled, y)
        
        # 训练结果
        train_accuracy = self.model.score(X_scaled, y)
        n_support = len(self.model.support_vectors_)
        
        print(f"✅ 训练完成!")
        print(f"  • 训练准确率: {train_accuracy:.4f}")
        print(f"  • 支持向量数: {n_support}")
        print(f"  • 支持向量比例: {n_support/len(X):.3f}")
        
        return self
    
    def trae_predict(self, X):
        """Trae风格的预测"""
        print(f"🔮 预测 {len(X)} 个样本...")
        
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        probabilities = self.model.predict_proba(X_scaled)
        
        print("✅ 预测完成!")
        
        return predictions, probabilities
    
    def trae_evaluate(self, X, y):
        """Trae风格的模型评估"""
        print("📈 开始模型评估...")
        
        predictions, probabilities = self.trae_predict(X)
        accuracy = accuracy_score(y, predictions)
        
        print(f"🎯 评估结果:")
        print(f"  • 准确率: {accuracy:.4f}")
        
        # 详细分类报告
        unique_classes = np.unique(y)
        if len(unique_classes) == 2:
            print(f"\n📊 二分类详细报告:")
            print(classification_report(y, predictions))
        
        return accuracy
    
    def trae_visualize_2d(self, X, y, title="Trae SVM 决策边界"):
        """Trae风格的2D可视化"""
        if X.shape[1] != 2:
            print("⚠️ 只支持2D数据可视化")
            return
        
        print("🎨 生成决策边界可视化...")
        
        X_scaled = self.scaler.transform(X)
        
        plt.figure(figsize=(12, 8))
        
        # 创建网格
        h = 0.02
        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        
        # 预测网格点
        Z = self.model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        # 绘制决策边界
        plt.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')
        
        # 绘制数据点
        scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, 
                            cmap='RdYlBu', edgecolors='black')
        
        # 绘制支持向量
        support_vectors = self.model.support_vectors_
        plt.scatter(support_vectors[:, 0], support_vectors[:, 1], 
                   s=200, linewidth=3, facecolors='none', 
                   edgecolors='yellow', label='支持向量')
        
        plt.xlabel('特征 1 (标准化后)')
        plt.ylabel('特征 2 (标准化后)')
        plt.title(f'{title}\n核函数: {self.kernel}, C: {self.C}')
        plt.colorbar(scatter, label='类别')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        print("✨ 可视化完成!")
    
    def trae_auto_tune(self, X, y, cv=3):
        """Trae风格的自动参数调优"""
        print("🔧 Trae 自动参数调优开始...")
        
        X_scaled = self.scaler.fit_transform(X)
        
        # 参数网格
        if self.kernel == 'rbf':
            param_grid = {
                'C': [0.1, 1, 10, 100],
                'gamma': [0.001, 0.01, 0.1, 1, 'scale']
            }
        elif self.kernel == 'linear':
            param_grid = {
                'C': [0.1, 1, 10, 100]
            }
        else:
            param_grid = {
                'C': [0.1, 1, 10, 100],
                'gamma': [0.001, 0.01, 0.1, 1]
            }
        
        # 网格搜索
        grid_search = GridSearchCV(
            SVC(kernel=self.kernel, probability=True),
            param_grid, cv=cv, scoring='accuracy', n_jobs=-1
        )
        
        grid_search.fit(X_scaled, y)
        
        # 更新最佳参数
        self.model = grid_search.best_estimator_
        self.C = grid_search.best_params_['C']
        if 'gamma' in grid_search.best_params_:
            self.gamma = grid_search.best_params_['gamma']
        
        print(f"🎯 调优完成!")
        print(f"  • 最佳参数: {grid_search.best_params_}")
        print(f"  • 最佳CV分数: {grid_search.best_score_:.4f}")
        
        return grid_search.best_params_

# Trae SVM演示
print("\n🌟 === Trae SVM 演示 === 🌟")

# 生成演示数据
X_demo, y_demo = make_moons(n_samples=200, noise=0.3, random_state=42)

# 创建Trae SVM
trae_svm = TraeSVM(kernel='rbf', C=1, gamma='scale')

# 训练
trae_svm.trae_fit(X_demo, y_demo)

# 可视化
trae_svm.trae_visualize_2d(X_demo, y_demo)

# 自动调优
best_params = trae_svm.trae_auto_tune(X_demo, y_demo)

# 重新可视化调优后的模型
trae_svm.trae_visualize_2d(X_demo, y_demo, "Trae SVM (调优后)")

# 评估
accuracy = trae_svm.trae_evaluate(X_demo, y_demo)
```

## 思考题

1. **间隔最大化**：为什么SVM要最大化间隔？这与其他分类算法有什么不同？

2. **核技巧**：核技巧如何解决非线性问题？为什么不直接在高维空间计算？

3. **参数选择**：C和γ参数如何影响SVM的性能？如何在实际应用中选择合适的参数？

4. **支持向量**：为什么只有支持向量对决策边界有影响？这有什么实际意义？

5. **SVM vs 其他算法**：在什么情况下应该选择SVM而不是决策树或逻辑回归？

## 本节小结

支持向量机是一种强大而优雅的机器学习算法，具有以下特点：

### 核心优势
- **最大间隔**：寻找最优分离超平面，泛化能力强
- **核技巧**：优雅处理非线性问题，无需显式映射
- **稀疏解**：只依赖支持向量，内存效率高
- **理论基础**：基于统计学习理论，有坚实的数学基础

### 关键技术
- **软间隔**：通过松弛变量处理噪声和重叠
- **核函数**：RBF、多项式、线性等不同核函数
- **参数调优**：C和γ参数的网格搜索优化

### 实际应用
- **文本分类**：垃圾邮件检测、情感分析
- **图像识别**：手写数字、人脸识别
- **生物信息学**：基因分类、蛋白质预测
- **金融分析**：信用评估、风险预测

### 使用建议
- **数据预处理**：特征标准化很重要
- **核函数选择**：RBF核通常是好的起点
- **参数调优**：使用交叉验证选择最优参数
- **计算复杂度**：大数据集可能需要考虑其他算法

### 下一步学习
- **集成方法**：将SVM与其他算法结合
- **深度学习**：了解神经网络的优势
- **在线学习**：处理流式数据的方法

SVM为我们展示了如何将几何直觉与数学优化相结合，创造出既优雅又实用的机器学习算法。它的核技巧思想也为后续的核方法奠定了基础。

demonstrate_svr()
```