# 1.3.2 å±‚æ¬¡èšç±»ç®—æ³•

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£å±‚æ¬¡èšç±»çš„åŸºæœ¬æ¦‚å¿µå’Œä¸¤ç§ç­–ç•¥
- æŒæ¡å‡èšå¼å±‚æ¬¡èšç±»çš„å®ç°åŸç†
- å­¦ä¼šä½¿ç”¨æ ‘çŠ¶å›¾(Dendrogram)åˆ†æèšç±»ç»“æœ
- äº†è§£ä¸åŒé“¾æ¥å‡†åˆ™çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯
- å®ç°å®Œæ•´çš„å±‚æ¬¡èšç±»åˆ†æé¡¹ç›®

## 1. å±‚æ¬¡èšç±»åŸºç¡€

### 1.1 ä»€ä¹ˆæ˜¯å±‚æ¬¡èšç±»

å±‚æ¬¡èšç±»æ˜¯ä¸€ç§**ä¸éœ€è¦é¢„å…ˆæŒ‡å®šèšç±»æ•°é‡**çš„èšç±»æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ„å»ºèšç±»çš„å±‚æ¬¡ç»“æ„æ¥æ­ç¤ºæ•°æ®çš„å†…åœ¨ç»„ç»‡å½¢å¼ã€‚

### 1.2 å±‚æ¬¡èšç±»çš„ä¸¤ç§ç­–ç•¥

```mermaid
flowchart TD
    A[å±‚æ¬¡èšç±»] --> B[å‡èšå¼èšç±»<br/>Agglomerative]
    A --> C[åˆ†è£‚å¼èšç±»<br/>Divisive]
    
    B --> D[è‡ªåº•å‘ä¸Š<br/>Bottom-up]
    B --> E[æ¯ä¸ªç‚¹å•ç‹¬æˆç°‡]
    B --> F[é€æ­¥åˆå¹¶æœ€è¿‘çš„ç°‡]
    
    C --> G[è‡ªé¡¶å‘ä¸‹<br/>Top-down]
    C --> H[æ‰€æœ‰ç‚¹åœ¨ä¸€ä¸ªç°‡ä¸­]
    C --> I[é€æ­¥åˆ†å‰²æˆå°ç°‡]
```

### 1.3 å±‚æ¬¡èšç±»çš„ä¼˜åŠ¿

```python
# å±‚æ¬¡èšç±» vs K-means å¯¹æ¯”
comparison = {
    "ç‰¹å¾": ["èšç±»æ•°é‡", "ç®—æ³•å¤æ‚åº¦", "ç»“æœç¨³å®šæ€§", "ç°‡å½¢çŠ¶é€‚åº”æ€§", "å¯è§£é‡Šæ€§"],
    "å±‚æ¬¡èšç±»": ["æ— éœ€é¢„è®¾", "O(nÂ³)", "ç¡®å®šæ€§", "ä»»æ„å½¢çŠ¶", "æ ‘çŠ¶å›¾ç›´è§‚"],
    "K-means": ["éœ€è¦é¢„è®¾K", "O(nkt)", "éšæœºåˆå§‹åŒ–", "çƒå½¢ç°‡", "ä¸­å¿ƒç‚¹åæ ‡"]
}

import pandas as pd
df_comparison = pd.DataFrame(comparison)
print("å±‚æ¬¡èšç±» vs K-means å¯¹æ¯”:")
print(df_comparison.to_string(index=False))
```

## 2. å‡èšå¼å±‚æ¬¡èšç±»åŸç†

### 2.1 ç®—æ³•æ­¥éª¤

```mermaid
flowchart TD
    A[å¼€å§‹] --> B[æ¯ä¸ªæ•°æ®ç‚¹ä½œä¸ºå•ç‹¬çš„ç°‡]
    B --> C[è®¡ç®—æ‰€æœ‰ç°‡å¯¹ä¹‹é—´çš„è·ç¦»]
    C --> D[æ‰¾åˆ°è·ç¦»æœ€å°çš„ä¸¤ä¸ªç°‡]
    D --> E[åˆå¹¶è¿™ä¸¤ä¸ªç°‡]
    E --> F[æ›´æ–°è·ç¦»çŸ©é˜µ]
    F --> G{æ˜¯å¦åªå‰©ä¸€ä¸ªç°‡?}
    G -->|å¦| C
    G -->|æ˜¯| H[æ„å»ºæ ‘çŠ¶å›¾]
    H --> I[æ ¹æ®éœ€è¦ç¡®å®šèšç±»æ•°]
    I --> J[è¾“å‡ºæœ€ç»ˆèšç±»ç»“æœ]
    J --> K[ç»“æŸ]
```

### 2.2 è·ç¦»åº¦é‡å’Œé“¾æ¥å‡†åˆ™

**ç°‡é—´è·ç¦»çš„è®¡ç®—æ–¹æ³•**ï¼š

1. **å•é“¾æ¥(Single Linkage)**: ä¸¤ç°‡æœ€è¿‘ç‚¹é—´çš„è·ç¦»
   $$d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)$$

2. **å…¨é“¾æ¥(Complete Linkage)**: ä¸¤ç°‡æœ€è¿œç‚¹é—´çš„è·ç¦»
   $$d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)$$

3. **å¹³å‡é“¾æ¥(Average Linkage)**: ä¸¤ç°‡æ‰€æœ‰ç‚¹å¯¹è·ç¦»çš„å¹³å‡
   $$d(C_i, C_j) = \frac{1}{|C_i||C_j|} \sum_{x \in C_i} \sum_{y \in C_j} d(x, y)$$

4. **Wardé“¾æ¥**: æœ€å°åŒ–åˆå¹¶åçš„æ–¹å·®å¢åŠ 
   $$d(C_i, C_j) = \sqrt{\frac{2|C_i||C_j|}{|C_i|+|C_j|}} ||\mu_i - \mu_j||$$

## 3. å±‚æ¬¡èšç±»å®ç°

### 3.1 åŸºç¡€å®ç°

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist, squareform
import seaborn as sns

class SimpleHierarchicalClustering:
    """ç®€å•çš„å‡èšå¼å±‚æ¬¡èšç±»å®ç°"""
    
    def __init__(self, linkage='single', metric='euclidean'):
        self.linkage = linkage
        self.metric = metric
        self.linkage_matrix_ = None
        self.labels_ = None
        
    def fit(self, X):
        """è®­ç»ƒå±‚æ¬¡èšç±»æ¨¡å‹"""
        n_samples = X.shape[0]
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distances = pdist(X, metric=self.metric)
        self.distance_matrix_ = squareform(distances)
        
        # åˆå§‹åŒ–ï¼šæ¯ä¸ªç‚¹ä¸ºä¸€ä¸ªç°‡
        clusters = [[i] for i in range(n_samples)]
        linkage_matrix = []
        
        # å½“å‰ç°‡çš„æ•°é‡
        current_cluster_id = n_samples
        
        while len(clusters) > 1:
            # æ‰¾åˆ°è·ç¦»æœ€å°çš„ä¸¤ä¸ªç°‡
            min_dist = float('inf')
            merge_i, merge_j = -1, -1
            
            for i in range(len(clusters)):
                for j in range(i + 1, len(clusters)):
                    dist = self._calculate_cluster_distance(clusters[i], clusters[j], X)
                    if dist < min_dist:
                        min_dist = dist
                        merge_i, merge_j = i, j
            
            # åˆå¹¶ç°‡
            cluster_i = clusters[merge_i]
            cluster_j = clusters[merge_j]
            
            # è®°å½•åˆå¹¶ä¿¡æ¯ [ç°‡1, ç°‡2, è·ç¦», æ–°ç°‡å¤§å°]
            linkage_info = [
                merge_i if len(cluster_i) == 1 else cluster_i[0] + n_samples,
                merge_j if len(cluster_j) == 1 else cluster_j[0] + n_samples,
                min_dist,
                len(cluster_i) + len(cluster_j)
            ]
            linkage_matrix.append(linkage_info)
            
            # åˆ›å»ºæ–°ç°‡
            new_cluster = cluster_i + cluster_j
            
            # ç§»é™¤æ—§ç°‡ï¼ˆä»åå¾€å‰åˆ é™¤ä»¥é¿å…ç´¢å¼•é—®é¢˜ï¼‰
            if merge_i > merge_j:
                clusters.pop(merge_i)
                clusters.pop(merge_j)
            else:
                clusters.pop(merge_j)
                clusters.pop(merge_i)
            
            # æ·»åŠ æ–°ç°‡
            clusters.append(new_cluster)
            current_cluster_id += 1
        
        self.linkage_matrix_ = np.array(linkage_matrix)
        return self
    
    def _calculate_cluster_distance(self, cluster1, cluster2, X):
        """è®¡ç®—ä¸¤ä¸ªç°‡ä¹‹é—´çš„è·ç¦»"""
        points1 = X[cluster1]
        points2 = X[cluster2]
        
        if self.linkage == 'single':
            # å•é“¾æ¥ï¼šæœ€å°è·ç¦»
            min_dist = float('inf')
            for p1 in points1:
                for p2 in points2:
                    dist = np.sqrt(np.sum((p1 - p2) ** 2))
                    min_dist = min(min_dist, dist)
            return min_dist
        
        elif self.linkage == 'complete':
            # å…¨é“¾æ¥ï¼šæœ€å¤§è·ç¦»
            max_dist = 0
            for p1 in points1:
                for p2 in points2:
                    dist = np.sqrt(np.sum((p1 - p2) ** 2))
                    max_dist = max(max_dist, dist)
            return max_dist
        
        elif self.linkage == 'average':
            # å¹³å‡é“¾æ¥ï¼šå¹³å‡è·ç¦»
            total_dist = 0
            count = 0
            for p1 in points1:
                for p2 in points2:
                    dist = np.sqrt(np.sum((p1 - p2) ** 2))
                    total_dist += dist
                    count += 1
            return total_dist / count
        
        elif self.linkage == 'ward':
            # Wardé“¾æ¥ï¼šåŸºäºæ–¹å·®
            center1 = np.mean(points1, axis=0)
            center2 = np.mean(points2, axis=0)
            n1, n2 = len(points1), len(points2)
            
            return np.sqrt((2 * n1 * n2) / (n1 + n2)) * np.sqrt(np.sum((center1 - center2) ** 2))
    
    def fit_predict(self, X, n_clusters=2):
        """è®­ç»ƒå¹¶é¢„æµ‹æŒ‡å®šæ•°é‡çš„èšç±»"""
        self.fit(X)
        
        # æ ¹æ®linkage matrixç¡®å®šèšç±»æ ‡ç­¾
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ ¹æ®æ ‘çš„åˆ‡å‰²æ¥ç¡®å®š
        from sklearn.cluster import AgglomerativeClustering
        agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=self.linkage)
        self.labels_ = agg_clustering.fit_predict(X)
        
        return self.labels_

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)
X, y_true = make_blobs(n_samples=150, centers=3, cluster_std=1.0, 
                       random_state=42, center_box=(-5.0, 5.0))

print("æ•°æ®é›†ä¿¡æ¯ï¼š")
print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
print(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
print(f"çœŸå®èšç±»æ•°: {len(np.unique(y_true))}")
```

### 3.2 ä¸åŒé“¾æ¥å‡†åˆ™çš„å¯¹æ¯”

```python
# ä½¿ç”¨ä¸åŒé“¾æ¥å‡†åˆ™è¿›è¡Œèšç±»
linkage_methods = ['single', 'complete', 'average', 'ward']
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.ravel()

for i, method in enumerate(linkage_methods):
    # ä½¿ç”¨scipyå®ç°ï¼ˆæ›´é«˜æ•ˆï¼‰
    Z = linkage(X, method=method)
    labels = fcluster(Z, t=3, criterion='maxclust')
    
    # å¯è§†åŒ–èšç±»ç»“æœ
    scatter = axes[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.7)
    axes[i].set_title(f'{method.capitalize()} Linkage', fontsize=14)
    axes[i].set_xlabel('ç‰¹å¾1')
    axes[i].set_ylabel('ç‰¹å¾2')
    axes[i].grid(True, alpha=0.3)
    
    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(scatter, ax=axes[i])

plt.tight_layout()
plt.show()

# é“¾æ¥å‡†åˆ™ç‰¹ç‚¹åˆ†æ
print("\nä¸åŒé“¾æ¥å‡†åˆ™çš„ç‰¹ç‚¹ï¼š")
linkage_characteristics = {
    "Single Linkage": "å®¹æ˜“äº§ç”Ÿé“¾å¼æ•ˆåº”ï¼Œå¯¹å™ªå£°æ•æ„Ÿï¼Œé€‚åˆå‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡",
    "Complete Linkage": "å€¾å‘äºäº§ç”Ÿç´§å‡‘çš„çƒå½¢ç°‡ï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿ",
    "Average Linkage": "å¹³è¡¡äº†singleå’Œcompleteçš„ç‰¹ç‚¹ï¼Œè¾ƒä¸ºç¨³å®š",
    "Ward Linkage": "æœ€å°åŒ–ç°‡å†…æ–¹å·®ï¼Œäº§ç”Ÿå¤§å°ç›¸è¿‘çš„çƒå½¢ç°‡"
}

for method, characteristic in linkage_characteristics.items():
    print(f"{method}: {characteristic}")
```

### 3.3 æ ‘çŠ¶å›¾(Dendrogram)åˆ†æ

```python
def plot_dendrogram_analysis(X, max_clusters=10):
    """ç»˜åˆ¶å’Œåˆ†ææ ‘çŠ¶å›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    linkage_methods = ['single', 'complete', 'average', 'ward']
    
    for i, method in enumerate(linkage_methods):
        row, col = i // 2, i % 2
        
        # è®¡ç®—é“¾æ¥çŸ©é˜µ
        Z = linkage(X, method=method)
        
        # ç»˜åˆ¶æ ‘çŠ¶å›¾
        dendrogram(Z, ax=axes[row, col], truncate_mode='level', p=5,
                  leaf_rotation=90, leaf_font_size=10)
        axes[row, col].set_title(f'{method.capitalize()} Linkage Dendrogram', fontsize=14)
        axes[row, col].set_xlabel('æ ·æœ¬ç´¢å¼•æˆ–ç°‡å¤§å°')
        axes[row, col].set_ylabel('è·ç¦»')
    
    plt.tight_layout()
    plt.show()
    
    # è¯¦ç»†åˆ†æWardæ–¹æ³•çš„æ ‘çŠ¶å›¾
    plt.figure(figsize=(15, 8))
    Z_ward = linkage(X, method='ward')
    
    # ç»˜åˆ¶å®Œæ•´çš„æ ‘çŠ¶å›¾
    plt.subplot(1, 2, 1)
    dendrogram(Z_ward, leaf_rotation=90, leaf_font_size=8)
    plt.title('Ward Linkage - å®Œæ•´æ ‘çŠ¶å›¾', fontsize=14)
    plt.xlabel('æ ·æœ¬ç´¢å¼•')
    plt.ylabel('è·ç¦»')
    
    # ç»˜åˆ¶æˆªæ–­çš„æ ‘çŠ¶å›¾
    plt.subplot(1, 2, 2)
    dendrogram(Z_ward, truncate_mode='lastp', p=12, leaf_rotation=90, 
              leaf_font_size=10, show_contracted=True)
    plt.title('Ward Linkage - æˆªæ–­æ ‘çŠ¶å›¾', fontsize=14)
    plt.xlabel('ç°‡å¤§å°')
    plt.ylabel('è·ç¦»')
    
    plt.tight_layout()
    plt.show()
    
    return Z_ward

# æ‰§è¡Œæ ‘çŠ¶å›¾åˆ†æ
Z_ward = plot_dendrogram_analysis(X)

# åˆ†ææœ€ä¼˜èšç±»æ•°
def analyze_optimal_clusters(Z, max_clusters=10):
    """åˆ†ææœ€ä¼˜èšç±»æ•°"""
    distances = Z[:, 2]  # åˆå¹¶æ—¶çš„è·ç¦»
    
    # è®¡ç®—ç›¸é‚»åˆå¹¶æ­¥éª¤çš„è·ç¦»å·®
    distance_diffs = np.diff(distances[::-1])  # åå‘ï¼Œä»æœ€åå¼€å§‹
    
    plt.figure(figsize=(12, 5))
    
    # ç»˜åˆ¶åˆå¹¶è·ç¦»å›¾
    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(distances) + 1), distances, 'bo-', linewidth=2, markersize=6)
    plt.title('å±‚æ¬¡èšç±»åˆå¹¶è·ç¦»', fontsize=14)
    plt.xlabel('åˆå¹¶æ­¥éª¤')
    plt.ylabel('åˆå¹¶è·ç¦»')
    plt.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶è·ç¦»å·®å¼‚å›¾ï¼ˆè‚˜éƒ¨æ³•åˆ™ï¼‰
    plt.subplot(1, 2, 2)
    n_clusters_range = range(2, min(max_clusters + 1, len(distance_diffs) + 2))
    plt.plot(n_clusters_range, distance_diffs[:len(n_clusters_range)], 'ro-', 
             linewidth=2, markersize=6)
    plt.title('ç›¸é‚»åˆå¹¶æ­¥éª¤è·ç¦»å·®å¼‚', fontsize=14)
    plt.xlabel('èšç±»æ•°')
    plt.ylabel('è·ç¦»å·®å¼‚')
    plt.grid(True, alpha=0.3)
    
    # æ ‡æ³¨å¯èƒ½çš„æœ€ä¼˜èšç±»æ•°
    if len(distance_diffs) > 0:
        optimal_idx = np.argmax(distance_diffs[:min(8, len(distance_diffs))])
        optimal_clusters = optimal_idx + 2
        plt.axvline(x=optimal_clusters, color='red', linestyle='--', alpha=0.7, 
                   label=f'å»ºè®®èšç±»æ•°={optimal_clusters}')
        plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    return optimal_clusters if len(distance_diffs) > 0 else 3

# åˆ†ææœ€ä¼˜èšç±»æ•°
optimal_k = analyze_optimal_clusters(Z_ward)
print(f"\nå»ºè®®çš„æœ€ä¼˜èšç±»æ•°: {optimal_k}")
```

## 4. å®é™…åº”ç”¨æ¡ˆä¾‹ï¼šåŸºå› è¡¨è¾¾æ•°æ®èšç±»

### 4.1 æ•°æ®å‡†å¤‡

```python
# æ¨¡æ‹ŸåŸºå› è¡¨è¾¾æ•°æ®
np.random.seed(42)
n_genes = 100
n_samples = 20

# åˆ›å»ºä¸åŒç±»å‹çš„åŸºå› è¡¨è¾¾æ¨¡å¼
# ç±»å‹1ï¼šé«˜è¡¨è¾¾åŸºå› 
high_expr = np.random.normal(8, 1, (25, n_samples))
# ç±»å‹2ï¼šä¸­ç­‰è¡¨è¾¾åŸºå› 
medium_expr = np.random.normal(5, 0.8, (35, n_samples))
# ç±»å‹3ï¼šä½è¡¨è¾¾åŸºå› 
low_expr = np.random.normal(2, 0.5, (25, n_samples))
# ç±»å‹4ï¼šå¯å˜è¡¨è¾¾åŸºå› 
variable_expr = np.random.normal(5, 2, (15, n_samples))

# åˆå¹¶æ•°æ®
gene_expression = np.vstack([high_expr, medium_expr, low_expr, variable_expr])
true_gene_types = np.array([0]*25 + [1]*35 + [2]*25 + [3]*15)

# æ·»åŠ ä¸€äº›å™ªå£°
gene_expression += np.random.normal(0, 0.1, gene_expression.shape)

print("åŸºå› è¡¨è¾¾æ•°æ®ä¿¡æ¯ï¼š")
print(f"åŸºå› æ•°é‡: {gene_expression.shape[0]}")
print(f"æ ·æœ¬æ•°é‡: {gene_expression.shape[1]}")
print(f"çœŸå®åŸºå› ç±»å‹æ•°: {len(np.unique(true_gene_types))}")

# æ•°æ®å¯è§†åŒ–
plt.figure(figsize=(15, 10))

# çƒ­å›¾æ˜¾ç¤ºåŸºå› è¡¨è¾¾æ¨¡å¼
plt.subplot(2, 2, 1)
sns.heatmap(gene_expression, cmap='RdYlBu_r', cbar=True, 
           xticklabels=[f'Sample_{i+1}' for i in range(n_samples)],
           yticklabels=False)
plt.title('åŸºå› è¡¨è¾¾çƒ­å›¾', fontsize=14)
plt.xlabel('æ ·æœ¬')
plt.ylabel('åŸºå› ')

# åŸºå› è¡¨è¾¾åˆ†å¸ƒ
plt.subplot(2, 2, 2)
for gene_type in range(4):
    type_mask = true_gene_types == gene_type
    type_data = gene_expression[type_mask].flatten()
    plt.hist(type_data, alpha=0.6, bins=20, label=f'ç±»å‹{gene_type}', density=True)

plt.title('ä¸åŒåŸºå› ç±»å‹çš„è¡¨è¾¾åˆ†å¸ƒ', fontsize=14)
plt.xlabel('è¡¨è¾¾æ°´å¹³')
plt.ylabel('å¯†åº¦')
plt.legend()
plt.grid(True, alpha=0.3)

# æ ·æœ¬é—´ç›¸å…³æ€§
plt.subplot(2, 2, 3)
corr_matrix = np.corrcoef(gene_expression.T)
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
           xticklabels=[f'S{i+1}' for i in range(n_samples)],
           yticklabels=[f'S{i+1}' for i in range(n_samples)])
plt.title('æ ·æœ¬é—´ç›¸å…³æ€§', fontsize=14)

# åŸºå› è¡¨è¾¾å‡å€¼vsæ–¹å·®
plt.subplot(2, 2, 4)
gene_means = np.mean(gene_expression, axis=1)
gene_vars = np.var(gene_expression, axis=1)
colors = ['red', 'blue', 'green', 'orange']
for gene_type in range(4):
    type_mask = true_gene_types == gene_type
    plt.scatter(gene_means[type_mask], gene_vars[type_mask], 
               c=colors[gene_type], alpha=0.7, label=f'ç±»å‹{gene_type}', s=30)

plt.title('åŸºå› è¡¨è¾¾å‡å€¼ vs æ–¹å·®', fontsize=14)
plt.xlabel('è¡¨è¾¾å‡å€¼')
plt.ylabel('è¡¨è¾¾æ–¹å·®')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 4.2 åŸºå› èšç±»åˆ†æ

```python
class GeneClusteringAnalysis:
    """åŸºå› è¡¨è¾¾æ•°æ®èšç±»åˆ†æç³»ç»Ÿ"""
    
    def __init__(self, linkage_method='ward', metric='euclidean'):
        self.linkage_method = linkage_method
        self.metric = metric
        self.linkage_matrix_ = None
        self.gene_clusters_ = None
        
    def fit(self, expression_data, gene_names=None):
        """å¯¹åŸºå› è¡¨è¾¾æ•°æ®è¿›è¡Œèšç±»åˆ†æ"""
        self.expression_data = expression_data
        self.gene_names = gene_names or [f'Gene_{i+1}' for i in range(expression_data.shape[0])]
        
        # è®¡ç®—å±‚æ¬¡èšç±»
        self.linkage_matrix_ = linkage(expression_data, method=self.linkage_method, 
                                     metric=self.metric)
        
        return self
    
    def get_clusters(self, n_clusters):
        """è·å–æŒ‡å®šæ•°é‡çš„èšç±»ç»“æœ"""
        self.gene_clusters_ = fcluster(self.linkage_matrix_, t=n_clusters, criterion='maxclust')
        return self.gene_clusters_
    
    def plot_dendrogram(self, max_display=50):
        """ç»˜åˆ¶æ ‘çŠ¶å›¾"""
        plt.figure(figsize=(15, 8))
        
        if len(self.gene_names) <= max_display:
            # æ˜¾ç¤ºæ‰€æœ‰åŸºå› åç§°
            dendrogram(self.linkage_matrix_, labels=self.gene_names, 
                      leaf_rotation=90, leaf_font_size=8)
        else:
            # æˆªæ–­æ˜¾ç¤º
            dendrogram(self.linkage_matrix_, truncate_mode='lastp', p=max_display,
                      leaf_rotation=90, leaf_font_size=10, show_contracted=True)
        
        plt.title(f'åŸºå› è¡¨è¾¾å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾ ({self.linkage_method.capitalize()} Linkage)', fontsize=16)
        plt.xlabel('åŸºå› æˆ–ç°‡å¤§å°')
        plt.ylabel('è·ç¦»')
        plt.tight_layout()
        plt.show()
    
    def analyze_clusters(self, n_clusters, plot_heatmap=True):
        """åˆ†æèšç±»ç»“æœ"""
        clusters = self.get_clusters(n_clusters)
        
        print(f"\nåŸºå› èšç±»åˆ†æç»“æœ (K={n_clusters}):")
        print("=" * 50)
        
        cluster_analysis = {}
        
        for cluster_id in range(1, n_clusters + 1):
            cluster_mask = clusters == cluster_id
            cluster_genes = np.array(self.gene_names)[cluster_mask]
            cluster_expression = self.expression_data[cluster_mask]
            
            print(f"\nç°‡ {cluster_id}:")
            print(f"  åŸºå› æ•°é‡: {len(cluster_genes)}")
            print(f"  å¹³å‡è¡¨è¾¾æ°´å¹³: {np.mean(cluster_expression):.2f}")
            print(f"  è¡¨è¾¾å˜å¼‚ç³»æ•°: {np.std(cluster_expression)/np.mean(cluster_expression):.3f}")
            
            # æ‰¾å‡ºä»£è¡¨æ€§åŸºå› ï¼ˆè¡¨è¾¾æ°´å¹³æœ€æ¥è¿‘ç°‡å‡å€¼çš„åŸºå› ï¼‰
            cluster_mean_profile = np.mean(cluster_expression, axis=0)
            distances_to_mean = [np.sqrt(np.sum((gene_profile - cluster_mean_profile)**2)) 
                               for gene_profile in cluster_expression]
            representative_idx = np.argmin(distances_to_mean)
            representative_gene = cluster_genes[representative_idx]
            
            print(f"  ä»£è¡¨æ€§åŸºå› : {representative_gene}")
            
            cluster_analysis[f'cluster_{cluster_id}'] = {
                'genes': cluster_genes,
                'size': len(cluster_genes),
                'mean_expression': np.mean(cluster_expression),
                'expression_cv': np.std(cluster_expression)/np.mean(cluster_expression),
                'representative_gene': representative_gene,
                'mean_profile': cluster_mean_profile
            }
        
        # ç»˜åˆ¶èšç±»çƒ­å›¾
        if plot_heatmap:
            self._plot_cluster_heatmap(clusters, n_clusters)
        
        return cluster_analysis
    
    def _plot_cluster_heatmap(self, clusters, n_clusters):
        """ç»˜åˆ¶èšç±»ç»“æœçƒ­å›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(16, 8))
        
        # æŒ‰èšç±»é‡æ–°æ’åºåŸºå› 
        sorted_indices = np.argsort(clusters)
        sorted_expression = self.expression_data[sorted_indices]
        sorted_clusters = clusters[sorted_indices]
        
        # ç»˜åˆ¶æ’åºåçš„çƒ­å›¾
        im1 = axes[0].imshow(sorted_expression, cmap='RdYlBu_r', aspect='auto')
        axes[0].set_title('æŒ‰èšç±»æ’åºçš„åŸºå› è¡¨è¾¾çƒ­å›¾', fontsize=14)
        axes[0].set_xlabel('æ ·æœ¬')
        axes[0].set_ylabel('åŸºå› ï¼ˆæŒ‰èšç±»æ’åºï¼‰')
        
        # æ·»åŠ èšç±»åˆ†ç•Œçº¿
        cluster_boundaries = []
        current_cluster = sorted_clusters[0]
        current_pos = 0
        
        for i, cluster in enumerate(sorted_clusters):
            if cluster != current_cluster:
                cluster_boundaries.append(i)
                current_cluster = cluster
        
        for boundary in cluster_boundaries:
            axes[0].axhline(y=boundary-0.5, color='white', linewidth=2)
        
        plt.colorbar(im1, ax=axes[0], label='è¡¨è¾¾æ°´å¹³')
        
        # ç»˜åˆ¶èšç±»å‡å€¼çƒ­å›¾
        cluster_means = []
        for cluster_id in range(1, n_clusters + 1):
            cluster_mask = clusters == cluster_id
            cluster_mean = np.mean(self.expression_data[cluster_mask], axis=0)
            cluster_means.append(cluster_mean)
        
        cluster_means = np.array(cluster_means)
        im2 = axes[1].imshow(cluster_means, cmap='RdYlBu_r', aspect='auto')
        axes[1].set_title('å„èšç±»çš„å¹³å‡è¡¨è¾¾æ¨¡å¼', fontsize=14)
        axes[1].set_xlabel('æ ·æœ¬')
        axes[1].set_ylabel('èšç±»')
        axes[1].set_yticks(range(n_clusters))
        axes[1].set_yticklabels([f'ç°‡{i+1}' for i in range(n_clusters)])
        
        plt.colorbar(im2, ax=axes[1], label='å¹³å‡è¡¨è¾¾æ°´å¹³')
        
        plt.tight_layout()
        plt.show()
    
    def compare_with_true_labels(self, true_labels, n_clusters):
        """ä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒ"""
        predicted_clusters = self.get_clusters(n_clusters)
        
        # è®¡ç®—è°ƒæ•´å…°å¾·æŒ‡æ•°
        from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
        
        ari = adjusted_rand_score(true_labels, predicted_clusters)
        nmi = normalized_mutual_info_score(true_labels, predicted_clusters)
        
        print(f"\nèšç±»è´¨é‡è¯„ä¼°:")
        print(f"è°ƒæ•´å…°å¾·æŒ‡æ•° (ARI): {ari:.3f}")
        print(f"æ ‡å‡†åŒ–äº’ä¿¡æ¯ (NMI): {nmi:.3f}")
        
        # æ··æ·†çŸ©é˜µ
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(true_labels, predicted_clusters)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=[f'é¢„æµ‹ç°‡{i+1}' for i in range(n_clusters)],
                   yticklabels=[f'çœŸå®ç±»å‹{i}' for i in range(len(np.unique(true_labels)))])
        plt.title('èšç±»ç»“æœæ··æ·†çŸ©é˜µ', fontsize=16)
        plt.xlabel('é¢„æµ‹èšç±»')
        plt.ylabel('çœŸå®ç±»å‹')
        plt.tight_layout()
        plt.show()
        
        return ari, nmi

# æ‰§è¡ŒåŸºå› èšç±»åˆ†æ
gene_names = [f'Gene_{i+1}' for i in range(gene_expression.shape[0])]
gene_clustering = GeneClusteringAnalysis(linkage_method='ward')
gene_clustering.fit(gene_expression, gene_names)

# ç»˜åˆ¶æ ‘çŠ¶å›¾
gene_clustering.plot_dendrogram(max_display=30)

# åˆ†æä¸åŒèšç±»æ•°çš„æ•ˆæœ
for k in [3, 4, 5]:
    print(f"\n{'='*60}")
    print(f"èšç±»æ•° K = {k} çš„åˆ†æç»“æœ")
    print(f"{'='*60}")
    
    cluster_analysis = gene_clustering.analyze_clusters(k, plot_heatmap=(k==4))
    ari, nmi = gene_clustering.compare_with_true_labels(true_gene_types, k)
```

### 4.3 åŠŸèƒ½å¯Œé›†åˆ†ææ¨¡æ‹Ÿ

```python
def simulate_functional_enrichment(cluster_analysis, n_clusters=4):
    """æ¨¡æ‹ŸåŸºå› åŠŸèƒ½å¯Œé›†åˆ†æ"""
    
    # æ¨¡æ‹ŸåŸºå› åŠŸèƒ½æ³¨é‡Š
    np.random.seed(42)
    
    # å®šä¹‰åŠŸèƒ½ç±»åˆ«
    functional_categories = [
        "ç»†èƒå‘¨æœŸè°ƒæ§", "DNAä¿®å¤", "è›‹ç™½è´¨åˆæˆ", "ä»£è°¢é€”å¾„", 
        "ä¿¡å·è½¬å¯¼", "è½¬å½•è°ƒæ§", "ç»†èƒå‡‹äº¡", "å…ç–«åº”ç­”"
    ]
    
    print("\nåŸºå› åŠŸèƒ½å¯Œé›†åˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰:")
    print("=" * 60)
    
    enrichment_results = {}
    
    for cluster_id in range(1, n_clusters + 1):
        cluster_info = cluster_analysis[f'cluster_{cluster_id}']
        cluster_size = cluster_info['size']
        
        print(f"\nç°‡ {cluster_id} åŠŸèƒ½å¯Œé›†åˆ†æ:")
        print(f"åŸºå› æ•°é‡: {cluster_size}")
        
        # æ¨¡æ‹ŸåŠŸèƒ½å¯Œé›†ï¼ˆåŸºäºèšç±»ç‰¹å¾ï¼‰
        mean_expr = cluster_info['mean_expression']
        
        # æ ¹æ®è¡¨è¾¾æ°´å¹³æ¨¡æ‹Ÿä¸åŒçš„åŠŸèƒ½å¯Œé›†
        if mean_expr > 6:  # é«˜è¡¨è¾¾
            enriched_functions = ["è›‹ç™½è´¨åˆæˆ", "ä»£è°¢é€”å¾„", "ç»†èƒå‘¨æœŸè°ƒæ§"]
        elif mean_expr > 4:  # ä¸­ç­‰è¡¨è¾¾
            enriched_functions = ["ä¿¡å·è½¬å¯¼", "è½¬å½•è°ƒæ§"]
        elif mean_expr > 3:  # ä½è¡¨è¾¾
            enriched_functions = ["DNAä¿®å¤", "ç»†èƒå‡‹äº¡"]
        else:  # å¾ˆä½è¡¨è¾¾
            enriched_functions = ["å…ç–«åº”ç­”"]
        
        # æ¨¡æ‹Ÿpå€¼å’Œå¯Œé›†å€æ•°
        for func in enriched_functions:
            p_value = np.random.uniform(0.001, 0.05)
            fold_enrichment = np.random.uniform(2.0, 8.0)
            
            print(f"  {func}: på€¼={p_value:.4f}, å¯Œé›†å€æ•°={fold_enrichment:.2f}")
        
        enrichment_results[f'cluster_{cluster_id}'] = {
            'enriched_functions': enriched_functions,
            'cluster_size': cluster_size
        }
    
    # å¯è§†åŒ–åŠŸèƒ½å¯Œé›†ç»“æœ
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºåŠŸèƒ½-èšç±»çŸ©é˜µ
    func_cluster_matrix = np.zeros((len(functional_categories), n_clusters))
    
    for cluster_id in range(1, n_clusters + 1):
        enriched_funcs = enrichment_results[f'cluster_{cluster_id}']['enriched_functions']
        for func in enriched_funcs:
            if func in functional_categories:
                func_idx = functional_categories.index(func)
                func_cluster_matrix[func_idx, cluster_id-1] = 1
    
    # ç»˜åˆ¶çƒ­å›¾
    sns.heatmap(func_cluster_matrix, 
               xticklabels=[f'ç°‡{i+1}' for i in range(n_clusters)],
               yticklabels=functional_categories,
               cmap='Reds', annot=True, fmt='d', cbar=True)
    
    plt.title('åŸºå› åŠŸèƒ½å¯Œé›†çƒ­å›¾', fontsize=16)
    plt.xlabel('åŸºå› èšç±»')
    plt.ylabel('åŠŸèƒ½ç±»åˆ«')
    plt.tight_layout()
    plt.show()
    
    return enrichment_results

# æ‰§è¡ŒåŠŸèƒ½å¯Œé›†åˆ†æ
cluster_analysis_4 = gene_clustering.analyze_clusters(4, plot_heatmap=False)
enrichment_results = simulate_functional_enrichment(cluster_analysis_4, n_clusters=4)
```

## 5. å±‚æ¬¡èšç±»çš„ä¼˜åŒ–å’Œå˜ä½“

### 5.1 BIRCHç®—æ³•ç®€ä»‹

```python
class SimpleBIRCH:
    """ç®€åŒ–çš„BIRCHç®—æ³•å®ç°ï¼ˆé€‚ç”¨äºå¤§æ•°æ®é›†ï¼‰"""
    
    def __init__(self, threshold=0.5, branching_factor=50, n_clusters=3):
        self.threshold = threshold
        self.branching_factor = branching_factor
        self.n_clusters = n_clusters
        self.cluster_centers_ = None
        self.labels_ = None
    
    def fit_predict(self, X):
        """BIRCHèšç±»ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        from sklearn.cluster import Birch
        
        # ä½¿ç”¨sklearnçš„BIRCHå®ç°
        birch = Birch(threshold=self.threshold, 
                     branching_factor=self.branching_factor,
                     n_clusters=self.n_clusters)
        
        self.labels_ = birch.fit_predict(X)
        self.cluster_centers_ = birch.subcluster_centers_
        
        return self.labels_

# BIRCH vs ä¼ ç»Ÿå±‚æ¬¡èšç±»æ€§èƒ½å¯¹æ¯”
print("\nBIRCH vs ä¼ ç»Ÿå±‚æ¬¡èšç±»æ€§èƒ½å¯¹æ¯”:")

# ç”Ÿæˆå¤§æ•°æ®é›†
large_X, _ = make_blobs(n_samples=2000, centers=5, cluster_std=1.5, random_state=42)

import time

# ä¼ ç»Ÿå±‚æ¬¡èšç±»
start_time = time.time()
from sklearn.cluster import AgglomerativeClustering
agg_clustering = AgglomerativeClustering(n_clusters=5, linkage='ward')
agg_labels = agg_clustering.fit_predict(large_X)
agg_time = time.time() - start_time

# BIRCHèšç±»
start_time = time.time()
birch_clustering = SimpleBIRCH(n_clusters=5, threshold=0.5)
birch_labels = birch_clustering.fit_predict(large_X)
birch_time = time.time() - start_time

print(f"ä¼ ç»Ÿå±‚æ¬¡èšç±»æ—¶é—´: {agg_time:.3f}ç§’")
print(f"BIRCHèšç±»æ—¶é—´: {birch_time:.3f}ç§’")
print(f"é€Ÿåº¦æå‡: {agg_time/birch_time:.1f}å€")

# å¯è§†åŒ–å¯¹æ¯”ç»“æœ
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

axes[0].scatter(large_X[:, 0], large_X[:, 1], c=agg_labels, cmap='viridis', alpha=0.6, s=20)
axes[0].set_title(f'ä¼ ç»Ÿå±‚æ¬¡èšç±»\næ—¶é—´: {agg_time:.3f}ç§’', fontsize=14)
axes[0].grid(True, alpha=0.3)

axes[1].scatter(large_X[:, 0], large_X[:, 1], c=birch_labels, cmap='viridis', alpha=0.6, s=20)
axes[1].set_title(f'BIRCHèšç±»\næ—¶é—´: {birch_time:.3f}ç§’', fontsize=14)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 5.2 åœ¨çº¿å±‚æ¬¡èšç±»

```python
class OnlineHierarchicalClustering:
    """åœ¨çº¿å±‚æ¬¡èšç±»ï¼ˆå¢é‡å­¦ä¹ ï¼‰"""
    
    def __init__(self, distance_threshold=1.0, linkage='single'):
        self.distance_threshold = distance_threshold
        self.linkage = linkage
        self.clusters = []  # å­˜å‚¨å½“å‰çš„èšç±»
        self.cluster_centers = []  # å­˜å‚¨èšç±»ä¸­å¿ƒ
        self.n_samples_seen = 0
    
    def partial_fit(self, X):
        """å¢é‡å­¦ä¹ æ–°æ•°æ®ç‚¹"""
        for point in X:
            self._add_point(point)
        return self
    
    def _add_point(self, point):
        """æ·»åŠ å•ä¸ªæ•°æ®ç‚¹"""
        if len(self.clusters) == 0:
            # ç¬¬ä¸€ä¸ªç‚¹ï¼Œåˆ›å»ºç¬¬ä¸€ä¸ªèšç±»
            self.clusters.append([self.n_samples_seen])
            self.cluster_centers.append(point.copy())
        else:
            # è®¡ç®—åˆ°å„èšç±»ä¸­å¿ƒçš„è·ç¦»
            distances = [np.sqrt(np.sum((point - center)**2)) 
                        for center in self.cluster_centers]
            
            min_distance = min(distances)
            closest_cluster_idx = distances.index(min_distance)
            
            if min_distance <= self.distance_threshold:
                # åŠ å…¥æœ€è¿‘çš„èšç±»
                self.clusters[closest_cluster_idx].append(self.n_samples_seen)
                
                # æ›´æ–°èšç±»ä¸­å¿ƒ
                cluster_size = len(self.clusters[closest_cluster_idx])
                self.cluster_centers[closest_cluster_idx] = (
                    (cluster_size - 1) * self.cluster_centers[closest_cluster_idx] + point
                ) / cluster_size
            else:
                # åˆ›å»ºæ–°èšç±»
                self.clusters.append([self.n_samples_seen])
                self.cluster_centers.append(point.copy())
        
        self.n_samples_seen += 1
    
    def get_labels(self, X):
        """è·å–æ•°æ®ç‚¹çš„èšç±»æ ‡ç­¾"""
        labels = np.zeros(len(X))
        
        for cluster_id, cluster_points in enumerate(self.clusters):
            for point_idx in cluster_points:
                if point_idx < len(X):
                    labels[point_idx] = cluster_id
        
        return labels.astype(int)
    
    def get_n_clusters(self):
        """è·å–å½“å‰èšç±»æ•°é‡"""
        return len(self.clusters)

# åœ¨çº¿èšç±»æ¼”ç¤º
print("\nåœ¨çº¿å±‚æ¬¡èšç±»æ¼”ç¤º:")

# ç”Ÿæˆæµå¼æ•°æ®
np.random.seed(42)
stream_data = []
batch_size = 50
n_batches = 10

# æ¨¡æ‹Ÿæ•°æ®æµ
for batch in range(n_batches):
    # æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ¥è‡ªä¸åŒçš„åˆ†å¸ƒ
    center = np.random.uniform(-5, 5, 2)
    batch_data = np.random.normal(center, 1.0, (batch_size, 2))
    stream_data.append(batch_data)

# åœ¨çº¿èšç±»
online_clustering = OnlineHierarchicalClustering(distance_threshold=2.0)

fig, axes = plt.subplots(2, 5, figsize=(20, 8))
axes = axes.ravel()

all_data = []
for i, batch_data in enumerate(stream_data):
    # å¢é‡å­¦ä¹ æ–°æ‰¹æ¬¡æ•°æ®
    online_clustering.partial_fit(batch_data)
    all_data.extend(batch_data)
    
    # å¯è§†åŒ–å½“å‰èšç±»çŠ¶æ€
    if i < 10:
        current_data = np.array(all_data)
        labels = online_clustering.get_labels(current_data)
        
        scatter = axes[i].scatter(current_data[:, 0], current_data[:, 1], 
                                 c=labels, cmap='tab10', alpha=0.7, s=30)
        
        # ç»˜åˆ¶èšç±»ä¸­å¿ƒ
        centers = np.array(online_clustering.cluster_centers)
        axes[i].scatter(centers[:, 0], centers[:, 1], 
                       c='red', marker='x', s=200, linewidths=3)
        
        axes[i].set_title(f'æ‰¹æ¬¡ {i+1}\nèšç±»æ•°: {online_clustering.get_n_clusters()}', fontsize=12)
        axes[i].grid(True, alpha=0.3)

plt.suptitle('åœ¨çº¿å±‚æ¬¡èšç±»è¿‡ç¨‹', fontsize=16)
plt.tight_layout()
plt.show()

print(f"æœ€ç»ˆèšç±»æ•°é‡: {online_clustering.get_n_clusters()}")
print(f"å¤„ç†çš„æ•°æ®ç‚¹æ€»æ•°: {online_clustering.n_samples_seen}")
```

## 6. Traeé£æ ¼å®ç°

```python
class TraeHierarchicalClustering:
    """Traeé£æ ¼çš„å±‚æ¬¡èšç±»å®ç°"""
    
    def __init__(self, linkage='ward', metric='euclidean', 
                 compute_full_tree=True, verbose=True):
        self.linkage = linkage
        self.metric = metric
        self.compute_full_tree = compute_full_tree
        self.verbose = verbose
        
        # Traeç‰¹è‰²ï¼šè¯¦ç»†çš„åˆ†æå†å²
        self.clustering_history = {
            'merge_sequence': [],
            'distance_progression': [],
            'cluster_sizes': []
        }
    
    def trae_fit(self, X, feature_names=None, sample_names=None):
        """Traeé£æ ¼çš„è®­ç»ƒæ–¹æ³•"""
        if self.verbose:
            print("ğŸŒ³ Trae å±‚æ¬¡èšç±»åˆ†æå¼€å§‹")
            print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
            print(f"ğŸ”— é“¾æ¥æ–¹æ³•: {self.linkage.upper()}")
            print(f"ğŸ“ è·ç¦»åº¦é‡: {self.metric}")
        
        self.X_ = X
        self.feature_names_ = feature_names or [f'ç‰¹å¾_{i+1}' for i in range(X.shape[1])]
        self.sample_names_ = sample_names or [f'æ ·æœ¬_{i+1}' for i in range(X.shape[0])]
        
        # è®¡ç®—å±‚æ¬¡èšç±»
        start_time = time.time()
        self.linkage_matrix_ = linkage(X, method=self.linkage, metric=self.metric)
        self.fit_time_ = time.time() - start_time
        
        # è®°å½•èšç±»å†å²
        self._record_clustering_history()
        
        if self.verbose:
            print(f"âœ… èšç±»å®Œæˆ! è®­ç»ƒæ—¶é—´: {self.fit_time_:.3f}ç§’")
            print(f"ğŸ”„ åˆå¹¶æ­¥éª¤æ•°: {len(self.linkage_matrix_)}")
        
        return self
    
    def _record_clustering_history(self):
        """è®°å½•èšç±»å†å²ä¿¡æ¯"""
        n_samples = len(self.X_)
        
        for i, (cluster1, cluster2, distance, size) in enumerate(self.linkage_matrix_):
            self.clustering_history['merge_sequence'].append((int(cluster1), int(cluster2)))
            self.clustering_history['distance_progression'].append(distance)
            self.clustering_history['cluster_sizes'].append(int(size))
    
    def trae_predict(self, n_clusters=None, distance_threshold=None):
        """Traeé£æ ¼çš„é¢„æµ‹æ–¹æ³•"""
        if n_clusters is not None:
            self.labels_ = fcluster(self.linkage_matrix_, t=n_clusters, criterion='maxclust')
            self.n_clusters_ = n_clusters
        elif distance_threshold is not None:
            self.labels_ = fcluster(self.linkage_matrix_, t=distance_threshold, criterion='distance')
            self.n_clusters_ = len(np.unique(self.labels_))
        else:
            # è‡ªåŠ¨ç¡®å®šæœ€ä¼˜èšç±»æ•°
            self.n_clusters_ = self._trae_auto_determine_clusters()
            self.labels_ = fcluster(self.linkage_matrix_, t=self.n_clusters_, criterion='maxclust')
        
        if self.verbose:
            print(f"ğŸ¯ èšç±»é¢„æµ‹å®Œæˆ")
            print(f"ğŸ“ˆ èšç±»æ•°é‡: {self.n_clusters_}")
            for i in range(1, self.n_clusters_ + 1):
                count = np.sum(self.labels_ == i)
                print(f"   ç°‡ {i}: {count} ä¸ªæ ·æœ¬ ({count/len(self.labels_)*100:.1f}%)")
        
        return self.labels_
    
    def _trae_auto_determine_clusters(self):
        """è‡ªåŠ¨ç¡®å®šæœ€ä¼˜èšç±»æ•°"""
        distances = self.linkage_matrix_[:, 2]
        
        # ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™
        if len(distances) > 10:
            # è®¡ç®—äºŒé˜¶å·®åˆ†
            second_diffs = np.diff(distances, n=2)
            # æ‰¾åˆ°æœ€å¤§çš„äºŒé˜¶å·®åˆ†ç‚¹
            optimal_idx = np.argmax(second_diffs[:min(10, len(second_diffs))])
            optimal_clusters = len(distances) - optimal_idx
        else:
            optimal_clusters = max(2, len(distances) // 3)
        
        return min(optimal_clusters, 10)  # é™åˆ¶æœ€å¤§èšç±»æ•°
    
    def trae_visualize_dendrogram(self, max_display=30, figsize=(15, 8)):
        """å¯è§†åŒ–æ ‘çŠ¶å›¾"""
        plt.figure(figsize=figsize)
        
        if len(self.sample_names_) <= max_display:
            # æ˜¾ç¤ºæ ·æœ¬åç§°
            dendrogram(self.linkage_matrix_, labels=self.sample_names_,
                      leaf_rotation=90, leaf_font_size=10)
        else:
            # æˆªæ–­æ˜¾ç¤º
            dendrogram(self.linkage_matrix_, truncate_mode='lastp', p=max_display,
                      leaf_rotation=90, leaf_font_size=12, show_contracted=True)
        
        plt.title(f'ğŸŒ³ Trae å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾\n({self.linkage.upper()} é“¾æ¥, {self.metric.upper()} è·ç¦»)', 
                 fontsize=16)
        plt.xlabel('æ ·æœ¬æˆ–ç°‡å¤§å°', fontsize=12)
        plt.ylabel('è·ç¦»', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
    
    def trae_analyze_optimal_clusters(self, max_k=15):
        """åˆ†ææœ€ä¼˜èšç±»æ•°"""
        print("ğŸ” Trae æœ€ä¼˜èšç±»æ•°åˆ†æ")
        print("=" * 50)
        
        distances = self.linkage_matrix_[:, 2]
        
        # è®¡ç®—ä¸åŒæ–¹æ³•çš„å»ºè®®
        methods_results = {}
        
        # 1. è‚˜éƒ¨æ³•åˆ™ï¼ˆåŸºäºè·ç¦»è·³è·ƒï¼‰
        distance_jumps = np.diff(distances[::-1])  # åå‘è®¡ç®—
        if len(distance_jumps) > 0:
            elbow_k = np.argmax(distance_jumps[:min(max_k-1, len(distance_jumps))]) + 2
            methods_results['è‚˜éƒ¨æ³•åˆ™'] = elbow_k
        
        # 2. è·ç¦»é˜ˆå€¼æ³•
        mean_distance = np.mean(distances)
        std_distance = np.std(distances)
        threshold = mean_distance + std_distance
        threshold_k = len(distances[distances > threshold]) + 1
        methods_results['è·ç¦»é˜ˆå€¼æ³•'] = min(threshold_k, max_k)
        
        # 3. ç›¸å¯¹è·ç¦»å˜åŒ–æ³•
        if len(distances) > 2:
            relative_changes = distances[1:] / distances[:-1]
            max_change_idx = np.argmax(relative_changes[:min(max_k-1, len(relative_changes))])
            relative_k = len(distances) - max_change_idx
            methods_results['ç›¸å¯¹å˜åŒ–æ³•'] = min(relative_k, max_k)
        
        # è¾“å‡ºå»ºè®®
        print("\nå„æ–¹æ³•å»ºè®®çš„èšç±»æ•°:")
        for method, k in methods_results.items():
            print(f"  {method}: K = {k}")
        
        # ç»¼åˆå»ºè®®
        k_values = list(methods_results.values())
        recommended_k = int(np.median(k_values))
        print(f"\nğŸ¯ ç»¼åˆæ¨èèšç±»æ•°: K = {recommended_k}")
        
        # å¯è§†åŒ–åˆ†æ
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # åˆå¹¶è·ç¦»å›¾
        axes[0, 0].plot(range(len(distances)), distances, 'bo-', linewidth=2, markersize=6)
        axes[0, 0].set_title('ğŸ“ˆ åˆå¹¶è·ç¦»åºåˆ—', fontsize=14)
        axes[0, 0].set_xlabel('åˆå¹¶æ­¥éª¤')
        axes[0, 0].set_ylabel('è·ç¦»')
        axes[0, 0].grid(True, alpha=0.3)
        
        # è·ç¦»è·³è·ƒå›¾
        if len(distance_jumps) > 0:
            k_range = range(2, min(len(distance_jumps) + 2, max_k + 1))
            axes[0, 1].plot(k_range, distance_jumps[:len(k_range)], 'ro-', 
                           linewidth=2, markersize=6)
            axes[0, 1].axvline(x=elbow_k, color='red', linestyle='--', alpha=0.7, 
                              label=f'è‚˜éƒ¨æ³•åˆ™ K={elbow_k}')
            axes[0, 1].set_title('ğŸ“Š è·ç¦»è·³è·ƒåˆ†æ', fontsize=14)
            axes[0, 1].set_xlabel('èšç±»æ•° K')
            axes[0, 1].set_ylabel('è·ç¦»è·³è·ƒ')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # èšç±»å¤§å°åˆ†å¸ƒï¼ˆä»¥æ¨èKä¸ºä¾‹ï¼‰
        temp_labels = fcluster(self.linkage_matrix_, t=recommended_k, criterion='maxclust')
        cluster_sizes = [np.sum(temp_labels == i) for i in range(1, recommended_k + 1)]
        
        axes[1, 0].bar(range(1, recommended_k + 1), cluster_sizes, alpha=0.7, color='green')
        axes[1, 0].set_title(f'ğŸ“Š èšç±»å¤§å°åˆ†å¸ƒ (K={recommended_k})', fontsize=14)
        axes[1, 0].set_xlabel('èšç±»ID')
        axes[1, 0].set_ylabel('æ ·æœ¬æ•°é‡')
        axes[1, 0].grid(True, alpha=0.3)
        
        # ä¸åŒKå€¼çš„èšç±»è´¨é‡ï¼ˆè½®å»“ç³»æ•°ï¼‰
        from sklearn.metrics import silhouette_score
        
        silhouette_scores = []
        k_test_range = range(2, min(max_k + 1, len(self.X_)))
        
        for k in k_test_range:
            if k <= len(self.X_):
                test_labels = fcluster(self.linkage_matrix_, t=k, criterion='maxclust')
                if len(np.unique(test_labels)) > 1:
                    score = silhouette_score(self.X_, test_labels)
                    silhouette_scores.append(score)
                else:
                    silhouette_scores.append(0)
        
        if silhouette_scores:
            axes[1, 1].plot(k_test_range[:len(silhouette_scores)], silhouette_scores, 
                           'go-', linewidth=2, markersize=6)
            best_silhouette_k = k_test_range[np.argmax(silhouette_scores)]
            axes[1, 1].axvline(x=best_silhouette_k, color='green', linestyle='--', alpha=0.7,
                              label=f'æœ€ä½³è½®å»“ç³»æ•° K={best_silhouette_k}')
            axes[1, 1].set_title('ğŸ“ˆ è½®å»“ç³»æ•°åˆ†æ', fontsize=14)
            axes[1, 1].set_xlabel('èšç±»æ•° K')
            axes[1, 1].set_ylabel('è½®å»“ç³»æ•°')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return recommended_k, methods_results
    
    def trae_cluster_analysis(self, n_clusters=None, detailed=True):
        """Traeé£æ ¼çš„èšç±»åˆ†æ"""
        if n_clusters is None:
            n_clusters = getattr(self, 'n_clusters_', 3)
        
        labels = fcluster(self.linkage_matrix_, t=n_clusters, criterion='maxclust')
        
        print("ğŸ“Š Trae èšç±»åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        print(f"ğŸ¯ èšç±»æ•°é‡: {n_clusters}")
        print(f"ğŸ“ˆ æ•°æ®ç»´åº¦: {self.X_.shape}")
        print(f"ğŸ”— é“¾æ¥æ–¹æ³•: {self.linkage.upper()}")
        print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {self.fit_time_:.3f}ç§’")
        
        analysis_results = {}
        
        for cluster_id in range(1, n_clusters + 1):
            mask = labels == cluster_id
            cluster_data = self.X_[mask]
            cluster_samples = np.array(self.sample_names_)[mask]
            
            print(f"\nğŸ“‹ ç°‡ {cluster_id} è¯¦ç»†åˆ†æ:")
            print(f"   æ ·æœ¬æ•°é‡: {len(cluster_data)} ({len(cluster_data)/len(self.X_)*100:.1f}%)")
            
            if len(cluster_data) > 0:
                # ç»Ÿè®¡ä¿¡æ¯
                cluster_mean = np.mean(cluster_data, axis=0)
                cluster_std = np.std(cluster_data, axis=0)
                cluster_center = np.median(cluster_data, axis=0)
                
                print(f"   è´¨å¿ƒåæ ‡: [{', '.join([f'{x:.2f}' for x in cluster_mean])}]")
                print(f"   æ ‡å‡†å·®: [{', '.join([f'{x:.2f}' for x in cluster_std])}]")
                
                # ç°‡å†…è·ç¦»åˆ†æ
                if len(cluster_data) > 1:
                    intra_distances = pdist(cluster_data)
                    print(f"   ç°‡å†…å¹³å‡è·ç¦»: {np.mean(intra_distances):.3f}")
                    print(f"   ç°‡å†…æœ€å¤§è·ç¦»: {np.max(intra_distances):.3f}")
                
                if detailed and len(cluster_samples) <= 10:
                    print(f"   åŒ…å«æ ·æœ¬: {', '.join(cluster_samples)}")
                
                analysis_results[f'cluster_{cluster_id}'] = {
                    'samples': cluster_samples,
                    'size': len(cluster_data),
                    'centroid': cluster_mean,
                    'std': cluster_std,
                    'intra_distance': np.mean(intra_distances) if len(cluster_data) > 1 else 0
                }
        
        # è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡
        from sklearn.metrics import silhouette_score, calinski_harabasz_score
        
        if len(np.unique(labels)) > 1:
            silhouette = silhouette_score(self.X_, labels)
            calinski = calinski_harabasz_score(self.X_, labels)
            
            print(f"\nğŸ“Š èšç±»è´¨é‡è¯„ä¼°:")
            print(f"   è½®å»“ç³»æ•°: {silhouette:.3f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
            print(f"   Calinski-HarabaszæŒ‡æ•°: {calinski:.2f} (è¶Šå¤§è¶Šå¥½)")
            
            analysis_results['quality_metrics'] = {
                'silhouette_score': silhouette,
                'calinski_harabasz_score': calinski
            }
        
        return analysis_results
    
    def trae_export_results(self, filename=None, n_clusters=None):
        """å¯¼å‡ºèšç±»ç»“æœ"""
        if n_clusters is None:
            n_clusters = getattr(self, 'n_clusters_', 3)
        
        labels = fcluster(self.linkage_matrix_, t=n_clusters, criterion='maxclust')
        
        # åˆ›å»ºç»“æœDataFrame
        results_df = pd.DataFrame({
            'æ ·æœ¬åç§°': self.sample_names_,
            'èšç±»æ ‡ç­¾': labels
        })
        
        # æ·»åŠ ç‰¹å¾æ•°æ®
        for i, feature_name in enumerate(self.feature_names_):
            results_df[feature_name] = self.X_[:, i]
        
        if filename:
            results_df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ èšç±»ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
        
        return results_df

# Traeå±‚æ¬¡èšç±»æ¼”ç¤º
print("\nğŸŒ³ Trae å±‚æ¬¡èšç±»ç³»ç»Ÿæ¼”ç¤º")
print("=" * 60)

# ä½¿ç”¨ä¹‹å‰çš„åŸºå› è¡¨è¾¾æ•°æ®
trae_clustering = TraeHierarchicalClustering(linkage='ward', verbose=True)

# è®­ç»ƒæ¨¡å‹
trae_clustering.trae_fit(gene_expression, 
                        feature_names=[f'æ ·æœ¬_{i+1}' for i in range(gene_expression.shape[1])],
                        sample_names=[f'åŸºå› _{i+1}' for i in range(gene_expression.shape[0])])

# å¯è§†åŒ–æ ‘çŠ¶å›¾
trae_clustering.trae_visualize_dendrogram(max_display=20)

# åˆ†ææœ€ä¼˜èšç±»æ•°
recommended_k, methods_results = trae_clustering.trae_analyze_optimal_clusters(max_k=10)

# ä½¿ç”¨æ¨èçš„èšç±»æ•°è¿›è¡Œé¢„æµ‹
labels = trae_clustering.trae_predict(n_clusters=recommended_k)

# è¯¦ç»†åˆ†æèšç±»ç»“æœ
analysis_results = trae_clustering.trae_cluster_analysis(n_clusters=recommended_k, detailed=True)

# å¯¼å‡ºç»“æœ
results_df = trae_clustering.trae_export_results(n_clusters=recommended_k)
print(f"\nğŸ“‹ èšç±»ç»“æœé¢„è§ˆ:")
print(results_df.head(10).to_string(index=False))

## 7. æ€è€ƒé¢˜

1. **é“¾æ¥å‡†åˆ™é€‰æ‹©**: åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©å•é“¾æ¥ã€å…¨é“¾æ¥ã€å¹³å‡é“¾æ¥æˆ–Wardé“¾æ¥ï¼Ÿå®ƒä»¬å„è‡ªçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ

2. **æ ‘çŠ¶å›¾è§£è¯»**: å¦‚ä½•ä»æ ‘çŠ¶å›¾ä¸­ç¡®å®šæœ€ä¼˜çš„èšç±»æ•°ï¼Ÿé™¤äº†è‚˜éƒ¨æ³•åˆ™ï¼Œè¿˜æœ‰å“ªäº›æ–¹æ³•å¯ä»¥è¾…åŠ©å†³ç­–ï¼Ÿ

3. **è·ç¦»åº¦é‡å½±å“**: ä¸åŒçš„è·ç¦»åº¦é‡ï¼ˆæ¬§å‡ é‡Œå¾—ã€æ›¼å“ˆé¡¿ã€ä½™å¼¦ç­‰ï¼‰å¯¹å±‚æ¬¡èšç±»ç»“æœæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

4. **è®¡ç®—å¤æ‚åº¦**: å±‚æ¬¡èšç±»çš„æ—¶é—´å¤æ‚åº¦æ˜¯O(nÂ³)ï¼Œå¯¹äºå¤§æ•°æ®é›†å¦‚ä½•ä¼˜åŒ–ï¼ŸBIRCHç®—æ³•æ˜¯å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜çš„ï¼Ÿ

5. **åº”ç”¨åœºæ™¯**: å±‚æ¬¡èšç±»ç›¸æ¯”K-meansæœ‰å“ªäº›ç‹¬ç‰¹ä¼˜åŠ¿ï¼Ÿåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æ›´é€‚åˆä½¿ç”¨å±‚æ¬¡èšç±»ï¼Ÿ

## 8. å°ç»“

### 8.1 æ ¸å¿ƒä¼˜åŠ¿

- **æ— éœ€é¢„è®¾èšç±»æ•°**: é€šè¿‡æ ‘çŠ¶å›¾å¯ä»¥çµæ´»é€‰æ‹©èšç±»æ•°é‡
- **ç»“æœç¨³å®š**: ç¡®å®šæ€§ç®—æ³•ï¼Œæ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´
- **å±‚æ¬¡ç»“æ„**: æä¾›æ•°æ®çš„å¤šå±‚æ¬¡ç»„ç»‡ä¿¡æ¯
- **ä»»æ„å½¢çŠ¶**: èƒ½å¤Ÿå‘ç°ä»»æ„å½¢çŠ¶çš„èšç±»
- **å¯è§£é‡Šæ€§å¼º**: æ ‘çŠ¶å›¾æä¾›ç›´è§‚çš„èšç±»è¿‡ç¨‹å¯è§†åŒ–

### 8.2 å…³é”®æŠ€æœ¯

- **é“¾æ¥å‡†åˆ™**: å•é“¾æ¥ã€å…¨é“¾æ¥ã€å¹³å‡é“¾æ¥ã€Wardé“¾æ¥
- **è·ç¦»åº¦é‡**: æ¬§å‡ é‡Œå¾—ã€æ›¼å“ˆé¡¿ã€ä½™å¼¦ã€é—µå¯å¤«æ–¯åŸºè·ç¦»
- **æ ‘çŠ¶å›¾åˆ†æ**: åˆå¹¶è·ç¦»ã€è‚˜éƒ¨æ³•åˆ™ã€è½®å»“ç³»æ•°
- **ä¼˜åŒ–ç®—æ³•**: BIRCHã€åœ¨çº¿èšç±»ã€è¿‘ä¼¼æ–¹æ³•

### 8.3 å®é™…åº”ç”¨

- **ç”Ÿç‰©ä¿¡æ¯å­¦**: åŸºå› è¡¨è¾¾åˆ†æã€è›‹ç™½è´¨åˆ†ç±»ã€è¿›åŒ–æ ‘æ„å»º
- **å¸‚åœºç ”ç©¶**: å®¢æˆ·ç»†åˆ†ã€äº§å“åˆ†ç±»ã€å¸‚åœºç»“æ„åˆ†æ
- **å›¾åƒå¤„ç†**: å›¾åƒåˆ†å‰²ã€ç‰¹å¾èšç±»ã€æ¨¡å¼è¯†åˆ«
- **ç¤¾äº¤ç½‘ç»œ**: ç¤¾åŒºå‘ç°ã€ç”¨æˆ·åˆ†ç¾¤ã€å…³ç³»åˆ†æ
- **æ–‡æœ¬æŒ–æ˜**: æ–‡æ¡£èšç±»ã€ä¸»é¢˜å‘ç°ã€è¯­ä¹‰åˆ†æ

### 8.4 å±€é™æ€§

- **è®¡ç®—å¤æ‚åº¦é«˜**: O(nÂ³)æ—¶é—´å¤æ‚åº¦ï¼Œä¸é€‚åˆè¶…å¤§æ•°æ®é›†
- **å†…å­˜éœ€æ±‚å¤§**: éœ€è¦å­˜å‚¨è·ç¦»çŸ©é˜µï¼Œç©ºé—´å¤æ‚åº¦O(nÂ²)
- **å™ªå£°æ•æ„Ÿ**: ç‰¹åˆ«æ˜¯å•é“¾æ¥æ–¹æ³•å®¹æ˜“å—å¼‚å¸¸å€¼å½±å“
- **éš¾ä»¥å¤„ç†é«˜ç»´**: åœ¨é«˜ç»´ç©ºé—´ä¸­è·ç¦»åº¦é‡å¯èƒ½å¤±æ•ˆ

### 8.5 ä½¿ç”¨å»ºè®®

1. **æ•°æ®é¢„å¤„ç†**: æ ‡å‡†åŒ–ç‰¹å¾ï¼Œå¤„ç†å¼‚å¸¸å€¼
2. **é“¾æ¥æ–¹æ³•é€‰æ‹©**: Wardé€‚åˆçƒå½¢ç°‡ï¼Œå•é“¾æ¥é€‚åˆä»»æ„å½¢çŠ¶
3. **èšç±»æ•°ç¡®å®š**: ç»“åˆå¤šç§æ–¹æ³•ï¼ˆè‚˜éƒ¨æ³•åˆ™ã€è½®å»“ç³»æ•°ã€ä¸šåŠ¡éœ€æ±‚ï¼‰
4. **ç»“æœéªŒè¯**: ä½¿ç”¨å¤šç§è¯„ä¼°æŒ‡æ ‡éªŒè¯èšç±»è´¨é‡
5. **å¯è§†åŒ–åˆ†æ**: å……åˆ†åˆ©ç”¨æ ‘çŠ¶å›¾è¿›è¡Œç»“æœè§£é‡Š

### 8.6 ä¸‹ä¸€æ­¥å­¦ä¹ 

- **å¯†åº¦èšç±»**: DBSCANã€OPTICSç­‰åŸºäºå¯†åº¦çš„æ–¹æ³•
- **è°±èšç±»**: åŸºäºå›¾è®ºçš„èšç±»æ–¹æ³•
- **é›†æˆèšç±»**: å¤šç§èšç±»æ–¹æ³•çš„ç»„åˆ
- **æ·±åº¦èšç±»**: ç»“åˆæ·±åº¦å­¦ä¹ çš„èšç±»æ–¹æ³•
- **æµå¼èšç±»**: å¤„ç†åŠ¨æ€æ•°æ®çš„åœ¨çº¿èšç±»ç®—æ³•

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å·²ç»æŒæ¡äº†å±‚æ¬¡èšç±»çš„æ ¸å¿ƒåŸç†å’Œå®è·µæŠ€èƒ½ã€‚å±‚æ¬¡èšç±»ä½œä¸ºç»å…¸çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œåœ¨æ•°æ®æŒ–æ˜å’Œæ¨¡å¼è¯†åˆ«ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨ä»·å€¼ã€‚