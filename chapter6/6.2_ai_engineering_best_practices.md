# 6.2 AI工程化最佳实践

随着AI技术的成熟，如何将AI模型从实验室环境成功部署到生产环境，并确保其稳定、可靠、可扩展的运行，成为了AI工程师面临的核心挑战。本节将深入探讨AI工程化的最佳实践。

## 📚 AI工程化概述

AI工程化（AI Engineering）是将机器学习模型从研发阶段转化为可生产部署的工程实践，涵盖了从数据管理、模型开发、训练、部署到监控的完整生命周期。

### 核心挑战
- **模型-代码鸿沟**：研究代码与生产代码的差异
- **数据漂移**：生产环境中数据分布的变化
- **性能要求**：延迟、吞吐量、资源消耗的平衡
- **可靠性保障**：系统稳定性和容错能力
- **可观测性**：模型行为的监控和调试

## 🏗️ MLOps完整流程

### 1. MLOps架构设计

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import mlflow
import mlflow.pytorch
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
import pandas as pd
from datetime import datetime
import json
import logging
from pathlib import Path
import pickle
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import hashlib
import yaml
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

@dataclass
class ModelConfig:
    """模型配置"""
    model_name: str
    model_version: str
    architecture: str
    hyperparameters: Dict[str, Any]
    training_config: Dict[str, Any]
    deployment_config: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'ModelConfig':
        return cls(**config_dict)
    
    def save(self, filepath: str):
        """保存配置到文件"""
        with open(filepath, 'w') as f:
            yaml.dump(self.to_dict(), f, default_flow_style=False)
    
    @classmethod
    def load(cls, filepath: str) -> 'ModelConfig':
        """从文件加载配置"""
        with open(filepath, 'r') as f:
            config_dict = yaml.safe_load(f)
        return cls.from_dict(config_dict)

@dataclass
class ExperimentMetrics:
    """实验指标"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    loss: float
    training_time: float
    inference_time: float
    model_size: int
    
    def to_dict(self) -> Dict[str, float]:
        return asdict(self)

class MLOpsLogger:
    """MLOps日志管理器"""
    
    def __init__(self, experiment_name: str, log_dir: str = "./mlops_logs"):
        self.experiment_name = experiment_name
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        # 设置日志
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.log_dir / f"{experiment_name}.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(experiment_name)
        
        # MLflow设置
        mlflow.set_experiment(experiment_name)
        
    def log_config(self, config: ModelConfig):
        """记录配置"""
        self.logger.info(f"实验配置: {config.model_name} v{config.model_version}")
        mlflow.log_params(config.hyperparameters)
        mlflow.log_params(config.training_config)
        
    def log_metrics(self, metrics: ExperimentMetrics, step: Optional[int] = None):
        """记录指标"""
        self.logger.info(f"实验指标: {metrics.to_dict()}")
        mlflow.log_metrics(metrics.to_dict(), step=step)
        
    def log_model(self, model: nn.Module, model_name: str):
        """记录模型"""
        self.logger.info(f"保存模型: {model_name}")
        mlflow.pytorch.log_model(model, model_name)
        
    def log_artifact(self, filepath: str, artifact_path: Optional[str] = None):
        """记录工件"""
        self.logger.info(f"保存工件: {filepath}")
        mlflow.log_artifact(filepath, artifact_path)

class DataVersionControl:
    """数据版本控制"""
    
    def __init__(self, data_dir: str = "./data_versions"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.metadata_file = self.data_dir / "metadata.json"
        
        # 加载或初始化元数据
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {"versions": {}, "latest_version": None}
    
    def create_data_hash(self, data: Any) -> str:
        """创建数据哈希"""
        if isinstance(data, pd.DataFrame):
            data_str = data.to_string()
        elif isinstance(data, np.ndarray):
            data_str = str(data.tobytes())
        else:
            data_str = str(data)
        
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def save_data_version(self, data: Any, version_name: str, 
                         description: str = "") -> str:
        """保存数据版本"""
        print(f"💾 保存数据版本: {version_name}")
        
        # 创建数据哈希
        data_hash = self.create_data_hash(data)
        
        # 检查是否已存在相同数据
        for existing_version, info in self.metadata["versions"].items():
            if info["hash"] == data_hash:
                print(f"   数据已存在于版本: {existing_version}")
                return existing_version
        
        # 保存数据
        version_dir = self.data_dir / version_name
        version_dir.mkdir(exist_ok=True)
        
        data_file = version_dir / "data.pkl"
        with open(data_file, 'wb') as f:
            pickle.dump(data, f)
        
        # 更新元数据
        self.metadata["versions"][version_name] = {
            "hash": data_hash,
            "description": description,
            "created_at": datetime.now().isoformat(),
            "file_path": str(data_file)
        }
        self.metadata["latest_version"] = version_name
        
        # 保存元数据
        with open(self.metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)
        
        print(f"   数据版本已保存: {version_name} (哈希: {data_hash[:8]}...)")
        return version_name
    
    def load_data_version(self, version_name: str) -> Any:
        """加载数据版本"""
        if version_name not in self.metadata["versions"]:
            raise ValueError(f"数据版本不存在: {version_name}")
        
        file_path = self.metadata["versions"][version_name]["file_path"]
        
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"📂 加载数据版本: {version_name}")
        return data
    
    def list_versions(self) -> List[Dict[str, Any]]:
        """列出所有版本"""
        versions = []
        for version_name, info in self.metadata["versions"].items():
            versions.append({
                "version": version_name,
                "hash": info["hash"][:8],
                "description": info["description"],
                "created_at": info["created_at"]
            })
        return versions

class ModelRegistry:
    """模型注册表"""
    
    def __init__(self, registry_dir: str = "./model_registry"):
        self.registry_dir = Path(registry_dir)
        self.registry_dir.mkdir(exist_ok=True)
        self.registry_file = self.registry_dir / "registry.json"
        
        # 加载或初始化注册表
        if self.registry_file.exists():
            with open(self.registry_file, 'r') as f:
                self.registry = json.load(f)
        else:
            self.registry = {"models": {}}
    
    def register_model(self, model: nn.Module, config: ModelConfig, 
                      metrics: ExperimentMetrics, stage: str = "staging") -> str:
        """注册模型"""
        model_id = f"{config.model_name}_v{config.model_version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"📝 注册模型: {model_id}")
        
        # 创建模型目录
        model_dir = self.registry_dir / model_id
        model_dir.mkdir(exist_ok=True)
        
        # 保存模型
        model_path = model_dir / "model.pth"
        torch.save(model.state_dict(), model_path)
        
        # 保存配置
        config_path = model_dir / "config.yaml"
        config.save(config_path)
        
        # 保存指标
        metrics_path = model_dir / "metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics.to_dict(), f, indent=2)
        
        # 更新注册表
        self.registry["models"][model_id] = {
            "model_name": config.model_name,
            "version": config.model_version,
            "stage": stage,
            "registered_at": datetime.now().isoformat(),
            "model_path": str(model_path),
            "config_path": str(config_path),
            "metrics_path": str(metrics_path),
            "metrics": metrics.to_dict()
        }
        
        # 保存注册表
        with open(self.registry_file, 'w') as f:
            json.dump(self.registry, f, indent=2)
        
        print(f"   模型已注册: {model_id} (阶段: {stage})")
        return model_id
    
    def load_model(self, model_id: str, model_class: type) -> Tuple[nn.Module, ModelConfig, ExperimentMetrics]:
        """加载模型"""
        if model_id not in self.registry["models"]:
            raise ValueError(f"模型不存在: {model_id}")
        
        model_info = self.registry["models"][model_id]
        
        # 加载模型
        model = model_class()
        model.load_state_dict(torch.load(model_info["model_path"]))
        
        # 加载配置
        config = ModelConfig.load(model_info["config_path"])
        
        # 加载指标
        with open(model_info["metrics_path"], 'r') as f:
            metrics_dict = json.load(f)
        metrics = ExperimentMetrics(**metrics_dict)
        
        print(f"📂 加载模型: {model_id}")
        return model, config, metrics
    
    def promote_model(self, model_id: str, new_stage: str):
        """提升模型阶段"""
        if model_id not in self.registry["models"]:
            raise ValueError(f"模型不存在: {model_id}")
        
        old_stage = self.registry["models"][model_id]["stage"]
        self.registry["models"][model_id]["stage"] = new_stage
        self.registry["models"][model_id]["promoted_at"] = datetime.now().isoformat()
        
        # 保存注册表
        with open(self.registry_file, 'w') as f:
            json.dump(self.registry, f, indent=2)
        
        print(f"🚀 模型阶段提升: {model_id} ({old_stage} -> {new_stage})")
    
    def list_models(self, stage: Optional[str] = None) -> List[Dict[str, Any]]:
        """列出模型"""
        models = []
        for model_id, info in self.registry["models"].items():
            if stage is None or info["stage"] == stage:
                models.append({
                    "model_id": model_id,
                    "model_name": info["model_name"],
                    "version": info["version"],
                    "stage": info["stage"],
                    "registered_at": info["registered_at"],
                    "accuracy": info["metrics"].get("accuracy", 0)
                })
        return models

class ContinuousIntegration:
    """持续集成管道"""
    
    def __init__(self, logger: MLOpsLogger, data_vc: DataVersionControl, 
                 model_registry: ModelRegistry):
        self.logger = logger
        self.data_vc = data_vc
        self.model_registry = model_registry
        self.test_results = []
    
    def run_data_validation(self, data: Any, expected_schema: Dict[str, Any]) -> bool:
        """数据验证"""
        print("🔍 执行数据验证...")
        
        validation_results = []
        
        if isinstance(data, pd.DataFrame):
            # 检查列名
            expected_columns = set(expected_schema.get("columns", []))
            actual_columns = set(data.columns)
            
            if expected_columns != actual_columns:
                missing_cols = expected_columns - actual_columns
                extra_cols = actual_columns - expected_columns
                
                if missing_cols:
                    validation_results.append(f"缺少列: {missing_cols}")
                if extra_cols:
                    validation_results.append(f"额外列: {extra_cols}")
            
            # 检查数据类型
            for col, expected_type in expected_schema.get("dtypes", {}).items():
                if col in data.columns:
                    actual_type = str(data[col].dtype)
                    if actual_type != expected_type:
                        validation_results.append(
                            f"列 {col} 类型不匹配: 期望 {expected_type}, 实际 {actual_type}"
                        )
            
            # 检查数据范围
            for col, range_info in expected_schema.get("ranges", {}).items():
                if col in data.columns:
                    min_val, max_val = range_info
                    actual_min, actual_max = data[col].min(), data[col].max()
                    
                    if actual_min < min_val or actual_max > max_val:
                        validation_results.append(
                            f"列 {col} 值超出范围: [{min_val}, {max_val}], 实际: [{actual_min}, {actual_max}]"
                        )
        
        # 记录验证结果
        if validation_results:
            for result in validation_results:
                self.logger.logger.error(f"数据验证失败: {result}")
            return False
        else:
            self.logger.logger.info("数据验证通过")
            return True
    
    def run_model_tests(self, model: nn.Module, test_data: DataLoader) -> Dict[str, Any]:
        """模型测试"""
        print("🧪 执行模型测试...")
        
        model.eval()
        test_results = {
            "unit_tests": self._run_unit_tests(model),
            "integration_tests": self._run_integration_tests(model, test_data),
            "performance_tests": self._run_performance_tests(model, test_data)
        }
        
        # 记录测试结果
        all_passed = all([
            test_results["unit_tests"]["passed"],
            test_results["integration_tests"]["passed"],
            test_results["performance_tests"]["passed"]
        ])
        
        test_results["overall_passed"] = all_passed
        self.test_results.append(test_results)
        
        if all_passed:
            self.logger.logger.info("所有模型测试通过")
        else:
            self.logger.logger.error("模型测试失败")
        
        return test_results
    
    def _run_unit_tests(self, model: nn.Module) -> Dict[str, Any]:
        """单元测试"""
        tests = []
        
        # 测试模型结构
        try:
            param_count = sum(p.numel() for p in model.parameters())
            tests.append({"test": "parameter_count", "passed": param_count > 0, "value": param_count})
        except Exception as e:
            tests.append({"test": "parameter_count", "passed": False, "error": str(e)})
        
        # 测试前向传播
        try:
            dummy_input = torch.randn(1, 784)  # 假设输入维度
            output = model(dummy_input)
            tests.append({"test": "forward_pass", "passed": True, "output_shape": list(output.shape)})
        except Exception as e:
            tests.append({"test": "forward_pass", "passed": False, "error": str(e)})
        
        # 测试梯度计算
        try:
            dummy_input = torch.randn(1, 784, requires_grad=True)
            output = model(dummy_input)
            loss = output.sum()
            loss.backward()
            
            has_gradients = any(p.grad is not None for p in model.parameters())
            tests.append({"test": "gradient_computation", "passed": has_gradients})
        except Exception as e:
            tests.append({"test": "gradient_computation", "passed": False, "error": str(e)})
        
        passed = all(test["passed"] for test in tests)
        return {"tests": tests, "passed": passed}
    
    def _run_integration_tests(self, model: nn.Module, test_data: DataLoader) -> Dict[str, Any]:
        """集成测试"""
        tests = []
        
        # 测试批量推理
        try:
            batch_count = 0
            total_samples = 0
            
            with torch.no_grad():
                for batch_idx, (data, target) in enumerate(test_data):
                    if batch_idx >= 3:  # 只测试前3个批次
                        break
                    
                    output = model(data)
                    batch_count += 1
                    total_samples += data.size(0)
            
            tests.append({
                "test": "batch_inference", 
                "passed": batch_count > 0, 
                "batches_processed": batch_count,
                "samples_processed": total_samples
            })
        except Exception as e:
            tests.append({"test": "batch_inference", "passed": False, "error": str(e)})
        
        # 测试输出一致性
        try:
            dummy_input = torch.randn(2, 784)
            output1 = model(dummy_input)
            output2 = model(dummy_input)
            
            consistency = torch.allclose(output1, output2, atol=1e-6)
            tests.append({"test": "output_consistency", "passed": consistency})
        except Exception as e:
            tests.append({"test": "output_consistency", "passed": False, "error": str(e)})
        
        passed = all(test["passed"] for test in tests)
        return {"tests": tests, "passed": passed}
    
    def _run_performance_tests(self, model: nn.Module, test_data: DataLoader) -> Dict[str, Any]:
        """性能测试"""
        tests = []
        
        # 测试推理延迟
        try:
            import time
            
            dummy_input = torch.randn(1, 784)
            
            # 预热
            for _ in range(10):
                _ = model(dummy_input)
            
            # 测试延迟
            start_time = time.time()
            for _ in range(100):
                _ = model(dummy_input)
            end_time = time.time()
            
            avg_latency = (end_time - start_time) / 100 * 1000  # 毫秒
            latency_ok = avg_latency < 100  # 100ms阈值
            
            tests.append({
                "test": "inference_latency", 
                "passed": latency_ok, 
                "avg_latency_ms": avg_latency
            })
        except Exception as e:
            tests.append({"test": "inference_latency", "passed": False, "error": str(e)})
        
        # 测试内存使用
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # 执行推理
            with torch.no_grad():
                for batch_idx, (data, target) in enumerate(test_data):
                    if batch_idx >= 10:  # 只测试前10个批次
                        break
                    _ = model(data)
            
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = memory_after - memory_before
            memory_ok = memory_increase < 500  # 500MB阈值
            
            tests.append({
                "test": "memory_usage", 
                "passed": memory_ok, 
                "memory_increase_mb": memory_increase
            })
        except Exception as e:
            tests.append({"test": "memory_usage", "passed": False, "error": str(e)})
        
        passed = all(test["passed"] for test in tests)
        return {"tests": tests, "passed": passed}
    
    def run_ci_pipeline(self, model: nn.Module, config: ModelConfig, 
                       train_data: Any, test_data: DataLoader, 
                       expected_schema: Dict[str, Any]) -> bool:
        """运行CI管道"""
        print("🚀 运行持续集成管道")
        print("=" * 50)
        
        # 1. 数据验证
        data_valid = self.run_data_validation(train_data, expected_schema)
        if not data_valid:
            self.logger.logger.error("CI管道失败: 数据验证未通过")
            return False
        
        # 2. 模型测试
        test_results = self.run_model_tests(model, test_data)
        if not test_results["overall_passed"]:
            self.logger.logger.error("CI管道失败: 模型测试未通过")
            return False
        
        # 3. 版本控制
        data_version = self.data_vc.save_data_version(
            train_data, 
            f"data_v{config.model_version}", 
            f"训练数据 for {config.model_name} v{config.model_version}"
        )
        
        self.logger.logger.info(f"CI管道成功完成，数据版本: {data_version}")
        return True
```

### 2. 模型部署策略

```python
from abc import ABC, abstractmethod
from typing import Union, List, Dict, Any, Optional
import torch
import torch.nn as nn
from torch.jit import ScriptModule
import onnx
import onnxruntime as ort
import numpy as np
from flask import Flask, request, jsonify
from concurrent.futures import ThreadPoolExecutor
import threading
import time
from dataclasses import dataclass
import queue

@dataclass
class DeploymentConfig:
    """部署配置"""
    deployment_type: str  # 'batch', 'online', 'streaming'
    serving_framework: str  # 'flask', 'fastapi', 'torchserve'
    model_format: str  # 'pytorch', 'onnx', 'torchscript'
    batch_size: int = 32
    max_latency_ms: int = 100
    max_throughput_rps: int = 1000
    auto_scaling: bool = True
    health_check_interval: int = 30

class ModelDeploymentStrategy(ABC):
    """模型部署策略基类"""
    
    def __init__(self, config: DeploymentConfig):
        self.config = config
        self.model = None
        self.is_loaded = False
    
    @abstractmethod
    def load_model(self, model_path: str):
        """加载模型"""
        pass
    
    @abstractmethod
    def predict(self, input_data: Any) -> Any:
        """预测"""
        pass
    
    @abstractmethod
    def health_check(self) -> bool:
        """健康检查"""
        pass

class PyTorchDeployment(ModelDeploymentStrategy):
    """PyTorch部署策略"""
    
    def __init__(self, config: DeploymentConfig, model_class: type):
        super().__init__(config)
        self.model_class = model_class
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def load_model(self, model_path: str):
        """加载PyTorch模型"""
        print(f"🔄 加载PyTorch模型: {model_path}")
        
        self.model = self.model_class()
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        
        self.is_loaded = True
        print(f"✅ PyTorch模型加载完成")
    
    def predict(self, input_data: Union[np.ndarray, torch.Tensor]) -> np.ndarray:
        """PyTorch预测"""
        if not self.is_loaded:
            raise RuntimeError("模型未加载")
        
        # 转换输入数据
        if isinstance(input_data, np.ndarray):
            input_tensor = torch.from_numpy(input_data).float().to(self.device)
        else:
            input_tensor = input_data.to(self.device)
        
        # 预测
        with torch.no_grad():
            output = self.model(input_tensor)
            
        return output.cpu().numpy()
    
    def health_check(self) -> bool:
        """健康检查"""
        if not self.is_loaded:
            return False
        
        try:
            # 使用虚拟输入测试模型
            dummy_input = torch.randn(1, 784).to(self.device)
            with torch.no_grad():
                _ = self.model(dummy_input)
            return True
        except Exception:
            return False

class ONNXDeployment(ModelDeploymentStrategy):
    """ONNX部署策略"""
    
    def __init__(self, config: DeploymentConfig):
        super().__init__(config)
        self.session = None
        self.input_name = None
        self.output_name = None
    
    def load_model(self, model_path: str):
        """加载ONNX模型"""
        print(f"🔄 加载ONNX模型: {model_path}")
        
        # 创建ONNX Runtime会话
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        self.session = ort.InferenceSession(model_path, providers=providers)
        
        # 获取输入输出名称
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name
        
        self.is_loaded = True
        print(f"✅ ONNX模型加载完成")
    
    def predict(self, input_data: np.ndarray) -> np.ndarray:
        """ONNX预测"""
        if not self.is_loaded:
            raise RuntimeError("模型未加载")
        
        # 确保输入数据类型正确
        if input_data.dtype != np.float32:
            input_data = input_data.astype(np.float32)
        
        # 预测
        result = self.session.run(
            [self.output_name], 
            {self.input_name: input_data}
        )
        
        return result[0]
    
    def health_check(self) -> bool:
        """健康检查"""
        if not self.is_loaded:
            return False
        
        try:
            # 使用虚拟输入测试模型
            dummy_input = np.random.randn(1, 784).astype(np.float32)
            _ = self.predict(dummy_input)
            return True
        except Exception:
            return False

class TorchScriptDeployment(ModelDeploymentStrategy):
    """TorchScript部署策略"""
    
    def __init__(self, config: DeploymentConfig):
        super().__init__(config)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def load_model(self, model_path: str):
        """加载TorchScript模型"""
        print(f"🔄 加载TorchScript模型: {model_path}")
        
        self.model = torch.jit.load(model_path, map_location=self.device)
        self.model.eval()
        
        self.is_loaded = True
        print(f"✅ TorchScript模型加载完成")
    
    def predict(self, input_data: Union[np.ndarray, torch.Tensor]) -> np.ndarray:
        """TorchScript预测"""
        if not self.is_loaded:
            raise RuntimeError("模型未加载")
        
        # 转换输入数据
        if isinstance(input_data, np.ndarray):
            input_tensor = torch.from_numpy(input_data).float().to(self.device)
        else:
            input_tensor = input_data.to(self.device)
        
        # 预测
        with torch.no_grad():
            output = self.model(input_tensor)
            
        return output.cpu().numpy()
    
    def health_check(self) -> bool:
        """健康检查"""
        if not self.is_loaded:
            return False
        
        try:
            # 使用虚拟输入测试模型
            dummy_input = torch.randn(1, 784).to(self.device)
            with torch.no_grad():
                _ = self.model(dummy_input)
            return True
        except Exception:
            return False

class ModelServer:
    """模型服务器"""
    
    def __init__(self, deployment_strategy: ModelDeploymentStrategy):
        self.deployment = deployment_strategy
        self.app = Flask(__name__)
        self.request_queue = queue.Queue(maxsize=1000)
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'avg_latency_ms': 0,
            'current_qps': 0
        }
        self.setup_routes()
    
    def setup_routes(self):
        """设置路由"""
        
        @self.app.route('/predict', methods=['POST'])
        def predict():
            start_time = time.time()
            
            try:
                # 获取输入数据
                data = request.get_json()
                input_array = np.array(data['input'])
                
                # 预测
                result = self.deployment.predict(input_array)
                
                # 更新指标
                latency = (time.time() - start_time) * 1000
                self._update_metrics(success=True, latency=latency)
                
                return jsonify({
                    'prediction': result.tolist(),
                    'latency_ms': latency,
                    'status': 'success'
                })
                
            except Exception as e:
                self._update_metrics(success=False)
                return jsonify({
                    'error': str(e),
                    'status': 'error'
                }), 500
        
        @self.app.route('/health', methods=['GET'])
        def health():
            is_healthy = self.deployment.health_check()
            status_code = 200 if is_healthy else 503
            
            return jsonify({
                'status': 'healthy' if is_healthy else 'unhealthy',
                'metrics': self.metrics
            }), status_code
        
        @self.app.route('/metrics', methods=['GET'])
        def metrics():
            return jsonify(self.metrics)
    
    def _update_metrics(self, success: bool, latency: float = 0):
        """更新指标"""
        self.metrics['total_requests'] += 1
        
        if success:
            self.metrics['successful_requests'] += 1
            # 更新平均延迟
            current_avg = self.metrics['avg_latency_ms']
            total_success = self.metrics['successful_requests']
            self.metrics['avg_latency_ms'] = (
                (current_avg * (total_success - 1) + latency) / total_success
            )
        else:
            self.metrics['failed_requests'] += 1
    
    def start_server(self, host: str = '0.0.0.0', port: int = 5000):
        """启动服务器"""
        print(f"🚀 启动模型服务器: http://{host}:{port}")
        self.app.run(host=host, port=port, threaded=True)

class BatchInferenceEngine:
    """批量推理引擎"""
    
    def __init__(self, deployment_strategy: ModelDeploymentStrategy, 
                 batch_size: int = 32):
        self.deployment = deployment_strategy
        self.batch_size = batch_size
        self.input_queue = queue.Queue()
        self.output_queue = queue.Queue()
        self.is_running = False
        self.worker_thread = None
    
    def start(self):
        """启动批量推理"""
        print(f"🚀 启动批量推理引擎 (批次大小: {self.batch_size})")
        
        self.is_running = True
        self.worker_thread = threading.Thread(target=self._batch_worker)
        self.worker_thread.start()
    
    def stop(self):
        """停止批量推理"""
        print("🛑 停止批量推理引擎")
        
        self.is_running = False
        if self.worker_thread:
            self.worker_thread.join()
    
    def submit_request(self, request_id: str, input_data: np.ndarray) -> str:
        """提交推理请求"""
        self.input_queue.put((request_id, input_data))
        return request_id
    
    def get_result(self, timeout: float = 10.0) -> Optional[Tuple[str, np.ndarray]]:
        """获取推理结果"""
        try:
            return self.output_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def _batch_worker(self):
        """批量处理工作线程"""
        while self.is_running:
            batch_requests = []
            batch_data = []
            
            # 收集批次数据
            try:
                # 等待第一个请求
                request_id, input_data = self.input_queue.get(timeout=1.0)
                batch_requests.append(request_id)
                batch_data.append(input_data)
                
                # 收集更多请求直到达到批次大小或超时
                start_time = time.time()
                while (len(batch_requests) < self.batch_size and 
                       time.time() - start_time < 0.1):  # 100ms超时
                    try:
                        request_id, input_data = self.input_queue.get(timeout=0.01)
                        batch_requests.append(request_id)
                        batch_data.append(input_data)
                    except queue.Empty:
                        break
                
            except queue.Empty:
                continue
            
            # 执行批量推理
            try:
                if batch_data:
                    batch_input = np.vstack(batch_data)
                    batch_output = self.deployment.predict(batch_input)
                    
                    # 分发结果
                    for i, request_id in enumerate(batch_requests):
                        self.output_queue.put((request_id, batch_output[i]))
                        
            except Exception as e:
                print(f"批量推理错误: {e}")
                # 为所有请求返回错误
                for request_id in batch_requests:
                    self.output_queue.put((request_id, None))

class AutoScaler:
    """自动扩缩容器"""
    
    def __init__(self, min_instances: int = 1, max_instances: int = 10):
        self.min_instances = min_instances
        self.max_instances = max_instances
        self.current_instances = min_instances
        self.metrics_history = []
        self.scaling_cooldown = 60  # 60秒冷却时间
        self.last_scaling_time = 0
    
    def should_scale_up(self, current_metrics: Dict[str, float]) -> bool:
        """判断是否需要扩容"""
        # 基于QPS和延迟决定扩容
        high_qps = current_metrics.get('current_qps', 0) > 100
        high_latency = current_metrics.get('avg_latency_ms', 0) > 200
        
        return (high_qps or high_latency) and self._can_scale()
    
    def should_scale_down(self, current_metrics: Dict[str, float]) -> bool:
        """判断是否需要缩容"""
        # 基于QPS和延迟决定缩容
        low_qps = current_metrics.get('current_qps', 0) < 20
        low_latency = current_metrics.get('avg_latency_ms', 0) < 50
        
        return (low_qps and low_latency) and self._can_scale()
    
    def _can_scale(self) -> bool:
        """检查是否可以扩缩容"""
        return time.time() - self.last_scaling_time > self.scaling_cooldown
    
    def scale_up(self) -> int:
        """扩容"""
        if self.current_instances < self.max_instances:
            self.current_instances += 1
            self.last_scaling_time = time.time()
            print(f"📈 扩容到 {self.current_instances} 个实例")
        
        return self.current_instances
    
    def scale_down(self) -> int:
        """缩容"""
        if self.current_instances > self.min_instances:
            self.current_instances -= 1
            self.last_scaling_time = time.time()
            print(f"📉 缩容到 {self.current_instances} 个实例")
        
        return self.current_instances
```

### 3. 模型监控与运维

```python
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, asdict
from collections import deque, defaultdict
import threading
import time
from sklearn.metrics import accuracy_score, precision_score, recall_score
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from abc import ABC, abstractmethod

@dataclass
class ModelMetrics:
    """模型指标"""
    timestamp: datetime
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    latency_ms: float
    throughput_rps: float
    error_rate: float
    memory_usage_mb: float
    cpu_usage_percent: float
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['timestamp'] = self.timestamp.isoformat()
        return result

@dataclass
class DataDriftMetrics:
    """数据漂移指标"""
    feature_name: str
    drift_score: float
    p_value: float
    is_drifted: bool
    drift_type: str  # 'mean', 'variance', 'distribution'
    baseline_stats: Dict[str, float]
    current_stats: Dict[str, float]

class DriftDetector(ABC):
    """数据漂移检测器基类"""
    
    @abstractmethod
    def fit(self, baseline_data: np.ndarray):
        """拟合基线数据"""
        pass
    
    @abstractmethod
    def detect(self, current_data: np.ndarray) -> DataDriftMetrics:
        """检测漂移"""
        pass

class KSTestDriftDetector(DriftDetector):
    """Kolmogorov-Smirnov检验漂移检测器"""
    
    def __init__(self, feature_name: str, significance_level: float = 0.05):
        self.feature_name = feature_name
        self.significance_level = significance_level
        self.baseline_data = None
        self.baseline_stats = {}
    
    def fit(self, baseline_data: np.ndarray):
        """拟合基线数据"""
        self.baseline_data = baseline_data.copy()
        self.baseline_stats = {
            'mean': np.mean(baseline_data),
            'std': np.std(baseline_data),
            'min': np.min(baseline_data),
            'max': np.max(baseline_data),
            'median': np.median(baseline_data)
        }
    
    def detect(self, current_data: np.ndarray) -> DataDriftMetrics:
        """使用KS检验检测分布漂移"""
        if self.baseline_data is None:
            raise ValueError("检测器未拟合基线数据")
        
        # 执行KS检验
        ks_statistic, p_value = stats.ks_2samp(self.baseline_data, current_data)
        
        # 判断是否漂移
        is_drifted = p_value < self.significance_level
        
        # 计算当前统计信息
        current_stats = {
            'mean': np.mean(current_data),
            'std': np.std(current_data),
            'min': np.min(current_data),
            'max': np.max(current_data),
            'median': np.median(current_data)
        }
        
        return DataDriftMetrics(
            feature_name=self.feature_name,
            drift_score=ks_statistic,
            p_value=p_value,
            is_drifted=is_drifted,
            drift_type='distribution',
            baseline_stats=self.baseline_stats,
            current_stats=current_stats
        )

class PSIDriftDetector(DriftDetector):
    """Population Stability Index (PSI) 漂移检测器"""
    
    def __init__(self, feature_name: str, n_bins: int = 10, psi_threshold: float = 0.2):
        self.feature_name = feature_name
        self.n_bins = n_bins
        self.psi_threshold = psi_threshold
        self.bin_edges = None
        self.baseline_proportions = None
    
    def fit(self, baseline_data: np.ndarray):
        """拟合基线数据"""
        # 创建分箱
        self.bin_edges = np.percentile(baseline_data, 
                                     np.linspace(0, 100, self.n_bins + 1))
        
        # 计算基线比例
        baseline_counts, _ = np.histogram(baseline_data, bins=self.bin_edges)
        self.baseline_proportions = baseline_counts / len(baseline_data)
        
        # 避免零比例
        self.baseline_proportions = np.maximum(self.baseline_proportions, 1e-6)
    
    def detect(self, current_data: np.ndarray) -> DataDriftMetrics:
        """使用PSI检测漂移"""
        if self.bin_edges is None:
            raise ValueError("检测器未拟合基线数据")
        
        # 计算当前比例
        current_counts, _ = np.histogram(current_data, bins=self.bin_edges)
        current_proportions = current_counts / len(current_data)
        current_proportions = np.maximum(current_proportions, 1e-6)
        
        # 计算PSI
        psi_values = (current_proportions - self.baseline_proportions) * \
                    np.log(current_proportions / self.baseline_proportions)
        psi_score = np.sum(psi_values)
        
        # 判断是否漂移
        is_drifted = psi_score > self.psi_threshold
        
        return DataDriftMetrics(
            feature_name=self.feature_name,
            drift_score=psi_score,
            p_value=0.0,  # PSI不提供p值
            is_drifted=is_drifted,
            drift_type='distribution',
            baseline_stats={'psi_threshold': self.psi_threshold},
            current_stats={'psi_score': psi_score}
        )

class ModelMonitor:
    """模型监控器"""
    
    def __init__(self, model_name: str, monitoring_window: int = 1000):
        self.model_name = model_name
        self.monitoring_window = monitoring_window
        
        # 指标存储
        self.metrics_history = deque(maxlen=monitoring_window)
        self.prediction_history = deque(maxlen=monitoring_window)
        self.feature_history = defaultdict(lambda: deque(maxlen=monitoring_window))
        
        # 漂移检测器
        self.drift_detectors = {}
        self.drift_history = []
        
        # 告警配置
        self.alert_thresholds = {
            'accuracy_drop': 0.05,  # 准确率下降5%
            'latency_increase': 2.0,  # 延迟增加2倍
            'error_rate': 0.1,  # 错误率10%
            'drift_detection': True  # 启用漂移检测告警
        }
        
        # 基线指标
        self.baseline_metrics = None
        
        # 监控线程
        self.monitoring_thread = None
        self.is_monitoring = False
    
    def set_baseline(self, baseline_data: Dict[str, np.ndarray], 
                    baseline_metrics: ModelMetrics):
        """设置基线数据和指标"""
        print(f"📊 设置模型 {self.model_name} 的基线")
        
        self.baseline_metrics = baseline_metrics
        
        # 为每个特征创建漂移检测器
        for feature_name, feature_data in baseline_data.items():
            # 使用KS检验检测器
            ks_detector = KSTestDriftDetector(feature_name)
            ks_detector.fit(feature_data)
            
            # 使用PSI检测器
            psi_detector = PSIDriftDetector(feature_name)
            psi_detector.fit(feature_data)
            
            self.drift_detectors[f"{feature_name}_ks"] = ks_detector
            self.drift_detectors[f"{feature_name}_psi"] = psi_detector
        
        print(f"   已为 {len(baseline_data)} 个特征设置漂移检测器")
    
    def log_prediction(self, features: Dict[str, float], prediction: Any, 
                      actual: Any = None, latency_ms: float = 0, 
                      error: Optional[str] = None):
        """记录预测"""
        timestamp = datetime.now()
        
        # 记录预测历史
        prediction_record = {
            'timestamp': timestamp,
            'features': features,
            'prediction': prediction,
            'actual': actual,
            'latency_ms': latency_ms,
            'error': error
        }
        self.prediction_history.append(prediction_record)
        
        # 记录特征历史
        for feature_name, feature_value in features.items():
            self.feature_history[feature_name].append(feature_value)
    
    def log_metrics(self, metrics: ModelMetrics):
        """记录指标"""
        self.metrics_history.append(metrics)
        
        # 检查告警
        self._check_alerts(metrics)
    
    def detect_drift(self) -> List[DataDriftMetrics]:
        """检测数据漂移"""
        if not self.drift_detectors:
            return []
        
        drift_results = []
        
        for detector_name, detector in self.drift_detectors.items():
            feature_name = detector.feature_name
            
            if feature_name in self.feature_history:
                current_data = np.array(list(self.feature_history[feature_name]))
                
                if len(current_data) >= 100:  # 至少100个样本
                    try:
                        drift_metrics = detector.detect(current_data)
                        drift_results.append(drift_metrics)
                        
                        if drift_metrics.is_drifted:
                            print(f"⚠️ 检测到特征 {feature_name} 漂移 "
                                  f"(检测器: {detector_name.split('_')[-1]}, "
                                  f"分数: {drift_metrics.drift_score:.4f})")
                    
                    except Exception as e:
                        print(f"漂移检测错误 {detector_name}: {e}")
        
        # 记录漂移历史
        if drift_results:
            self.drift_history.append({
                'timestamp': datetime.now(),
                'drift_results': drift_results
            })
        
        return drift_results
    
    def _check_alerts(self, current_metrics: ModelMetrics):
        """检查告警条件"""
        if self.baseline_metrics is None:
            return
        
        alerts = []
        
        # 检查准确率下降
        accuracy_drop = self.baseline_metrics.accuracy - current_metrics.accuracy
        if accuracy_drop > self.alert_thresholds['accuracy_drop']:
            alerts.append(f"准确率下降 {accuracy_drop:.3f} "
                         f"({self.baseline_metrics.accuracy:.3f} -> {current_metrics.accuracy:.3f})")
        
        # 检查延迟增加
        latency_ratio = current_metrics.latency_ms / self.baseline_metrics.latency_ms
        if latency_ratio > self.alert_thresholds['latency_increase']:
            alerts.append(f"延迟增加 {latency_ratio:.2f}x "
                         f"({self.baseline_metrics.latency_ms:.1f}ms -> {current_metrics.latency_ms:.1f}ms)")
        
        # 检查错误率
        if current_metrics.error_rate > self.alert_thresholds['error_rate']:
            alerts.append(f"错误率过高: {current_metrics.error_rate:.3f}")
        
        # 发送告警
        if alerts:
            self._send_alerts(alerts)
    
    def _send_alerts(self, alerts: List[str]):
        """发送告警"""
        print(f"🚨 模型 {self.model_name} 告警:")
        for alert in alerts:
            print(f"   - {alert}")
    
    def start_monitoring(self, check_interval: int = 60):
        """启动监控"""
        print(f"🔍 启动模型 {self.model_name} 监控 (检查间隔: {check_interval}s)")
        
        self.is_monitoring = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(check_interval,)
        )
        self.monitoring_thread.start()
    
    def stop_monitoring(self):
        """停止监控"""
        print(f"🛑 停止模型 {self.model_name} 监控")
        
        self.is_monitoring = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
    
    def _monitoring_loop(self, check_interval: int):
        """监控循环"""
        while self.is_monitoring:
            try:
                # 检测漂移
                if self.alert_thresholds['drift_detection']:
                    drift_results = self.detect_drift()
                    
                    # 发送漂移告警
                    drifted_features = [dr.feature_name for dr in drift_results if dr.is_drifted]
                    if drifted_features:
                        self._send_alerts([f"检测到特征漂移: {', '.join(drifted_features)}"])
                
                time.sleep(check_interval)
                
            except Exception as e:
                print(f"监控循环错误: {e}")
                time.sleep(check_interval)
    
    def generate_report(self) -> Dict[str, Any]:
        """生成监控报告"""
        if not self.metrics_history:
            return {"error": "无监控数据"}
        
        # 计算统计信息
        recent_metrics = list(self.metrics_history)[-100:]  # 最近100条记录
        
        accuracy_values = [m.accuracy for m in recent_metrics]
        latency_values = [m.latency_ms for m in recent_metrics]
        error_rates = [m.error_rate for m in recent_metrics]
        
        report = {
            "model_name": self.model_name,
            "report_time": datetime.now().isoformat(),
            "monitoring_period": {
                "start": recent_metrics[0].timestamp.isoformat(),
                "end": recent_metrics[-1].timestamp.isoformat(),
                "total_predictions": len(recent_metrics)
            },
            "performance_summary": {
                "accuracy": {
                    "mean": np.mean(accuracy_values),
                    "std": np.std(accuracy_values),
                    "min": np.min(accuracy_values),
                    "max": np.max(accuracy_values)
                },
                "latency_ms": {
                    "mean": np.mean(latency_values),
                    "std": np.std(latency_values),
                    "p95": np.percentile(latency_values, 95),
                    "p99": np.percentile(latency_values, 99)
                },
                "error_rate": {
                    "mean": np.mean(error_rates),
                    "max": np.max(error_rates)
                }
            },
            "drift_summary": {
                "total_drift_checks": len(self.drift_history),
                "features_with_drift": len(set(
                    dr.feature_name for drift_check in self.drift_history 
                    for dr in drift_check['drift_results'] if dr.is_drifted
                ))
            }
        }
        
        return report
    
    def visualize_metrics(self, save_path: Optional[str] = None):
        """可视化监控指标"""
        if not self.metrics_history:
            print("无监控数据可视化")
            return
        
        # 准备数据
        timestamps = [m.timestamp for m in self.metrics_history]
        accuracies = [m.accuracy for m in self.metrics_history]
        latencies = [m.latency_ms for m in self.metrics_history]
        error_rates = [m.error_rate for m in self.metrics_history]
        
        # 创建图表
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'模型监控报告 - {self.model_name}', fontsize=16)
        
        # 准确率趋势
        axes[0, 0].plot(timestamps, accuracies, 'b-', linewidth=2)
        axes[0, 0].set_title('准确率趋势')
        axes[0, 0].set_ylabel('准确率')
        axes[0, 0].grid(True, alpha=0.3)
        
        if self.baseline_metrics:
            axes[0, 0].axhline(y=self.baseline_metrics.accuracy, 
                              color='r', linestyle='--', label='基线')
            axes[0, 0].legend()
        
        # 延迟趋势
        axes[0, 1].plot(timestamps, latencies, 'g-', linewidth=2)
        axes[0, 1].set_title('延迟趋势')
        axes[0, 1].set_ylabel('延迟 (ms)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 错误率趋势
        axes[1, 0].plot(timestamps, error_rates, 'r-', linewidth=2)
        axes[1, 0].set_title('错误率趋势')
        axes[1, 0].set_ylabel('错误率')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 延迟分布
        axes[1, 1].hist(latencies, bins=30, alpha=0.7, color='orange')
        axes[1, 1].set_title('延迟分布')
        axes[1, 1].set_xlabel('延迟 (ms)')
        axes[1, 1].set_ylabel('频次')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"监控图表已保存: {save_path}")
        else:
            plt.show()

class ModelGovernance:
    """模型治理"""
    
    def __init__(self):
        self.models = {}  # 模型注册表
        self.monitors = {}  # 监控器
        self.policies = {}  # 治理策略
        self.audit_log = []  # 审计日志
    
    def register_model(self, model_id: str, model_info: Dict[str, Any]):
        """注册模型"""
        print(f"📝 注册模型: {model_id}")
        
        self.models[model_id] = {
            **model_info,
            'registered_at': datetime.now(),
            'status': 'registered'
        }
        
        self._log_audit('model_registered', model_id, model_info)
    
    def set_governance_policy(self, policy_name: str, policy_config: Dict[str, Any]):
        """设置治理策略"""
        print(f"📋 设置治理策略: {policy_name}")
        
        self.policies[policy_name] = policy_config
        self._log_audit('policy_set', policy_name, policy_config)
    
    def evaluate_model_compliance(self, model_id: str) -> Dict[str, Any]:
        """评估模型合规性"""
        if model_id not in self.models:
            return {'error': f'模型 {model_id} 未注册'}
        
        model_info = self.models[model_id]
        compliance_results = {}
        
        # 检查各项策略
        for policy_name, policy_config in self.policies.items():
            compliance_results[policy_name] = self._check_policy_compliance(
                model_info, policy_config
            )
        
        # 计算总体合规分数
        compliance_scores = [r['score'] for r in compliance_results.values() if 'score' in r]
        overall_score = np.mean(compliance_scores) if compliance_scores else 0
        
        result = {
            'model_id': model_id,
            'overall_compliance_score': overall_score,
            'policy_results': compliance_results,
            'is_compliant': overall_score >= 0.8,
            'evaluation_time': datetime.now().isoformat()
        }
        
        self._log_audit('compliance_evaluated', model_id, result)
        return result
    
    def _check_policy_compliance(self, model_info: Dict[str, Any], 
                                policy_config: Dict[str, Any]) -> Dict[str, Any]:
        """检查策略合规性"""
        policy_type = policy_config.get('type')
        
        if policy_type == 'performance_threshold':
            return self._check_performance_policy(model_info, policy_config)
        elif policy_type == 'data_quality':
            return self._check_data_quality_policy(model_info, policy_config)
        elif policy_type == 'model_documentation':
            return self._check_documentation_policy(model_info, policy_config)
        else:
            return {'error': f'未知策略类型: {policy_type}'}
    
    def _check_performance_policy(self, model_info: Dict[str, Any], 
                                 policy_config: Dict[str, Any]) -> Dict[str, Any]:
        """检查性能策略"""
        required_accuracy = policy_config.get('min_accuracy', 0.8)
        max_latency = policy_config.get('max_latency_ms', 100)
        
        model_accuracy = model_info.get('accuracy', 0)
        model_latency = model_info.get('latency_ms', float('inf'))
        
        accuracy_ok = model_accuracy >= required_accuracy
        latency_ok = model_latency <= max_latency
        
        score = (int(accuracy_ok) + int(latency_ok)) / 2
        
        return {
            'type': 'performance_threshold',
            'score': score,
            'details': {
                'accuracy_check': {
                    'required': required_accuracy,
                    'actual': model_accuracy,
                    'passed': accuracy_ok
                },
                'latency_check': {
                    'required': max_latency,
                    'actual': model_latency,
                    'passed': latency_ok
                }
            }
        }
    
    def _check_data_quality_policy(self, model_info: Dict[str, Any], 
                                  policy_config: Dict[str, Any]) -> Dict[str, Any]:
        """检查数据质量策略"""
        required_checks = policy_config.get('required_checks', [])
        
        data_quality = model_info.get('data_quality', {})
        passed_checks = 0
        total_checks = len(required_checks)
        
        check_results = {}
        for check in required_checks:
            check_passed = data_quality.get(check, False)
            check_results[check] = check_passed
            if check_passed:
                passed_checks += 1
        
        score = passed_checks / total_checks if total_checks > 0 else 1.0
        
        return {
            'type': 'data_quality',
            'score': score,
            'details': {
                'total_checks': total_checks,
                'passed_checks': passed_checks,
                'check_results': check_results
            }
        }
    
    def _check_documentation_policy(self, model_info: Dict[str, Any], 
                                   policy_config: Dict[str, Any]) -> Dict[str, Any]:
        """检查文档策略"""
        required_docs = policy_config.get('required_documents', [])
        
        documentation = model_info.get('documentation', {})
        available_docs = 0
        total_docs = len(required_docs)
        
        doc_results = {}
        for doc in required_docs:
            doc_available = doc in documentation and documentation[doc] is not None
            doc_results[doc] = doc_available
            if doc_available:
                available_docs += 1
        
        score = available_docs / total_docs if total_docs > 0 else 1.0
        
        return {
            'type': 'model_documentation',
            'score': score,
            'details': {
                'total_documents': total_docs,
                'available_documents': available_docs,
                'document_results': doc_results
            }
        }
    
    def _log_audit(self, action: str, target: str, details: Any):
        """记录审计日志"""
        audit_entry = {
            'timestamp': datetime.now(),
            'action': action,
            'target': target,
            'details': details
        }
        self.audit_log.append(audit_entry)
    
    def get_audit_log(self, start_time: Optional[datetime] = None, 
                     end_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """获取审计日志"""
        filtered_log = self.audit_log
        
        if start_time:
            filtered_log = [entry for entry in filtered_log 
                          if entry['timestamp'] >= start_time]
        
        if end_time:
            filtered_log = [entry for entry in filtered_log 
                          if entry['timestamp'] <= end_time]
        
        return [{
            'timestamp': entry['timestamp'].isoformat(),
            'action': entry['action'],
            'target': entry['target'],
            'details': entry['details']
        } for entry in filtered_log]

# 演示代码
if __name__ == "__main__":
    print("🚀 AI工程化最佳实践演示")
    print("=" * 50)
    
    # 1. MLOps流程演示
    print("\n📋 1. MLOps完整流程演示")
    
    # 创建配置
    config = ModelConfig(
        model_name="demo_classifier",
        model_version="1.0.0",
        architecture="simple_nn",
        hyperparameters={"lr": 0.001, "batch_size": 32},
        training_config={"epochs": 10, "validation_split": 0.2},
        deployment_config={"framework": "pytorch", "device": "cpu"}
    )
    
    # 创建MLOps组件
    logger = MLOpsLogger("demo_experiment")
    data_vc = DataVersionControl()
    model_registry = ModelRegistry()
    ci = ContinuousIntegration(logger, data_vc, model_registry)
    
    print("   MLOps组件初始化完成")
    
    # 2. 模型部署演示
    print("\n🚀 2. 模型部署策略演示")
    
    # 部署配置
    deploy_config = DeploymentConfig(
        deployment_type="online",
        serving_framework="flask",
        model_format="pytorch",
        batch_size=32,
        max_latency_ms=100
    )
    
    print(f"   部署配置: {deploy_config.deployment_type} 服务")
    print(f"   服务框架: {deploy_config.serving_framework}")
    print(f"   模型格式: {deploy_config.model_format}")
    
    # 3. 模型监控演示
    print("\n🔍 3. 模型监控演示")
    
    monitor = ModelMonitor("demo_classifier")
    
    # 模拟基线数据
    baseline_data = {
        "feature_1": np.random.normal(0, 1, 1000),
        "feature_2": np.random.normal(5, 2, 1000)
    }
    
    baseline_metrics = ModelMetrics(
        timestamp=datetime.now(),
        accuracy=0.85,
        precision=0.83,
        recall=0.87,
        f1_score=0.85,
        latency_ms=45.0,
        throughput_rps=100.0,
        error_rate=0.02,
        memory_usage_mb=256.0,
        cpu_usage_percent=15.0
    )
    
    monitor.set_baseline(baseline_data, baseline_metrics)
    
    # 模拟一些预测和指标
    for i in range(10):
        # 模拟特征漂移
        drift_factor = 0.1 * i
        features = {
            "feature_1": np.random.normal(drift_factor, 1),
            "feature_2": np.random.normal(5 + drift_factor, 2)
        }
        
        monitor.log_prediction(
            features=features,
            prediction=np.random.choice([0, 1]),
            actual=np.random.choice([0, 1]),
            latency_ms=45 + np.random.normal(0, 5)
        )
        
        # 模拟性能下降
        current_metrics = ModelMetrics(
            timestamp=datetime.now(),
            accuracy=0.85 - 0.01 * i,
            precision=0.83 - 0.01 * i,
            recall=0.87 - 0.01 * i,
            f1_score=0.85 - 0.01 * i,
            latency_ms=45.0 + 2 * i,
            throughput_rps=100.0 - i,
            error_rate=0.02 + 0.005 * i,
            memory_usage_mb=256.0 + 10 * i,
            cpu_usage_percent=15.0 + i
        )
        
        monitor.log_metrics(current_metrics)
    
    # 检测漂移
    drift_results = monitor.detect_drift()
    print(f"   检测到 {len([dr for dr in drift_results if dr.is_drifted])} 个特征漂移")
    
    # 生成监控报告
    report = monitor.generate_report()
    print(f"   监控报告生成完成，覆盖 {report['monitoring_period']['total_predictions']} 次预测")
    
    # 4. 模型治理演示
    print("\n📋 4. 模型治理演示")
    
    governance = ModelGovernance()
    
    # 设置治理策略
    governance.set_governance_policy("performance_policy", {
        "type": "performance_threshold",
        "min_accuracy": 0.8,
        "max_latency_ms": 100
    })
    
    governance.set_governance_policy("documentation_policy", {
        "type": "model_documentation",
        "required_documents": ["model_card", "training_data", "evaluation_report"]
    })
    
    # 注册模型
    model_info = {
        "accuracy": 0.85,
        "latency_ms": 45,
        "documentation": {
            "model_card": "完整的模型卡片",
            "training_data": "训练数据描述"
            # 缺少 evaluation_report
        },
        "data_quality": {
            "completeness_check": True,
            "consistency_check": True
        }
    }
    
    governance.register_model("demo_classifier_v1", model_info)
    
    # 评估合规性
    compliance_result = governance.evaluate_model_compliance("demo_classifier_v1")
    print(f"   模型合规性评分: {compliance_result['overall_compliance_score']:.2f}")
    print(f"   是否合规: {'是' if compliance_result['is_compliant'] else '否'}")
    
    print("\n✅ AI工程化最佳实践演示完成")

## 🏗️ 大规模系统架构

### 1. 微服务架构设计

在大规模AI系统中，微服务架构能够提供更好的可扩展性、可维护性和容错能力。

**核心组件：**
- **模型服务**：独立的模型推理服务
- **数据服务**：数据预处理和特征工程
- **监控服务**：模型性能和系统监控
- **配置服务**：统一配置管理
- **网关服务**：API网关和负载均衡

### 2. 容器化部署

使用Docker和Kubernetes实现模型的容器化部署：

```dockerfile
# Dockerfile示例
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "model_server.py"]
```

### 3. 负载均衡与自动扩缩容

- **水平扩展**：根据负载自动增减实例
- **垂直扩展**：动态调整资源配置
- **智能路由**：基于模型版本和A/B测试的流量分配

## 🔒 安全与合规

### 1. 数据安全

- **数据加密**：传输和存储加密
- **访问控制**：基于角色的权限管理
- **数据脱敏**：敏感信息保护
- **审计日志**：完整的操作记录

### 2. 模型安全

- **模型加密**：保护模型参数
- **对抗攻击防护**：检测和防御恶意输入
- **模型水印**：知识产权保护
- **安全推理**：隔离执行环境

### 3. 合规要求

- **GDPR合规**：数据隐私保护
- **行业标准**：金融、医疗等行业规范
- **可解释性**：模型决策透明度
- **公平性**：算法偏见检测和缓解

## 📊 小结

### 核心要点

1. **MLOps流程**：建立完整的机器学习运维体系
2. **部署策略**：选择合适的模型部署方案
3. **监控运维**：实时监控模型性能和数据漂移
4. **系统架构**：设计可扩展的大规模AI系统
5. **安全合规**：确保系统安全和法规遵从

### 实践建议

1. **渐进式实施**：从简单场景开始，逐步完善工程化能力
2. **工具选择**：根据团队技术栈选择合适的MLOps工具
3. **标准化**：建立统一的开发、测试、部署标准
4. **自动化**：尽可能自动化重复性工作
5. **持续改进**：基于监控数据持续优化系统

### 发展方向

1. **AIOps**：AI驱动的运维自动化
2. **边缘部署**：模型在边缘设备的部署优化
3. **联邦MLOps**：分布式机器学习运维
4. **绿色AI**：能耗优化和可持续发展
5. **智能运维**：自适应的系统管理和优化