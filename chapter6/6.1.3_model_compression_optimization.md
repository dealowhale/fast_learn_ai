# 6.1.3 模型压缩与优化技术

随着大模型规模的不断增长，模型压缩与优化技术变得越来越重要。本节将深入探讨各种模型压缩技术，帮助你在保持模型性能的同时，显著减少模型大小和推理时间。

## 📚 技术概述

模型压缩与优化是将大型深度学习模型转换为更小、更快、更高效版本的技术集合，主要目标包括：

- **减少模型大小**：降低存储和传输成本
- **加速推理**：提高模型响应速度
- **降低功耗**：适应移动设备和边缘计算
- **保持性能**：在压缩过程中尽量保持原始精度

## 🔧 核心技术方法

### 1. 知识蒸馏（Knowledge Distillation）

知识蒸馏通过让小模型（学生）学习大模型（教师）的知识来实现压缩。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt

class TeacherModel(nn.Module):
    """教师模型（大模型）"""
    
    def __init__(self, input_size: int = 784, hidden_sizes: List[int] = [512, 256, 128], num_classes: int = 10):
        super(TeacherModel, self).__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, num_classes))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.view(x.size(0), -1)  # 展平输入
        return self.network(x)

class StudentModel(nn.Module):
    """学生模型（小模型）"""
    
    def __init__(self, input_size: int = 784, hidden_sizes: List[int] = [64, 32], num_classes: int = 10):
        super(StudentModel, self).__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, num_classes))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.view(x.size(0), -1)
        return self.network(x)

class KnowledgeDistillationTrainer:
    """知识蒸馏训练器"""
    
    def __init__(self, teacher_model: nn.Module, student_model: nn.Module, 
                 temperature: float = 4.0, alpha: float = 0.7, device: str = 'cpu'):
        self.teacher_model = teacher_model.to(device)
        self.student_model = student_model.to(device)
        self.temperature = temperature
        self.alpha = alpha  # 蒸馏损失权重
        self.device = device
        
        # 冻结教师模型
        self.teacher_model.eval()
        for param in self.teacher_model.parameters():
            param.requires_grad = False
    
    def distillation_loss(self, student_logits: torch.Tensor, teacher_logits: torch.Tensor, 
                         labels: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, float]]:
        """计算蒸馏损失"""
        
        # 软目标损失（蒸馏损失）
        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)
        soft_targets_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (self.temperature ** 2)
        
        # 硬目标损失（标准交叉熵）
        hard_targets_loss = F.cross_entropy(student_logits, labels)
        
        # 总损失
        total_loss = self.alpha * soft_targets_loss + (1 - self.alpha) * hard_targets_loss
        
        loss_info = {
            'total_loss': total_loss.item(),
            'soft_loss': soft_targets_loss.item(),
            'hard_loss': hard_targets_loss.item()
        }
        
        return total_loss, loss_info
    
    def train_epoch(self, dataloader: DataLoader, optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """训练一个epoch"""
        self.student_model.train()
        
        total_loss = 0
        total_soft_loss = 0
        total_hard_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            
            # 获取教师和学生的输出
            with torch.no_grad():
                teacher_logits = self.teacher_model(data)
            
            student_logits = self.student_model(data)
            
            # 计算损失
            loss, loss_info = self.distillation_loss(student_logits, teacher_logits, target)
            
            # 反向传播
            loss.backward()
            optimizer.step()
            
            # 统计
            total_loss += loss_info['total_loss']
            total_soft_loss += loss_info['soft_loss']
            total_hard_loss += loss_info['hard_loss']
            
            pred = student_logits.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
        
        return {
            'loss': total_loss / len(dataloader),
            'soft_loss': total_soft_loss / len(dataloader),
            'hard_loss': total_hard_loss / len(dataloader),
            'accuracy': 100. * correct / total
        }
    
    def evaluate(self, dataloader: DataLoader) -> Dict[str, float]:
        """评估模型"""
        self.student_model.eval()
        
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(self.device), target.to(self.device)
                
                teacher_logits = self.teacher_model(data)
                student_logits = self.student_model(data)
                
                loss, _ = self.distillation_loss(student_logits, teacher_logits, target)
                total_loss += loss.item()
                
                pred = student_logits.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        return {
            'loss': total_loss / len(dataloader),
            'accuracy': 100. * correct / total
        }

# 模型大小比较工具
class ModelAnalyzer:
    """模型分析器"""
    
    @staticmethod
    def count_parameters(model: nn.Module) -> Dict[str, int]:
        """统计模型参数"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'non_trainable_parameters': total_params - trainable_params
        }
    
    @staticmethod
    def model_size_mb(model: nn.Module) -> float:
        """计算模型大小（MB）"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_mb = (param_size + buffer_size) / 1024 / 1024
        return size_mb
    
    @staticmethod
    def compare_models(teacher: nn.Module, student: nn.Module) -> Dict[str, any]:
        """比较教师和学生模型"""
        teacher_params = ModelAnalyzer.count_parameters(teacher)
        student_params = ModelAnalyzer.count_parameters(student)
        
        teacher_size = ModelAnalyzer.model_size_mb(teacher)
        student_size = ModelAnalyzer.model_size_mb(student)
        
        compression_ratio = teacher_params['total_parameters'] / student_params['total_parameters']
        size_reduction = (teacher_size - student_size) / teacher_size * 100
        
        return {
            'teacher': {
                'parameters': teacher_params['total_parameters'],
                'size_mb': teacher_size
            },
            'student': {
                'parameters': student_params['total_parameters'],
                'size_mb': student_size
            },
            'compression_ratio': compression_ratio,
            'size_reduction_percent': size_reduction
        }
```

### 2. 模型剪枝（Model Pruning）

模型剪枝通过移除不重要的权重或神经元来减少模型复杂度。

```python
import torch.nn.utils.prune as prune
from collections import OrderedDict

class ModelPruner:
    """模型剪枝器"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.original_state = None
    
    def magnitude_pruning(self, pruning_ratio: float = 0.2) -> Dict[str, float]:
        """基于权重幅度的剪枝"""
        print(f"🔪 执行幅度剪枝，剪枝比例: {pruning_ratio:.1%}")
        
        # 保存原始状态
        self.original_state = self.model.state_dict().copy()
        
        # 收集所有线性层
        modules_to_prune = []
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                modules_to_prune.append((module, 'weight'))
        
        # 执行全局幅度剪枝
        prune.global_unstructured(
            modules_to_prune,
            pruning_method=prune.L1Unstructured,
            amount=pruning_ratio,
        )
        
        # 统计剪枝效果
        stats = self._calculate_pruning_stats()
        
        print(f"   剪枝完成: {stats['pruned_params']:,} / {stats['total_params']:,} 参数被剪枝")
        print(f"   剪枝比例: {stats['pruning_ratio']:.1%}")
        
        return stats
    
    def structured_pruning(self, pruning_ratio: float = 0.3) -> Dict[str, float]:
        """结构化剪枝（移除整个神经元）"""
        print(f"🏗️ 执行结构化剪枝，剪枝比例: {pruning_ratio:.1%}")
        
        self.original_state = self.model.state_dict().copy()
        
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear) and hasattr(module, 'weight'):
                # 计算每个神经元的重要性（L2范数）
                weight = module.weight.data
                neuron_importance = torch.norm(weight, dim=1)
                
                # 确定要剪枝的神经元数量
                num_neurons = weight.size(0)
                num_to_prune = int(num_neurons * pruning_ratio)
                
                if num_to_prune > 0:
                    # 找到重要性最低的神经元
                    _, indices_to_prune = torch.topk(neuron_importance, num_to_prune, largest=False)
                    
                    # 创建掩码
                    mask = torch.ones_like(neuron_importance, dtype=torch.bool)
                    mask[indices_to_prune] = False
                    
                    # 应用结构化剪枝
                    prune.custom_from_mask(module, 'weight', mask.unsqueeze(1).expand_as(weight))
        
        stats = self._calculate_pruning_stats()
        print(f"   结构化剪枝完成: {stats['pruning_ratio']:.1%} 的参数被移除")
        
        return stats
    
    def gradual_pruning(self, target_ratio: float = 0.5, num_steps: int = 5) -> List[Dict[str, float]]:
        """渐进式剪枝"""
        print(f"📈 执行渐进式剪枝，目标比例: {target_ratio:.1%}，步骤数: {num_steps}")
        
        self.original_state = self.model.state_dict().copy()
        
        # 计算每步的剪枝比例
        step_ratio = target_ratio / num_steps
        current_ratio = 0
        
        stats_history = []
        
        for step in range(num_steps):
            current_ratio += step_ratio
            print(f"   步骤 {step + 1}/{num_steps}: 累计剪枝比例 {current_ratio:.1%}")
            
            # 执行当前步骤的剪枝
            step_stats = self.magnitude_pruning(step_ratio)
            step_stats['step'] = step + 1
            step_stats['cumulative_ratio'] = current_ratio
            
            stats_history.append(step_stats)
        
        return stats_history
    
    def _calculate_pruning_stats(self) -> Dict[str, float]:
        """计算剪枝统计信息"""
        total_params = 0
        pruned_params = 0
        
        for name, module in self.model.named_modules():
            if hasattr(module, 'weight_mask'):
                mask = module.weight_mask
                total_params += mask.numel()
                pruned_params += (mask == 0).sum().item()
        
        pruning_ratio = pruned_params / total_params if total_params > 0 else 0
        
        return {
            'total_params': total_params,
            'pruned_params': pruned_params,
            'remaining_params': total_params - pruned_params,
            'pruning_ratio': pruning_ratio
        }
    
    def make_permanent(self):
        """使剪枝永久化"""
        print("🔒 使剪枝永久化...")
        
        for name, module in self.model.named_modules():
            if hasattr(module, 'weight_mask'):
                prune.remove(module, 'weight')
        
        print("   剪枝已永久化")
    
    def restore_original(self):
        """恢复原始模型"""
        if self.original_state is not None:
            print("🔄 恢复原始模型状态...")
            
            # 移除剪枝掩码
            for name, module in self.model.named_modules():
                if hasattr(module, 'weight_mask'):
                    prune.remove(module, 'weight')
            
            # 恢复原始权重
            self.model.load_state_dict(self.original_state)
            print("   模型已恢复到原始状态")
        else:
            print("⚠️ 没有保存的原始状态")
```

### 3. 量化技术（Quantization）

量化通过降低数值精度来减少模型大小和计算复杂度。

```python
import torch.quantization as quant
from torch.quantization import QuantStub, DeQuantStub

class QuantizableModel(nn.Module):
    """可量化的模型"""
    
    def __init__(self, original_model: nn.Module):
        super(QuantizableModel, self).__init__()
        
        # 量化和反量化存根
        self.quant = QuantStub()
        self.dequant = DeQuantStub()
        
        # 原始模型
        self.model = original_model
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.quant(x)
        x = self.model(x)
        x = self.dequant(x)
        return x

class ModelQuantizer:
    """模型量化器"""
    
    def __init__(self, model: nn.Module, device: str = 'cpu'):
        self.original_model = model.to(device)
        self.device = device
    
    def post_training_quantization(self, calibration_loader: DataLoader) -> nn.Module:
        """训练后量化（PTQ）"""
        print("🔢 执行训练后量化...")
        
        # 创建可量化模型
        quantizable_model = QuantizableModel(self.original_model)
        quantizable_model.eval()
        
        # 设置量化配置
        quantizable_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # 准备量化
        prepared_model = torch.quantization.prepare(quantizable_model)
        
        # 校准（使用少量数据）
        print("   正在校准量化参数...")
        with torch.no_grad():
            for i, (data, _) in enumerate(calibration_loader):
                if i >= 100:  # 只使用100个批次进行校准
                    break
                data = data.to(self.device)
                prepared_model(data)
        
        # 转换为量化模型
        quantized_model = torch.quantization.convert(prepared_model)
        
        print("   训练后量化完成")
        return quantized_model
    
    def quantization_aware_training(self, train_loader: DataLoader, 
                                  num_epochs: int = 5) -> nn.Module:
        """量化感知训练（QAT）"""
        print(f"🎯 执行量化感知训练，训练轮数: {num_epochs}")
        
        # 创建可量化模型
        quantizable_model = QuantizableModel(self.original_model)
        
        # 设置量化配置
        quantizable_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        
        # 准备QAT
        prepared_model = torch.quantization.prepare_qat(quantizable_model)
        prepared_model.train()
        
        # 训练
        optimizer = torch.optim.Adam(prepared_model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(num_epochs):
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = prepared_model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
                
                if batch_idx % 100 == 0:
                    print(f"   Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, "
                          f"Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%")
        
        # 转换为量化模型
        prepared_model.eval()
        quantized_model = torch.quantization.convert(prepared_model)
        
        print("   量化感知训练完成")
        return quantized_model
    
    def dynamic_quantization(self) -> nn.Module:
        """动态量化"""
        print("⚡ 执行动态量化...")
        
        # 动态量化主要针对线性层
        quantized_model = torch.quantization.quantize_dynamic(
            self.original_model,
            {nn.Linear},
            dtype=torch.qint8
        )
        
        print("   动态量化完成")
        return quantized_model
    
    @staticmethod
    def compare_quantized_models(original: nn.Module, quantized: nn.Module, 
                               test_loader: DataLoader, device: str = 'cpu') -> Dict[str, any]:
        """比较量化前后的模型"""
        print("📊 比较量化效果...")
        
        # 模型大小比较
        original_size = ModelAnalyzer.model_size_mb(original)
        quantized_size = ModelAnalyzer.model_size_mb(quantized)
        
        # 精度比较
        def evaluate_model(model, loader):
            model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, target in loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    pred = output.argmax(dim=1, keepdim=True)
                    correct += pred.eq(target.view_as(pred)).sum().item()
                    total += target.size(0)
            
            return 100. * correct / total
        
        original_acc = evaluate_model(original, test_loader)
        quantized_acc = evaluate_model(quantized, test_loader)
        
        # 推理速度比较
        def measure_inference_time(model, sample_input, num_runs=100):
            model.eval()
            
            # 预热
            for _ in range(10):
                with torch.no_grad():
                    _ = model(sample_input)
            
            # 测量时间
            import time
            start_time = time.time()
            
            for _ in range(num_runs):
                with torch.no_grad():
                    _ = model(sample_input)
            
            end_time = time.time()
            return (end_time - start_time) / num_runs
        
        # 获取样本输入
        sample_input = next(iter(test_loader))[0][:1].to(device)
        
        original_time = measure_inference_time(original, sample_input)
        quantized_time = measure_inference_time(quantized, sample_input)
        
        results = {
            'size_comparison': {
                'original_mb': original_size,
                'quantized_mb': quantized_size,
                'compression_ratio': original_size / quantized_size,
                'size_reduction_percent': (original_size - quantized_size) / original_size * 100
            },
            'accuracy_comparison': {
                'original_accuracy': original_acc,
                'quantized_accuracy': quantized_acc,
                'accuracy_drop': original_acc - quantized_acc
            },
            'speed_comparison': {
                'original_time_ms': original_time * 1000,
                'quantized_time_ms': quantized_time * 1000,
                'speedup': original_time / quantized_time
            }
        }
        
        print(f"   模型大小: {original_size:.2f}MB -> {quantized_size:.2f}MB "
              f"(压缩 {results['size_comparison']['size_reduction_percent']:.1f}%)")
        print(f"   准确率: {original_acc:.2f}% -> {quantized_acc:.2f}% "
              f"(下降 {results['accuracy_comparison']['accuracy_drop']:.2f}%)")
        print(f"   推理速度: {original_time*1000:.2f}ms -> {quantized_time*1000:.2f}ms "
              f"(加速 {results['speed_comparison']['speedup']:.2f}x)")
        
        return results
```

### 4. 低秩分解（Low-Rank Decomposition）

通过矩阵分解技术减少参数数量。

```python
class LowRankDecomposer:
    """低秩分解器"""
    
    def __init__(self, model: nn.Module):
        self.model = model
    
    def svd_decomposition(self, layer: nn.Linear, rank_ratio: float = 0.5) -> nn.Module:
        """SVD分解线性层"""
        print(f"🔍 对线性层执行SVD分解，秩比例: {rank_ratio:.1%}")
        
        # 获取权重矩阵
        weight = layer.weight.data
        bias = layer.bias.data if layer.bias is not None else None
        
        # 执行SVD分解
        U, S, V = torch.svd(weight)
        
        # 确定保留的秩
        original_rank = min(weight.size())
        target_rank = max(1, int(original_rank * rank_ratio))
        
        print(f"   原始秩: {original_rank}, 目标秩: {target_rank}")
        
        # 截断SVD
        U_truncated = U[:, :target_rank]
        S_truncated = S[:target_rank]
        V_truncated = V[:, :target_rank]
        
        # 创建分解后的层
        input_size = weight.size(1)
        output_size = weight.size(0)
        
        # 第一层：input_size -> target_rank
        layer1 = nn.Linear(input_size, target_rank, bias=False)
        layer1.weight.data = (V_truncated * S_truncated).t()
        
        # 第二层：target_rank -> output_size
        layer2 = nn.Linear(target_rank, output_size, bias=(bias is not None))
        layer2.weight.data = U_truncated.t()
        if bias is not None:
            layer2.bias.data = bias
        
        # 组合成序列模块
        decomposed_layer = nn.Sequential(
            layer1,
            layer2
        )
        
        # 计算参数减少量
        original_params = weight.numel() + (bias.numel() if bias is not None else 0)
        new_params = (layer1.weight.numel() + layer2.weight.numel() + 
                     (layer2.bias.numel() if layer2.bias is not None else 0))
        
        reduction = (original_params - new_params) / original_params * 100
        
        print(f"   参数减少: {original_params:,} -> {new_params:,} ({reduction:.1f}%)")
        
        return decomposed_layer
    
    def decompose_model(self, rank_ratio: float = 0.5) -> nn.Module:
        """分解整个模型"""
        print(f"🏗️ 分解整个模型，秩比例: {rank_ratio:.1%}")
        
        # 创建新模型
        decomposed_model = nn.Sequential()
        
        for name, module in self.model.named_children():
            if isinstance(module, nn.Linear):
                # 分解线性层
                decomposed_layer = self.svd_decomposition(module, rank_ratio)
                decomposed_model.add_module(f"{name}_decomposed", decomposed_layer)
            else:
                # 保持其他层不变
                decomposed_model.add_module(name, module)
        
        return decomposed_model
```

## 🎯 Trae实践：模型压缩工作流

让我们在Trae中实现一个完整的模型压缩工作流：

```python
class ModelCompressionPipeline:
    """模型压缩流水线"""
    
    def __init__(self, original_model: nn.Module, device: str = 'cpu'):
        self.original_model = original_model.to(device)
        self.device = device
        self.compression_results = {}
    
    def run_compression_pipeline(self, train_loader: DataLoader, 
                               test_loader: DataLoader) -> Dict[str, any]:
        """运行完整的压缩流水线"""
        print("🚀 启动模型压缩流水线")
        print("=" * 60)
        
        # 1. 基线评估
        print("\n📊 步骤1: 基线模型评估")
        baseline_stats = self._evaluate_baseline(test_loader)
        self.compression_results['baseline'] = baseline_stats
        
        # 2. 知识蒸馏
        print("\n🎓 步骤2: 知识蒸馏")
        distilled_model = self._apply_knowledge_distillation(train_loader, test_loader)
        
        # 3. 模型剪枝
        print("\n✂️ 步骤3: 模型剪枝")
        pruned_model = self._apply_pruning(distilled_model, test_loader)
        
        # 4. 模型量化
        print("\n🔢 步骤4: 模型量化")
        quantized_model = self._apply_quantization(pruned_model, train_loader, test_loader)
        
        # 5. 综合评估
        print("\n📈 步骤5: 综合评估")
        final_comparison = self._final_evaluation(quantized_model, test_loader)
        
        return {
            'compression_results': self.compression_results,
            'final_model': quantized_model,
            'final_comparison': final_comparison
        }
    
    def _evaluate_baseline(self, test_loader: DataLoader) -> Dict[str, any]:
        """评估基线模型"""
        analyzer = ModelAnalyzer()
        
        # 模型统计
        params_info = analyzer.count_parameters(self.original_model)
        model_size = analyzer.model_size_mb(self.original_model)
        
        # 准确率评估
        accuracy = self._evaluate_accuracy(self.original_model, test_loader)
        
        stats = {
            'parameters': params_info['total_parameters'],
            'size_mb': model_size,
            'accuracy': accuracy
        }
        
        print(f"   基线模型参数: {stats['parameters']:,}")
        print(f"   基线模型大小: {stats['size_mb']:.2f} MB")
        print(f"   基线准确率: {stats['accuracy']:.2f}%")
        
        return stats
    
    def _apply_knowledge_distillation(self, train_loader: DataLoader, 
                                    test_loader: DataLoader) -> nn.Module:
        """应用知识蒸馏"""
        # 创建学生模型（更小的架构）
        student_model = StudentModel()
        
        # 知识蒸馏训练
        distiller = KnowledgeDistillationTrainer(
            teacher_model=self.original_model,
            student_model=student_model,
            device=self.device
        )
        
        optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)
        
        # 训练几个epoch
        for epoch in range(3):
            train_stats = distiller.train_epoch(train_loader, optimizer)
            print(f"   Epoch {epoch+1}: Loss={train_stats['loss']:.4f}, "
                  f"Acc={train_stats['accuracy']:.2f}%")
        
        # 评估蒸馏效果
        student_accuracy = self._evaluate_accuracy(student_model, test_loader)
        student_size = ModelAnalyzer.model_size_mb(student_model)
        
        self.compression_results['distillation'] = {
            'accuracy': student_accuracy,
            'size_mb': student_size,
            'compression_ratio': self.compression_results['baseline']['size_mb'] / student_size
        }
        
        print(f"   蒸馏后准确率: {student_accuracy:.2f}%")
        print(f"   蒸馏后大小: {student_size:.2f} MB")
        
        return student_model
    
    def _apply_pruning(self, model: nn.Module, test_loader: DataLoader) -> nn.Module:
        """应用模型剪枝"""
        pruner = ModelPruner(model)
        
        # 执行渐进式剪枝
        pruning_stats = pruner.gradual_pruning(target_ratio=0.3, num_steps=3)
        
        # 使剪枝永久化
        pruner.make_permanent()
        
        # 评估剪枝效果
        pruned_accuracy = self._evaluate_accuracy(model, test_loader)
        pruned_size = ModelAnalyzer.model_size_mb(model)
        
        self.compression_results['pruning'] = {
            'accuracy': pruned_accuracy,
            'size_mb': pruned_size,
            'pruning_stats': pruning_stats[-1]  # 最终剪枝统计
        }
        
        print(f"   剪枝后准确率: {pruned_accuracy:.2f}%")
        print(f"   剪枝后大小: {pruned_size:.2f} MB")
        
        return model
    
    def _apply_quantization(self, model: nn.Module, train_loader: DataLoader, 
                          test_loader: DataLoader) -> nn.Module:
        """应用模型量化"""
        quantizer = ModelQuantizer(model, self.device)
        
        # 使用训练后量化
        quantized_model = quantizer.post_training_quantization(train_loader)
        
        # 评估量化效果
        quantized_accuracy = self._evaluate_accuracy(quantized_model, test_loader)
        quantized_size = ModelAnalyzer.model_size_mb(quantized_model)
        
        self.compression_results['quantization'] = {
            'accuracy': quantized_accuracy,
            'size_mb': quantized_size
        }
        
        print(f"   量化后准确率: {quantized_accuracy:.2f}%")
        print(f"   量化后大小: {quantized_size:.2f} MB")
        
        return quantized_model
    
    def _evaluate_accuracy(self, model: nn.Module, test_loader: DataLoader) -> float:
        """评估模型准确率"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        return 100. * correct / total
    
    def _final_evaluation(self, final_model: nn.Module, test_loader: DataLoader) -> Dict[str, any]:
        """最终评估"""
        baseline = self.compression_results['baseline']
        
        final_accuracy = self._evaluate_accuracy(final_model, test_loader)
        final_size = ModelAnalyzer.model_size_mb(final_model)
        
        comparison = {
            'original': {
                'accuracy': baseline['accuracy'],
                'size_mb': baseline['size_mb'],
                'parameters': baseline['parameters']
            },
            'compressed': {
                'accuracy': final_accuracy,
                'size_mb': final_size,
                'parameters': ModelAnalyzer.count_parameters(final_model)['total_parameters']
            },
            'improvements': {
                'size_reduction_percent': (baseline['size_mb'] - final_size) / baseline['size_mb'] * 100,
                'parameter_reduction_percent': (baseline['parameters'] - ModelAnalyzer.count_parameters(final_model)['total_parameters']) / baseline['parameters'] * 100,
                'accuracy_drop': baseline['accuracy'] - final_accuracy,
                'compression_ratio': baseline['size_mb'] / final_size
            }
        }
        
        print(f"\n🎉 压缩完成！")
        print(f"   模型大小: {baseline['size_mb']:.2f}MB -> {final_size:.2f}MB "
              f"(减少 {comparison['improvements']['size_reduction_percent']:.1f}%)")
        print(f"   参数数量: {baseline['parameters']:,} -> {comparison['compressed']['parameters']:,} "
              f"(减少 {comparison['improvements']['parameter_reduction_percent']:.1f}%)")
        print(f"   准确率: {baseline['accuracy']:.2f}% -> {final_accuracy:.2f}% "
              f"(下降 {comparison['improvements']['accuracy_drop']:.2f}%)")
        print(f"   压缩比: {comparison['improvements']['compression_ratio']:.2f}x")
        
        return comparison

# 压缩流水线演示
if __name__ == "__main__":
    # 创建示例数据
    from torch.utils.data import TensorDataset
    
    # 模拟数据
    X_train = torch.randn(1000, 784)
    y_train = torch.randint(0, 10, (1000,))
    X_test = torch.randn(200, 784)
    y_test = torch.randint(0, 10, (200,))
    
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # 创建原始模型
    original_model = TeacherModel()
    
    # 运行压缩流水线
    pipeline = ModelCompressionPipeline(original_model)
    results = pipeline.run_compression_pipeline(train_loader, test_loader)
    
    print("\n📋 压缩流水线完成！")
    print("详细结果已保存在 results 变量中")
```

## 小结

模型压缩与优化技术是部署大模型的关键技术。通过本节的学习，你应该掌握：

### 🎯 核心技术
1. **知识蒸馏**：通过师生网络传递知识
2. **模型剪枝**：移除不重要的参数或结构
3. **量化技术**：降低数值精度减少存储
4. **低秩分解**：通过矩阵分解减少参数

### 💡 实践要点
1. **渐进式压缩**：逐步应用多种技术
2. **性能监控**：平衡压缩率和精度损失
3. **场景适配**：根据部署环境选择技术
4. **效果评估**：全面评估压缩效果

### 🚀 应用建议
1. **移动端部署**：优先考虑量化和剪枝
2. **边缘计算**：结合多种压缩技术
3. **云端服务**：重点关注推理速度优化
4. **实时应用**：平衡延迟和准确率

模型压缩技术正在快速发展，掌握这些技术将帮助你更好地部署和应用大模型。在下一小节中，我们将探讨联邦学习与隐私保护技术。