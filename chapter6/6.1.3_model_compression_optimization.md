# 6.1.3 æ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–æŠ€æœ¯

éšç€å¤§æ¨¡å‹è§„æ¨¡çš„ä¸æ–­å¢é•¿ï¼Œæ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–æŠ€æœ¯å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬èŠ‚å°†æ·±å…¥æ¢è®¨å„ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œå¸®åŠ©ä½ åœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æ—¶é—´ã€‚

## ğŸ“š æŠ€æœ¯æ¦‚è¿°

æ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–æ˜¯å°†å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹è½¬æ¢ä¸ºæ›´å°ã€æ›´å¿«ã€æ›´é«˜æ•ˆç‰ˆæœ¬çš„æŠ€æœ¯é›†åˆï¼Œä¸»è¦ç›®æ ‡åŒ…æ‹¬ï¼š

- **å‡å°‘æ¨¡å‹å¤§å°**ï¼šé™ä½å­˜å‚¨å’Œä¼ è¾“æˆæœ¬
- **åŠ é€Ÿæ¨ç†**ï¼šæé«˜æ¨¡å‹å“åº”é€Ÿåº¦
- **é™ä½åŠŸè€—**ï¼šé€‚åº”ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—
- **ä¿æŒæ€§èƒ½**ï¼šåœ¨å‹ç¼©è¿‡ç¨‹ä¸­å°½é‡ä¿æŒåŸå§‹ç²¾åº¦

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•

### 1. çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰

çŸ¥è¯†è’¸é¦é€šè¿‡è®©å°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰å­¦ä¹ å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰çš„çŸ¥è¯†æ¥å®ç°å‹ç¼©ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt

class TeacherModel(nn.Module):
    """æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§æ¨¡å‹ï¼‰"""
    
    def __init__(self, input_size: int = 784, hidden_sizes: List[int] = [512, 256, 128], num_classes: int = 10):
        super(TeacherModel, self).__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, num_classes))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.view(x.size(0), -1)  # å±•å¹³è¾“å…¥
        return self.network(x)

class StudentModel(nn.Module):
    """å­¦ç”Ÿæ¨¡å‹ï¼ˆå°æ¨¡å‹ï¼‰"""
    
    def __init__(self, input_size: int = 784, hidden_sizes: List[int] = [64, 32], num_classes: int = 10):
        super(StudentModel, self).__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, num_classes))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.view(x.size(0), -1)
        return self.network(x)

class KnowledgeDistillationTrainer:
    """çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, teacher_model: nn.Module, student_model: nn.Module, 
                 temperature: float = 4.0, alpha: float = 0.7, device: str = 'cpu'):
        self.teacher_model = teacher_model.to(device)
        self.student_model = student_model.to(device)
        self.temperature = temperature
        self.alpha = alpha  # è’¸é¦æŸå¤±æƒé‡
        self.device = device
        
        # å†»ç»“æ•™å¸ˆæ¨¡å‹
        self.teacher_model.eval()
        for param in self.teacher_model.parameters():
            param.requires_grad = False
    
    def distillation_loss(self, student_logits: torch.Tensor, teacher_logits: torch.Tensor, 
                         labels: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, float]]:
        """è®¡ç®—è’¸é¦æŸå¤±"""
        
        # è½¯ç›®æ ‡æŸå¤±ï¼ˆè’¸é¦æŸå¤±ï¼‰
        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)
        soft_targets_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (self.temperature ** 2)
        
        # ç¡¬ç›®æ ‡æŸå¤±ï¼ˆæ ‡å‡†äº¤å‰ç†µï¼‰
        hard_targets_loss = F.cross_entropy(student_logits, labels)
        
        # æ€»æŸå¤±
        total_loss = self.alpha * soft_targets_loss + (1 - self.alpha) * hard_targets_loss
        
        loss_info = {
            'total_loss': total_loss.item(),
            'soft_loss': soft_targets_loss.item(),
            'hard_loss': hard_targets_loss.item()
        }
        
        return total_loss, loss_info
    
    def train_epoch(self, dataloader: DataLoader, optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.student_model.train()
        
        total_loss = 0
        total_soft_loss = 0
        total_hard_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            
            # è·å–æ•™å¸ˆå’Œå­¦ç”Ÿçš„è¾“å‡º
            with torch.no_grad():
                teacher_logits = self.teacher_model(data)
            
            student_logits = self.student_model(data)
            
            # è®¡ç®—æŸå¤±
            loss, loss_info = self.distillation_loss(student_logits, teacher_logits, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss_info['total_loss']
            total_soft_loss += loss_info['soft_loss']
            total_hard_loss += loss_info['hard_loss']
            
            pred = student_logits.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
        
        return {
            'loss': total_loss / len(dataloader),
            'soft_loss': total_soft_loss / len(dataloader),
            'hard_loss': total_hard_loss / len(dataloader),
            'accuracy': 100. * correct / total
        }
    
    def evaluate(self, dataloader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.student_model.eval()
        
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(self.device), target.to(self.device)
                
                teacher_logits = self.teacher_model(data)
                student_logits = self.student_model(data)
                
                loss, _ = self.distillation_loss(student_logits, teacher_logits, target)
                total_loss += loss.item()
                
                pred = student_logits.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        return {
            'loss': total_loss / len(dataloader),
            'accuracy': 100. * correct / total
        }

# æ¨¡å‹å¤§å°æ¯”è¾ƒå·¥å…·
class ModelAnalyzer:
    """æ¨¡å‹åˆ†æå™¨"""
    
    @staticmethod
    def count_parameters(model: nn.Module) -> Dict[str, int]:
        """ç»Ÿè®¡æ¨¡å‹å‚æ•°"""
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'non_trainable_parameters': total_params - trainable_params
        }
    
    @staticmethod
    def model_size_mb(model: nn.Module) -> float:
        """è®¡ç®—æ¨¡å‹å¤§å°ï¼ˆMBï¼‰"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_mb = (param_size + buffer_size) / 1024 / 1024
        return size_mb
    
    @staticmethod
    def compare_models(teacher: nn.Module, student: nn.Module) -> Dict[str, any]:
        """æ¯”è¾ƒæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹"""
        teacher_params = ModelAnalyzer.count_parameters(teacher)
        student_params = ModelAnalyzer.count_parameters(student)
        
        teacher_size = ModelAnalyzer.model_size_mb(teacher)
        student_size = ModelAnalyzer.model_size_mb(student)
        
        compression_ratio = teacher_params['total_parameters'] / student_params['total_parameters']
        size_reduction = (teacher_size - student_size) / teacher_size * 100
        
        return {
            'teacher': {
                'parameters': teacher_params['total_parameters'],
                'size_mb': teacher_size
            },
            'student': {
                'parameters': student_params['total_parameters'],
                'size_mb': student_size
            },
            'compression_ratio': compression_ratio,
            'size_reduction_percent': size_reduction
        }
```

### 2. æ¨¡å‹å‰ªæï¼ˆModel Pruningï¼‰

æ¨¡å‹å‰ªæé€šè¿‡ç§»é™¤ä¸é‡è¦çš„æƒé‡æˆ–ç¥ç»å…ƒæ¥å‡å°‘æ¨¡å‹å¤æ‚åº¦ã€‚

```python
import torch.nn.utils.prune as prune
from collections import OrderedDict

class ModelPruner:
    """æ¨¡å‹å‰ªæå™¨"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.original_state = None
    
    def magnitude_pruning(self, pruning_ratio: float = 0.2) -> Dict[str, float]:
        """åŸºäºæƒé‡å¹…åº¦çš„å‰ªæ"""
        print(f"ğŸ”ª æ‰§è¡Œå¹…åº¦å‰ªæï¼Œå‰ªææ¯”ä¾‹: {pruning_ratio:.1%}")
        
        # ä¿å­˜åŸå§‹çŠ¶æ€
        self.original_state = self.model.state_dict().copy()
        
        # æ”¶é›†æ‰€æœ‰çº¿æ€§å±‚
        modules_to_prune = []
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear):
                modules_to_prune.append((module, 'weight'))
        
        # æ‰§è¡Œå…¨å±€å¹…åº¦å‰ªæ
        prune.global_unstructured(
            modules_to_prune,
            pruning_method=prune.L1Unstructured,
            amount=pruning_ratio,
        )
        
        # ç»Ÿè®¡å‰ªææ•ˆæœ
        stats = self._calculate_pruning_stats()
        
        print(f"   å‰ªæå®Œæˆ: {stats['pruned_params']:,} / {stats['total_params']:,} å‚æ•°è¢«å‰ªæ")
        print(f"   å‰ªææ¯”ä¾‹: {stats['pruning_ratio']:.1%}")
        
        return stats
    
    def structured_pruning(self, pruning_ratio: float = 0.3) -> Dict[str, float]:
        """ç»“æ„åŒ–å‰ªæï¼ˆç§»é™¤æ•´ä¸ªç¥ç»å…ƒï¼‰"""
        print(f"ğŸ—ï¸ æ‰§è¡Œç»“æ„åŒ–å‰ªæï¼Œå‰ªææ¯”ä¾‹: {pruning_ratio:.1%}")
        
        self.original_state = self.model.state_dict().copy()
        
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear) and hasattr(module, 'weight'):
                # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§ï¼ˆL2èŒƒæ•°ï¼‰
                weight = module.weight.data
                neuron_importance = torch.norm(weight, dim=1)
                
                # ç¡®å®šè¦å‰ªæçš„ç¥ç»å…ƒæ•°é‡
                num_neurons = weight.size(0)
                num_to_prune = int(num_neurons * pruning_ratio)
                
                if num_to_prune > 0:
                    # æ‰¾åˆ°é‡è¦æ€§æœ€ä½çš„ç¥ç»å…ƒ
                    _, indices_to_prune = torch.topk(neuron_importance, num_to_prune, largest=False)
                    
                    # åˆ›å»ºæ©ç 
                    mask = torch.ones_like(neuron_importance, dtype=torch.bool)
                    mask[indices_to_prune] = False
                    
                    # åº”ç”¨ç»“æ„åŒ–å‰ªæ
                    prune.custom_from_mask(module, 'weight', mask.unsqueeze(1).expand_as(weight))
        
        stats = self._calculate_pruning_stats()
        print(f"   ç»“æ„åŒ–å‰ªæå®Œæˆ: {stats['pruning_ratio']:.1%} çš„å‚æ•°è¢«ç§»é™¤")
        
        return stats
    
    def gradual_pruning(self, target_ratio: float = 0.5, num_steps: int = 5) -> List[Dict[str, float]]:
        """æ¸è¿›å¼å‰ªæ"""
        print(f"ğŸ“ˆ æ‰§è¡Œæ¸è¿›å¼å‰ªæï¼Œç›®æ ‡æ¯”ä¾‹: {target_ratio:.1%}ï¼Œæ­¥éª¤æ•°: {num_steps}")
        
        self.original_state = self.model.state_dict().copy()
        
        # è®¡ç®—æ¯æ­¥çš„å‰ªææ¯”ä¾‹
        step_ratio = target_ratio / num_steps
        current_ratio = 0
        
        stats_history = []
        
        for step in range(num_steps):
            current_ratio += step_ratio
            print(f"   æ­¥éª¤ {step + 1}/{num_steps}: ç´¯è®¡å‰ªææ¯”ä¾‹ {current_ratio:.1%}")
            
            # æ‰§è¡Œå½“å‰æ­¥éª¤çš„å‰ªæ
            step_stats = self.magnitude_pruning(step_ratio)
            step_stats['step'] = step + 1
            step_stats['cumulative_ratio'] = current_ratio
            
            stats_history.append(step_stats)
        
        return stats_history
    
    def _calculate_pruning_stats(self) -> Dict[str, float]:
        """è®¡ç®—å‰ªæç»Ÿè®¡ä¿¡æ¯"""
        total_params = 0
        pruned_params = 0
        
        for name, module in self.model.named_modules():
            if hasattr(module, 'weight_mask'):
                mask = module.weight_mask
                total_params += mask.numel()
                pruned_params += (mask == 0).sum().item()
        
        pruning_ratio = pruned_params / total_params if total_params > 0 else 0
        
        return {
            'total_params': total_params,
            'pruned_params': pruned_params,
            'remaining_params': total_params - pruned_params,
            'pruning_ratio': pruning_ratio
        }
    
    def make_permanent(self):
        """ä½¿å‰ªææ°¸ä¹…åŒ–"""
        print("ğŸ”’ ä½¿å‰ªææ°¸ä¹…åŒ–...")
        
        for name, module in self.model.named_modules():
            if hasattr(module, 'weight_mask'):
                prune.remove(module, 'weight')
        
        print("   å‰ªæå·²æ°¸ä¹…åŒ–")
    
    def restore_original(self):
        """æ¢å¤åŸå§‹æ¨¡å‹"""
        if self.original_state is not None:
            print("ğŸ”„ æ¢å¤åŸå§‹æ¨¡å‹çŠ¶æ€...")
            
            # ç§»é™¤å‰ªææ©ç 
            for name, module in self.model.named_modules():
                if hasattr(module, 'weight_mask'):
                    prune.remove(module, 'weight')
            
            # æ¢å¤åŸå§‹æƒé‡
            self.model.load_state_dict(self.original_state)
            print("   æ¨¡å‹å·²æ¢å¤åˆ°åŸå§‹çŠ¶æ€")
        else:
            print("âš ï¸ æ²¡æœ‰ä¿å­˜çš„åŸå§‹çŠ¶æ€")
```

### 3. é‡åŒ–æŠ€æœ¯ï¼ˆQuantizationï¼‰

é‡åŒ–é€šè¿‡é™ä½æ•°å€¼ç²¾åº¦æ¥å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—å¤æ‚åº¦ã€‚

```python
import torch.quantization as quant
from torch.quantization import QuantStub, DeQuantStub

class QuantizableModel(nn.Module):
    """å¯é‡åŒ–çš„æ¨¡å‹"""
    
    def __init__(self, original_model: nn.Module):
        super(QuantizableModel, self).__init__()
        
        # é‡åŒ–å’Œåé‡åŒ–å­˜æ ¹
        self.quant = QuantStub()
        self.dequant = DeQuantStub()
        
        # åŸå§‹æ¨¡å‹
        self.model = original_model
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.quant(x)
        x = self.model(x)
        x = self.dequant(x)
        return x

class ModelQuantizer:
    """æ¨¡å‹é‡åŒ–å™¨"""
    
    def __init__(self, model: nn.Module, device: str = 'cpu'):
        self.original_model = model.to(device)
        self.device = device
    
    def post_training_quantization(self, calibration_loader: DataLoader) -> nn.Module:
        """è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰"""
        print("ğŸ”¢ æ‰§è¡Œè®­ç»ƒåé‡åŒ–...")
        
        # åˆ›å»ºå¯é‡åŒ–æ¨¡å‹
        quantizable_model = QuantizableModel(self.original_model)
        quantizable_model.eval()
        
        # è®¾ç½®é‡åŒ–é…ç½®
        quantizable_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # å‡†å¤‡é‡åŒ–
        prepared_model = torch.quantization.prepare(quantizable_model)
        
        # æ ¡å‡†ï¼ˆä½¿ç”¨å°‘é‡æ•°æ®ï¼‰
        print("   æ­£åœ¨æ ¡å‡†é‡åŒ–å‚æ•°...")
        with torch.no_grad():
            for i, (data, _) in enumerate(calibration_loader):
                if i >= 100:  # åªä½¿ç”¨100ä¸ªæ‰¹æ¬¡è¿›è¡Œæ ¡å‡†
                    break
                data = data.to(self.device)
                prepared_model(data)
        
        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        quantized_model = torch.quantization.convert(prepared_model)
        
        print("   è®­ç»ƒåé‡åŒ–å®Œæˆ")
        return quantized_model
    
    def quantization_aware_training(self, train_loader: DataLoader, 
                                  num_epochs: int = 5) -> nn.Module:
        """é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰"""
        print(f"ğŸ¯ æ‰§è¡Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œè®­ç»ƒè½®æ•°: {num_epochs}")
        
        # åˆ›å»ºå¯é‡åŒ–æ¨¡å‹
        quantizable_model = QuantizableModel(self.original_model)
        
        # è®¾ç½®é‡åŒ–é…ç½®
        quantizable_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        
        # å‡†å¤‡QAT
        prepared_model = torch.quantization.prepare_qat(quantizable_model)
        prepared_model.train()
        
        # è®­ç»ƒ
        optimizer = torch.optim.Adam(prepared_model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        for epoch in range(num_epochs):
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = prepared_model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
                
                if batch_idx % 100 == 0:
                    print(f"   Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, "
                          f"Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%")
        
        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        prepared_model.eval()
        quantized_model = torch.quantization.convert(prepared_model)
        
        print("   é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå®Œæˆ")
        return quantized_model
    
    def dynamic_quantization(self) -> nn.Module:
        """åŠ¨æ€é‡åŒ–"""
        print("âš¡ æ‰§è¡ŒåŠ¨æ€é‡åŒ–...")
        
        # åŠ¨æ€é‡åŒ–ä¸»è¦é’ˆå¯¹çº¿æ€§å±‚
        quantized_model = torch.quantization.quantize_dynamic(
            self.original_model,
            {nn.Linear},
            dtype=torch.qint8
        )
        
        print("   åŠ¨æ€é‡åŒ–å®Œæˆ")
        return quantized_model
    
    @staticmethod
    def compare_quantized_models(original: nn.Module, quantized: nn.Module, 
                               test_loader: DataLoader, device: str = 'cpu') -> Dict[str, any]:
        """æ¯”è¾ƒé‡åŒ–å‰åçš„æ¨¡å‹"""
        print("ğŸ“Š æ¯”è¾ƒé‡åŒ–æ•ˆæœ...")
        
        # æ¨¡å‹å¤§å°æ¯”è¾ƒ
        original_size = ModelAnalyzer.model_size_mb(original)
        quantized_size = ModelAnalyzer.model_size_mb(quantized)
        
        # ç²¾åº¦æ¯”è¾ƒ
        def evaluate_model(model, loader):
            model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, target in loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    pred = output.argmax(dim=1, keepdim=True)
                    correct += pred.eq(target.view_as(pred)).sum().item()
                    total += target.size(0)
            
            return 100. * correct / total
        
        original_acc = evaluate_model(original, test_loader)
        quantized_acc = evaluate_model(quantized, test_loader)
        
        # æ¨ç†é€Ÿåº¦æ¯”è¾ƒ
        def measure_inference_time(model, sample_input, num_runs=100):
            model.eval()
            
            # é¢„çƒ­
            for _ in range(10):
                with torch.no_grad():
                    _ = model(sample_input)
            
            # æµ‹é‡æ—¶é—´
            import time
            start_time = time.time()
            
            for _ in range(num_runs):
                with torch.no_grad():
                    _ = model(sample_input)
            
            end_time = time.time()
            return (end_time - start_time) / num_runs
        
        # è·å–æ ·æœ¬è¾“å…¥
        sample_input = next(iter(test_loader))[0][:1].to(device)
        
        original_time = measure_inference_time(original, sample_input)
        quantized_time = measure_inference_time(quantized, sample_input)
        
        results = {
            'size_comparison': {
                'original_mb': original_size,
                'quantized_mb': quantized_size,
                'compression_ratio': original_size / quantized_size,
                'size_reduction_percent': (original_size - quantized_size) / original_size * 100
            },
            'accuracy_comparison': {
                'original_accuracy': original_acc,
                'quantized_accuracy': quantized_acc,
                'accuracy_drop': original_acc - quantized_acc
            },
            'speed_comparison': {
                'original_time_ms': original_time * 1000,
                'quantized_time_ms': quantized_time * 1000,
                'speedup': original_time / quantized_time
            }
        }
        
        print(f"   æ¨¡å‹å¤§å°: {original_size:.2f}MB -> {quantized_size:.2f}MB "
              f"(å‹ç¼© {results['size_comparison']['size_reduction_percent']:.1f}%)")
        print(f"   å‡†ç¡®ç‡: {original_acc:.2f}% -> {quantized_acc:.2f}% "
              f"(ä¸‹é™ {results['accuracy_comparison']['accuracy_drop']:.2f}%)")
        print(f"   æ¨ç†é€Ÿåº¦: {original_time*1000:.2f}ms -> {quantized_time*1000:.2f}ms "
              f"(åŠ é€Ÿ {results['speed_comparison']['speedup']:.2f}x)")
        
        return results
```

### 4. ä½ç§©åˆ†è§£ï¼ˆLow-Rank Decompositionï¼‰

é€šè¿‡çŸ©é˜µåˆ†è§£æŠ€æœ¯å‡å°‘å‚æ•°æ•°é‡ã€‚

```python
class LowRankDecomposer:
    """ä½ç§©åˆ†è§£å™¨"""
    
    def __init__(self, model: nn.Module):
        self.model = model
    
    def svd_decomposition(self, layer: nn.Linear, rank_ratio: float = 0.5) -> nn.Module:
        """SVDåˆ†è§£çº¿æ€§å±‚"""
        print(f"ğŸ” å¯¹çº¿æ€§å±‚æ‰§è¡ŒSVDåˆ†è§£ï¼Œç§©æ¯”ä¾‹: {rank_ratio:.1%}")
        
        # è·å–æƒé‡çŸ©é˜µ
        weight = layer.weight.data
        bias = layer.bias.data if layer.bias is not None else None
        
        # æ‰§è¡ŒSVDåˆ†è§£
        U, S, V = torch.svd(weight)
        
        # ç¡®å®šä¿ç•™çš„ç§©
        original_rank = min(weight.size())
        target_rank = max(1, int(original_rank * rank_ratio))
        
        print(f"   åŸå§‹ç§©: {original_rank}, ç›®æ ‡ç§©: {target_rank}")
        
        # æˆªæ–­SVD
        U_truncated = U[:, :target_rank]
        S_truncated = S[:target_rank]
        V_truncated = V[:, :target_rank]
        
        # åˆ›å»ºåˆ†è§£åçš„å±‚
        input_size = weight.size(1)
        output_size = weight.size(0)
        
        # ç¬¬ä¸€å±‚ï¼šinput_size -> target_rank
        layer1 = nn.Linear(input_size, target_rank, bias=False)
        layer1.weight.data = (V_truncated * S_truncated).t()
        
        # ç¬¬äºŒå±‚ï¼štarget_rank -> output_size
        layer2 = nn.Linear(target_rank, output_size, bias=(bias is not None))
        layer2.weight.data = U_truncated.t()
        if bias is not None:
            layer2.bias.data = bias
        
        # ç»„åˆæˆåºåˆ—æ¨¡å—
        decomposed_layer = nn.Sequential(
            layer1,
            layer2
        )
        
        # è®¡ç®—å‚æ•°å‡å°‘é‡
        original_params = weight.numel() + (bias.numel() if bias is not None else 0)
        new_params = (layer1.weight.numel() + layer2.weight.numel() + 
                     (layer2.bias.numel() if layer2.bias is not None else 0))
        
        reduction = (original_params - new_params) / original_params * 100
        
        print(f"   å‚æ•°å‡å°‘: {original_params:,} -> {new_params:,} ({reduction:.1f}%)")
        
        return decomposed_layer
    
    def decompose_model(self, rank_ratio: float = 0.5) -> nn.Module:
        """åˆ†è§£æ•´ä¸ªæ¨¡å‹"""
        print(f"ğŸ—ï¸ åˆ†è§£æ•´ä¸ªæ¨¡å‹ï¼Œç§©æ¯”ä¾‹: {rank_ratio:.1%}")
        
        # åˆ›å»ºæ–°æ¨¡å‹
        decomposed_model = nn.Sequential()
        
        for name, module in self.model.named_children():
            if isinstance(module, nn.Linear):
                # åˆ†è§£çº¿æ€§å±‚
                decomposed_layer = self.svd_decomposition(module, rank_ratio)
                decomposed_model.add_module(f"{name}_decomposed", decomposed_layer)
            else:
                # ä¿æŒå…¶ä»–å±‚ä¸å˜
                decomposed_model.add_module(name, module)
        
        return decomposed_model
```

## ğŸ¯ Traeå®è·µï¼šæ¨¡å‹å‹ç¼©å·¥ä½œæµ

è®©æˆ‘ä»¬åœ¨Traeä¸­å®ç°ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹å‹ç¼©å·¥ä½œæµï¼š

```python
class ModelCompressionPipeline:
    """æ¨¡å‹å‹ç¼©æµæ°´çº¿"""
    
    def __init__(self, original_model: nn.Module, device: str = 'cpu'):
        self.original_model = original_model.to(device)
        self.device = device
        self.compression_results = {}
    
    def run_compression_pipeline(self, train_loader: DataLoader, 
                               test_loader: DataLoader) -> Dict[str, any]:
        """è¿è¡Œå®Œæ•´çš„å‹ç¼©æµæ°´çº¿"""
        print("ğŸš€ å¯åŠ¨æ¨¡å‹å‹ç¼©æµæ°´çº¿")
        print("=" * 60)
        
        # 1. åŸºçº¿è¯„ä¼°
        print("\nğŸ“Š æ­¥éª¤1: åŸºçº¿æ¨¡å‹è¯„ä¼°")
        baseline_stats = self._evaluate_baseline(test_loader)
        self.compression_results['baseline'] = baseline_stats
        
        # 2. çŸ¥è¯†è’¸é¦
        print("\nğŸ“ æ­¥éª¤2: çŸ¥è¯†è’¸é¦")
        distilled_model = self._apply_knowledge_distillation(train_loader, test_loader)
        
        # 3. æ¨¡å‹å‰ªæ
        print("\nâœ‚ï¸ æ­¥éª¤3: æ¨¡å‹å‰ªæ")
        pruned_model = self._apply_pruning(distilled_model, test_loader)
        
        # 4. æ¨¡å‹é‡åŒ–
        print("\nğŸ”¢ æ­¥éª¤4: æ¨¡å‹é‡åŒ–")
        quantized_model = self._apply_quantization(pruned_model, train_loader, test_loader)
        
        # 5. ç»¼åˆè¯„ä¼°
        print("\nğŸ“ˆ æ­¥éª¤5: ç»¼åˆè¯„ä¼°")
        final_comparison = self._final_evaluation(quantized_model, test_loader)
        
        return {
            'compression_results': self.compression_results,
            'final_model': quantized_model,
            'final_comparison': final_comparison
        }
    
    def _evaluate_baseline(self, test_loader: DataLoader) -> Dict[str, any]:
        """è¯„ä¼°åŸºçº¿æ¨¡å‹"""
        analyzer = ModelAnalyzer()
        
        # æ¨¡å‹ç»Ÿè®¡
        params_info = analyzer.count_parameters(self.original_model)
        model_size = analyzer.model_size_mb(self.original_model)
        
        # å‡†ç¡®ç‡è¯„ä¼°
        accuracy = self._evaluate_accuracy(self.original_model, test_loader)
        
        stats = {
            'parameters': params_info['total_parameters'],
            'size_mb': model_size,
            'accuracy': accuracy
        }
        
        print(f"   åŸºçº¿æ¨¡å‹å‚æ•°: {stats['parameters']:,}")
        print(f"   åŸºçº¿æ¨¡å‹å¤§å°: {stats['size_mb']:.2f} MB")
        print(f"   åŸºçº¿å‡†ç¡®ç‡: {stats['accuracy']:.2f}%")
        
        return stats
    
    def _apply_knowledge_distillation(self, train_loader: DataLoader, 
                                    test_loader: DataLoader) -> nn.Module:
        """åº”ç”¨çŸ¥è¯†è’¸é¦"""
        # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹ï¼ˆæ›´å°çš„æ¶æ„ï¼‰
        student_model = StudentModel()
        
        # çŸ¥è¯†è’¸é¦è®­ç»ƒ
        distiller = KnowledgeDistillationTrainer(
            teacher_model=self.original_model,
            student_model=student_model,
            device=self.device
        )
        
        optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)
        
        # è®­ç»ƒå‡ ä¸ªepoch
        for epoch in range(3):
            train_stats = distiller.train_epoch(train_loader, optimizer)
            print(f"   Epoch {epoch+1}: Loss={train_stats['loss']:.4f}, "
                  f"Acc={train_stats['accuracy']:.2f}%")
        
        # è¯„ä¼°è’¸é¦æ•ˆæœ
        student_accuracy = self._evaluate_accuracy(student_model, test_loader)
        student_size = ModelAnalyzer.model_size_mb(student_model)
        
        self.compression_results['distillation'] = {
            'accuracy': student_accuracy,
            'size_mb': student_size,
            'compression_ratio': self.compression_results['baseline']['size_mb'] / student_size
        }
        
        print(f"   è’¸é¦åå‡†ç¡®ç‡: {student_accuracy:.2f}%")
        print(f"   è’¸é¦åå¤§å°: {student_size:.2f} MB")
        
        return student_model
    
    def _apply_pruning(self, model: nn.Module, test_loader: DataLoader) -> nn.Module:
        """åº”ç”¨æ¨¡å‹å‰ªæ"""
        pruner = ModelPruner(model)
        
        # æ‰§è¡Œæ¸è¿›å¼å‰ªæ
        pruning_stats = pruner.gradual_pruning(target_ratio=0.3, num_steps=3)
        
        # ä½¿å‰ªææ°¸ä¹…åŒ–
        pruner.make_permanent()
        
        # è¯„ä¼°å‰ªææ•ˆæœ
        pruned_accuracy = self._evaluate_accuracy(model, test_loader)
        pruned_size = ModelAnalyzer.model_size_mb(model)
        
        self.compression_results['pruning'] = {
            'accuracy': pruned_accuracy,
            'size_mb': pruned_size,
            'pruning_stats': pruning_stats[-1]  # æœ€ç»ˆå‰ªæç»Ÿè®¡
        }
        
        print(f"   å‰ªæåå‡†ç¡®ç‡: {pruned_accuracy:.2f}%")
        print(f"   å‰ªæåå¤§å°: {pruned_size:.2f} MB")
        
        return model
    
    def _apply_quantization(self, model: nn.Module, train_loader: DataLoader, 
                          test_loader: DataLoader) -> nn.Module:
        """åº”ç”¨æ¨¡å‹é‡åŒ–"""
        quantizer = ModelQuantizer(model, self.device)
        
        # ä½¿ç”¨è®­ç»ƒåé‡åŒ–
        quantized_model = quantizer.post_training_quantization(train_loader)
        
        # è¯„ä¼°é‡åŒ–æ•ˆæœ
        quantized_accuracy = self._evaluate_accuracy(quantized_model, test_loader)
        quantized_size = ModelAnalyzer.model_size_mb(quantized_model)
        
        self.compression_results['quantization'] = {
            'accuracy': quantized_accuracy,
            'size_mb': quantized_size
        }
        
        print(f"   é‡åŒ–åå‡†ç¡®ç‡: {quantized_accuracy:.2f}%")
        print(f"   é‡åŒ–åå¤§å°: {quantized_size:.2f} MB")
        
        return quantized_model
    
    def _evaluate_accuracy(self, model: nn.Module, test_loader: DataLoader) -> float:
        """è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        return 100. * correct / total
    
    def _final_evaluation(self, final_model: nn.Module, test_loader: DataLoader) -> Dict[str, any]:
        """æœ€ç»ˆè¯„ä¼°"""
        baseline = self.compression_results['baseline']
        
        final_accuracy = self._evaluate_accuracy(final_model, test_loader)
        final_size = ModelAnalyzer.model_size_mb(final_model)
        
        comparison = {
            'original': {
                'accuracy': baseline['accuracy'],
                'size_mb': baseline['size_mb'],
                'parameters': baseline['parameters']
            },
            'compressed': {
                'accuracy': final_accuracy,
                'size_mb': final_size,
                'parameters': ModelAnalyzer.count_parameters(final_model)['total_parameters']
            },
            'improvements': {
                'size_reduction_percent': (baseline['size_mb'] - final_size) / baseline['size_mb'] * 100,
                'parameter_reduction_percent': (baseline['parameters'] - ModelAnalyzer.count_parameters(final_model)['total_parameters']) / baseline['parameters'] * 100,
                'accuracy_drop': baseline['accuracy'] - final_accuracy,
                'compression_ratio': baseline['size_mb'] / final_size
            }
        }
        
        print(f"\nğŸ‰ å‹ç¼©å®Œæˆï¼")
        print(f"   æ¨¡å‹å¤§å°: {baseline['size_mb']:.2f}MB -> {final_size:.2f}MB "
              f"(å‡å°‘ {comparison['improvements']['size_reduction_percent']:.1f}%)")
        print(f"   å‚æ•°æ•°é‡: {baseline['parameters']:,} -> {comparison['compressed']['parameters']:,} "
              f"(å‡å°‘ {comparison['improvements']['parameter_reduction_percent']:.1f}%)")
        print(f"   å‡†ç¡®ç‡: {baseline['accuracy']:.2f}% -> {final_accuracy:.2f}% "
              f"(ä¸‹é™ {comparison['improvements']['accuracy_drop']:.2f}%)")
        print(f"   å‹ç¼©æ¯”: {comparison['improvements']['compression_ratio']:.2f}x")
        
        return comparison

# å‹ç¼©æµæ°´çº¿æ¼”ç¤º
if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    from torch.utils.data import TensorDataset
    
    # æ¨¡æ‹Ÿæ•°æ®
    X_train = torch.randn(1000, 784)
    y_train = torch.randint(0, 10, (1000,))
    X_test = torch.randn(200, 784)
    y_test = torch.randint(0, 10, (200,))
    
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºåŸå§‹æ¨¡å‹
    original_model = TeacherModel()
    
    # è¿è¡Œå‹ç¼©æµæ°´çº¿
    pipeline = ModelCompressionPipeline(original_model)
    results = pipeline.run_compression_pipeline(train_loader, test_loader)
    
    print("\nğŸ“‹ å‹ç¼©æµæ°´çº¿å®Œæˆï¼")
    print("è¯¦ç»†ç»“æœå·²ä¿å­˜åœ¨ results å˜é‡ä¸­")
```

## å°ç»“

æ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–æŠ€æœ¯æ˜¯éƒ¨ç½²å¤§æ¨¡å‹çš„å…³é”®æŠ€æœ¯ã€‚é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œä½ åº”è¯¥æŒæ¡ï¼š

### ğŸ¯ æ ¸å¿ƒæŠ€æœ¯
1. **çŸ¥è¯†è’¸é¦**ï¼šé€šè¿‡å¸ˆç”Ÿç½‘ç»œä¼ é€’çŸ¥è¯†
2. **æ¨¡å‹å‰ªæ**ï¼šç§»é™¤ä¸é‡è¦çš„å‚æ•°æˆ–ç»“æ„
3. **é‡åŒ–æŠ€æœ¯**ï¼šé™ä½æ•°å€¼ç²¾åº¦å‡å°‘å­˜å‚¨
4. **ä½ç§©åˆ†è§£**ï¼šé€šè¿‡çŸ©é˜µåˆ†è§£å‡å°‘å‚æ•°

### ğŸ’¡ å®è·µè¦ç‚¹
1. **æ¸è¿›å¼å‹ç¼©**ï¼šé€æ­¥åº”ç”¨å¤šç§æŠ€æœ¯
2. **æ€§èƒ½ç›‘æ§**ï¼šå¹³è¡¡å‹ç¼©ç‡å’Œç²¾åº¦æŸå¤±
3. **åœºæ™¯é€‚é…**ï¼šæ ¹æ®éƒ¨ç½²ç¯å¢ƒé€‰æ‹©æŠ€æœ¯
4. **æ•ˆæœè¯„ä¼°**ï¼šå…¨é¢è¯„ä¼°å‹ç¼©æ•ˆæœ

### ğŸš€ åº”ç”¨å»ºè®®
1. **ç§»åŠ¨ç«¯éƒ¨ç½²**ï¼šä¼˜å…ˆè€ƒè™‘é‡åŒ–å’Œå‰ªæ
2. **è¾¹ç¼˜è®¡ç®—**ï¼šç»“åˆå¤šç§å‹ç¼©æŠ€æœ¯
3. **äº‘ç«¯æœåŠ¡**ï¼šé‡ç‚¹å…³æ³¨æ¨ç†é€Ÿåº¦ä¼˜åŒ–
4. **å®æ—¶åº”ç”¨**ï¼šå¹³è¡¡å»¶è¿Ÿå’Œå‡†ç¡®ç‡

æ¨¡å‹å‹ç¼©æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼ŒæŒæ¡è¿™äº›æŠ€æœ¯å°†å¸®åŠ©ä½ æ›´å¥½åœ°éƒ¨ç½²å’Œåº”ç”¨å¤§æ¨¡å‹ã€‚åœ¨ä¸‹ä¸€å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è”é‚¦å­¦ä¹ ä¸éšç§ä¿æŠ¤æŠ€æœ¯ã€‚