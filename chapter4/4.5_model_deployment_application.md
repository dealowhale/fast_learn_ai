# 4.5 æ¨¡å‹éƒ¨ç½²ä¸åº”ç”¨å®ä¾‹

æœ¬èŠ‚å°†è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨Traeç¯å¢ƒä¸­å°†è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼ŒåŒ…æ‹¬æ¨¡å‹ä¿å­˜ä¸åŠ è½½ã€Web APIå¼€å‘ã€å®¹å™¨åŒ–éƒ¨ç½²å’Œæ€§èƒ½ç›‘æ§ç­‰å…³é”®æŠ€æœ¯ã€‚é€šè¿‡å®é™…æ¡ˆä¾‹å±•ç¤ºå®Œæ•´çš„æ¨¡å‹éƒ¨ç½²æµç¨‹ã€‚

## 4.5.1 æ¨¡å‹ä¿å­˜ä¸åŠ è½½ç³»ç»Ÿ

### æ™ºèƒ½æ¨¡å‹åºåˆ—åŒ–ç®¡ç†å™¨

```python
# æ¨¡å‹ä¿å­˜ä¸åŠ è½½ç³»ç»Ÿ
import pickle
import joblib
import json
import os
from datetime import datetime
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator
import hashlib
import warnings

class ModelSerializer:
    """æ™ºèƒ½æ¨¡å‹åºåˆ—åŒ–ç®¡ç†å™¨"""
    
    def __init__(self, base_path="./models"):
        self.base_path = base_path
        self.metadata_file = os.path.join(base_path, "model_registry.json")
        self.ensure_directory()
        self.model_registry = self.load_registry()
    
    def ensure_directory(self):
        """ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.base_path):
            os.makedirs(self.base_path)
            print(f"ğŸ“ åˆ›å»ºæ¨¡å‹ç›®å½•: {self.base_path}")
    
    def load_registry(self):
        """åŠ è½½æ¨¡å‹æ³¨å†Œè¡¨"""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ³¨å†Œè¡¨å¤±è´¥: {e}")
                return {}
        return {}
    
    def save_registry(self):
        """ä¿å­˜æ¨¡å‹æ³¨å†Œè¡¨"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.model_registry, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ³¨å†Œè¡¨å¤±è´¥: {e}")
    
    def calculate_model_hash(self, model):
        """è®¡ç®—æ¨¡å‹å“ˆå¸Œå€¼"""
        try:
            model_bytes = pickle.dumps(model)
            return hashlib.md5(model_bytes).hexdigest()
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—å“ˆå¸Œå¤±è´¥: {e}")
            return None
    
    def save_model(self, model, model_name, version=None, metadata=None, 
                   compression=True, format_type='joblib'):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        print(f"\n=== ä¿å­˜æ¨¡å‹: {model_name} ===")
        
        # ç”Ÿæˆç‰ˆæœ¬å·
        if version is None:
            version = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        model_dir = os.path.join(self.base_path, model_name)
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        
        # ç¡®å®šæ–‡ä»¶æ ¼å¼å’Œè·¯å¾„
        if format_type == 'joblib':
            if compression:
                model_file = os.path.join(model_dir, f"{model_name}_v{version}.joblib")
                joblib.dump(model, model_file, compress=3)
            else:
                model_file = os.path.join(model_dir, f"{model_name}_v{version}.joblib")
                joblib.dump(model, model_file)
        elif format_type == 'pickle':
            model_file = os.path.join(model_dir, f"{model_name}_v{version}.pkl")
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")
        
        # è®¡ç®—æ–‡ä»¶å¤§å°å’Œæ¨¡å‹å“ˆå¸Œ
        file_size = os.path.getsize(model_file)
        model_hash = self.calculate_model_hash(model)
        
        # å‡†å¤‡å…ƒæ•°æ®
        model_metadata = {
            'model_name': model_name,
            'version': version,
            'file_path': model_file,
            'format': format_type,
            'compression': compression,
            'file_size_bytes': file_size,
            'file_size_mb': round(file_size / (1024 * 1024), 2),
            'model_hash': model_hash,
            'model_type': type(model).__name__,
            'save_time': datetime.now().isoformat(),
            'metadata': metadata or {}
        }
        
        # æ·»åŠ æ¨¡å‹ç‰¹å®šä¿¡æ¯
        if hasattr(model, 'get_params'):
            model_metadata['parameters'] = model.get_params()
        
        if hasattr(model, 'feature_importances_'):
            model_metadata['has_feature_importance'] = True
        
        # æ›´æ–°æ³¨å†Œè¡¨
        if model_name not in self.model_registry:
            self.model_registry[model_name] = {'versions': {}}
        
        self.model_registry[model_name]['versions'][version] = model_metadata
        self.model_registry[model_name]['latest_version'] = version
        
        # ä¿å­˜æ³¨å†Œè¡¨
        self.save_registry()
        
        print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ")
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {model_file}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {model_metadata['file_size_mb']} MB")
        print(f"ğŸ”‘ æ¨¡å‹å“ˆå¸Œ: {model_hash[:8]}...")
        print(f"ğŸ“… ä¿å­˜æ—¶é—´: {model_metadata['save_time']}")
        
        return model_metadata
    
    def load_model(self, model_name, version=None):
        """ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½æ¨¡å‹"""
        print(f"\n=== åŠ è½½æ¨¡å‹: {model_name} ===")
        
        if model_name not in self.model_registry:
            raise ValueError(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
        
        # ç¡®å®šç‰ˆæœ¬
        if version is None:
            version = self.model_registry[model_name]['latest_version']
            print(f"ğŸ”„ ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬: {version}")
        
        if version not in self.model_registry[model_name]['versions']:
            available_versions = list(self.model_registry[model_name]['versions'].keys())
            raise ValueError(f"ç‰ˆæœ¬ {version} ä¸å­˜åœ¨ã€‚å¯ç”¨ç‰ˆæœ¬: {available_versions}")
        
        # è·å–æ¨¡å‹å…ƒæ•°æ®
        model_metadata = self.model_registry[model_name]['versions'][version]
        model_file = model_metadata['file_path']
        
        if not os.path.exists(model_file):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
        
        # åŠ è½½æ¨¡å‹
        try:
            start_time = datetime.now()
            
            if model_metadata['format'] == 'joblib':
                model = joblib.load(model_file)
            elif model_metadata['format'] == 'pickle':
                with open(model_file, 'rb') as f:
                    model = pickle.load(f)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {model_metadata['format']}")
            
            load_time = (datetime.now() - start_time).total_seconds()
            
            # éªŒè¯æ¨¡å‹å“ˆå¸Œï¼ˆå¯é€‰ï¼‰
            current_hash = self.calculate_model_hash(model)
            if current_hash != model_metadata['model_hash']:
                warnings.warn(f"æ¨¡å‹å“ˆå¸Œä¸åŒ¹é…ï¼Œå¯èƒ½æ–‡ä»¶å·²æŸå")
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {load_time:.3f}ç§’)")
            print(f"ğŸ“Š æ¨¡å‹ç±»å‹: {model_metadata['model_type']}")
            print(f"ğŸ“… ä¿å­˜æ—¶é—´: {model_metadata['save_time']}")
            print(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {model_metadata['file_size_mb']} MB")
            
            return model, model_metadata
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰å·²ä¿å­˜çš„æ¨¡å‹"""
        print("\n=== æ¨¡å‹æ³¨å†Œè¡¨ ===")
        
        if not self.model_registry:
            print("ğŸ“­ æ²¡æœ‰å·²ä¿å­˜çš„æ¨¡å‹")
            return None
        
        models_info = []
        
        for model_name, model_info in self.model_registry.items():
            latest_version = model_info['latest_version']
            latest_metadata = model_info['versions'][latest_version]
            
            models_info.append({
                'model_name': model_name,
                'latest_version': latest_version,
                'model_type': latest_metadata['model_type'],
                'file_size_mb': latest_metadata['file_size_mb'],
                'save_time': latest_metadata['save_time'],
                'total_versions': len(model_info['versions'])
            })
        
        models_df = pd.DataFrame(models_info)
        models_df = models_df.sort_values('save_time', ascending=False)
        
        print("ğŸ“‹ æ¨¡å‹åˆ—è¡¨:")
        print(models_df.to_string(index=False))
        
        return models_df
    
    def get_model_versions(self, model_name):
        """è·å–æ¨¡å‹çš„æ‰€æœ‰ç‰ˆæœ¬ä¿¡æ¯"""
        if model_name not in self.model_registry:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return None
        
        print(f"\n=== {model_name} ç‰ˆæœ¬å†å² ===")
        
        versions_info = []
        for version, metadata in self.model_registry[model_name]['versions'].items():
            versions_info.append({
                'version': version,
                'model_type': metadata['model_type'],
                'file_size_mb': metadata['file_size_mb'],
                'save_time': metadata['save_time'],
                'format': metadata['format'],
                'compression': metadata['compression']
            })
        
        versions_df = pd.DataFrame(versions_info)
        versions_df = versions_df.sort_values('save_time', ascending=False)
        
        print("ğŸ“Š ç‰ˆæœ¬åˆ—è¡¨:")
        print(versions_df.to_string(index=False))
        
        return versions_df
    
    def delete_model_version(self, model_name, version):
        """åˆ é™¤æŒ‡å®šç‰ˆæœ¬çš„æ¨¡å‹"""
        print(f"\n=== åˆ é™¤æ¨¡å‹ç‰ˆæœ¬: {model_name} v{version} ===")
        
        if model_name not in self.model_registry:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return False
        
        if version not in self.model_registry[model_name]['versions']:
            print(f"âŒ ç‰ˆæœ¬ {version} ä¸å­˜åœ¨")
            return False
        
        # è·å–æ–‡ä»¶è·¯å¾„
        model_metadata = self.model_registry[model_name]['versions'][version]
        model_file = model_metadata['file_path']
        
        try:
            # åˆ é™¤æ–‡ä»¶
            if os.path.exists(model_file):
                os.remove(model_file)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ–‡ä»¶: {model_file}")
            
            # ä»æ³¨å†Œè¡¨ä¸­ç§»é™¤
            del self.model_registry[model_name]['versions'][version]
            
            # å¦‚æœæ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ›´æ–°æœ€æ–°ç‰ˆæœ¬æŒ‡é’ˆ
            if self.model_registry[model_name]['latest_version'] == version:
                remaining_versions = list(self.model_registry[model_name]['versions'].keys())
                if remaining_versions:
                    # é€‰æ‹©æœ€æ–°çš„ç‰ˆæœ¬ä½œä¸ºæ–°çš„latest_version
                    latest_time = None
                    latest_ver = None
                    for ver in remaining_versions:
                        ver_time = self.model_registry[model_name]['versions'][ver]['save_time']
                        if latest_time is None or ver_time > latest_time:
                            latest_time = ver_time
                            latest_ver = ver
                    self.model_registry[model_name]['latest_version'] = latest_ver
                else:
                    # å¦‚æœæ²¡æœ‰å‰©ä½™ç‰ˆæœ¬ï¼Œåˆ é™¤æ•´ä¸ªæ¨¡å‹æ¡ç›®
                    del self.model_registry[model_name]
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ¨¡å‹ {model_name} (æ— å‰©ä½™ç‰ˆæœ¬)")
            
            # ä¿å­˜æ³¨å†Œè¡¨
            self.save_registry()
            
            print(f"âœ… ç‰ˆæœ¬ {version} åˆ é™¤æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ é™¤å¤±è´¥: {e}")
            return False
    
    def cleanup_old_versions(self, model_name, keep_latest=3):
        """æ¸…ç†æ—§ç‰ˆæœ¬ï¼Œåªä¿ç•™æœ€æ–°çš„å‡ ä¸ªç‰ˆæœ¬"""
        print(f"\n=== æ¸…ç†æ—§ç‰ˆæœ¬: {model_name} (ä¿ç•™æœ€æ–°{keep_latest}ä¸ª) ===")
        
        if model_name not in self.model_registry:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return
        
        versions = self.model_registry[model_name]['versions']
        
        if len(versions) <= keep_latest:
            print(f"ğŸ“‹ å½“å‰ç‰ˆæœ¬æ•°({len(versions)}) <= ä¿ç•™æ•°({keep_latest})ï¼Œæ— éœ€æ¸…ç†")
            return
        
        # æŒ‰ä¿å­˜æ—¶é—´æ’åº
        sorted_versions = sorted(versions.items(), 
                               key=lambda x: x[1]['save_time'], 
                               reverse=True)
        
        # åˆ é™¤å¤šä½™çš„ç‰ˆæœ¬
        deleted_count = 0
        for version, metadata in sorted_versions[keep_latest:]:
            if self.delete_model_version(model_name, version):
                deleted_count += 1
        
        print(f"ğŸ§¹ æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§ç‰ˆæœ¬")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹åºåˆ—åŒ–å™¨
    serializer = ModelSerializer("./saved_models")
    
    # å‡è®¾æœ‰ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.datasets import make_classification
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®å’Œæ¨¡å‹
    X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X, y)
    
    # ä¿å­˜æ¨¡å‹
    metadata = {
        'dataset': 'synthetic_classification',
        'features': 20,
        'samples': 1000,
        'accuracy': model.score(X, y)
    }
    
    serializer.save_model(
        model=model,
        model_name="random_forest_classifier",
        metadata=metadata,
        compression=True
    )
    
    # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
    serializer.list_models()
    
    # åŠ è½½æ¨¡å‹
    loaded_model, loaded_metadata = serializer.load_model("random_forest_classifier")
    
    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    print(f"\nğŸ” æ¨¡å‹éªŒè¯:")
    print(f"åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {model.score(X, y):.4f}")
    print(f"åŠ è½½æ¨¡å‹å‡†ç¡®ç‡: {loaded_model.score(X, y):.4f}")
```

## 4.5.2 Web APIå¼€å‘ä¸éƒ¨ç½²

### Flask RESTful APIæœåŠ¡

```python
# Flask Web API å¼€å‘
from flask import Flask, request, jsonify
from flask_cors import CORS
import numpy as np
import pandas as pd
from datetime import datetime
import logging
import traceback
import os
from functools import wraps
import time

class ModelAPIServer:
    """æ¨¡å‹APIæœåŠ¡å™¨"""
    
    def __init__(self, model_serializer, host='0.0.0.0', port=5000, debug=False):
        self.app = Flask(__name__)
        CORS(self.app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ
        
        self.serializer = model_serializer
        self.host = host
        self.port = port
        self.debug = debug
        
        # åŠ è½½çš„æ¨¡å‹ç¼“å­˜
        self.loaded_models = {}
        self.model_metadata = {}
        
        # è¯·æ±‚ç»Ÿè®¡
        self.request_stats = {
            'total_requests': 0,
            'successful_predictions': 0,
            'failed_predictions': 0,
            'start_time': datetime.now()
        }
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # æ³¨å†Œè·¯ç”±
        self.register_routes()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('model_api.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log_request(self, f):
        """è¯·æ±‚æ—¥å¿—è£…é¥°å™¨"""
        @wraps(f)
        def decorated_function(*args, **kwargs):
            start_time = time.time()
            self.request_stats['total_requests'] += 1
            
            try:
                result = f(*args, **kwargs)
                duration = time.time() - start_time
                
                self.logger.info(f"Request to {request.endpoint} completed in {duration:.3f}s")
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                self.logger.error(f"Request to {request.endpoint} failed in {duration:.3f}s: {str(e)}")
                raise
        
        return decorated_function
    
    def register_routes(self):
        """æ³¨å†ŒAPIè·¯ç”±"""
        
        @self.app.route('/health', methods=['GET'])
        @self.log_request
        def health_check():
            """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
            return jsonify({
                'status': 'healthy',
                'timestamp': datetime.now().isoformat(),
                'uptime_seconds': (datetime.now() - self.request_stats['start_time']).total_seconds()
            })
        
        @self.app.route('/models', methods=['GET'])
        @self.log_request
        def list_models():
            """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
            try:
                models_df = self.serializer.list_models()
                if models_df is not None:
                    return jsonify({
                        'status': 'success',
                        'models': models_df.to_dict('records')
                    })
                else:
                    return jsonify({
                        'status': 'success',
                        'models': []
                    })
            except Exception as e:
                return jsonify({
                    'status': 'error',
                    'message': str(e)
                }), 500
        
        @self.app.route('/models/<model_name>/load', methods=['POST'])
        @self.log_request
        def load_model(model_name):
            """åŠ è½½æŒ‡å®šæ¨¡å‹åˆ°å†…å­˜"""
            try:
                data = request.get_json() or {}
                version = data.get('version', None)
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
                model_key = f"{model_name}_{version or 'latest'}"
                if model_key in self.loaded_models:
                    return jsonify({
                        'status': 'success',
                        'message': f'æ¨¡å‹ {model_name} å·²åœ¨å†…å­˜ä¸­',
                        'metadata': self.model_metadata[model_key]
                    })
                
                # åŠ è½½æ¨¡å‹
                model, metadata = self.serializer.load_model(model_name, version)
                
                # ç¼“å­˜æ¨¡å‹
                self.loaded_models[model_key] = model
                self.model_metadata[model_key] = metadata
                
                return jsonify({
                    'status': 'success',
                    'message': f'æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ',
                    'metadata': metadata
                })
                
            except Exception as e:
                return jsonify({
                    'status': 'error',
                    'message': str(e)
                }), 500
        
        @self.app.route('/models/<model_name>/predict', methods=['POST'])
        @self.log_request
        def predict(model_name):
            """æ¨¡å‹é¢„æµ‹ç«¯ç‚¹"""
            try:
                data = request.get_json()
                if not data:
                    return jsonify({
                        'status': 'error',
                        'message': 'è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º'
                    }), 400
                
                version = data.get('version', None)
                features = data.get('features')
                
                if features is None:
                    return jsonify({
                        'status': 'error',
                        'message': 'ç¼ºå°‘featureså­—æ®µ'
                    }), 400
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
                model_key = f"{model_name}_{version or 'latest'}"
                if model_key not in self.loaded_models:
                    # å°è¯•è‡ªåŠ¨åŠ è½½æ¨¡å‹
                    try:
                        model, metadata = self.serializer.load_model(model_name, version)
                        self.loaded_models[model_key] = model
                        self.model_metadata[model_key] = metadata
                    except Exception as e:
                        return jsonify({
                            'status': 'error',
                            'message': f'æ¨¡å‹æœªåŠ è½½ä¸”è‡ªåŠ¨åŠ è½½å¤±è´¥: {str(e)}'
                        }), 404
                
                model = self.loaded_models[model_key]
                
                # é¢„å¤„ç†è¾“å…¥æ•°æ®
                if isinstance(features, list):
                    if isinstance(features[0], list):
                        # å¤šä¸ªæ ·æœ¬
                        X = np.array(features)
                    else:
                        # å•ä¸ªæ ·æœ¬
                        X = np.array([features])
                elif isinstance(features, dict):
                    # ç‰¹å¾å­—å…¸æ ¼å¼
                    X = pd.DataFrame([features])
                else:
                    return jsonify({
                        'status': 'error',
                        'message': 'ä¸æ”¯æŒçš„featuresæ ¼å¼'
                    }), 400
                
                # æ‰§è¡Œé¢„æµ‹
                start_time = time.time()
                predictions = model.predict(X)
                prediction_time = time.time() - start_time
                
                # è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœæ”¯æŒï¼‰
                probabilities = None
                if hasattr(model, 'predict_proba'):
                    try:
                        probabilities = model.predict_proba(X).tolist()
                    except:
                        pass
                
                # æ ¼å¼åŒ–ç»“æœ
                result = {
                    'status': 'success',
                    'predictions': predictions.tolist(),
                    'prediction_time_seconds': round(prediction_time, 4),
                    'model_metadata': {
                        'model_name': model_name,
                        'version': self.model_metadata[model_key]['version'],
                        'model_type': self.model_metadata[model_key]['model_type']
                    }
                }
                
                if probabilities is not None:
                    result['probabilities'] = probabilities
                
                self.request_stats['successful_predictions'] += 1
                return jsonify(result)
                
            except Exception as e:
                self.request_stats['failed_predictions'] += 1
                self.logger.error(f"Prediction error: {str(e)}\n{traceback.format_exc()}")
                return jsonify({
                    'status': 'error',
                    'message': str(e)
                }), 500
        
        @self.app.route('/models/<model_name>/batch_predict', methods=['POST'])
        @self.log_request
        def batch_predict(model_name):
            """æ‰¹é‡é¢„æµ‹ç«¯ç‚¹"""
            try:
                data = request.get_json()
                if not data:
                    return jsonify({
                        'status': 'error',
                        'message': 'è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º'
                    }), 400
                
                version = data.get('version', None)
                batch_features = data.get('batch_features')
                batch_size = data.get('batch_size', 100)
                
                if batch_features is None:
                    return jsonify({
                        'status': 'error',
                        'message': 'ç¼ºå°‘batch_featureså­—æ®µ'
                    }), 400
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
                model_key = f"{model_name}_{version or 'latest'}"
                if model_key not in self.loaded_models:
                    return jsonify({
                        'status': 'error',
                        'message': f'æ¨¡å‹ {model_name} æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨loadæ¥å£'
                    }), 404
                
                model = self.loaded_models[model_key]
                
                # è½¬æ¢è¾“å…¥æ•°æ®
                X = np.array(batch_features)
                total_samples = len(X)
                
                # åˆ†æ‰¹å¤„ç†
                all_predictions = []
                all_probabilities = []
                
                start_time = time.time()
                
                for i in range(0, total_samples, batch_size):
                    batch_X = X[i:i+batch_size]
                    
                    # é¢„æµ‹
                    batch_predictions = model.predict(batch_X)
                    all_predictions.extend(batch_predictions.tolist())
                    
                    # é¢„æµ‹æ¦‚ç‡
                    if hasattr(model, 'predict_proba'):
                        try:
                            batch_proba = model.predict_proba(batch_X)
                            all_probabilities.extend(batch_proba.tolist())
                        except:
                            pass
                
                prediction_time = time.time() - start_time
                
                result = {
                    'status': 'success',
                    'predictions': all_predictions,
                    'total_samples': total_samples,
                    'prediction_time_seconds': round(prediction_time, 4),
                    'throughput_samples_per_second': round(total_samples / prediction_time, 2)
                }
                
                if all_probabilities:
                    result['probabilities'] = all_probabilities
                
                self.request_stats['successful_predictions'] += total_samples
                return jsonify(result)
                
            except Exception as e:
                self.request_stats['failed_predictions'] += 1
                return jsonify({
                    'status': 'error',
                    'message': str(e)
                }), 500
        
        @self.app.route('/stats', methods=['GET'])
        @self.log_request
        def get_stats():
            """è·å–APIç»Ÿè®¡ä¿¡æ¯"""
            uptime = datetime.now() - self.request_stats['start_time']
            
            return jsonify({
                'status': 'success',
                'stats': {
                    'total_requests': self.request_stats['total_requests'],
                    'successful_predictions': self.request_stats['successful_predictions'],
                    'failed_predictions': self.request_stats['failed_predictions'],
                    'success_rate': round(
                        self.request_stats['successful_predictions'] / 
                        max(1, self.request_stats['successful_predictions'] + self.request_stats['failed_predictions']) * 100, 2
                    ),
                    'uptime_seconds': uptime.total_seconds(),
                    'uptime_formatted': str(uptime),
                    'loaded_models': list(self.loaded_models.keys()),
                    'memory_usage_mb': self.get_memory_usage()
                }
            })
        
        @self.app.errorhandler(404)
        def not_found(error):
            return jsonify({
                'status': 'error',
                'message': 'APIç«¯ç‚¹ä¸å­˜åœ¨'
            }), 404
        
        @self.app.errorhandler(500)
        def internal_error(error):
            return jsonify({
                'status': 'error',
                'message': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'
            }), 500
    
    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            process = psutil.Process(os.getpid())
            return round(process.memory_info().rss / 1024 / 1024, 2)
        except ImportError:
            return 'N/A (psutil not installed)'
    
    def run(self):
        """å¯åŠ¨APIæœåŠ¡å™¨"""
        print(f"\nğŸš€ å¯åŠ¨æ¨¡å‹APIæœåŠ¡å™¨")
        print(f"ğŸŒ åœ°å€: http://{self.host}:{self.port}")
        print(f"ğŸ“‹ å¯ç”¨ç«¯ç‚¹:")
        print(f"   GET  /health - å¥åº·æ£€æŸ¥")
        print(f"   GET  /models - åˆ—å‡ºæ‰€æœ‰æ¨¡å‹")
        print(f"   POST /models/<name>/load - åŠ è½½æ¨¡å‹")
        print(f"   POST /models/<name>/predict - å•æ¬¡é¢„æµ‹")
        print(f"   POST /models/<name>/batch_predict - æ‰¹é‡é¢„æµ‹")
        print(f"   GET  /stats - è·å–ç»Ÿè®¡ä¿¡æ¯")
        print(f"\nğŸ’¡ ä½¿ç”¨ Ctrl+C åœæ­¢æœåŠ¡å™¨")
        
        try:
            self.app.run(
                host=self.host,
                port=self.port,
                debug=self.debug,
                threaded=True
            )
        except KeyboardInterrupt:
            print("\nğŸ›‘ æœåŠ¡å™¨å·²åœæ­¢")

# APIå®¢æˆ·ç«¯ç¤ºä¾‹
class ModelAPIClient:
    """æ¨¡å‹APIå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url="http://localhost:5000"):
        self.base_url = base_url.rstrip('/')
        import requests
        self.session = requests.Session()
    
    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        response = self.session.get(f"{self.base_url}/health")
        return response.json()
    
    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        response = self.session.get(f"{self.base_url}/models")
        return response.json()
    
    def load_model(self, model_name, version=None):
        """åŠ è½½æ¨¡å‹"""
        data = {}
        if version:
            data['version'] = version
        
        response = self.session.post(
            f"{self.base_url}/models/{model_name}/load",
            json=data
        )
        return response.json()
    
    def predict(self, model_name, features, version=None):
        """å•æ¬¡é¢„æµ‹"""
        data = {
            'features': features
        }
        if version:
            data['version'] = version
        
        response = self.session.post(
            f"{self.base_url}/models/{model_name}/predict",
            json=data
        )
        return response.json()
    
    def batch_predict(self, model_name, batch_features, version=None, batch_size=100):
        """æ‰¹é‡é¢„æµ‹"""
        data = {
            'batch_features': batch_features,
            'batch_size': batch_size
        }
        if version:
            data['version'] = version
        
        response = self.session.post(
            f"{self.base_url}/models/{model_name}/batch_predict",
            json=data
        )
        return response.json()
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        response = self.session.get(f"{self.base_url}/stats")
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºAPIæœåŠ¡å™¨
    serializer = ModelSerializer("./saved_models")
    api_server = ModelAPIServer(serializer, host='0.0.0.0', port=5000)
    
    # å¯åŠ¨æœåŠ¡å™¨
    api_server.run()
```

## 4.5.3 å®¹å™¨åŒ–éƒ¨ç½²ä¸ç¼–æ’

### Dockerå®¹å™¨åŒ–é…ç½®

```dockerfile
# Dockerfile - æ¨¡å‹APIæœåŠ¡å®¹å™¨åŒ–
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºæ¨¡å‹ç›®å½•
RUN mkdir -p /app/saved_models

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV FLASK_APP=model_api.py
ENV FLASK_ENV=production
ENV PYTHONPATH=/app

# æš´éœ²ç«¯å£
EXPOSE 5000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "model_api.py"]
```

```yaml
# docker-compose.yml - æœåŠ¡ç¼–æ’é…ç½®
version: '3.8'

services:
  model-api:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./saved_models:/app/saved_models
      - ./logs:/app/logs
    environment:
      - FLASK_ENV=production
      - MODEL_PATH=/app/saved_models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - model-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - model-api
    restart: unless-stopped
    networks:
      - model-network

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    networks:
      - model-network

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped
    networks:
      - model-network

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    restart: unless-stopped
    networks:
      - model-network

volumes:
  redis-data:
  prometheus-data:
  grafana-data:

networks:
  model-network:
    driver: bridge
```

```nginx
# nginx.conf - åå‘ä»£ç†é…ç½®
events {
    worker_connections 1024;
}

http {
    upstream model_api {
        server model-api:5000;
    }
    
    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    
    server {
        listen 80;
        server_name localhost;
        
        # é‡å®šå‘åˆ°HTTPS
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name localhost;
        
        # SSLé…ç½®
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        
        # å®‰å…¨å¤´
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        
        # APIä»£ç†
        location /api/ {
            # é™æµ
            limit_req zone=api_limit burst=20 nodelay;
            
            # ä»£ç†è®¾ç½®
            proxy_pass http://model_api/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 30s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
            
            # ç¼“å­˜è®¾ç½®
            proxy_cache_bypass $http_upgrade;
        }
        
        # å¥åº·æ£€æŸ¥
        location /health {
            proxy_pass http://model_api/health;
            access_log off;
        }
        
        # é™æ€æ–‡ä»¶
        location /static/ {
            alias /app/static/;
            expires 1d;
            add_header Cache-Control "public, immutable";
        }
    }
}
```

### Kuberneteséƒ¨ç½²é…ç½®

```python
# k8s_deployment_generator.py - Kubernetesé…ç½®ç”Ÿæˆå™¨
import yaml
from datetime import datetime

class KubernetesDeploymentGenerator:
    """Kuberneteséƒ¨ç½²é…ç½®ç”Ÿæˆå™¨"""
    
    def __init__(self, app_name="model-api", namespace="default"):
        self.app_name = app_name
        self.namespace = namespace
    
    def generate_deployment(self, image_name, replicas=3, cpu_request="100m", 
                          memory_request="256Mi", cpu_limit="500m", memory_limit="512Mi"):
        """ç”ŸæˆDeploymenté…ç½®"""
        deployment = {
            'apiVersion': 'apps/v1',
            'kind': 'Deployment',
            'metadata': {
                'name': self.app_name,
                'namespace': self.namespace,
                'labels': {
                    'app': self.app_name,
                    'version': 'v1'
                }
            },
            'spec': {
                'replicas': replicas,
                'selector': {
                    'matchLabels': {
                        'app': self.app_name
                    }
                },
                'template': {
                    'metadata': {
                        'labels': {
                            'app': self.app_name,
                            'version': 'v1'
                        }
                    },
                    'spec': {
                        'containers': [{
                            'name': self.app_name,
                            'image': image_name,
                            'ports': [{
                                'containerPort': 5000,
                                'name': 'http'
                            }],
                            'env': [
                                {
                                    'name': 'FLASK_ENV',
                                    'value': 'production'
                                },
                                {
                                    'name': 'MODEL_PATH',
                                    'value': '/app/saved_models'
                                }
                            ],
                            'resources': {
                                'requests': {
                                    'cpu': cpu_request,
                                    'memory': memory_request
                                },
                                'limits': {
                                    'cpu': cpu_limit,
                                    'memory': memory_limit
                                }
                            },
                            'livenessProbe': {
                                'httpGet': {
                                    'path': '/health',
                                    'port': 5000
                                },
                                'initialDelaySeconds': 30,
                                'periodSeconds': 10
                            },
                            'readinessProbe': {
                                'httpGet': {
                                    'path': '/health',
                                    'port': 5000
                                },
                                'initialDelaySeconds': 5,
                                'periodSeconds': 5
                            },
                            'volumeMounts': [{
                                'name': 'model-storage',
                                'mountPath': '/app/saved_models'
                            }]
                        }],
                        'volumes': [{
                            'name': 'model-storage',
                            'persistentVolumeClaim': {
                                'claimName': f'{self.app_name}-pvc'
                            }
                        }]
                    }
                }
            }
        }
        return deployment
    
    def generate_service(self, service_type="ClusterIP", port=80):
        """ç”ŸæˆServiceé…ç½®"""
        service = {
            'apiVersion': 'v1',
            'kind': 'Service',
            'metadata': {
                'name': f'{self.app_name}-service',
                'namespace': self.namespace,
                'labels': {
                    'app': self.app_name
                }
            },
            'spec': {
                'type': service_type,
                'ports': [{
                    'port': port,
                    'targetPort': 5000,
                    'protocol': 'TCP',
                    'name': 'http'
                }],
                'selector': {
                    'app': self.app_name
                }
            }
        }
        return service
    
    def generate_ingress(self, host, tls_secret=None):
        """ç”ŸæˆIngressé…ç½®"""
        ingress = {
            'apiVersion': 'networking.k8s.io/v1',
            'kind': 'Ingress',
            'metadata': {
                'name': f'{self.app_name}-ingress',
                'namespace': self.namespace,
                'annotations': {
                    'nginx.ingress.kubernetes.io/rewrite-target': '/',
                    'nginx.ingress.kubernetes.io/rate-limit': '100',
                    'nginx.ingress.kubernetes.io/rate-limit-window': '1m'
                }
            },
            'spec': {
                'rules': [{
                    'host': host,
                    'http': {
                        'paths': [{
                            'path': '/',
                            'pathType': 'Prefix',
                            'backend': {
                                'service': {
                                    'name': f'{self.app_name}-service',
                                    'port': {
                                        'number': 80
                                    }
                                }
                            }
                        }]
                    }
                }]
            }
        }
        
        if tls_secret:
            ingress['spec']['tls'] = [{
                'hosts': [host],
                'secretName': tls_secret
            }]
        
        return ingress
    
    def generate_hpa(self, min_replicas=2, max_replicas=10, cpu_threshold=70):
        """ç”ŸæˆHorizontalPodAutoscaleré…ç½®"""
        hpa = {
            'apiVersion': 'autoscaling/v2',
            'kind': 'HorizontalPodAutoscaler',
            'metadata': {
                'name': f'{self.app_name}-hpa',
                'namespace': self.namespace
            },
            'spec': {
                'scaleTargetRef': {
                    'apiVersion': 'apps/v1',
                    'kind': 'Deployment',
                    'name': self.app_name
                },
                'minReplicas': min_replicas,
                'maxReplicas': max_replicas,
                'metrics': [{
                    'type': 'Resource',
                    'resource': {
                        'name': 'cpu',
                        'target': {
                            'type': 'Utilization',
                            'averageUtilization': cpu_threshold
                        }
                    }
                }]
            }
        }
        return hpa
    
    def generate_pvc(self, storage_size="10Gi", storage_class="standard"):
        """ç”ŸæˆPersistentVolumeClaimé…ç½®"""
        pvc = {
            'apiVersion': 'v1',
            'kind': 'PersistentVolumeClaim',
            'metadata': {
                'name': f'{self.app_name}-pvc',
                'namespace': self.namespace
            },
            'spec': {
                'accessModes': ['ReadWriteOnce'],
                'storageClassName': storage_class,
                'resources': {
                    'requests': {
                        'storage': storage_size
                    }
                }
            }
        }
        return pvc
    
    def save_all_configs(self, image_name, host, output_dir="./k8s"):
        """ç”Ÿæˆå¹¶ä¿å­˜æ‰€æœ‰Kubernetesé…ç½®æ–‡ä»¶"""
        import os
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        configs = {
            'deployment.yaml': self.generate_deployment(image_name),
            'service.yaml': self.generate_service(),
            'ingress.yaml': self.generate_ingress(host),
            'hpa.yaml': self.generate_hpa(),
            'pvc.yaml': self.generate_pvc()
        }
        
        for filename, config in configs.items():
            filepath = os.path.join(output_dir, filename)
            with open(filepath, 'w') as f:
                yaml.dump(config, f, default_flow_style=False)
            print(f"âœ… ç”Ÿæˆé…ç½®æ–‡ä»¶: {filepath}")
        
        # ç”Ÿæˆéƒ¨ç½²è„šæœ¬
        deploy_script = f"""#!/bin/bash
# Kuberneteséƒ¨ç½²è„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}

echo "ğŸš€ å¼€å§‹éƒ¨ç½² {self.app_name}"

# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace {self.namespace} --dry-run=client -o yaml | kubectl apply -f -

# åº”ç”¨é…ç½®
kubectl apply -f {output_dir}/pvc.yaml
kubectl apply -f {output_dir}/deployment.yaml
kubectl apply -f {output_dir}/service.yaml
kubectl apply -f {output_dir}/ingress.yaml
kubectl apply -f {output_dir}/hpa.yaml

# ç­‰å¾…éƒ¨ç½²å®Œæˆ
echo "â³ ç­‰å¾…éƒ¨ç½²å®Œæˆ..."
kubectl rollout status deployment/{self.app_name} -n {self.namespace}

# æ˜¾ç¤ºéƒ¨ç½²çŠ¶æ€
echo "ğŸ“Š éƒ¨ç½²çŠ¶æ€:"
kubectl get pods -n {self.namespace} -l app={self.app_name}
kubectl get svc -n {self.namespace} -l app={self.app_name}
kubectl get ingress -n {self.namespace}

echo "âœ… éƒ¨ç½²å®Œæˆ!"
"""
        
        script_path = os.path.join(output_dir, 'deploy.sh')
        with open(script_path, 'w') as f:
            f.write(deploy_script)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_path, 0o755)
        print(f"âœ… ç”Ÿæˆéƒ¨ç½²è„šæœ¬: {script_path}")
        
        return configs

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    generator = KubernetesDeploymentGenerator("model-api", "ml-models")
    
    configs = generator.save_all_configs(
        image_name="your-registry/model-api:latest",
        host="api.yourcompany.com",
        output_dir="./k8s-configs"
    )
    
    print("\nğŸ¯ Kubernetesé…ç½®ç”Ÿæˆå®Œæˆ!")
    print("ğŸ“‹ éƒ¨ç½²æ­¥éª¤:")
    print("1. æ„å»ºå¹¶æ¨é€Dockeré•œåƒ")
    print("2. è¿è¡Œ ./k8s-configs/deploy.sh")
    print("3. é…ç½®DNSæŒ‡å‘Ingress IP")
```

## 4.5.4 æ€§èƒ½ç›‘æ§ä¸æ—¥å¿—ç®¡ç†

### æ€§èƒ½ç›‘æ§ç³»ç»Ÿ

```python
# monitoring.py - æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
import time
import psutil
import threading
from datetime import datetime, timedelta
import json
import os
from collections import deque, defaultdict
import matplotlib.pyplot as plt
import pandas as pd

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, monitor_interval=5, history_size=1000):
        self.monitor_interval = monitor_interval
        self.history_size = history_size
        
        # ç›‘æ§æ•°æ®å­˜å‚¨
        self.metrics_history = {
            'timestamp': deque(maxlen=history_size),
            'cpu_percent': deque(maxlen=history_size),
            'memory_percent': deque(maxlen=history_size),
            'memory_mb': deque(maxlen=history_size),
            'disk_io_read': deque(maxlen=history_size),
            'disk_io_write': deque(maxlen=history_size),
            'network_sent': deque(maxlen=history_size),
            'network_recv': deque(maxlen=history_size)
        }
        
        # APIæ€§èƒ½æŒ‡æ ‡
        self.api_metrics = {
            'request_count': 0,
            'total_response_time': 0,
            'error_count': 0,
            'response_times': deque(maxlen=1000),
            'requests_per_minute': deque(maxlen=60),
            'endpoint_stats': defaultdict(lambda: {
                'count': 0,
                'total_time': 0,
                'errors': 0
            })
        }
        
        # æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        self.model_metrics = defaultdict(lambda: {
            'prediction_count': 0,
            'total_prediction_time': 0,
            'prediction_times': deque(maxlen=1000),
            'error_count': 0,
            'last_used': None
        })
        
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        if self.monitoring:
            print("âš ï¸ ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        print(f"ğŸ” æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ (é—´éš”: {self.monitor_interval}ç§’)")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        print("ğŸ›‘ æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                self._collect_system_metrics()
                time.sleep(self.monitor_interval)
            except Exception as e:
                print(f"âŒ ç›‘æ§é”™è¯¯: {e}")
                time.sleep(self.monitor_interval)
    
    def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        now = datetime.now()
        
        # CPUå’Œå†…å­˜
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        
        # ç£ç›˜IO
        disk_io = psutil.disk_io_counters()
        
        # ç½‘ç»œIO
        network_io = psutil.net_io_counters()
        
        # å­˜å‚¨æŒ‡æ ‡
        self.metrics_history['timestamp'].append(now)
        self.metrics_history['cpu_percent'].append(cpu_percent)
        self.metrics_history['memory_percent'].append(memory.percent)
        self.metrics_history['memory_mb'].append(memory.used / 1024 / 1024)
        
        if disk_io:
            self.metrics_history['disk_io_read'].append(disk_io.read_bytes)
            self.metrics_history['disk_io_write'].append(disk_io.write_bytes)
        else:
            self.metrics_history['disk_io_read'].append(0)
            self.metrics_history['disk_io_write'].append(0)
        
        if network_io:
            self.metrics_history['network_sent'].append(network_io.bytes_sent)
            self.metrics_history['network_recv'].append(network_io.bytes_recv)
        else:
            self.metrics_history['network_sent'].append(0)
            self.metrics_history['network_recv'].append(0)
    
    def record_api_request(self, endpoint, response_time, success=True):
        """è®°å½•APIè¯·æ±‚"""
        self.api_metrics['request_count'] += 1
        self.api_metrics['total_response_time'] += response_time
        self.api_metrics['response_times'].append(response_time)
        
        if not success:
            self.api_metrics['error_count'] += 1
        
        # ç«¯ç‚¹ç»Ÿè®¡
        endpoint_stats = self.api_metrics['endpoint_stats'][endpoint]
        endpoint_stats['count'] += 1
        endpoint_stats['total_time'] += response_time
        if not success:
            endpoint_stats['errors'] += 1
        
        # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
        current_minute = datetime.now().replace(second=0, microsecond=0)
        if not self.api_metrics['requests_per_minute'] or \
           self.api_metrics['requests_per_minute'][-1][0] != current_minute:
            self.api_metrics['requests_per_minute'].append([current_minute, 1])
        else:
            self.api_metrics['requests_per_minute'][-1][1] += 1
    
    def record_model_prediction(self, model_name, prediction_time, success=True):
        """è®°å½•æ¨¡å‹é¢„æµ‹"""
        model_stats = self.model_metrics[model_name]
        model_stats['prediction_count'] += 1
        model_stats['total_prediction_time'] += prediction_time
        model_stats['prediction_times'].append(prediction_time)
        model_stats['last_used'] = datetime.now()
        
        if not success:
            model_stats['error_count'] += 1
    
    def get_system_summary(self):
        """è·å–ç³»ç»Ÿæ€§èƒ½æ‘˜è¦"""
        if not self.metrics_history['timestamp']:
            return None
        
        recent_data = {
            'cpu_avg': sum(list(self.metrics_history['cpu_percent'])[-10:]) / min(10, len(self.metrics_history['cpu_percent'])),
            'memory_avg': sum(list(self.metrics_history['memory_percent'])[-10:]) / min(10, len(self.metrics_history['memory_percent'])),
            'memory_mb': list(self.metrics_history['memory_mb'])[-1] if self.metrics_history['memory_mb'] else 0
        }
        
        return recent_data
    
    def get_api_summary(self):
        """è·å–APIæ€§èƒ½æ‘˜è¦"""
        if self.api_metrics['request_count'] == 0:
            return None
        
        avg_response_time = self.api_metrics['total_response_time'] / self.api_metrics['request_count']
        error_rate = self.api_metrics['error_count'] / self.api_metrics['request_count'] * 100
        
        # è®¡ç®—P95å“åº”æ—¶é—´
        response_times = sorted(self.api_metrics['response_times'])
        p95_index = int(len(response_times) * 0.95)
        p95_response_time = response_times[p95_index] if response_times else 0
        
        # æœ€è¿‘ä¸€åˆ†é’Ÿçš„è¯·æ±‚æ•°
        recent_rpm = 0
        if self.api_metrics['requests_per_minute']:
            recent_rpm = self.api_metrics['requests_per_minute'][-1][1]
        
        return {
            'total_requests': self.api_metrics['request_count'],
            'avg_response_time': avg_response_time,
            'p95_response_time': p95_response_time,
            'error_rate': error_rate,
            'requests_per_minute': recent_rpm
        }
    
    def get_model_summary(self):
        """è·å–æ¨¡å‹æ€§èƒ½æ‘˜è¦"""
        model_summaries = {}
        
        for model_name, stats in self.model_metrics.items():
            if stats['prediction_count'] > 0:
                avg_prediction_time = stats['total_prediction_time'] / stats['prediction_count']
                error_rate = stats['error_count'] / stats['prediction_count'] * 100
                
                # è®¡ç®—P95é¢„æµ‹æ—¶é—´
                prediction_times = sorted(stats['prediction_times'])
                p95_index = int(len(prediction_times) * 0.95)
                p95_prediction_time = prediction_times[p95_index] if prediction_times else 0
                
                model_summaries[model_name] = {
                    'prediction_count': stats['prediction_count'],
                    'avg_prediction_time': avg_prediction_time,
                    'p95_prediction_time': p95_prediction_time,
                    'error_rate': error_rate,
                    'last_used': stats['last_used'].isoformat() if stats['last_used'] else None
                }
        
        return model_summaries
    
    def generate_performance_report(self, figsize=(15, 12)):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n=== æ€§èƒ½ç›‘æ§æŠ¥å‘Š ===")
        
        if not self.metrics_history['timestamp']:
            print("âŒ æ²¡æœ‰ç›‘æ§æ•°æ®")
            return
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(3, 2, figsize=figsize)
        
        # è½¬æ¢æ—¶é—´æˆ³
        timestamps = list(self.metrics_history['timestamp'])
        
        # CPUä½¿ç”¨ç‡
        axes[0, 0].plot(timestamps, list(self.metrics_history['cpu_percent']), 'b-', linewidth=2)
        axes[0, 0].set_title('CPUä½¿ç”¨ç‡ (%)')
        axes[0, 0].set_ylabel('CPU %')
        axes[0, 0].grid(True, alpha=0.3)
        
        # å†…å­˜ä½¿ç”¨ç‡
        axes[0, 1].plot(timestamps, list(self.metrics_history['memory_percent']), 'g-', linewidth=2)
        axes[0, 1].set_title('å†…å­˜ä½¿ç”¨ç‡ (%)')
        axes[0, 1].set_ylabel('Memory %')
        axes[0, 1].grid(True, alpha=0.3)
        
        # APIå“åº”æ—¶é—´åˆ†å¸ƒ
        if self.api_metrics['response_times']:
            axes[1, 0].hist(list(self.api_metrics['response_times']), bins=50, alpha=0.7, color='orange')
            axes[1, 0].set_title('APIå“åº”æ—¶é—´åˆ†å¸ƒ')
            axes[1, 0].set_xlabel('å“åº”æ—¶é—´ (ç§’)')
            axes[1, 0].set_ylabel('é¢‘æ¬¡')
        
        # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
        if self.api_metrics['requests_per_minute']:
            rpm_times = [item[0] for item in self.api_metrics['requests_per_minute']]
            rpm_counts = [item[1] for item in self.api_metrics['requests_per_minute']]
            axes[1, 1].plot(rpm_times, rpm_counts, 'r-', marker='o', linewidth=2)
            axes[1, 1].set_title('æ¯åˆ†é’Ÿè¯·æ±‚æ•°')
            axes[1, 1].set_ylabel('è¯·æ±‚æ•°')
            axes[1, 1].grid(True, alpha=0.3)
        
        # æ¨¡å‹é¢„æµ‹æ—¶é—´å¯¹æ¯”
        if self.model_metrics:
            model_names = list(self.model_metrics.keys())
            avg_times = []
            for model_name in model_names:
                stats = self.model_metrics[model_name]
                if stats['prediction_count'] > 0:
                    avg_time = stats['total_prediction_time'] / stats['prediction_count']
                    avg_times.append(avg_time)
                else:
                    avg_times.append(0)
            
            axes[2, 0].bar(model_names, avg_times, color='purple', alpha=0.7)
            axes[2, 0].set_title('æ¨¡å‹å¹³å‡é¢„æµ‹æ—¶é—´')
            axes[2, 0].set_ylabel('é¢„æµ‹æ—¶é—´ (ç§’)')
            axes[2, 0].tick_params(axis='x', rotation=45)
        
        # ç«¯ç‚¹æ€§èƒ½å¯¹æ¯”
        if self.api_metrics['endpoint_stats']:
            endpoints = list(self.api_metrics['endpoint_stats'].keys())
            avg_times = []
            for endpoint in endpoints:
                stats = self.api_metrics['endpoint_stats'][endpoint]
                if stats['count'] > 0:
                    avg_time = stats['total_time'] / stats['count']
                    avg_times.append(avg_time)
                else:
                    avg_times.append(0)
            
            axes[2, 1].bar(endpoints, avg_times, color='teal', alpha=0.7)
            axes[2, 1].set_title('ç«¯ç‚¹å¹³å‡å“åº”æ—¶é—´')
            axes[2, 1].set_ylabel('å“åº”æ—¶é—´ (ç§’)')
            axes[2, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°æ‘˜è¦ä¿¡æ¯
        system_summary = self.get_system_summary()
        api_summary = self.get_api_summary()
        model_summary = self.get_model_summary()
        
        if system_summary:
            print(f"\nğŸ–¥ï¸ ç³»ç»Ÿæ€§èƒ½:")
            print(f"   CPUå¹³å‡ä½¿ç”¨ç‡: {system_summary['cpu_avg']:.1f}%")
            print(f"   å†…å­˜å¹³å‡ä½¿ç”¨ç‡: {system_summary['memory_avg']:.1f}%")
            print(f"   å½“å‰å†…å­˜ä½¿ç”¨: {system_summary['memory_mb']:.1f} MB")
        
        if api_summary:
            print(f"\nğŸŒ APIæ€§èƒ½:")
            print(f"   æ€»è¯·æ±‚æ•°: {api_summary['total_requests']}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {api_summary['avg_response_time']:.3f}ç§’")
            print(f"   P95å“åº”æ—¶é—´: {api_summary['p95_response_time']:.3f}ç§’")
            print(f"   é”™è¯¯ç‡: {api_summary['error_rate']:.2f}%")
            print(f"   æœ€è¿‘æ¯åˆ†é’Ÿè¯·æ±‚æ•°: {api_summary['requests_per_minute']}")
        
        if model_summary:
            print(f"\nğŸ¤– æ¨¡å‹æ€§èƒ½:")
            for model_name, stats in model_summary.items():
                print(f"   {model_name}:")
                print(f"     é¢„æµ‹æ¬¡æ•°: {stats['prediction_count']}")
                print(f"     å¹³å‡é¢„æµ‹æ—¶é—´: {stats['avg_prediction_time']:.3f}ç§’")
                print(f"     P95é¢„æµ‹æ—¶é—´: {stats['p95_prediction_time']:.3f}ç§’")
                print(f"     é”™è¯¯ç‡: {stats['error_rate']:.2f}%")
    
    def export_metrics(self, filepath):
        """å¯¼å‡ºç›‘æ§æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        export_data = {
            'export_time': datetime.now().isoformat(),
            'system_metrics': {
                'timestamps': [ts.isoformat() for ts in self.metrics_history['timestamp']],
                'cpu_percent': list(self.metrics_history['cpu_percent']),
                'memory_percent': list(self.metrics_history['memory_percent']),
                'memory_mb': list(self.metrics_history['memory_mb'])
            },
            'api_metrics': {
                'request_count': self.api_metrics['request_count'],
                'total_response_time': self.api_metrics['total_response_time'],
                'error_count': self.api_metrics['error_count'],
                'response_times': list(self.api_metrics['response_times']),
                'endpoint_stats': dict(self.api_metrics['endpoint_stats'])
            },
            'model_metrics': {
                model_name: {
                    'prediction_count': stats['prediction_count'],
                    'total_prediction_time': stats['total_prediction_time'],
                    'error_count': stats['error_count'],
                    'last_used': stats['last_used'].isoformat() if stats['last_used'] else None,
                    'prediction_times': list(stats['prediction_times'])
                }
                for model_name, stats in self.model_metrics.items()
            }
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        print(f"ğŸ“Š ç›‘æ§æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    monitor = PerformanceMonitor(monitor_interval=1)
    
    # å¯åŠ¨ç›‘æ§
    monitor.start_monitoring()
    
    # æ¨¡æ‹Ÿä¸€äº›APIè¯·æ±‚å’Œæ¨¡å‹é¢„æµ‹
    import random
    
    for i in range(100):
        # æ¨¡æ‹ŸAPIè¯·æ±‚
        response_time = random.uniform(0.1, 2.0)
        success = random.random() > 0.05  # 95%æˆåŠŸç‡
        monitor.record_api_request('/predict', response_time, success)
        
        # æ¨¡æ‹Ÿæ¨¡å‹é¢„æµ‹
        prediction_time = random.uniform(0.05, 0.5)
        model_success = random.random() > 0.02  # 98%æˆåŠŸç‡
        monitor.record_model_prediction('random_forest', prediction_time, model_success)
        
        time.sleep(0.1)
    
    # ç­‰å¾…æ”¶é›†ä¸€äº›ç³»ç»ŸæŒ‡æ ‡
    time.sleep(10)
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    monitor.generate_performance_report()
    
    # å¯¼å‡ºæŒ‡æ ‡
    monitor.export_metrics('performance_metrics.json')
    
    # åœæ­¢ç›‘æ§
    monitor.stop_monitoring()
```

## 4.5.5 æœ¬èŠ‚æ€»ç»“

æœ¬èŠ‚å…¨é¢ä»‹ç»äº†Traeç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²ä¸åº”ç”¨å®ä¾‹ï¼Œæ„å»ºäº†ä»æ¨¡å‹ä¿å­˜åˆ°ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å››ä¸ªæ ¸å¿ƒç³»ç»Ÿçš„å®ç°ï¼Œä¸ºAIå¼€å‘è€…æä¾›äº†ä¼ä¸šçº§çš„æ¨¡å‹éƒ¨ç½²èƒ½åŠ›ã€‚

### æ ¸å¿ƒæˆæœæ€»ç»“

| éƒ¨ç½²æ¨¡å— | ä¸»è¦åŠŸèƒ½ | æŠ€æœ¯ç‰¹è‰² | å®ç”¨ä»·å€¼ |
|---------|---------|---------|----------|
| **æ¨¡å‹åºåˆ—åŒ–ç®¡ç†** | æ™ºèƒ½ä¿å­˜ã€ç‰ˆæœ¬æ§åˆ¶ã€å¿«é€ŸåŠ è½½ | å¤šæ ¼å¼æ”¯æŒã€å“ˆå¸ŒéªŒè¯ã€å…ƒæ•°æ®ç®¡ç† | ç¡®ä¿æ¨¡å‹å®‰å…¨æ€§å’Œå¯è¿½æº¯æ€§ |
| **Web APIæœåŠ¡** | RESTfulæ¥å£ã€æ‰¹é‡é¢„æµ‹ã€æ€§èƒ½ç›‘æ§ | å¼‚æ­¥å¤„ç†ã€é”™è¯¯å¤„ç†ã€è¯·æ±‚é™æµ | æä¾›æ ‡å‡†åŒ–çš„æ¨¡å‹æœåŠ¡æ¥å£ |
| **å®¹å™¨åŒ–éƒ¨ç½²** | Dockerå®¹å™¨ã€K8sç¼–æ’ã€è‡ªåŠ¨æ‰©ç¼©å®¹ | å¾®æœåŠ¡æ¶æ„ã€è´Ÿè½½å‡è¡¡ã€å¥åº·æ£€æŸ¥ | å®ç°å¼¹æ€§ã€å¯æ‰©å±•çš„éƒ¨ç½²æ–¹æ¡ˆ |
| **æ€§èƒ½ç›‘æ§** | å®æ—¶ç›‘æ§ã€æŒ‡æ ‡æ”¶é›†ã€æŠ¥å‘Šç”Ÿæˆ | å¤šç»´åº¦ç›‘æ§ã€å¯è§†åŒ–åˆ†æã€å‘Šè­¦æœºåˆ¶ | ä¿éšœæœåŠ¡ç¨³å®šæ€§å’Œæ€§èƒ½ä¼˜åŒ– |

### æŠ€æœ¯äº®ç‚¹ä¸æœ€ä½³å®è·µ

```python
# éƒ¨ç½²æ¶æ„ä¼˜åŠ¿å±•ç¤º
deployment_advantages = {
    "å¯é æ€§ä¿éšœ": {
        "æ¨¡å‹ç‰ˆæœ¬ç®¡ç†": "å®Œæ•´çš„ç‰ˆæœ¬å†å²å’Œå›æ»šæœºåˆ¶",
        "å¥åº·æ£€æŸ¥": "å¤šå±‚æ¬¡çš„æœåŠ¡å¥åº·ç›‘æ§",
        "å®¹é”™å¤„ç†": "ä¼˜é›…çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶",
        "æ•°æ®éªŒè¯": "è¾“å…¥è¾“å‡ºæ•°æ®çš„å®Œæ•´æ€§æ ¡éªŒ"
    },
    
    "æ€§èƒ½ä¼˜åŒ–": {
        "æ¨¡å‹ç¼“å­˜": "å†…å­˜ä¸­çš„æ¨¡å‹ç¼“å­˜æœºåˆ¶",
        "æ‰¹é‡å¤„ç†": "é«˜æ•ˆçš„æ‰¹é‡é¢„æµ‹æ¥å£",
        "å¼‚æ­¥å¤„ç†": "éé˜»å¡çš„è¯·æ±‚å¤„ç†",
        "èµ„æºç›‘æ§": "å®æ—¶çš„èµ„æºä½¿ç”¨ç›‘æ§"
    },
    
    "æ‰©å±•èƒ½åŠ›": {
        "æ°´å¹³æ‰©å±•": "åŸºäºKubernetesçš„è‡ªåŠ¨æ‰©ç¼©å®¹",
        "è´Ÿè½½å‡è¡¡": "æ™ºèƒ½çš„è¯·æ±‚åˆ†å‘æœºåˆ¶",
        "å¤šæ¨¡å‹æ”¯æŒ": "åŒæ—¶éƒ¨ç½²å¤šä¸ªæ¨¡å‹ç‰ˆæœ¬",
        "A/Bæµ‹è¯•": "æ”¯æŒæ¨¡å‹ç‰ˆæœ¬å¯¹æ¯”æµ‹è¯•"
    },
    
    "è¿ç»´å‹å¥½": {
        "å®¹å™¨åŒ–": "æ ‡å‡†åŒ–çš„éƒ¨ç½²å’Œè¿è¡Œç¯å¢ƒ",
        "æ—¥å¿—ç®¡ç†": "ç»“æ„åŒ–çš„æ—¥å¿—è®°å½•å’Œåˆ†æ",
        "ç›‘æ§å‘Šè­¦": "å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»",
        "è‡ªåŠ¨åŒ–éƒ¨ç½²": "ä¸€é”®å¼çš„éƒ¨ç½²å’Œæ›´æ–°æµç¨‹"
    }
}

# å¯è§†åŒ–éƒ¨ç½²æ¶æ„ä¼˜åŠ¿
import matplotlib.pyplot as plt
import numpy as np

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# å¯é æ€§æŒ‡æ ‡
reliability_metrics = ['ç‰ˆæœ¬ç®¡ç†', 'å¥åº·æ£€æŸ¥', 'å®¹é”™å¤„ç†', 'æ•°æ®éªŒè¯']
reliability_scores = [0.95, 0.92, 0.88, 0.90]

axes[0, 0].bar(reliability_metrics, reliability_scores, color='green', alpha=0.7)
axes[0, 0].set_title('å¯é æ€§ä¿éšœæŒ‡æ ‡')
axes[0, 0].set_ylabel('å®Œæˆåº¦')
axes[0, 0].set_ylim(0, 1)
for i, v in enumerate(reliability_scores):
    axes[0, 0].text(i, v + 0.01, f'{v:.2f}', ha='center')

# æ€§èƒ½ä¼˜åŒ–æ•ˆæœ
performance_aspects = ['å“åº”æ—¶é—´', 'ååé‡', 'èµ„æºåˆ©ç”¨', 'å¹¶å‘å¤„ç†']
baseline_performance = [1.0, 1.0, 1.0, 1.0]
optimized_performance = [0.3, 3.2, 0.7, 5.0]

x = np.arange(len(performance_aspects))
width = 0.35

axes[0, 1].bar(x - width/2, baseline_performance, width, label='ä¼˜åŒ–å‰', alpha=0.7)
axes[0, 1].bar(x + width/2, optimized_performance, width, label='ä¼˜åŒ–å', alpha=0.7)
axes[0, 1].set_title('æ€§èƒ½ä¼˜åŒ–æ•ˆæœå¯¹æ¯”')
axes[0, 1].set_ylabel('ç›¸å¯¹æ€§èƒ½')
axes[0, 1].set_xticks(x)
axes[0, 1].set_xticklabels(performance_aspects)
axes[0, 1].legend()

# æ‰©å±•èƒ½åŠ›é›·è¾¾å›¾
categories = ['æ°´å¹³æ‰©å±•', 'è´Ÿè½½å‡è¡¡', 'å¤šæ¨¡å‹æ”¯æŒ', 'A/Bæµ‹è¯•', 'å¼¹æ€§ä¼¸ç¼©']
values = [0.9, 0.85, 0.95, 0.8, 0.88]

angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
values += values[:1]
angles = np.concatenate((angles, [angles[0]]))

axes[1, 0].plot(angles, values, 'o-', linewidth=2, color='blue')
axes[1, 0].fill(angles, values, alpha=0.25, color='blue')
axes[1, 0].set_xticks(angles[:-1])
axes[1, 0].set_xticklabels(categories)
axes[1, 0].set_ylim(0, 1)
axes[1, 0].set_title('æ‰©å±•èƒ½åŠ›è¯„ä¼°')

# è¿ç»´æˆæœ¬å¯¹æ¯”
deployment_methods = ['ä¼ ç»Ÿéƒ¨ç½²', 'å®¹å™¨åŒ–éƒ¨ç½²', 'K8séƒ¨ç½²']
setup_cost = [100, 60, 40]
maintenance_cost = [100, 40, 20]
scaling_cost = [100, 30, 10]

x = np.arange(len(deployment_methods))
width = 0.25

axes[1, 1].bar(x - width, setup_cost, width, label='åˆå§‹æˆæœ¬', alpha=0.7)
axes[1, 1].bar(x, maintenance_cost, width, label='ç»´æŠ¤æˆæœ¬', alpha=0.7)
axes[1, 1].bar(x + width, scaling_cost, width, label='æ‰©å±•æˆæœ¬', alpha=0.7)
axes[1, 1].set_title('è¿ç»´æˆæœ¬å¯¹æ¯”')
axes[1, 1].set_ylabel('ç›¸å¯¹æˆæœ¬')
axes[1, 1].set_xticks(x)
axes[1, 1].set_xticklabels(deployment_methods)
axes[1, 1].legend()

plt.tight_layout()
plt.show()
```

### å®è·µä»·å€¼ä¸åº”ç”¨åœºæ™¯

**1. ä¼ä¸šçº§AIæœåŠ¡**
- é«˜å¯ç”¨çš„æ¨¡å‹APIæœåŠ¡
- å¤šç§Ÿæˆ·çš„æ¨¡å‹ç®¡ç†å¹³å°
- ä¼ä¸šå†…éƒ¨AIèƒ½åŠ›ä¸­å°

**2. äº‘åŸç”ŸAIåº”ç”¨**
- å¾®æœåŠ¡æ¶æ„çš„AIç³»ç»Ÿ
- å¼¹æ€§ä¼¸ç¼©çš„æ¨ç†æœåŠ¡
- å¤šäº‘ç¯å¢ƒçš„æ¨¡å‹éƒ¨ç½²

**3. è¾¹ç¼˜è®¡ç®—åœºæ™¯**
- è½»é‡åŒ–çš„æ¨¡å‹éƒ¨ç½²
- ç¦»çº¿æ¨ç†æœåŠ¡
- IoTè®¾å¤‡é›†æˆ

### éƒ¨ç½²æœ€ä½³å®è·µå»ºè®®

1. **å®‰å…¨æ€§ä¼˜å…ˆ**ï¼šå®æ–½APIè®¤è¯ã€æ•°æ®åŠ å¯†ã€è®¿é—®æ§åˆ¶
2. **æ€§èƒ½ç›‘æ§**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ï¼ŒåŠæ—¶å‘ç°æ€§èƒ½ç“¶é¢ˆ
3. **ç‰ˆæœ¬ç®¡ç†**ï¼šç»´æŠ¤æ¸…æ™°çš„æ¨¡å‹ç‰ˆæœ¬å†å²ï¼Œæ”¯æŒå¿«é€Ÿå›æ»š
4. **èµ„æºä¼˜åŒ–**ï¼šæ ¹æ®è´Ÿè½½æƒ…å†µåŠ¨æ€è°ƒæ•´èµ„æºé…ç½®
5. **æµ‹è¯•éªŒè¯**ï¼šå»ºç«‹å®Œæ•´çš„æµ‹è¯•æµç¨‹ï¼Œç¡®ä¿éƒ¨ç½²è´¨é‡

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œå¼€å‘è€…å¯ä»¥æŒæ¡å®Œæ•´çš„æ¨¡å‹éƒ¨ç½²æµç¨‹ï¼Œä»å¼€å‘ç¯å¢ƒåˆ°ç”Ÿäº§ç¯å¢ƒçš„æ— ç¼è¿ç§»ã€‚ä¸‹ä¸€èŠ‚å°†é€šè¿‡ç»¼åˆé¡¹ç›®æ¡ˆä¾‹ï¼Œå±•ç¤ºå¦‚ä½•å°†å‰é¢å­¦åˆ°çš„æ‰€æœ‰æŠ€æœ¯æ•´åˆåˆ°å®é™…é¡¹ç›®ä¸­ã€‚
```