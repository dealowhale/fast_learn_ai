# 4.6 ç»¼åˆé¡¹ç›®æ¡ˆä¾‹ï¼šæ™ºèƒ½å®¢æˆ·æœåŠ¡ç³»ç»Ÿ

æœ¬èŠ‚å°†é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„æ™ºèƒ½å®¢æˆ·æœåŠ¡ç³»ç»Ÿé¡¹ç›®ï¼Œå±•ç¤ºå¦‚ä½•åœ¨Traeç¯å¢ƒä¸­æ•´åˆå‰é¢ç« èŠ‚å­¦åˆ°çš„æ‰€æœ‰æŠ€æœ¯ï¼Œä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²çš„å…¨æµç¨‹å®è·µã€‚

## 4.6.1 é¡¹ç›®æ¦‚è¿°ä¸æ¶æ„è®¾è®¡

### é¡¹ç›®èƒŒæ™¯

æ™ºèƒ½å®¢æˆ·æœåŠ¡ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ çš„ç»¼åˆAIåº”ç”¨ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å¤„ç†å®¢æˆ·å’¨è¯¢ã€æƒ…æ„Ÿåˆ†æã€æ™ºèƒ½è·¯ç”±å’ŒæœåŠ¡è´¨é‡è¯„ä¼°ã€‚

### ç³»ç»Ÿæ¶æ„

```python
# project_architecture.py - é¡¹ç›®æ¶æ„è®¾è®¡
import os
from dataclasses import dataclass
from typing import Dict, List, Optional
from enum import Enum

class ServiceType(Enum):
    """æœåŠ¡ç±»å‹æšä¸¾"""
    INTENT_CLASSIFICATION = "intent_classification"
    SENTIMENT_ANALYSIS = "sentiment_analysis"
    RESPONSE_GENERATION = "response_generation"
    QUALITY_ASSESSMENT = "quality_assessment"

@dataclass
class ProjectConfig:
    """é¡¹ç›®é…ç½®ç±»"""
    project_name: str = "intelligent_customer_service"
    version: str = "1.0.0"
    
    # æ•°æ®è·¯å¾„é…ç½®
    data_dir: str = "./data"
    raw_data_dir: str = "./data/raw"
    processed_data_dir: str = "./data/processed"
    
    # æ¨¡å‹è·¯å¾„é…ç½®
    models_dir: str = "./models"
    checkpoints_dir: str = "./checkpoints"
    
    # æ—¥å¿—é…ç½®
    logs_dir: str = "./logs"
    
    # APIé…ç½®
    api_host: str = "0.0.0.0"
    api_port: int = 8080
    
    # æ•°æ®åº“é…ç½®
    db_url: str = "sqlite:///customer_service.db"
    
    def create_directories(self):
        """åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„"""
        directories = [
            self.data_dir,
            self.raw_data_dir,
            self.processed_data_dir,
            self.models_dir,
            self.checkpoints_dir,
            self.logs_dir
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            print(f"âœ… åˆ›å»ºç›®å½•: {directory}")

class SystemArchitecture:
    """ç³»ç»Ÿæ¶æ„ç®¡ç†å™¨"""
    
    def __init__(self, config: ProjectConfig):
        self.config = config
        self.services = {}
        self.data_pipeline = None
        self.model_manager = None
        self.api_server = None
    
    def initialize_project(self):
        """åˆå§‹åŒ–é¡¹ç›®ç»“æ„"""
        print("ğŸš€ åˆå§‹åŒ–æ™ºèƒ½å®¢æˆ·æœåŠ¡ç³»ç»Ÿ")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.config.create_directories()
        
        # ç”Ÿæˆé¡¹ç›®æ–‡ä»¶
        self._generate_requirements_file()
        self._generate_docker_files()
        self._generate_config_files()
        
        print("âœ… é¡¹ç›®åˆå§‹åŒ–å®Œæˆ")
    
    def _generate_requirements_file(self):
        """ç”Ÿæˆä¾èµ–æ–‡ä»¶"""
        requirements = """
# æ ¸å¿ƒæœºå™¨å­¦ä¹ åº“
scikit-learn==1.3.0
pandas==2.0.3
numpy==1.24.3
scipy==1.11.1

# æ·±åº¦å­¦ä¹ æ¡†æ¶
torch==2.0.1
transformers==4.30.2
sentence-transformers==2.2.2

# è‡ªç„¶è¯­è¨€å¤„ç†
nltk==3.8.1
spacy==3.6.0
jieba==0.42.1

# Webæ¡†æ¶å’ŒAPI
flask==2.3.2
flask-restful==0.3.10
flask-cors==4.0.0
fastapi==0.100.0
uvicorn==0.22.0

# æ•°æ®åº“
sqlalchemy==2.0.17
alembic==1.11.1
psycopg2-binary==2.9.6

# æ•°æ®å¯è§†åŒ–
matplotlib==3.7.1
seaborn==0.12.2
plotly==5.15.0

# å·¥å…·åº“
requests==2.31.0
joblib==1.3.1
tqdm==4.65.0
pyyaml==6.0
click==8.1.3

# ç›‘æ§å’Œæ—¥å¿—
prometheus-client==0.17.0
loguru==0.7.0

# æµ‹è¯•
pytest==7.4.0
pytest-cov==4.1.0

# å¼€å‘å·¥å…·
black==23.3.0
flake8==6.0.0
mypy==1.4.1
"""
        
        with open("requirements.txt", "w", encoding="utf-8") as f:
            f.write(requirements.strip())
        
        print("ğŸ“¦ ç”Ÿæˆ requirements.txt")
    
    def _generate_docker_files(self):
        """ç”ŸæˆDockeré…ç½®æ–‡ä»¶"""
        dockerfile_content = """
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# ä¸‹è½½spaCyæ¨¡å‹
RUN python -m spacy download en_core_web_sm
RUN python -m spacy download zh_core_web_sm

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV FLASK_ENV=production

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "app.py"]
"""
        
        with open("Dockerfile", "w", encoding="utf-8") as f:
            f.write(dockerfile_content.strip())
        
        docker_compose_content = """
version: '3.8'

services:
  customer-service-api:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=postgresql://user:password@db:5432/customer_service
    depends_on:
      - db
      - redis
    restart: unless-stopped
    networks:
      - customer-service-network

  db:
    image: postgres:13
    environment:
      - POSTGRES_DB=customer_service
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - customer-service-network

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - customer-service-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - customer-service-api
    restart: unless-stopped
    networks:
      - customer-service-network

volumes:
  postgres_data:
  redis_data:

networks:
  customer-service-network:
    driver: bridge
"""
        
        with open("docker-compose.yml", "w", encoding="utf-8") as f:
            f.write(docker_compose_content.strip())
        
        print("ğŸ³ ç”Ÿæˆ Docker é…ç½®æ–‡ä»¶")
    
    def _generate_config_files(self):
        """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
        config_yaml = f"""
# æ™ºèƒ½å®¢æˆ·æœåŠ¡ç³»ç»Ÿé…ç½®æ–‡ä»¶
project:
  name: {self.config.project_name}
  version: {self.config.version}

data:
  raw_dir: {self.config.raw_data_dir}
  processed_dir: {self.config.processed_data_dir}
  batch_size: 32
  max_sequence_length: 512

models:
  intent_classifier:
    model_type: "bert"
    model_name: "bert-base-chinese"
    num_classes: 20
    learning_rate: 2e-5
    epochs: 10
  
  sentiment_analyzer:
    model_type: "roberta"
    model_name: "hfl/chinese-roberta-wwm-ext"
    num_classes: 3
    learning_rate: 1e-5
    epochs: 5
  
  response_generator:
    model_type: "gpt2"
    model_name: "uer/gpt2-chinese-cluecorpussmall"
    max_length: 256
    temperature: 0.7

api:
  host: {self.config.api_host}
  port: {self.config.api_port}
  debug: false
  cors_enabled: true

database:
  url: {self.config.db_url}
  pool_size: 10
  max_overflow: 20

logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "{self.config.logs_dir}/app.log"
  max_bytes: 10485760
  backup_count: 5

monitoring:
  enabled: true
  metrics_port: 9090
  health_check_interval: 30
"""
        
        with open("config.yaml", "w", encoding="utf-8") as f:
            f.write(config_yaml)
        
        print("âš™ï¸ ç”Ÿæˆ config.yaml")
    
    def get_architecture_diagram(self):
        """ç”Ÿæˆç³»ç»Ÿæ¶æ„å›¾"""
        import matplotlib.pyplot as plt
        import matplotlib.patches as patches
        from matplotlib.patches import FancyBboxPatch
        
        fig, ax = plt.subplots(1, 1, figsize=(16, 12))
        
        # å®šä¹‰é¢œè‰²
        colors = {
            'frontend': '#E3F2FD',
            'api': '#FFF3E0',
            'ml': '#E8F5E8',
            'data': '#F3E5F5',
            'infra': '#FAFAFA'
        }
        
        # å‰ç«¯å±‚
        frontend_box = FancyBboxPatch((1, 9), 14, 1.5, 
                                     boxstyle="round,pad=0.1", 
                                     facecolor=colors['frontend'], 
                                     edgecolor='blue', linewidth=2)
        ax.add_patch(frontend_box)
        ax.text(8, 9.75, 'å®¢æˆ·ç«¯ç•Œé¢å±‚\n(Webç•Œé¢ / ç§»åŠ¨åº”ç”¨ / ç¬¬ä¸‰æ–¹é›†æˆ)', 
                ha='center', va='center', fontsize=12, weight='bold')
        
        # APIç½‘å…³å±‚
        api_box = FancyBboxPatch((1, 7), 14, 1.5, 
                                boxstyle="round,pad=0.1", 
                                facecolor=colors['api'], 
                                edgecolor='orange', linewidth=2)
        ax.add_patch(api_box)
        ax.text(8, 7.75, 'APIç½‘å…³å±‚\n(è´Ÿè½½å‡è¡¡ / è®¤è¯æˆæƒ / é™æµæ§åˆ¶)', 
                ha='center', va='center', fontsize=12, weight='bold')
        
        # ä¸šåŠ¡æœåŠ¡å±‚
        services = [
            ('æ„å›¾è¯†åˆ«\næœåŠ¡', 2, 5),
            ('æƒ…æ„Ÿåˆ†æ\næœåŠ¡', 6, 5),
            ('å“åº”ç”Ÿæˆ\næœåŠ¡', 10, 5),
            ('è´¨é‡è¯„ä¼°\næœåŠ¡', 14, 5)
        ]
        
        for service_name, x, y in services:
            service_box = FancyBboxPatch((x-1, y), 3, 1.5, 
                                        boxstyle="round,pad=0.1", 
                                        facecolor=colors['ml'], 
                                        edgecolor='green', linewidth=2)
            ax.add_patch(service_box)
            ax.text(x+0.5, y+0.75, service_name, 
                    ha='center', va='center', fontsize=10, weight='bold')
        
        # æ¨¡å‹ç®¡ç†å±‚
        model_box = FancyBboxPatch((1, 2.5), 14, 1.5, 
                                  boxstyle="round,pad=0.1", 
                                  facecolor=colors['ml'], 
                                  edgecolor='green', linewidth=2)
        ax.add_patch(model_box)
        ax.text(8, 3.25, 'æ¨¡å‹ç®¡ç†å±‚\n(æ¨¡å‹åŠ è½½ / ç‰ˆæœ¬æ§åˆ¶ / æ€§èƒ½ç›‘æ§ / A/Bæµ‹è¯•)', 
                ha='center', va='center', fontsize=12, weight='bold')
        
        # æ•°æ®å±‚
        data_components = [
            ('æ•°æ®å¤„ç†\nå¼•æ“', 2, 0.5),
            ('ç‰¹å¾å­˜å‚¨', 6, 0.5),
            ('æ¨¡å‹å­˜å‚¨', 10, 0.5),
            ('æ—¥å¿—å­˜å‚¨', 14, 0.5)
        ]
        
        for comp_name, x, y in data_components:
            comp_box = FancyBboxPatch((x-1, y), 3, 1, 
                                     boxstyle="round,pad=0.1", 
                                     facecolor=colors['data'], 
                                     edgecolor='purple', linewidth=2)
            ax.add_patch(comp_box)
            ax.text(x+0.5, y+0.5, comp_name, 
                    ha='center', va='center', fontsize=10, weight='bold')
        
        # æ·»åŠ è¿æ¥çº¿
        connections = [
            # å‰ç«¯åˆ°API
            ((8, 9), (8, 8.5)),
            # APIåˆ°æœåŠ¡
            ((8, 7), (3.5, 6.5)),
            ((8, 7), (7.5, 6.5)),
            ((8, 7), (11.5, 6.5)),
            ((8, 7), (15.5, 6.5)),
            # æœåŠ¡åˆ°æ¨¡å‹ç®¡ç†
            ((3.5, 5), (8, 4)),
            ((7.5, 5), (8, 4)),
            ((11.5, 5), (8, 4)),
            ((15.5, 5), (8, 4)),
            # æ¨¡å‹ç®¡ç†åˆ°æ•°æ®å±‚
            ((8, 2.5), (3.5, 1.5)),
            ((8, 2.5), (7.5, 1.5)),
            ((8, 2.5), (11.5, 1.5)),
            ((8, 2.5), (15.5, 1.5))
        ]
        
        for (x1, y1), (x2, y2) in connections:
            ax.plot([x1, x2], [y1, y2], 'k-', alpha=0.6, linewidth=1.5)
            # æ·»åŠ ç®­å¤´
            ax.annotate('', xy=(x2, y2), xytext=(x1, y1),
                       arrowprops=dict(arrowstyle='->', color='black', alpha=0.6))
        
        # è®¾ç½®å›¾å½¢å±æ€§
        ax.set_xlim(0, 17)
        ax.set_ylim(-0.5, 11)
        ax.set_aspect('equal')
        ax.axis('off')
        
        # æ·»åŠ æ ‡é¢˜
        plt.title('æ™ºèƒ½å®¢æˆ·æœåŠ¡ç³»ç»Ÿæ¶æ„å›¾', fontsize=16, weight='bold', pad=20)
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = [
            patches.Patch(color=colors['frontend'], label='å‰ç«¯å±‚'),
            patches.Patch(color=colors['api'], label='APIå±‚'),
            patches.Patch(color=colors['ml'], label='AIæœåŠ¡å±‚'),
            patches.Patch(color=colors['data'], label='æ•°æ®å±‚')
        ]
        ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))
        
        plt.tight_layout()
        plt.show()
        
        return fig

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºé¡¹ç›®é…ç½®
    config = ProjectConfig()
    
    # åˆå§‹åŒ–ç³»ç»Ÿæ¶æ„
    architecture = SystemArchitecture(config)
    
    # åˆå§‹åŒ–é¡¹ç›®
    architecture.initialize_project()
    
    # æ˜¾ç¤ºæ¶æ„å›¾
    architecture.get_architecture_diagram()
    
    print("\nğŸ¯ æ™ºèƒ½å®¢æˆ·æœåŠ¡ç³»ç»Ÿæ¶æ„è®¾è®¡å®Œæˆ!")
    print("ğŸ“‹ é¡¹ç›®ç»“æ„:")
    print("â”œâ”€â”€ data/                    # æ•°æ®ç›®å½•")
    print("â”‚   â”œâ”€â”€ raw/                # åŸå§‹æ•°æ®")
    print("â”‚   â””â”€â”€ processed/          # å¤„ç†åæ•°æ®")
    print("â”œâ”€â”€ models/                 # æ¨¡å‹ç›®å½•")
    print("â”œâ”€â”€ logs/                   # æ—¥å¿—ç›®å½•")
    print("â”œâ”€â”€ src/                    # æºä»£ç ")
    print("â”‚   â”œâ”€â”€ data/              # æ•°æ®å¤„ç†æ¨¡å—")
    print("â”‚   â”œâ”€â”€ models/            # æ¨¡å‹æ¨¡å—")
    print("â”‚   â”œâ”€â”€ api/               # APIæ¨¡å—")
    print("â”‚   â””â”€â”€ utils/             # å·¥å…·æ¨¡å—")
    print("â”œâ”€â”€ tests/                  # æµ‹è¯•ä»£ç ")
    print("â”œâ”€â”€ config.yaml            # é…ç½®æ–‡ä»¶")
    print("â”œâ”€â”€ requirements.txt       # ä¾èµ–æ–‡ä»¶")
    print("â”œâ”€â”€ Dockerfile             # Dockeré…ç½®")
    print("â””â”€â”€ docker-compose.yml     # å®¹å™¨ç¼–æ’")
```

## 4.6.2 æ•°æ®å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹

### å®¢æˆ·æœåŠ¡æ•°æ®å¤„ç†ç®¡é“

```python
# data_pipeline.py - å®¢æˆ·æœåŠ¡æ•°æ®å¤„ç†ç®¡é“
import pandas as pd
import numpy as np
import re
import jieba
import json
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import seaborn as sns

class CustomerServiceDataProcessor:
    """å®¢æˆ·æœåŠ¡æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = self._load_config(config_path)
        self.tokenizer = None
        self.label_encoders = {}
        self.intent_categories = []
        self.sentiment_labels = ['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢']
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        jieba.initialize()
        
        print("ğŸ”§ å®¢æˆ·æœåŠ¡æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        import yaml
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'data': {
                    'max_sequence_length': 512,
                    'batch_size': 32
                },
                'models': {
                    'intent_classifier': {
                        'model_name': 'bert-base-chinese'
                    }
                }
            }
    
    def load_raw_data(self, data_sources: Dict[str, str]) -> Dict[str, pd.DataFrame]:
        """åŠ è½½åŸå§‹æ•°æ®"""
        print("ğŸ“¥ åŠ è½½åŸå§‹æ•°æ®...")
        
        datasets = {}
        
        for dataset_name, file_path in data_sources.items():
            try:
                if file_path.endswith('.csv'):
                    df = pd.read_csv(file_path, encoding='utf-8')
                elif file_path.endswith('.json'):
                    df = pd.read_json(file_path, encoding='utf-8')
                elif file_path.endswith('.xlsx'):
                    df = pd.read_excel(file_path)
                else:
                    print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                    continue
                
                datasets[dataset_name] = df
                print(f"âœ… åŠ è½½ {dataset_name}: {df.shape[0]} æ¡è®°å½•")
                
            except Exception as e:
                print(f"âŒ åŠ è½½ {dataset_name} å¤±è´¥: {e}")
        
        return datasets
    
    def generate_sample_data(self, num_samples: int = 10000) -> Dict[str, pd.DataFrame]:
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º"""
        print(f"ğŸ² ç”Ÿæˆ {num_samples} æ¡ç¤ºä¾‹æ•°æ®...")
        
        # å®šä¹‰æ„å›¾ç±»åˆ«
        self.intent_categories = [
            'è´¦æˆ·æŸ¥è¯¢', 'ä½™é¢æŸ¥è¯¢', 'è½¬è´¦æ±‡æ¬¾', 'ä¿¡ç”¨å¡ç”³è¯·', 'è´·æ¬¾å’¨è¯¢',
            'æŠ•èµ„ç†è´¢', 'ä¿é™©å’¨è¯¢', 'æŠ€æœ¯æ”¯æŒ', 'æŠ•è¯‰å»ºè®®', 'äº§å“ä»‹ç»',
            'è´¹ç”¨æŸ¥è¯¢', 'å¯†ç é‡ç½®', 'å¼€æˆ·ç”³è¯·', 'é”€æˆ·ç”³è¯·', 'ä¼˜æƒ æ´»åŠ¨',
            'ç½‘ç‚¹æŸ¥è¯¢', 'æ±‡ç‡æŸ¥è¯¢', 'è¿˜æ¬¾æé†’', 'é£é™©è¯„ä¼°', 'å…¶ä»–å’¨è¯¢'
        ]
        
        # ç¤ºä¾‹å®¢æˆ·é—®é¢˜æ¨¡æ¿
        question_templates = {
            'è´¦æˆ·æŸ¥è¯¢': [
                'æˆ‘æƒ³æŸ¥è¯¢ä¸€ä¸‹æˆ‘çš„è´¦æˆ·ä¿¡æ¯',
                'è¯·å¸®æˆ‘æŸ¥çœ‹è´¦æˆ·çŠ¶æ€',
                'æˆ‘çš„è´¦æˆ·æ˜¯å¦æ­£å¸¸',
                'è´¦æˆ·ä½™é¢æ˜¯å¤šå°‘'
            ],
            'ä½™é¢æŸ¥è¯¢': [
                'æˆ‘çš„å¡é‡Œè¿˜æœ‰å¤šå°‘é’±',
                'è¯·æŸ¥è¯¢ä½™é¢',
                'è´¦æˆ·ä½™é¢æŸ¥è¯¢',
                'æˆ‘æƒ³çŸ¥é“ä½™é¢'
            ],
            'è½¬è´¦æ±‡æ¬¾': [
                'æˆ‘è¦è½¬è´¦ç»™æœ‹å‹',
                'å¦‚ä½•è¿›è¡Œæ±‡æ¬¾',
                'è½¬è´¦æ‰‹ç»­è´¹æ˜¯å¤šå°‘',
                'è·¨è¡Œè½¬è´¦æ€ä¹ˆæ“ä½œ'
            ],
            'æŠ€æœ¯æ”¯æŒ': [
                'æ‰‹æœºé“¶è¡Œç™»å½•ä¸äº†',
                'ç½‘é“¶å¯†ç å¿˜è®°äº†',
                'APPé—ªé€€æ€ä¹ˆåŠ',
                'ç³»ç»Ÿæç¤ºé”™è¯¯'
            ],
            'æŠ•è¯‰å»ºè®®': [
                'æˆ‘è¦æŠ•è¯‰ä½ ä»¬çš„æœåŠ¡',
                'å¯¹æœåŠ¡ä¸æ»¡æ„',
                'å»ºè®®æ”¹è¿›ç³»ç»Ÿ',
                'æœåŠ¡æ€åº¦æœ‰é—®é¢˜'
            ]
        }
        
        # ç”Ÿæˆå¯¹è¯æ•°æ®
        conversations = []
        
        for i in range(num_samples):
            # éšæœºé€‰æ‹©æ„å›¾
            intent = np.random.choice(self.intent_categories)
            
            # ç”Ÿæˆå®¢æˆ·é—®é¢˜
            if intent in question_templates:
                base_question = np.random.choice(question_templates[intent])
            else:
                base_question = f"å…³äº{intent}çš„é—®é¢˜"
            
            # æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
            variations = [
                f"ä½ å¥½ï¼Œ{base_question}",
                f"è¯·é—®{base_question}",
                f"{base_question}ï¼Œè°¢è°¢",
                f"éº»çƒ¦{base_question}",
                base_question
            ]
            
            customer_message = np.random.choice(variations)
            
            # ç”Ÿæˆæƒ…æ„Ÿæ ‡ç­¾
            if intent == 'æŠ•è¯‰å»ºè®®':
                sentiment = np.random.choice(['è´Ÿé¢', 'ä¸­æ€§'], p=[0.7, 0.3])
            elif intent in ['è´¦æˆ·æŸ¥è¯¢', 'ä½™é¢æŸ¥è¯¢', 'äº§å“ä»‹ç»']:
                sentiment = np.random.choice(['ä¸­æ€§', 'æ­£é¢'], p=[0.6, 0.4])
            else:
                sentiment = np.random.choice(['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢'], p=[0.2, 0.5, 0.3])
            
            # ç”Ÿæˆå®¢æœå›å¤
            response_templates = {
                'è´¦æˆ·æŸ¥è¯¢': 'æ‚¨çš„è´¦æˆ·çŠ¶æ€æ­£å¸¸ï¼Œè¯¦ç»†ä¿¡æ¯å·²ä¸ºæ‚¨æŸ¥è¯¢ã€‚',
                'ä½™é¢æŸ¥è¯¢': 'æ‚¨çš„è´¦æˆ·ä½™é¢ä¸ºXXXå…ƒï¼Œæ„Ÿè°¢æ‚¨çš„æŸ¥è¯¢ã€‚',
                'è½¬è´¦æ±‡æ¬¾': 'è½¬è´¦åŠŸèƒ½å¯é€šè¿‡æ‰‹æœºé“¶è¡Œæ“ä½œï¼Œæ‰‹ç»­è´¹æ ¹æ®é‡‘é¢è®¡ç®—ã€‚',
                'æŠ€æœ¯æ”¯æŒ': 'è¯·å°è¯•é‡æ–°ç™»å½•ï¼Œå¦‚ä»æœ‰é—®é¢˜è¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚',
                'æŠ•è¯‰å»ºè®®': 'éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ï¼Œæˆ‘ä»¬ä¼šè®¤çœŸå¤„ç†æ‚¨çš„åé¦ˆã€‚'
            }
            
            agent_response = response_templates.get(intent, f"å…³äº{intent}ï¼Œæˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†è§£ç­”ã€‚")
            
            # ç”Ÿæˆè´¨é‡è¯„åˆ†
            if sentiment == 'æ­£é¢':
                quality_score = np.random.uniform(4.0, 5.0)
            elif sentiment == 'ä¸­æ€§':
                quality_score = np.random.uniform(3.0, 4.5)
            else:
                quality_score = np.random.uniform(1.0, 3.5)
            
            conversations.append({
                'conversation_id': f"conv_{i+1:06d}",
                'timestamp': datetime.now() - timedelta(days=np.random.randint(0, 365)),
                'customer_id': f"cust_{np.random.randint(1, 5000):06d}",
                'customer_message': customer_message,
                'intent': intent,
                'sentiment': sentiment,
                'agent_response': agent_response,
                'quality_score': round(quality_score, 2),
                'resolution_time': np.random.randint(30, 1800),  # ç§’
                'channel': np.random.choice(['ç½‘é¡µ', 'æ‰‹æœºAPP', 'ç”µè¯', 'å¾®ä¿¡']),
                'agent_id': f"agent_{np.random.randint(1, 50):03d}"
            })
        
        # è½¬æ¢ä¸ºDataFrame
        df_conversations = pd.DataFrame(conversations)
        
        # ç”Ÿæˆå®¢æˆ·ä¿¡æ¯æ•°æ®
        customers = []
        unique_customer_ids = df_conversations['customer_id'].unique()
        
        for customer_id in unique_customer_ids:
            customers.append({
                'customer_id': customer_id,
                'age': np.random.randint(18, 80),
                'gender': np.random.choice(['ç”·', 'å¥³']),
                'city': np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'æˆéƒ½', 'æ­¦æ±‰']),
                'customer_type': np.random.choice(['ä¸ªäºº', 'ä¼ä¸š'], p=[0.8, 0.2]),
                'vip_level': np.random.choice(['æ™®é€š', 'é“¶å¡', 'é‡‘å¡', 'é’»çŸ³'], p=[0.6, 0.25, 0.12, 0.03]),
                'registration_date': datetime.now() - timedelta(days=np.random.randint(30, 2000))
            })
        
        df_customers = pd.DataFrame(customers)
        
        print(f"âœ… ç”Ÿæˆå¯¹è¯æ•°æ®: {len(df_conversations)} æ¡")
        print(f"âœ… ç”Ÿæˆå®¢æˆ·æ•°æ®: {len(df_customers)} æ¡")
        
        return {
            'conversations': df_conversations,
            'customers': df_customers
        }
    
    def clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—"""
        if pd.isna(text):
            return ""
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        text = str(text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰\[\]{}ã€\-_+=<>@#$%^&*~`|\\/.\s]', '', text)
        
        # å»é™¤é¦–å°¾ç©ºæ ¼
        text = text.strip()
        
        return text
    
    def preprocess_conversations(self, df: pd.DataFrame) -> pd.DataFrame:
        """é¢„å¤„ç†å¯¹è¯æ•°æ®"""
        print("ğŸ”„ é¢„å¤„ç†å¯¹è¯æ•°æ®...")
        
        df_processed = df.copy()
        
        # æ–‡æœ¬æ¸…æ´—
        df_processed['customer_message_clean'] = df_processed['customer_message'].apply(self.clean_text)
        df_processed['agent_response_clean'] = df_processed['agent_response'].apply(self.clean_text)
        
        # ç§»é™¤ç©ºæ–‡æœ¬
        df_processed = df_processed[df_processed['customer_message_clean'].str.len() > 0]
        
        # è®¡ç®—æ–‡æœ¬é•¿åº¦
        df_processed['message_length'] = df_processed['customer_message_clean'].str.len()
        df_processed['response_length'] = df_processed['agent_response_clean'].str.len()
        
        # åˆ†è¯ï¼ˆç”¨äºåç»­ç‰¹å¾æå–ï¼‰
        df_processed['customer_tokens'] = df_processed['customer_message_clean'].apply(
            lambda x: list(jieba.cut(x))
        )
        
        # è®¡ç®—è¯æ•°
        df_processed['token_count'] = df_processed['customer_tokens'].apply(len)
        
        # æ—¶é—´ç‰¹å¾
        df_processed['timestamp'] = pd.to_datetime(df_processed['timestamp'])
        df_processed['hour'] = df_processed['timestamp'].dt.hour
        df_processed['day_of_week'] = df_processed['timestamp'].dt.dayofweek
        df_processed['is_weekend'] = df_processed['day_of_week'].isin([5, 6])
        
        # ç¼–ç åˆ†ç±»å˜é‡
        categorical_columns = ['intent', 'sentiment', 'channel']
        
        for col in categorical_columns:
            if col not in self.label_encoders:
                self.label_encoders[col] = LabelEncoder()
                df_processed[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df_processed[col])
            else:
                df_processed[f'{col}_encoded'] = self.label_encoders[col].transform(df_processed[col])
        
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œä¿ç•™ {len(df_processed)} æ¡æœ‰æ•ˆè®°å½•")
        
        return df_processed
    
    def extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹"""
        print("âš™ï¸ æ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
        
        df_features = df.copy()
        
        # æ–‡æœ¬ç‰¹å¾
        df_features['has_question_mark'] = df_features['customer_message_clean'].str.contains('ï¼Ÿ|\?').astype(int)
        df_features['has_exclamation'] = df_features['customer_message_clean'].str.contains('ï¼|!').astype(int)
        df_features['has_numbers'] = df_features['customer_message_clean'].str.contains('\d').astype(int)
        df_features['has_english'] = df_features['customer_message_clean'].str.contains('[a-zA-Z]').astype(int)
        
        # æƒ…æ„Ÿè¯ç‰¹å¾
        positive_words = ['å¥½', 'è°¢è°¢', 'æ»¡æ„', 'ä¸é”™', 'ä¼˜ç§€', 'æ£’', 'èµ']
        negative_words = ['ä¸å¥½', 'å·®', 'çƒ‚', 'åƒåœ¾', 'æŠ•è¯‰', 'é—®é¢˜', 'é”™è¯¯', 'å¤±æœ›']
        
        df_features['positive_word_count'] = df_features['customer_message_clean'].apply(
            lambda x: sum(1 for word in positive_words if word in x)
        )
        df_features['negative_word_count'] = df_features['customer_message_clean'].apply(
            lambda x: sum(1 for word in negative_words if word in x)
        )
        
        # æ—¶é—´ç‰¹å¾
        df_features['is_business_hour'] = ((df_features['hour'] >= 9) & (df_features['hour'] <= 17)).astype(int)
        df_features['is_peak_hour'] = df_features['hour'].isin([10, 11, 14, 15, 16]).astype(int)
        
        # å®¢æˆ·è¡Œä¸ºç‰¹å¾ï¼ˆåŸºäºå†å²æ•°æ®ï¼‰
        customer_stats = df_features.groupby('customer_id').agg({
            'conversation_id': 'count',
            'quality_score': 'mean',
            'resolution_time': 'mean'
        }).rename(columns={
            'conversation_id': 'customer_conversation_count',
            'quality_score': 'customer_avg_quality',
            'resolution_time': 'customer_avg_resolution_time'
        })
        
        df_features = df_features.merge(customer_stats, on='customer_id', how='left')
        
        # ä»£ç†ç‰¹å¾
        agent_stats = df_features.groupby('agent_id').agg({
            'conversation_id': 'count',
            'quality_score': 'mean',
            'resolution_time': 'mean'
        }).rename(columns={
            'conversation_id': 'agent_conversation_count',
            'quality_score': 'agent_avg_quality',
            'resolution_time': 'agent_avg_resolution_time'
        })
        
        df_features = df_features.merge(agent_stats, on='agent_id', how='left')
        
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç”Ÿæˆ {df_features.shape[1]} ä¸ªç‰¹å¾")
        
        return df_features
    
    def prepare_model_data(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®...")
        
        # åˆå§‹åŒ–tokenizer
        model_name = self.config.get('models', {}).get('intent_classifier', {}).get('model_name', 'bert-base-chinese')
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        datasets = {}
        
        # 1. æ„å›¾åˆ†ç±»æ•°æ®
        intent_features = ['message_length', 'token_count', 'has_question_mark', 
                          'has_exclamation', 'has_numbers', 'positive_word_count', 
                          'negative_word_count', 'is_business_hour', 'hour']
        
        X_intent_text = df['customer_message_clean'].tolist()
        X_intent_features = df[intent_features].values
        y_intent = df['intent_encoded'].values
        
        X_intent_text_train, X_intent_text_test, X_intent_feat_train, X_intent_feat_test, \
        y_intent_train, y_intent_test = train_test_split(
            X_intent_text, X_intent_features, y_intent, 
            test_size=0.2, random_state=42, stratify=y_intent
        )
        
        datasets['intent_classification'] = {
            'train': {
                'text': X_intent_text_train,
                'features': X_intent_feat_train,
                'labels': y_intent_train
            },
            'test': {
                'text': X_intent_text_test,
                'features': X_intent_feat_test,
                'labels': y_intent_test
            },
            'label_encoder': self.label_encoders['intent'],
            'feature_names': intent_features
        }
        
        # 2. æƒ…æ„Ÿåˆ†ææ•°æ®
        sentiment_features = ['message_length', 'token_count', 'positive_word_count', 
                             'negative_word_count', 'has_exclamation']
        
        X_sentiment_text = df['customer_message_clean'].tolist()
        X_sentiment_features = df[sentiment_features].values
        y_sentiment = df['sentiment_encoded'].values
        
        X_sentiment_text_train, X_sentiment_text_test, X_sentiment_feat_train, X_sentiment_feat_test, \
        y_sentiment_train, y_sentiment_test = train_test_split(
            X_sentiment_text, X_sentiment_features, y_sentiment, 
            test_size=0.2, random_state=42, stratify=y_sentiment
        )
        
        datasets['sentiment_analysis'] = {
            'train': {
                'text': X_sentiment_text_train,
                'features': X_sentiment_feat_train,
                'labels': y_sentiment_train
            },
            'test': {
                'text': X_sentiment_text_test,
                'features': X_sentiment_feat_test,
                'labels': y_sentiment_test
            },
            'label_encoder': self.label_encoders['sentiment'],
            'feature_names': sentiment_features
        }
        
        # 3. è´¨é‡è¯„ä¼°æ•°æ®ï¼ˆå›å½’ä»»åŠ¡ï¼‰
        quality_features = ['message_length', 'resolution_time', 'agent_avg_quality', 
                           'customer_avg_quality', 'sentiment_encoded', 'intent_encoded']
        
        # ç§»é™¤ç¼ºå¤±å€¼
        quality_mask = df[quality_features + ['quality_score']].notna().all(axis=1)
        df_quality = df[quality_mask]
        
        X_quality_text = df_quality['agent_response_clean'].tolist()
        X_quality_features = df_quality[quality_features].values
        y_quality = df_quality['quality_score'].values
        
        X_quality_text_train, X_quality_text_test, X_quality_feat_train, X_quality_feat_test, \
        y_quality_train, y_quality_test = train_test_split(
            X_quality_text, X_quality_features, y_quality, 
            test_size=0.2, random_state=42
        )
        
        datasets['quality_assessment'] = {
            'train': {
                'text': X_quality_text_train,
                'features': X_quality_feat_train,
                'labels': y_quality_train
            },
            'test': {
                'text': X_quality_text_test,
                'features': X_quality_feat_test,
                'labels': y_quality_test
            },
            'feature_names': quality_features
        }
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
        for task, data in datasets.items():
            print(f"   {task}: è®­ç»ƒé›† {len(data['train']['text'])} æ¡ï¼Œæµ‹è¯•é›† {len(data['test']['text'])} æ¡")
        
        return datasets
    
    def visualize_data_analysis(self, df: pd.DataFrame):
        """æ•°æ®åˆ†æå¯è§†åŒ–"""
        print("ğŸ“ˆ ç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Š...")
        
        fig, axes = plt.subplots(3, 2, figsize=(16, 18))
        
        # 1. æ„å›¾åˆ†å¸ƒ
        intent_counts = df['intent'].value_counts()
        axes[0, 0].pie(intent_counts.values, labels=intent_counts.index, autopct='%1.1f%%')
        axes[0, 0].set_title('å®¢æˆ·æ„å›¾åˆ†å¸ƒ')
        
        # 2. æƒ…æ„Ÿåˆ†å¸ƒ
        sentiment_counts = df['sentiment'].value_counts()
        colors = ['red', 'gray', 'green']
        axes[0, 1].bar(sentiment_counts.index, sentiment_counts.values, color=colors)
        axes[0, 1].set_title('æƒ…æ„Ÿåˆ†å¸ƒ')
        axes[0, 1].set_ylabel('æ•°é‡')
        
        # 3. è´¨é‡è¯„åˆ†åˆ†å¸ƒ
        axes[1, 0].hist(df['quality_score'], bins=30, alpha=0.7, color='blue')
        axes[1, 0].set_title('æœåŠ¡è´¨é‡è¯„åˆ†åˆ†å¸ƒ')
        axes[1, 0].set_xlabel('è´¨é‡è¯„åˆ†')
        axes[1, 0].set_ylabel('é¢‘æ¬¡')
        
        # 4. æ¸ é“ä½¿ç”¨æƒ…å†µ
        channel_counts = df['channel'].value_counts()
        axes[1, 1].bar(channel_counts.index, channel_counts.values, color='orange')
        axes[1, 1].set_title('æœåŠ¡æ¸ é“ä½¿ç”¨æƒ…å†µ')
        axes[1, 1].set_ylabel('æ•°é‡')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        # 5. æ—¶é—´åˆ†å¸ƒ
        hourly_counts = df['hour'].value_counts().sort_index()
        axes[2, 0].plot(hourly_counts.index, hourly_counts.values, marker='o')
        axes[2, 0].set_title('24å°æ—¶å’¨è¯¢åˆ†å¸ƒ')
        axes[2, 0].set_xlabel('å°æ—¶')
        axes[2, 0].set_ylabel('å’¨è¯¢æ•°é‡')
        axes[2, 0].grid(True, alpha=0.3)
        
        # 6. æ„å›¾ä¸æƒ…æ„Ÿå…³ç³»
        intent_sentiment = pd.crosstab(df['intent'], df['sentiment'])
        sns.heatmap(intent_sentiment, annot=True, fmt='d', cmap='YlOrRd', ax=axes[2, 1])
        axes[2, 1].set_title('æ„å›¾ä¸æƒ…æ„Ÿå…³ç³»çƒ­åŠ›å›¾')
        axes[2, 1].tick_params(axis='x', rotation=45)
        axes[2, 1].tick_params(axis='y', rotation=0)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡æ‘˜è¦:")
        print(f"æ€»å¯¹è¯æ•°: {len(df):,}")
        print(f"ç‹¬ç‰¹å®¢æˆ·æ•°: {df['customer_id'].nunique():,}")
        print(f"ç‹¬ç‰¹ä»£ç†æ•°: {df['agent_id'].nunique():,}")
        print(f"å¹³å‡æ¶ˆæ¯é•¿åº¦: {df['message_length'].mean():.1f} å­—ç¬¦")
        print(f"å¹³å‡è´¨é‡è¯„åˆ†: {df['quality_score'].mean():.2f}")
        print(f"å¹³å‡è§£å†³æ—¶é—´: {df['resolution_time'].mean():.0f} ç§’")
        
        print("\nğŸ¯ æ„å›¾åˆ†å¸ƒ (Top 10):")
        for intent, count in intent_counts.head(10).items():
            percentage = count / len(df) * 100
            print(f"  {intent}: {count:,} ({percentage:.1f}%)")
    
    def save_processed_data(self, datasets: Dict, output_dir: str = "./data/processed"):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        import os
        import pickle
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®é›†
        for task_name, task_data in datasets.items():
            task_dir = os.path.join(output_dir, task_name)
            os.makedirs(task_dir, exist_ok=True)
            
            # ä¿å­˜è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            for split in ['train', 'test']:
                split_data = task_data[split]
                
                # ä¿å­˜æ–‡æœ¬æ•°æ®
                with open(os.path.join(task_dir, f'{split}_texts.txt'), 'w', encoding='utf-8') as f:
                    for text in split_data['text']:
                        f.write(text + '\n')
                
                # ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾
                np.save(os.path.join(task_dir, f'{split}_features.npy'), split_data['features'])
                np.save(os.path.join(task_dir, f'{split}_labels.npy'), split_data['labels'])
            
            # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
            if 'label_encoder' in task_data:
                with open(os.path.join(task_dir, 'label_encoder.pkl'), 'wb') as f:
                    pickle.dump(task_data['label_encoder'], f)
            
            # ä¿å­˜ç‰¹å¾åç§°
            if 'feature_names' in task_data:
                with open(os.path.join(task_dir, 'feature_names.txt'), 'w', encoding='utf-8') as f:
                    for name in task_data['feature_names']:
                        f.write(name + '\n')
        
        # ä¿å­˜æ‰€æœ‰æ ‡ç­¾ç¼–ç å™¨
        with open(os.path.join(output_dir, 'label_encoders.pkl'), 'wb') as f:
            pickle.dump(self.label_encoders, f)
        
        print(f"ğŸ’¾ å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    processor = CustomerServiceDataProcessor()
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    raw_datasets = processor.generate_sample_data(num_samples=5000)
    
    # é¢„å¤„ç†å¯¹è¯æ•°æ®
    df_processed = processor.preprocess_conversations(raw_datasets['conversations'])
    
    # ç‰¹å¾å·¥ç¨‹
    df_features = processor.extract_features(df_processed)
    
    # æ•°æ®åˆ†æå¯è§†åŒ–
    processor.visualize_data_analysis(df_features)
    
    # å‡†å¤‡æ¨¡å‹æ•°æ®
    model_datasets = processor.prepare_model_data(df_features)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    processor.save_processed_data(model_datasets)
    
    print("\nğŸ‰ æ•°æ®å¤„ç†ç®¡é“æ‰§è¡Œå®Œæˆ!")
```

## 4.6.3 å¤šæ¨¡å‹è®­ç»ƒä¸é›†æˆ

### æ™ºèƒ½å®¢æœæ¨¡å‹è®­ç»ƒç³»ç»Ÿ

```python
# model_training.py - æ™ºèƒ½å®¢æœæ¨¡å‹è®­ç»ƒç³»ç»Ÿ
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModel, AutoConfig,
    AdamW, get_linear_schedule_with_warmup
)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, mean_squared_error, r2_score
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json
import os
from datetime import datetime

class CustomerServiceDataset(Dataset):
    """å®¢æˆ·æœåŠ¡æ•°æ®é›†ç±»"""
    
    def __init__(self, texts: List[str], features: np.ndarray, labels: np.ndarray, 
                 tokenizer, max_length: int = 512):
        self.texts = texts
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels) if labels.dtype == int else torch.FloatTensor(labels)
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        # æ–‡æœ¬ç¼–ç 
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'features': self.features[idx],
            'labels': self.labels[idx]
        }

class MultiModalClassifier(nn.Module):
    """å¤šæ¨¡æ€åˆ†ç±»å™¨ï¼ˆæ–‡æœ¬+ç‰¹å¾ï¼‰"""
    
    def __init__(self, model_name: str, num_classes: int, feature_dim: int, 
                 dropout_rate: float = 0.3):
        super().__init__()
        
        # BERTç¼–ç å™¨
        self.bert = AutoModel.from_pretrained(model_name)
        self.bert_dim = self.bert.config.hidden_size
        
        # ç‰¹å¾å¤„ç†å±‚
        self.feature_layer = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 64)
        )
        
        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(self.bert_dim + 64, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # åˆ†ç±»å±‚
        self.classifier = nn.Linear(128, num_classes)
        
        # Dropout
        self.dropout = nn.Dropout(dropout_rate)
    
    def forward(self, input_ids, attention_mask, features):
        # BERTç¼–ç 
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        text_features = bert_output.pooler_output
        text_features = self.dropout(text_features)
        
        # ç‰¹å¾å¤„ç†
        processed_features = self.feature_layer(features)
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([text_features, processed_features], dim=1)
        fused_features = self.fusion_layer(combined_features)
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)
        
        return logits

class MultiModalRegressor(nn.Module):
    """å¤šæ¨¡æ€å›å½’å™¨ï¼ˆç”¨äºè´¨é‡è¯„ä¼°ï¼‰"""
    
    def __init__(self, model_name: str, feature_dim: int, dropout_rate: float = 0.3):
        super().__init__()
        
        # BERTç¼–ç å™¨
        self.bert = AutoModel.from_pretrained(model_name)
        self.bert_dim = self.bert.config.hidden_size
        
        # ç‰¹å¾å¤„ç†å±‚
        self.feature_layer = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 64)
        )
        
        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(self.bert_dim + 64, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # å›å½’å±‚
        self.regressor = nn.Linear(128, 1)
        
        # Dropout
        self.dropout = nn.Dropout(dropout_rate)
    
    def forward(self, input_ids, attention_mask, features):
        # BERTç¼–ç 
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        text_features = bert_output.pooler_output
        text_features = self.dropout(text_features)
        
        # ç‰¹å¾å¤„ç†
        processed_features = self.feature_layer(features)
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([text_features, processed_features], dim=1)
        fused_features = self.fusion_layer(combined_features)
        
        # å›å½’
        output = self.regressor(fused_features)
        
        return output.squeeze()

class CustomerServiceModelTrainer:
    """å®¢æˆ·æœåŠ¡æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.models = {}
        self.tokenizers = {}
        self.training_history = {}
        
        print(f"ğŸ”§ æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_data(self, data_path: str) -> Dict:
        """åŠ è½½å¤„ç†åçš„æ•°æ®"""
        print(f"ğŸ“¥ ä» {data_path} åŠ è½½æ•°æ®...")
        
        datasets = {}
        
        for task_name in ['intent_classification', 'sentiment_analysis', 'quality_assessment']:
            task_dir = os.path.join(data_path, task_name)
            
            if not os.path.exists(task_dir):
                print(f"âš ï¸ ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨: {task_dir}")
                continue
            
            task_data = {'train': {}, 'test': {}}
            
            for split in ['train', 'test']:
                # åŠ è½½æ–‡æœ¬
                text_file = os.path.join(task_dir, f'{split}_texts.txt')
                with open(text_file, 'r', encoding='utf-8') as f:
                    texts = [line.strip() for line in f.readlines()]
                
                # åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾
                features = np.load(os.path.join(task_dir, f'{split}_features.npy'))
                labels = np.load(os.path.join(task_dir, f'{split}_labels.npy'))
                
                task_data[split] = {
                    'texts': texts,
                    'features': features,
                    'labels': labels
                }
            
            # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
            label_encoder_file = os.path.join(task_dir, 'label_encoder.pkl')
            if os.path.exists(label_encoder_file):
                import pickle
                with open(label_encoder_file, 'rb') as f:
                    task_data['label_encoder'] = pickle.load(f)
            
            # åŠ è½½ç‰¹å¾åç§°
            feature_names_file = os.path.join(task_dir, 'feature_names.txt')
            if os.path.exists(feature_names_file):
                with open(feature_names_file, 'r', encoding='utf-8') as f:
                    task_data['feature_names'] = [line.strip() for line in f.readlines()]
            
            datasets[task_name] = task_data
            print(f"âœ… åŠ è½½ {task_name} æ•°æ®å®Œæˆ")
        
        return datasets
    
    def train_intent_classifier(self, data: Dict) -> Dict:
        """è®­ç»ƒæ„å›¾åˆ†ç±»æ¨¡å‹"""
        print("ğŸ¯ è®­ç»ƒæ„å›¾åˆ†ç±»æ¨¡å‹...")
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = self.config.get('models', {}).get('intent_classifier', {})
        model_name = model_config.get('model_name', 'bert-base-chinese')
        num_classes = len(data['label_encoder'].classes_)
        feature_dim = data['train']['features'].shape[1]
        
        # åˆå§‹åŒ–tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizers['intent'] = tokenizer
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = CustomerServiceDataset(
            data['train']['texts'],
            data['train']['features'],
            data['train']['labels'],
            tokenizer
        )
        
        test_dataset = CustomerServiceDataset(
            data['test']['texts'],
            data['test']['features'],
            data['test']['labels'],
            tokenizer
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = self.config.get('data', {}).get('batch_size', 16)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = MultiModalClassifier(model_name, num_classes, feature_dim)
        model.to(self.device)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        epochs = model_config.get('epochs', 5)
        learning_rate = model_config.get('learning_rate', 2e-5)
        
        optimizer = AdamW(model.parameters(), lr=learning_rate)
        total_steps = len(train_loader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps=0, num_training_steps=total_steps
        )
        
        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå†å²
        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            print(f"\nEpoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            train_pbar = tqdm(train_loader, desc="Training")
            for batch in train_pbar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                features = batch['features'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                
                outputs = model(input_ids, attention_mask, features)
                loss = criterion(outputs, labels)
                
                loss.backward()
                optimizer.step()
                scheduler.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
                
                train_pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{100.*train_correct/train_total:.2f}%'
                })
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch in tqdm(test_loader, desc="Validation"):
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    features = batch['features'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    outputs = model(input_ids, attention_mask, features)
                    loss = criterion(outputs, labels)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
            
            # è®°å½•å†å²
            epoch_train_loss = train_loss / len(train_loader)
            epoch_train_acc = 100. * train_correct / train_total
            epoch_val_loss = val_loss / len(test_loader)
            epoch_val_acc = 100. * val_correct / val_total
            
            history['train_loss'].append(epoch_train_loss)
            history['train_acc'].append(epoch_train_acc)
            history['val_loss'].append(epoch_val_loss)
            history['val_acc'].append(epoch_val_acc)
            
            print(f"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%")
            print(f"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%")
        
        # ä¿å­˜æ¨¡å‹
        self.models['intent'] = model
        self.training_history['intent'] = history
        
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        model.eval()
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                features = batch['features'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = model(input_ids, attention_mask, features)
                _, predicted = torch.max(outputs.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è½¬æ¢å›åŸå§‹æ ‡ç­¾
        label_encoder = data['label_encoder']
        pred_labels = label_encoder.inverse_transform(all_predictions)
        true_labels = label_encoder.inverse_transform(all_labels)
        
        report = classification_report(true_labels, pred_labels, output_dict=True)
        
        print("\nğŸ“Š æ„å›¾åˆ†ç±»æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {epoch_val_acc:.2f}%")
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'history': history,
            'classification_report': report,
            'label_encoder': label_encoder
        }
    
    def train_sentiment_analyzer(self, data: Dict) -> Dict:
        """è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹"""
        print("ğŸ˜Š è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = self.config.get('models', {}).get('sentiment_analyzer', {})
        model_name = model_config.get('model_name', 'hfl/chinese-roberta-wwm-ext')
        num_classes = len(data['label_encoder'].classes_)
        feature_dim = data['train']['features'].shape[1]
        
        # åˆå§‹åŒ–tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizers['sentiment'] = tokenizer
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = CustomerServiceDataset(
            data['train']['texts'],
            data['train']['features'],
            data['train']['labels'],
            tokenizer
        )
        
        test_dataset = CustomerServiceDataset(
            data['test']['texts'],
            data['test']['features'],
            data['test']['labels'],
            tokenizer
        )
        
        batch_size = self.config.get('data', {}).get('batch_size', 16)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = MultiModalClassifier(model_name, num_classes, feature_dim)
        model.to(self.device)
        
        # è®­ç»ƒé…ç½®
        epochs = model_config.get('epochs', 3)
        learning_rate = model_config.get('learning_rate', 1e-5)
        
        optimizer = AdamW(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå†å²
        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
        
        # è®­ç»ƒå¾ªç¯ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸æ„å›¾åˆ†ç±»ç±»ä¼¼ï¼‰
        for epoch in range(epochs):
            print(f"\nEpoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0
            
            for batch in tqdm(train_loader, desc="Training"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                features = batch['features'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                outputs = model(input_ids, attention_mask, features)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch in test_loader:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    features = batch['features'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    outputs = model(input_ids, attention_mask, features)
                    loss = criterion(outputs, labels)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += labels.size(0)
                    val_correct += (predicted == labels).sum().item()
            
            # è®°å½•å†å²
            epoch_train_acc = 100. * train_correct / train_total
            epoch_val_acc = 100. * val_correct / val_total
            
            history['train_loss'].append(train_loss / len(train_loader))
            history['train_acc'].append(epoch_train_acc)
            history['val_loss'].append(val_loss / len(test_loader))
            history['val_acc'].append(epoch_val_acc)
            
            print(f"Train Acc: {epoch_train_acc:.2f}%, Val Acc: {epoch_val_acc:.2f}%")
        
        self.models['sentiment'] = model
        self.training_history['sentiment'] = history
        
        print("\nğŸ˜Š æƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒå®Œæˆ!")
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'history': history,
            'label_encoder': data['label_encoder']
        }
    
    def train_quality_assessor(self, data: Dict) -> Dict:
        """è®­ç»ƒè´¨é‡è¯„ä¼°æ¨¡å‹"""
        print("â­ è®­ç»ƒè´¨é‡è¯„ä¼°æ¨¡å‹...")
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = self.config.get('models', {}).get('response_generator', {})
        model_name = model_config.get('model_name', 'bert-base-chinese')
        feature_dim = data['train']['features'].shape[1]
        
        # åˆå§‹åŒ–tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizers['quality'] = tokenizer
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = CustomerServiceDataset(
            data['train']['texts'],
            data['train']['features'],
            data['train']['labels'],
            tokenizer
        )
        
        test_dataset = CustomerServiceDataset(
            data['test']['texts'],
            data['test']['features'],
            data['test']['labels'],
            tokenizer
        )
        
        batch_size = self.config.get('data', {}).get('batch_size', 16)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # åˆå§‹åŒ–å›å½’æ¨¡å‹
        model = MultiModalRegressor(model_name, feature_dim)
        model.to(self.device)
        
        # è®­ç»ƒé…ç½®
        epochs = 5
        learning_rate = 1e-5
        
        optimizer = AdamW(model.parameters(), lr=learning_rate)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒå†å²
        history = {'train_loss': [], 'val_loss': [], 'val_r2': []}
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            print(f"\nEpoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            
            for batch in tqdm(train_loader, desc="Training"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                features = batch['features'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                outputs = model(input_ids, attention_mask, features)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0
            all_predictions = []
            all_labels = []
            
            with torch.no_grad():
                for batch in test_loader:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    features = batch['features'].to(self.device)
                    labels = batch['labels'].to(self.device)
                    
                    outputs = model(input_ids, attention_mask, features)
                    loss = criterion(outputs, labels)
                    
                    val_loss += loss.item()
                    all_predictions.extend(outputs.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
            
            # è®¡ç®—RÂ²åˆ†æ•°
            r2 = r2_score(all_labels, all_predictions)
            
            history['train_loss'].append(train_loss / len(train_loader))
            history['val_loss'].append(val_loss / len(test_loader))
            history['val_r2'].append(r2)
            
            print(f"Train Loss: {train_loss/len(train_loader):.4f}")
            print(f"Val Loss: {val_loss/len(test_loader):.4f}, RÂ²: {r2:.4f}")
        
        self.models['quality'] = model
        self.training_history['quality'] = history
        
        print("\nâ­ è´¨é‡è¯„ä¼°æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'history': history,
            'r2_score': r2
        }
    
    def train_ensemble_model(self, datasets: Dict) -> Dict:
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        print("ğŸ­ è®­ç»ƒé›†æˆæ¨¡å‹ç³»ç»Ÿ...")
        
        ensemble_results = {}
        
        # è®­ç»ƒå„ä¸ªå­æ¨¡å‹
        intent_result = self.train_intent_classifier(datasets['intent_classification'])
        sentiment_result = self.train_sentiment_analyzer(datasets['sentiment_analysis'])
        quality_result = self.train_quality_assessor(datasets['quality_assessment'])
        
        ensemble_results['intent'] = intent_result
        ensemble_results['sentiment'] = sentiment_result
        ensemble_results['quality'] = quality_result
        
        # è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ä½œä¸ºå¤‡é€‰
        print("\nğŸ”„ è®­ç»ƒä¼ ç»ŸMLæ¨¡å‹ä½œä¸ºå¤‡é€‰...")
        
        # æ„å›¾åˆ†ç±»çš„éšæœºæ£®æ—æ¨¡å‹
        rf_intent = RandomForestClassifier(n_estimators=100, random_state=42)
        intent_data = datasets['intent_classification']
        rf_intent.fit(intent_data['train']['features'], intent_data['train']['labels'])
        
        # æƒ…æ„Ÿåˆ†æçš„é€»è¾‘å›å½’æ¨¡å‹
        lr_sentiment = LogisticRegression(random_state=42, max_iter=1000)
        sentiment_data = datasets['sentiment_analysis']
        lr_sentiment.fit(sentiment_data['train']['features'], sentiment_data['train']['labels'])
        
        # è´¨é‡è¯„ä¼°çš„æ¢¯åº¦æå‡å›å½’
        gb_quality = GradientBoostingRegressor(n_estimators=100, random_state=42)
        quality_data = datasets['quality_assessment']
        gb_quality.fit(quality_data['train']['features'], quality_data['train']['labels'])
        
        ensemble_results['traditional_models'] = {
            'intent_rf': rf_intent,
            'sentiment_lr': lr_sentiment,
            'quality_gb': gb_quality
        }
        
        print("\nğŸ­ é›†æˆæ¨¡å‹ç³»ç»Ÿè®­ç»ƒå®Œæˆ!")
        
        return ensemble_results
    
    def visualize_training_results(self):
        """å¯è§†åŒ–è®­ç»ƒç»“æœ"""
        print("ğŸ“Š ç”Ÿæˆè®­ç»ƒç»“æœå¯è§†åŒ–...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # æ„å›¾åˆ†ç±»è®­ç»ƒæ›²çº¿
        if 'intent' in self.training_history:
            history = self.training_history['intent']
            axes[0, 0].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', color='blue')
            axes[0, 0].plot(history['val_loss'], label='éªŒè¯æŸå¤±', color='red')
            axes[0, 0].set_title('æ„å›¾åˆ†ç±» - æŸå¤±æ›²çº¿')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            axes[1, 0].plot(history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
            axes[1, 0].plot(history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', color='red')
            axes[1, 0].set_title('æ„å›¾åˆ†ç±» - å‡†ç¡®ç‡æ›²çº¿')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Accuracy (%)')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # æƒ…æ„Ÿåˆ†æè®­ç»ƒæ›²çº¿
        if 'sentiment' in self.training_history:
            history = self.training_history['sentiment']
            axes[0, 1].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', color='green')
            axes[0, 1].plot(history['val_loss'], label='éªŒè¯æŸå¤±', color='orange')
            axes[0, 1].set_title('æƒ…æ„Ÿåˆ†æ - æŸå¤±æ›²çº¿')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            axes[1, 1].plot(history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡', color='green')
            axes[1, 1].plot(history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', color='orange')
            axes[1, 1].set_title('æƒ…æ„Ÿåˆ†æ - å‡†ç¡®ç‡æ›²çº¿')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Accuracy (%)')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        # è´¨é‡è¯„ä¼°è®­ç»ƒæ›²çº¿
        if 'quality' in self.training_history:
            history = self.training_history['quality']
            axes[0, 2].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', color='purple')
            axes[0, 2].plot(history['val_loss'], label='éªŒè¯æŸå¤±', color='brown')
            axes[0, 2].set_title('è´¨é‡è¯„ä¼° - æŸå¤±æ›²çº¿')
            axes[0, 2].set_xlabel('Epoch')
            axes[0, 2].set_ylabel('MSE Loss')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)
            
            axes[1, 2].plot(history['val_r2'], label='RÂ² åˆ†æ•°', color='purple')
            axes[1, 2].set_title('è´¨é‡è¯„ä¼° - RÂ² åˆ†æ•°')
            axes[1, 2].set_xlabel('Epoch')
            axes[1, 2].set_ylabel('RÂ² Score')
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def save_models(self, output_dir: str = "./models"):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        import os
        import joblib
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜PyTorchæ¨¡å‹
        for model_name, model in self.models.items():
            model_path = os.path.join(output_dir, f"{model_name}_model.pth")
            torch.save(model.state_dict(), model_path)
            print(f"ğŸ’¾ ä¿å­˜ {model_name} æ¨¡å‹: {model_path}")
        
        # ä¿å­˜tokenizers
        for tokenizer_name, tokenizer in self.tokenizers.items():
            tokenizer_path = os.path.join(output_dir, f"{tokenizer_name}_tokenizer")
            tokenizer.save_pretrained(tokenizer_path)
            print(f"ğŸ’¾ ä¿å­˜ {tokenizer_name} tokenizer: {tokenizer_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(output_dir, "training_history.json")
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®
    config = {
        'data': {
            'batch_size': 16,
            'max_sequence_length': 512
        },
        'models': {
            'intent_classifier': {
                'model_name': 'bert-base-chinese',
                'epochs': 3,
                'learning_rate': 2e-5
            },
            'sentiment_analyzer': {
                'model_name': 'hfl/chinese-roberta-wwm-ext',
                'epochs': 3,
                'learning_rate': 1e-5
            }
        }
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CustomerServiceModelTrainer(config)
    
    # åŠ è½½æ•°æ®
    datasets = trainer.load_data("./data/processed")
    
    # è®­ç»ƒé›†æˆæ¨¡å‹
    ensemble_results = trainer.train_ensemble_model(datasets)
    
    # å¯è§†åŒ–ç»“æœ
    trainer.visualize_training_results()
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_models()
    
    print("\nğŸ‰ æ™ºèƒ½å®¢æœæ¨¡å‹è®­ç»ƒå®Œæˆ!")
```

## 4.6.4 APIæœåŠ¡å¼€å‘ä¸éƒ¨ç½²

### æ™ºèƒ½å®¢æœAPIæœåŠ¡ç³»ç»Ÿ

```python
# api_service.py - æ™ºèƒ½å®¢æœAPIæœåŠ¡ç³»ç»Ÿ
from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
from transformers import AutoTokenizer
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
import json
import os
import logging
from datetime import datetime
import time
import pickle
from dataclasses import dataclass
import threading
from queue import Queue
import redis
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('./logs/api_service.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“æ¨¡å‹
Base = declarative_base()

class ConversationLog(Base):
    """å¯¹è¯æ—¥å¿—è¡¨"""
    __tablename__ = 'conversation_logs'
    
    id = Column(Integer, primary_key=True)
    session_id = Column(String(100), nullable=False)
    customer_message = Column(Text, nullable=False)
    predicted_intent = Column(String(50))
    predicted_sentiment = Column(String(20))
    confidence_score = Column(Float)
    response_time = Column(Float)
    timestamp = Column(DateTime, default=datetime.utcnow)
    
class ServiceMetrics(Base):
    """æœåŠ¡æŒ‡æ ‡è¡¨"""
    __tablename__ = 'service_metrics'
    
    id = Column(Integer, primary_key=True)
    endpoint = Column(String(100), nullable=False)
    request_count = Column(Integer, default=0)
    avg_response_time = Column(Float, default=0.0)
    error_count = Column(Integer, default=0)
    timestamp = Column(DateTime, default=datetime.utcnow)

@dataclass
class PredictionRequest:
    """é¢„æµ‹è¯·æ±‚æ•°æ®ç±»"""
    text: str
    session_id: Optional[str] = None
    context: Optional[Dict] = None

@dataclass
class PredictionResponse:
    """é¢„æµ‹å“åº”æ•°æ®ç±»"""
    intent: str
    intent_confidence: float
    sentiment: str
    sentiment_confidence: float
    quality_score: float
    response_time: float
    suggestions: List[str]
    session_id: str

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, models_dir: str = "./models"):
        self.models_dir = models_dir
        self.models = {}
        self.tokenizers = {}
        self.label_encoders = {}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self._load_models()
        
        logger.info(f"æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _load_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        try:
            # åŠ è½½tokenizers
            for model_type in ['intent', 'sentiment', 'quality']:
                tokenizer_path = os.path.join(self.models_dir, f"{model_type}_tokenizer")
                if os.path.exists(tokenizer_path):
                    self.tokenizers[model_type] = AutoTokenizer.from_pretrained(tokenizer_path)
                    logger.info(f"åŠ è½½ {model_type} tokenizer æˆåŠŸ")
            
            # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
            label_encoders_path = os.path.join(self.models_dir, "../data/processed/label_encoders.pkl")
            if os.path.exists(label_encoders_path):
                with open(label_encoders_path, 'rb') as f:
                    self.label_encoders = pickle.load(f)
                logger.info("åŠ è½½æ ‡ç­¾ç¼–ç å™¨æˆåŠŸ")
            
            # æ³¨æ„ï¼šå®é™…éƒ¨ç½²æ—¶éœ€è¦åŠ è½½å®Œæ•´çš„PyTorchæ¨¡å‹
            # è¿™é‡Œä¸ºæ¼”ç¤ºç›®çš„ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
            self._create_mock_models()
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _create_mock_models(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹ç”¨äºæ¼”ç¤º"""
        # æ¨¡æ‹Ÿæ„å›¾åˆ†ç±»ç»“æœ
        self.intent_classes = [
            'è´¦æˆ·æŸ¥è¯¢', 'ä½™é¢æŸ¥è¯¢', 'è½¬è´¦æ±‡æ¬¾', 'ä¿¡ç”¨å¡ç”³è¯·', 'è´·æ¬¾å’¨è¯¢',
            'æŠ•èµ„ç†è´¢', 'ä¿é™©å’¨è¯¢', 'æŠ€æœ¯æ”¯æŒ', 'æŠ•è¯‰å»ºè®®', 'äº§å“ä»‹ç»'
        ]
        
        # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æç»“æœ
        self.sentiment_classes = ['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢']
        
        logger.info("åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å®Œæˆ")
    
    def predict_intent(self, text: str) -> Tuple[str, float]:
        """é¢„æµ‹æ„å›¾"""
        # å®é™…å®ç°ä¸­ä¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ä½œä¸ºæ¼”ç¤º
        
        intent_keywords = {
            'è´¦æˆ·æŸ¥è¯¢': ['è´¦æˆ·', 'æŸ¥è¯¢', 'çŠ¶æ€'],
            'ä½™é¢æŸ¥è¯¢': ['ä½™é¢', 'å¤šå°‘é’±', 'æŸ¥è¯¢'],
            'è½¬è´¦æ±‡æ¬¾': ['è½¬è´¦', 'æ±‡æ¬¾', 'è½¬é’±'],
            'æŠ€æœ¯æ”¯æŒ': ['ç™»å½•', 'å¯†ç ', 'é”™è¯¯', 'é—®é¢˜'],
            'æŠ•è¯‰å»ºè®®': ['æŠ•è¯‰', 'ä¸æ»¡æ„', 'å»ºè®®']
        }
        
        text_lower = text.lower()
        best_intent = 'å…¶ä»–å’¨è¯¢'
        best_score = 0.5
        
        for intent, keywords in intent_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                confidence = min(0.9, 0.6 + score * 0.1)
                if confidence > best_score:
                    best_intent = intent
                    best_score = confidence
        
        return best_intent, best_score
    
    def predict_sentiment(self, text: str) -> Tuple[str, float]:
        """é¢„æµ‹æƒ…æ„Ÿ"""
        # ç®€å•çš„æƒ…æ„Ÿåˆ†æå®ç°
        positive_words = ['å¥½', 'è°¢è°¢', 'æ»¡æ„', 'ä¸é”™', 'ä¼˜ç§€']
        negative_words = ['ä¸å¥½', 'å·®', 'çƒ‚', 'æŠ•è¯‰', 'é—®é¢˜', 'é”™è¯¯']
        
        text_lower = text.lower()
        
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if pos_count > neg_count:
            return 'æ­£é¢', 0.8
        elif neg_count > pos_count:
            return 'è´Ÿé¢', 0.8
        else:
            return 'ä¸­æ€§', 0.7
    
    def predict_quality(self, response_text: str, context: Dict) -> float:
        """é¢„æµ‹å›å¤è´¨é‡"""
        # ç®€å•çš„è´¨é‡è¯„ä¼°
        base_score = 3.5
        
        # æ ¹æ®å›å¤é•¿åº¦è°ƒæ•´
        if len(response_text) > 50:
            base_score += 0.3
        
        # æ ¹æ®æƒ…æ„Ÿè°ƒæ•´
        sentiment = context.get('sentiment', 'ä¸­æ€§')
        if sentiment == 'æ­£é¢':
            base_score += 0.5
        elif sentiment == 'è´Ÿé¢':
            base_score -= 0.3
        
        return min(5.0, max(1.0, base_score))

class CustomerServiceAPI:
    """æ™ºèƒ½å®¢æœAPIæœåŠ¡"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.app = Flask(__name__)
        CORS(self.app)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model_manager = ModelManager()
        self.request_queue = Queue(maxsize=1000)
        self.redis_client = None
        self.db_session = None
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'avg_response_time': 0.0
        }
        
        self._setup_database()
        self._setup_redis()
        self._setup_routes()
        
        logger.info("æ™ºèƒ½å®¢æœAPIæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_database(self):
        """è®¾ç½®æ•°æ®åº“è¿æ¥"""
        try:
            db_url = self.config.get('database', {}).get('url', 'sqlite:///customer_service.db')
            engine = create_engine(db_url)
            Base.metadata.create_all(engine)
            
            Session = sessionmaker(bind=engine)
            self.db_session = Session()
            
            logger.info("æ•°æ®åº“è¿æ¥è®¾ç½®æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ•°æ®åº“è®¾ç½®å¤±è´¥: {e}")
    
    def _setup_redis(self):
        """è®¾ç½®Redisè¿æ¥"""
        try:
            redis_config = self.config.get('redis', {})
            self.redis_client = redis.Redis(
                host=redis_config.get('host', 'localhost'),
                port=redis_config.get('port', 6379),
                db=redis_config.get('db', 0),
                decode_responses=True
            )
            
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            logger.info("Redisè¿æ¥è®¾ç½®æˆåŠŸ")
        except Exception as e:
            logger.warning(f"Redisè®¾ç½®å¤±è´¥: {e}")
            self.redis_client = None
    
    def _setup_routes(self):
        """è®¾ç½®APIè·¯ç”±"""
        
        @self.app.route('/health', methods=['GET'])
        def health_check():
            """å¥åº·æ£€æŸ¥"""
            return jsonify({
                'status': 'healthy',
                'timestamp': datetime.utcnow().isoformat(),
                'version': '1.0.0'
            })
        
        @self.app.route('/predict', methods=['POST'])
        def predict():
            """å•æ¡é¢„æµ‹"""
            start_time = time.time()
            
            try:
                data = request.get_json()
                
                if not data or 'text' not in data:
                    return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: text'}), 400
                
                # åˆ›å»ºé¢„æµ‹è¯·æ±‚
                pred_request = PredictionRequest(
                    text=data['text'],
                    session_id=data.get('session_id'),
                    context=data.get('context', {})
                )
                
                # æ‰§è¡Œé¢„æµ‹
                result = self._process_prediction(pred_request)
                
                # è®°å½•å“åº”æ—¶é—´
                response_time = time.time() - start_time
                result.response_time = response_time
                
                # è®°å½•æ—¥å¿—
                self._log_conversation(pred_request, result)
                
                # æ›´æ–°æŒ‡æ ‡
                self._update_metrics('predict', response_time, True)
                
                return jsonify(result.__dict__)
                
            except Exception as e:
                logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
                self._update_metrics('predict', time.time() - start_time, False)
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/batch_predict', methods=['POST'])
        def batch_predict():
            """æ‰¹é‡é¢„æµ‹"""
            start_time = time.time()
            
            try:
                data = request.get_json()
                
                if not data or 'texts' not in data:
                    return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°: texts'}), 400
                
                texts = data['texts']
                if len(texts) > 100:  # é™åˆ¶æ‰¹é‡å¤§å°
                    return jsonify({'error': 'æ‰¹é‡å¤§å°ä¸èƒ½è¶…è¿‡100'}), 400
                
                results = []
                for i, text in enumerate(texts):
                    pred_request = PredictionRequest(
                        text=text,
                        session_id=data.get('session_id', f'batch_{i}'),
                        context=data.get('context', {})
                    )
                    
                    result = self._process_prediction(pred_request)
                    results.append(result.__dict__)
                
                response_time = time.time() - start_time
                self._update_metrics('batch_predict', response_time, True)
                
                return jsonify({
                    'results': results,
                    'total_count': len(results),
                    'response_time': response_time
                })
                
            except Exception as e:
                logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
                self._update_metrics('batch_predict', time.time() - start_time, False)
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/metrics', methods=['GET'])
        def get_metrics():
            """è·å–æœåŠ¡æŒ‡æ ‡"""
            return jsonify(self.metrics)
        
        @self.app.route('/conversation_history/<session_id>', methods=['GET'])
        def get_conversation_history(session_id):
            """è·å–å¯¹è¯å†å²"""
            try:
                if self.db_session:
                    logs = self.db_session.query(ConversationLog).filter(
                        ConversationLog.session_id == session_id
                    ).order_by(ConversationLog.timestamp.desc()).limit(50).all()
                    
                    history = []
                    for log in logs:
                        history.append({
                            'timestamp': log.timestamp.isoformat(),
                            'message': log.customer_message,
                            'intent': log.predicted_intent,
                            'sentiment': log.predicted_sentiment,
                            'confidence': log.confidence_score
                        })
                    
                    return jsonify({'history': history})
                else:
                    return jsonify({'error': 'æ•°æ®åº“æœªé…ç½®'}), 500
                    
            except Exception as e:
                logger.error(f"è·å–å¯¹è¯å†å²å¤±è´¥: {e}")
                return jsonify({'error': str(e)}), 500
    
    def _process_prediction(self, request: PredictionRequest) -> PredictionResponse:
        """å¤„ç†é¢„æµ‹è¯·æ±‚"""
        # é¢„æµ‹æ„å›¾
        intent, intent_confidence = self.model_manager.predict_intent(request.text)
        
        # é¢„æµ‹æƒ…æ„Ÿ
        sentiment, sentiment_confidence = self.model_manager.predict_sentiment(request.text)
        
        # é¢„æµ‹è´¨é‡ï¼ˆåŸºäºä¸Šä¸‹æ–‡ï¼‰
        context = request.context or {}
        context['sentiment'] = sentiment
        quality_score = self.model_manager.predict_quality(request.text, context)
        
        # ç”Ÿæˆå»ºè®®
        suggestions = self._generate_suggestions(intent, sentiment, quality_score)
        
        # ç”Ÿæˆæˆ–ä½¿ç”¨ä¼šè¯ID
        session_id = request.session_id or f"session_{int(time.time())}"
        
        return PredictionResponse(
            intent=intent,
            intent_confidence=intent_confidence,
            sentiment=sentiment,
            sentiment_confidence=sentiment_confidence,
            quality_score=quality_score,
            response_time=0.0,  # å°†åœ¨è·¯ç”±ä¸­è®¾ç½®
            suggestions=suggestions,
            session_id=session_id
        )
    
    def _generate_suggestions(self, intent: str, sentiment: str, quality_score: float) -> List[str]:
        """ç”Ÿæˆå¤„ç†å»ºè®®"""
        suggestions = []
        
        # åŸºäºæ„å›¾çš„å»ºè®®
        intent_suggestions = {
            'è´¦æˆ·æŸ¥è¯¢': ['æä¾›è´¦æˆ·è¯¦ç»†ä¿¡æ¯', 'ç¡®è®¤èº«ä»½éªŒè¯', 'å±•ç¤ºè´¦æˆ·çŠ¶æ€'],
            'ä½™é¢æŸ¥è¯¢': ['æ˜¾ç¤ºå½“å‰ä½™é¢', 'æä¾›äº¤æ˜“å†å²', 'æ¨èç†è´¢äº§å“'],
            'è½¬è´¦æ±‡æ¬¾': ['éªŒè¯æ”¶æ¬¾ä¿¡æ¯', 'ç¡®è®¤è½¬è´¦é‡‘é¢', 'æä¾›æ‰‹ç»­è´¹è¯´æ˜'],
            'æŠ€æœ¯æ”¯æŒ': ['æä¾›è¯¦ç»†æ“ä½œæ­¥éª¤', 'å®‰æ’æŠ€æœ¯ä¸“å‘˜', 'å‘é€æ“ä½œæŒ‡å—'],
            'æŠ•è¯‰å»ºè®®': ['è®°å½•æŠ•è¯‰å†…å®¹', 'å®‰æ’å®¢æœç»ç†', 'æä¾›è§£å†³æ–¹æ¡ˆ']
        }
        
        if intent in intent_suggestions:
            suggestions.extend(intent_suggestions[intent])
        
        # åŸºäºæƒ…æ„Ÿçš„å»ºè®®
        if sentiment == 'è´Ÿé¢':
            suggestions.extend(['è¡¨ç¤ºæ­‰æ„', 'æä¾›è¡¥å¿æ–¹æ¡ˆ', 'å‡çº§å¤„ç†çº§åˆ«'])
        elif sentiment == 'æ­£é¢':
            suggestions.extend(['æ„Ÿè°¢å®¢æˆ·', 'æ¨èç›¸å…³æœåŠ¡', 'æ”¶é›†æ»¡æ„åº¦åé¦ˆ'])
        
        # åŸºäºè´¨é‡åˆ†æ•°çš„å»ºè®®
        if quality_score < 3.0:
            suggestions.append('éœ€è¦äººå·¥ä»‹å…¥')
        elif quality_score > 4.0:
            suggestions.append('å¯ä»¥è‡ªåŠ¨å¤„ç†')
        
        return suggestions[:5]  # é™åˆ¶å»ºè®®æ•°é‡
    
    def _log_conversation(self, request: PredictionRequest, response: PredictionResponse):
        """è®°å½•å¯¹è¯æ—¥å¿—"""
        try:
            if self.db_session:
                log = ConversationLog(
                    session_id=response.session_id,
                    customer_message=request.text,
                    predicted_intent=response.intent,
                    predicted_sentiment=response.sentiment,
                    confidence_score=response.intent_confidence,
                    response_time=response.response_time
                )
                
                self.db_session.add(log)
                self.db_session.commit()
            
            # ç¼“å­˜åˆ°Redis
            if self.redis_client:
                cache_key = f"conversation:{response.session_id}:latest"
                cache_data = {
                    'text': request.text,
                    'intent': response.intent,
                    'sentiment': response.sentiment,
                    'timestamp': datetime.utcnow().isoformat()
                }
                self.redis_client.setex(cache_key, 3600, json.dumps(cache_data))
                
        except Exception as e:
            logger.error(f"è®°å½•å¯¹è¯æ—¥å¿—å¤±è´¥: {e}")
    
    def _update_metrics(self, endpoint: str, response_time: float, success: bool):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.metrics['total_requests'] += 1
        
        if success:
            self.metrics['successful_requests'] += 1
        else:
            self.metrics['failed_requests'] += 1
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        current_avg = self.metrics['avg_response_time']
        total_requests = self.metrics['total_requests']
        
        self.metrics['avg_response_time'] = (
            (current_avg * (total_requests - 1) + response_time) / total_requests
        )
    
    def run(self, host: str = '0.0.0.0', port: int = 5000, debug: bool = False):
        """å¯åŠ¨APIæœåŠ¡"""
        logger.info(f"å¯åŠ¨æ™ºèƒ½å®¢æœAPIæœåŠ¡: {host}:{port}")
        self.app.run(host=host, port=port, debug=debug, threaded=True)

# é…ç½®æ–‡ä»¶ç¤ºä¾‹
API_CONFIG = {
    'database': {
        'url': 'sqlite:///customer_service.db'
    },
    'redis': {
        'host': 'localhost',
        'port': 6379,
        'db': 0
    },
    'models': {
        'models_dir': './models'
    },
    'server': {
        'host': '0.0.0.0',
        'port': 5000,
        'debug': False
    }
}

# å¯åŠ¨è„šæœ¬
if __name__ == "__main__":
    # åˆ›å»ºAPIæœåŠ¡
    api_service = CustomerServiceAPI(API_CONFIG)
    
    # å¯åŠ¨æœåŠ¡
    server_config = API_CONFIG['server']
    api_service.run(
        host=server_config['host'],
        port=server_config['port'],
        debug=server_config['debug']
    )
```

### å®¢æˆ·ç«¯SDKå¼€å‘

```python
# client_sdk.py - æ™ºèƒ½å®¢æœå®¢æˆ·ç«¯SDK
import requests
import json
from typing import Dict, List, Optional
from dataclasses import dataclass
import time

@dataclass
class ClientConfig:
    """å®¢æˆ·ç«¯é…ç½®"""
    base_url: str
    api_key: Optional[str] = None
    timeout: int = 30
    retry_count: int = 3

class CustomerServiceClient:
    """æ™ºèƒ½å®¢æœå®¢æˆ·ç«¯"""
    
    def __init__(self, config: ClientConfig):
        self.config = config
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'CustomerService-Client/1.0'
        })
        
        if config.api_key:
            self.session.headers['Authorization'] = f'Bearer {config.api_key}'
    
    def predict(self, text: str, session_id: Optional[str] = None, 
                context: Optional[Dict] = None) -> Dict:
        """å•æ¡é¢„æµ‹"""
        url = f"{self.config.base_url}/predict"
        
        payload = {
            'text': text,
            'session_id': session_id,
            'context': context or {}
        }
        
        return self._make_request('POST', url, payload)
    
    def batch_predict(self, texts: List[str], session_id: Optional[str] = None,
                     context: Optional[Dict] = None) -> Dict:
        """æ‰¹é‡é¢„æµ‹"""
        url = f"{self.config.base_url}/batch_predict"
        
        payload = {
            'texts': texts,
            'session_id': session_id,
            'context': context or {}
        }
        
        return self._make_request('POST', url, payload)
    
    def get_conversation_history(self, session_id: str) -> Dict:
        """è·å–å¯¹è¯å†å²"""
        url = f"{self.config.base_url}/conversation_history/{session_id}"
        return self._make_request('GET', url)
    
    def get_metrics(self) -> Dict:
        """è·å–æœåŠ¡æŒ‡æ ‡"""
        url = f"{self.config.base_url}/metrics"
        return self._make_request('GET', url)
    
    def health_check(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        url = f"{self.config.base_url}/health"
        return self._make_request('GET', url)
    
    def _make_request(self, method: str, url: str, payload: Optional[Dict] = None) -> Dict:
        """å‘é€HTTPè¯·æ±‚"""
        for attempt in range(self.config.retry_count):
            try:
                if method == 'GET':
                    response = self.session.get(url, timeout=self.config.timeout)
                elif method == 'POST':
                    response = self.session.post(
                        url, 
                        json=payload, 
                        timeout=self.config.timeout
                    )
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„HTTPæ–¹æ³•: {method}")
                
                response.raise_for_status()
                return response.json()
                
            except requests.exceptions.RequestException as e:
                if attempt == self.config.retry_count - 1:
                    raise Exception(f"è¯·æ±‚å¤±è´¥: {e}")
                
                # æŒ‡æ•°é€€é¿é‡è¯•
                time.sleep(2 ** attempt)
        
        raise Exception("è¯·æ±‚é‡è¯•æ¬¡æ•°å·²ç”¨å®Œ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®å®¢æˆ·ç«¯
    config = ClientConfig(
        base_url="http://localhost:5000",
        timeout=30,
        retry_count=3
    )
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = CustomerServiceClient(config)
    
    # å¥åº·æ£€æŸ¥
    health = client.health_check()
    print(f"æœåŠ¡çŠ¶æ€: {health}")
    
    # å•æ¡é¢„æµ‹
    result = client.predict(
        text="æˆ‘æƒ³æŸ¥è¯¢ä¸€ä¸‹è´¦æˆ·ä½™é¢",
        session_id="test_session_001"
    )
    print(f"é¢„æµ‹ç»“æœ: {result}")
    
    # æ‰¹é‡é¢„æµ‹
    batch_result = client.batch_predict([
        "æˆ‘è¦è½¬è´¦ç»™æœ‹å‹",
        "å¯†ç å¿˜è®°äº†æ€ä¹ˆåŠ",
        "ä½ ä»¬çš„æœåŠ¡çœŸä¸é”™"
    ])
    print(f"æ‰¹é‡é¢„æµ‹ç»“æœ: {batch_result}")
    
    # è·å–æŒ‡æ ‡
    metrics = client.get_metrics()
    print(f"æœåŠ¡æŒ‡æ ‡: {metrics}")
```

## 4.6.5 å®¹å™¨åŒ–éƒ¨ç½²ä¸ç›‘æ§

### Dockerå®¹å™¨åŒ–é…ç½®

```dockerfile
# Dockerfile - æ™ºèƒ½å®¢æœAPIæœåŠ¡å®¹å™¨åŒ–
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p logs models data

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV FLASK_APP=api_service.py
ENV FLASK_ENV=production

# æš´éœ²ç«¯å£
EXPOSE 5000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "api_service.py"]
```

```yaml
# docker-compose.yml - å®Œæ•´æœåŠ¡ç¼–æ’
version: '3.8'

services:
  # æ™ºèƒ½å®¢æœAPIæœåŠ¡
  customer-service-api:
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/customer_service
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
      - ./data:/app/data
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    networks:
      - customer-service-network
  
  # PostgreSQLæ•°æ®åº“
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_DB=customer_service
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - customer-service-network
  
  # Redisç¼“å­˜
  redis:
    image: redis:6-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - customer-service-network
  
  # Nginxåå‘ä»£ç†
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - customer-service-api
    restart: unless-stopped
    networks:
      - customer-service-network
  
  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped
    networks:
      - customer-service-network
  
  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - customer-service-network

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  customer-service-network:
    driver: bridge
```

### ç›‘æ§ä¸æ—¥å¿—ç³»ç»Ÿ

```python
# monitoring.py - ç›‘æ§ç³»ç»Ÿ
import psutil
import time
import json
from datetime import datetime
from typing import Dict, List
import logging
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import threading

# PrometheusæŒ‡æ ‡
REQUEST_COUNT = Counter('api_requests_total', 'APIè¯·æ±‚æ€»æ•°', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('api_request_duration_seconds', 'APIè¯·æ±‚è€—æ—¶', ['method', 'endpoint'])
SYSTEM_CPU_USAGE = Gauge('system_cpu_usage_percent', 'CPUä½¿ç”¨ç‡')
SYSTEM_MEMORY_USAGE = Gauge('system_memory_usage_percent', 'å†…å­˜ä½¿ç”¨ç‡')
MODEL_PREDICTION_COUNT = Counter('model_predictions_total', 'æ¨¡å‹é¢„æµ‹æ€»æ•°', ['model_type'])
MODEL_PREDICTION_LATENCY = Histogram('model_prediction_latency_seconds', 'æ¨¡å‹é¢„æµ‹å»¶è¿Ÿ', ['model_type'])

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self, update_interval: int = 10):
        self.update_interval = update_interval
        self.running = False
        self.monitor_thread = None
        
        # å¯åŠ¨PrometheusæŒ‡æ ‡æœåŠ¡å™¨
        start_http_server(8000)
        logging.info("PrometheusæŒ‡æ ‡æœåŠ¡å™¨å¯åŠ¨åœ¨ç«¯å£8000")
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        logging.info("ç³»ç»Ÿç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join()
        logging.info("ç³»ç»Ÿç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            try:
                # æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
                cpu_percent = psutil.cpu_percent(interval=1)
                memory_percent = psutil.virtual_memory().percent
                
                SYSTEM_CPU_USAGE.set(cpu_percent)
                SYSTEM_MEMORY_USAGE.set(memory_percent)
                
                # è®°å½•ç³»ç»ŸçŠ¶æ€
                logging.info(f"ç³»ç»ŸçŠ¶æ€ - CPU: {cpu_percent:.1f}%, å†…å­˜: {memory_percent:.1f}%")
                
                time.sleep(self.update_interval)
                
            except Exception as e:
                logging.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(5)
    
    def record_api_request(self, method: str, endpoint: str, status_code: int, duration: float):
        """è®°å½•APIè¯·æ±‚æŒ‡æ ‡"""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=str(status_code)).inc()
        REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
    
    def record_model_prediction(self, model_type: str, latency: float):
        """è®°å½•æ¨¡å‹é¢„æµ‹æŒ‡æ ‡"""
        MODEL_PREDICTION_COUNT.labels(model_type=model_type).inc()
        MODEL_PREDICTION_LATENCY.labels(model_type=model_type).observe(latency)

class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, log_file: str = './logs/api_service.log'):
        self.log_file = log_file
        self.error_patterns = [
            'ERROR',
            'CRITICAL',
            'Exception',
            'Failed',
            'Timeout'
        ]
    
    def analyze_logs(self, hours: int = 24) -> Dict:
        """åˆ†ææ—¥å¿—"""
        try:
            with open(self.log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # ç»Ÿè®¡é”™è¯¯
            error_count = 0
            warning_count = 0
            info_count = 0
            
            error_details = []
            
            for line in lines:
                if 'ERROR' in line or 'CRITICAL' in line:
                    error_count += 1
                    error_details.append(line.strip())
                elif 'WARNING' in line:
                    warning_count += 1
                elif 'INFO' in line:
                    info_count += 1
            
            return {
                'total_lines': len(lines),
                'error_count': error_count,
                'warning_count': warning_count,
                'info_count': info_count,
                'error_rate': error_count / len(lines) if lines else 0,
                'recent_errors': error_details[-10:],  # æœ€è¿‘10ä¸ªé”™è¯¯
                'analysis_time': datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            logging.error(f"æ—¥å¿—åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        try:
            # åˆ†æå“åº”æ—¶é—´åˆ†å¸ƒ
            response_times = []
            
            with open(self.log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if 'response_time' in line:
                        # æå–å“åº”æ—¶é—´ï¼ˆç®€åŒ–å®ç°ï¼‰
                        try:
                            parts = line.split('response_time:')
                            if len(parts) > 1:
                                time_str = parts[1].split()[0]
                                response_times.append(float(time_str))
                        except:
                            continue
            
            if response_times:
                return {
                    'avg_response_time': sum(response_times) / len(response_times),
                    'min_response_time': min(response_times),
                    'max_response_time': max(response_times),
                    'total_requests': len(response_times),
                    'p95_response_time': sorted(response_times)[int(len(response_times) * 0.95)]
                }
            else:
                return {'message': 'æœªæ‰¾åˆ°å“åº”æ—¶é—´æ•°æ®'}
                
        except Exception as e:
            return {'error': str(e)}

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å¯åŠ¨ç³»ç»Ÿç›‘æ§
    monitor = SystemMonitor(update_interval=5)
    monitor.start_monitoring()
    
    # åˆ›å»ºæ—¥å¿—åˆ†æå™¨
    log_analyzer = LogAnalyzer()
    
    try:
        # è¿è¡Œä¸€æ®µæ—¶é—´
        time.sleep(60)
        
        # åˆ†ææ—¥å¿—
        log_analysis = log_analyzer.analyze_logs()
        print(f"æ—¥å¿—åˆ†æç»“æœ: {json.dumps(log_analysis, indent=2, ensure_ascii=False)}")
        
        # æ€§èƒ½ç»Ÿè®¡
        perf_stats = log_analyzer.get_performance_stats()
        print(f"æ€§èƒ½ç»Ÿè®¡: {json.dumps(perf_stats, indent=2, ensure_ascii=False)}")
        
    finally:
        monitor.stop_monitoring()
```

## 4.6.6 é¡¹ç›®æ€»ç»“ä¸æœ€ä½³å®è·µ

### é¡¹ç›®æˆæœæ€»ç»“

æœ¬æ™ºèƒ½å®¢æˆ·æœåŠ¡ç³»ç»Ÿé¡¹ç›®å±•ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯AIåº”ç”¨å¼€å‘æµç¨‹ï¼Œä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒï¼Œå†åˆ°APIæœåŠ¡éƒ¨ç½²å’Œç›‘æ§ï¼Œæ¶µç›–äº†ç°ä»£AIç³»ç»Ÿå¼€å‘çš„å„ä¸ªå…³é”®ç¯èŠ‚ã€‚

#### æ ¸å¿ƒæŠ€æœ¯æˆæœ

| æŠ€æœ¯æ¨¡å— | å®ç°åŠŸèƒ½ | æŠ€æœ¯äº®ç‚¹ | ä¸šåŠ¡ä»·å€¼ |
|---------|---------|---------|----------|
| **æ•°æ®å¤„ç†ç³»ç»Ÿ** | å¤šæºæ•°æ®æ•´åˆã€æ™ºèƒ½æ¸…æ´—ã€ç‰¹å¾å·¥ç¨‹ | è‡ªåŠ¨åŒ–æ•°æ®è´¨é‡æ£€æµ‹ã€å¤šæ¨¡æ€ç‰¹å¾æå– | æå‡æ•°æ®è´¨é‡ï¼Œå‡å°‘äººå·¥å¤„ç†æˆæœ¬ |
| **AIæ¨¡å‹é›†æˆ** | æ„å›¾è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€è´¨é‡è¯„ä¼° | BERT/RoBERTaæ·±åº¦å­¦ä¹ +ä¼ ç»ŸMLå¤‡é€‰ | æ™ºèƒ½åŒ–å®¢æœå“åº”ï¼Œæå‡æœåŠ¡æ•ˆç‡ |
| **APIæœåŠ¡æ¶æ„** | RESTfulæ¥å£ã€æ‰¹é‡å¤„ç†ã€å®æ—¶é¢„æµ‹ | å¼‚æ­¥å¤„ç†ã€ç¼“å­˜ä¼˜åŒ–ã€é”™è¯¯æ¢å¤ | é«˜å¹¶å‘æ”¯æŒï¼Œç¨³å®šå¯é çš„æœåŠ¡ |
| **å®¹å™¨åŒ–éƒ¨ç½²** | Dockerç¼–æ’ã€å¾®æœåŠ¡æ¶æ„ | è‡ªåŠ¨æ‰©ç¼©å®¹ã€æœåŠ¡å‘ç°ã€è´Ÿè½½å‡è¡¡ | äº‘åŸç”Ÿéƒ¨ç½²ï¼Œè¿ç»´æˆæœ¬é™ä½ |
| **ç›‘æ§å‘Šè­¦ç³»ç»Ÿ** | å®æ—¶ç›‘æ§ã€æ€§èƒ½åˆ†æã€æ—¥å¿—ç®¡ç† | Prometheus+Grafanaå¯è§†åŒ– | ä¸»åŠ¨è¿ç»´ï¼Œå¿«é€Ÿé—®é¢˜å®šä½ |

#### ç³»ç»Ÿæ¶æ„ä¼˜åŠ¿

```python
# æ¶æ„ä¼˜åŠ¿å±•ç¤ºä»£ç 
class SystemAdvantages:
    """ç³»ç»Ÿæ¶æ„ä¼˜åŠ¿åˆ†æ"""
    
    def __init__(self):
        self.advantages = {
            'å¯æ‰©å±•æ€§': {
                'å¾®æœåŠ¡æ¶æ„': 'ç‹¬ç«‹éƒ¨ç½²ã€æ°´å¹³æ‰©å±•',
                'å®¹å™¨åŒ–éƒ¨ç½²': 'Docker + Kubernetesè‡ªåŠ¨ç¼–æ’',
                'è´Ÿè½½å‡è¡¡': 'Nginxåå‘ä»£ç†ï¼Œå¤šå®ä¾‹æ”¯æŒ'
            },
            'å¯é æ€§': {
                'å®¹é”™æœºåˆ¶': 'å¤šæ¨¡å‹å¤‡é€‰ã€å¼‚å¸¸è‡ªåŠ¨æ¢å¤',
                'æ•°æ®æŒä¹…åŒ–': 'PostgreSQL + RedisåŒé‡ä¿éšœ',
                'å¥åº·æ£€æŸ¥': 'è‡ªåŠ¨æ•…éšœæ£€æµ‹å’ŒæœåŠ¡é‡å¯'
            },
            'æ€§èƒ½ä¼˜åŒ–': {
                'ç¼“å­˜ç­–ç•¥': 'Redisç¼“å­˜çƒ­ç‚¹æ•°æ®',
                'æ‰¹é‡å¤„ç†': 'æ”¯æŒæ‰¹é‡é¢„æµ‹ï¼Œæå‡ååé‡',
                'å¼‚æ­¥å¤„ç†': 'éé˜»å¡I/Oï¼Œæå‡å¹¶å‘èƒ½åŠ›'
            },
            'è¿ç»´å‹å¥½': {
                'ç›‘æ§ä½“ç³»': 'Prometheus + Grafanaå…¨æ–¹ä½ç›‘æ§',
                'æ—¥å¿—ç®¡ç†': 'ç»“æ„åŒ–æ—¥å¿—ï¼Œä¾¿äºé—®é¢˜è¿½è¸ª',
                'è‡ªåŠ¨åŒ–éƒ¨ç½²': 'CI/CDæµæ°´çº¿ï¼Œä¸€é”®éƒ¨ç½²'
            }
        }
    
    def generate_architecture_report(self) -> str:
        """ç”Ÿæˆæ¶æ„ä¼˜åŠ¿æŠ¥å‘Š"""
        report = "# æ™ºèƒ½å®¢æœç³»ç»Ÿæ¶æ„ä¼˜åŠ¿æŠ¥å‘Š\n\n"
        
        for category, items in self.advantages.items():
            report += f"## {category}\n\n"
            for feature, description in items.items():
                report += f"- **{feature}**: {description}\n"
            report += "\n"
        
        return report
    
    def calculate_performance_metrics(self) -> Dict:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        return {
            'å“åº”æ—¶é—´': {
                'å¹³å‡å“åº”æ—¶é—´': '< 200ms',
                'P95å“åº”æ—¶é—´': '< 500ms',
                'P99å“åº”æ—¶é—´': '< 1s'
            },
            'ååé‡': {
                'å•å®ä¾‹QPS': '1000+',
                'é›†ç¾¤æ€»QPS': '10000+',
                'å¹¶å‘ç”¨æˆ·æ•°': '5000+'
            },
            'å¯ç”¨æ€§': {
                'ç³»ç»Ÿå¯ç”¨æ€§': '99.9%',
                'æ•…éšœæ¢å¤æ—¶é—´': '< 30s',
                'æ•°æ®ä¸€è‡´æ€§': '99.99%'
            }
        }

# æ€§èƒ½åŸºå‡†æµ‹è¯•
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, api_client):
        self.client = api_client
        self.test_data = [
            "æˆ‘æƒ³æŸ¥è¯¢è´¦æˆ·ä½™é¢",
            "å¦‚ä½•åŠç†ä¿¡ç”¨å¡",
            "è½¬è´¦å¤±è´¥äº†æ€ä¹ˆåŠ",
            "ä½ ä»¬çš„æœåŠ¡æ€åº¦å¾ˆå¥½",
            "æˆ‘è¦æŠ•è¯‰è¿™ä¸ªé—®é¢˜"
        ]
    
    def run_latency_test(self, iterations: int = 1000) -> Dict:
        """å»¶è¿Ÿæµ‹è¯•"""
        import time
        import statistics
        
        latencies = []
        
        for i in range(iterations):
            start_time = time.time()
            
            try:
                result = self.client.predict(
                    text=self.test_data[i % len(self.test_data)],
                    session_id=f"benchmark_{i}"
                )
                
                end_time = time.time()
                latency = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                latencies.append(latency)
                
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥: {e}")
        
        return {
            'total_requests': len(latencies),
            'avg_latency': statistics.mean(latencies),
            'median_latency': statistics.median(latencies),
            'p95_latency': sorted(latencies)[int(len(latencies) * 0.95)],
            'p99_latency': sorted(latencies)[int(len(latencies) * 0.99)],
            'min_latency': min(latencies),
            'max_latency': max(latencies)
        }
    
    def run_throughput_test(self, duration: int = 60) -> Dict:
        """ååé‡æµ‹è¯•"""
        import threading
        import time
        
        results = {'success': 0, 'failure': 0}
        start_time = time.time()
        
        def worker():
            while time.time() - start_time < duration:
                try:
                    self.client.predict(
                        text=self.test_data[0],
                        session_id="throughput_test"
                    )
                    results['success'] += 1
                except:
                    results['failure'] += 1
        
        # å¯åŠ¨å¤šä¸ªçº¿ç¨‹
        threads = []
        for _ in range(10):  # 10ä¸ªå¹¶å‘çº¿ç¨‹
            t = threading.Thread(target=worker)
            t.start()
            threads.append(t)
        
        # ç­‰å¾…æµ‹è¯•å®Œæˆ
        for t in threads:
            t.join()
        
        total_requests = results['success'] + results['failure']
        qps = total_requests / duration
        
        return {
            'duration': duration,
            'total_requests': total_requests,
            'successful_requests': results['success'],
            'failed_requests': results['failure'],
            'success_rate': results['success'] / total_requests * 100,
            'qps': qps
        }
```

### æœ€ä½³å®è·µä¸ç»éªŒæ€»ç»“

#### 1. å¼€å‘é˜¶æ®µæœ€ä½³å®è·µ

```python
# å¼€å‘æœ€ä½³å®è·µç¤ºä¾‹
class DevelopmentBestPractices:
    """å¼€å‘æœ€ä½³å®è·µ"""
    
    @staticmethod
    def data_processing_practices():
        """æ•°æ®å¤„ç†æœ€ä½³å®è·µ"""
        return {
            'æ•°æ®è´¨é‡ä¿è¯': [
                'å»ºç«‹æ•°æ®éªŒè¯è§„åˆ™å’Œå¼‚å¸¸æ£€æµ‹æœºåˆ¶',
                'å®æ–½æ•°æ®ç‰ˆæœ¬æ§åˆ¶å’Œè¡€ç¼˜è¿½è¸ª',
                'å®šæœŸè¿›è¡Œæ•°æ®è´¨é‡è¯„ä¼°å’Œæ¸…æ´—'
            ],
            'ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–': [
                'ä½¿ç”¨è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©å’Œé™ç»´æŠ€æœ¯',
                'å»ºç«‹ç‰¹å¾é‡è¦æ€§è¯„ä¼°ä½“ç³»',
                'å®ç°ç‰¹å¾å·¥ç¨‹æµæ°´çº¿çš„å¯å¤ç”¨æ€§'
            ],
            'æ•°æ®å®‰å…¨åˆè§„': [
                'æ•æ„Ÿæ•°æ®è„±æ•å’ŒåŠ å¯†å­˜å‚¨',
                'è®¿é—®æƒé™æ§åˆ¶å’Œå®¡è®¡æ—¥å¿—',
                'ç¬¦åˆGDPRç­‰æ•°æ®ä¿æŠ¤æ³•è§„'
            ]
        }
    
    @staticmethod
    def model_development_practices():
        """æ¨¡å‹å¼€å‘æœ€ä½³å®è·µ"""
        return {
            'æ¨¡å‹è®¾è®¡åŸåˆ™': [
                'é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•',
                'å®ç°æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’ŒA/Bæµ‹è¯•',
                'å»ºç«‹æ¨¡å‹æ€§èƒ½åŸºçº¿å’Œè¯„ä¼°æ ‡å‡†'
            ],
            'è®­ç»ƒä¼˜åŒ–ç­–ç•¥': [
                'ä½¿ç”¨äº¤å‰éªŒè¯å’Œæ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ',
                'å®æ–½è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–å’Œç½‘æ ¼æœç´¢',
                'å»ºç«‹è®­ç»ƒè¿‡ç¨‹ç›‘æ§å’Œå¯è§†åŒ–'
            ],
            'æ¨¡å‹é›†æˆæ–¹æ³•': [
                'ç»“åˆæ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¼˜åŠ¿',
                'å®ç°å¤šæ¨¡å‹æŠ•ç¥¨å’ŒåŠ æƒé›†æˆ',
                'å»ºç«‹æ¨¡å‹æ€§èƒ½å¯¹æ¯”å’Œé€‰æ‹©æœºåˆ¶'
            ]
        }
    
    @staticmethod
    def api_development_practices():
        """APIå¼€å‘æœ€ä½³å®è·µ"""
        return {
            'æ¥å£è®¾è®¡è§„èŒƒ': [
                'éµå¾ªRESTful APIè®¾è®¡åŸåˆ™',
                'å®ç°ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œå“åº”æ ¼å¼',
                'æä¾›å®Œæ•´çš„APIæ–‡æ¡£å’Œç¤ºä¾‹'
            ],
            'æ€§èƒ½ä¼˜åŒ–æŠ€å·§': [
                'ä½¿ç”¨å¼‚æ­¥å¤„ç†å’Œè¿æ¥æ± ä¼˜åŒ–',
                'å®ç°æ™ºèƒ½ç¼“å­˜å’Œæ•°æ®é¢„åŠ è½½',
                'å»ºç«‹è¯·æ±‚é™æµå’Œç†”æ–­æœºåˆ¶'
            ],
            'å®‰å…¨é˜²æŠ¤æªæ–½': [
                'å®æ–½APIè®¤è¯å’Œæˆæƒæœºåˆ¶',
                'æ·»åŠ è¾“å…¥éªŒè¯å’ŒSQLæ³¨å…¥é˜²æŠ¤',
                'å»ºç«‹è®¿é—®æ—¥å¿—å’Œå¼‚å¸¸ç›‘æ§'
            ]
        }

# éƒ¨ç½²è¿ç»´æœ€ä½³å®è·µ
class DeploymentBestPractices:
    """éƒ¨ç½²è¿ç»´æœ€ä½³å®è·µ"""
    
    @staticmethod
    def containerization_practices():
        """å®¹å™¨åŒ–æœ€ä½³å®è·µ"""
        return {
            'Dockerä¼˜åŒ–': [
                'ä½¿ç”¨å¤šé˜¶æ®µæ„å»ºå‡å°‘é•œåƒå¤§å°',
                'åˆç†è®¾ç½®èµ„æºé™åˆ¶å’Œå¥åº·æ£€æŸ¥',
                'å®ç°é•œåƒå®‰å…¨æ‰«æå’Œæ¼æ´ä¿®å¤'
            ],
            'Kuberneteséƒ¨ç½²': [
                'ä½¿ç”¨Helm Chartsç®¡ç†åº”ç”¨é…ç½®',
                'å®ç°è‡ªåŠ¨æ‰©ç¼©å®¹å’Œæ»šåŠ¨æ›´æ–°',
                'å»ºç«‹æœåŠ¡ç½‘æ ¼å’Œæµé‡ç®¡ç†'
            ],
            'é…ç½®ç®¡ç†': [
                'ä½¿ç”¨ConfigMapå’ŒSecretç®¡ç†é…ç½®',
                'å®ç°ç¯å¢ƒéš”ç¦»å’Œé…ç½®ç‰ˆæœ¬æ§åˆ¶',
                'å»ºç«‹é…ç½®å˜æ›´å®¡è®¡å’Œå›æ»šæœºåˆ¶'
            ]
        }
    
    @staticmethod
    def monitoring_practices():
        """ç›‘æ§æœ€ä½³å®è·µ"""
        return {
            'æŒ‡æ ‡ç›‘æ§': [
                'å»ºç«‹ä¸šåŠ¡æŒ‡æ ‡å’ŒæŠ€æœ¯æŒ‡æ ‡ç›‘æ§ä½“ç³»',
                'è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼å’Œå‡çº§ç­–ç•¥',
                'å®ç°å¤šç»´åº¦æŒ‡æ ‡èšåˆå’Œåˆ†æ'
            ],
            'æ—¥å¿—ç®¡ç†': [
                'ç»Ÿä¸€æ—¥å¿—æ ¼å¼å’Œç»“æ„åŒ–å­˜å‚¨',
                'å®ç°æ—¥å¿—èšåˆå’Œå®æ—¶åˆ†æ',
                'å»ºç«‹æ—¥å¿—ä¿ç•™ç­–ç•¥å’Œå½’æ¡£æœºåˆ¶'
            ],
            'é“¾è·¯è¿½è¸ª': [
                'å®ç°åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ªå’Œæ€§èƒ½åˆ†æ',
                'å»ºç«‹æœåŠ¡ä¾èµ–å…³ç³»å›¾è°±',
                'æä¾›ç«¯åˆ°ç«¯çš„è¯·æ±‚è¿½è¸ªèƒ½åŠ›'
            ]
        }
```

#### 2. ç”Ÿäº§ç¯å¢ƒè¿ç»´æŒ‡å—

```python
# ç”Ÿäº§ç¯å¢ƒè¿ç»´æŒ‡å—
class ProductionOperationsGuide:
    """ç”Ÿäº§ç¯å¢ƒè¿ç»´æŒ‡å—"""
    
    def __init__(self):
        self.operational_procedures = {
            'æ—¥å¸¸è¿ç»´': {
                'ç³»ç»Ÿå·¡æ£€': 'æ¯æ—¥æ£€æŸ¥ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å’Œå…³é”®æŒ‡æ ‡',
                'æ€§èƒ½ç›‘æ§': 'å®æ—¶ç›‘æ§APIå“åº”æ—¶é—´å’Œç³»ç»Ÿèµ„æºä½¿ç”¨',
                'æ—¥å¿—åˆ†æ': 'å®šæœŸåˆ†æé”™è¯¯æ—¥å¿—å’Œå¼‚å¸¸æ¨¡å¼',
                'å¤‡ä»½éªŒè¯': 'éªŒè¯æ•°æ®å¤‡ä»½å®Œæ•´æ€§å’Œæ¢å¤èƒ½åŠ›'
            },
            'æ•…éšœå¤„ç†': {
                'å‘Šè­¦å“åº”': 'å»ºç«‹7x24å°æ—¶å‘Šè­¦å“åº”æœºåˆ¶',
                'é—®é¢˜å®šä½': 'ä½¿ç”¨ç›‘æ§å·¥å…·å¿«é€Ÿå®šä½æ•…éšœæ ¹å› ',
                'åº”æ€¥å¤„ç†': 'æ‰§è¡Œé¢„å®šä¹‰çš„åº”æ€¥å¤„ç†æµç¨‹',
                'äº‹åå¤ç›˜': 'è¿›è¡Œæ•…éšœå¤ç›˜å’Œæ”¹è¿›æªæ–½åˆ¶å®š'
            },
            'å®¹é‡è§„åˆ’': {
                'æ€§èƒ½è¯„ä¼°': 'å®šæœŸè¯„ä¼°ç³»ç»Ÿæ€§èƒ½å’Œå®¹é‡éœ€æ±‚',
                'æ‰©å®¹ç­–ç•¥': 'åˆ¶å®šè‡ªåŠ¨å’Œæ‰‹åŠ¨æ‰©å®¹ç­–ç•¥',
                'æˆæœ¬ä¼˜åŒ–': 'ä¼˜åŒ–èµ„æºé…ç½®å’Œæˆæœ¬æ§åˆ¶',
                'æŠ€æœ¯å‡çº§': 'è§„åˆ’æŠ€æœ¯æ ˆå‡çº§å’Œè¿ç§»æ–¹æ¡ˆ'
            }
        }
    
    def generate_runbook(self) -> str:
        """ç”Ÿæˆè¿ç»´æ‰‹å†Œ"""
        runbook = "# æ™ºèƒ½å®¢æœç³»ç»Ÿè¿ç»´æ‰‹å†Œ\n\n"
        
        for category, procedures in self.operational_procedures.items():
            runbook += f"## {category}\n\n"
            for procedure, description in procedures.items():
                runbook += f"### {procedure}\n{description}\n\n"
        
        return runbook
    
    def create_incident_response_plan(self) -> Dict:
        """åˆ›å»ºäº‹ä»¶å“åº”è®¡åˆ’"""
        return {
            'ä¸¥é‡çº§åˆ«å®šä¹‰': {
                'P0 - ç´§æ€¥': 'ç³»ç»Ÿå®Œå…¨ä¸å¯ç”¨ï¼Œå½±å“æ‰€æœ‰ç”¨æˆ·',
                'P1 - é«˜': 'æ ¸å¿ƒåŠŸèƒ½ä¸å¯ç”¨ï¼Œå½±å“å¤§éƒ¨åˆ†ç”¨æˆ·',
                'P2 - ä¸­': 'éƒ¨åˆ†åŠŸèƒ½å¼‚å¸¸ï¼Œå½±å“å°‘æ•°ç”¨æˆ·',
                'P3 - ä½': 'éæ ¸å¿ƒåŠŸèƒ½é—®é¢˜ï¼Œç”¨æˆ·ä½“éªŒè½»å¾®å½±å“'
            },
            'å“åº”æ—¶é—´è¦æ±‚': {
                'P0': '15åˆ†é’Ÿå†…å“åº”ï¼Œ1å°æ—¶å†…è§£å†³',
                'P1': '30åˆ†é’Ÿå†…å“åº”ï¼Œ4å°æ—¶å†…è§£å†³',
                'P2': '2å°æ—¶å†…å“åº”ï¼Œ24å°æ—¶å†…è§£å†³',
                'P3': '24å°æ—¶å†…å“åº”ï¼Œ72å°æ—¶å†…è§£å†³'
            },
            'å‡çº§æœºåˆ¶': {
                'ä¸€çº§æ”¯æŒ': 'è¿ç»´å·¥ç¨‹å¸ˆåˆæ­¥å¤„ç†',
                'äºŒçº§æ”¯æŒ': 'å¼€å‘å·¥ç¨‹å¸ˆæ·±å…¥åˆ†æ',
                'ä¸‰çº§æ”¯æŒ': 'æ¶æ„å¸ˆå’ŒæŠ€æœ¯ä¸“å®¶ä»‹å…¥',
                'ç®¡ç†å±‚': 'é‡å¤§äº‹ä»¶éœ€è¦ç®¡ç†å±‚å†³ç­–'
            }
        }
```

### é¡¹ç›®ä»·å€¼ä¸åº”ç”¨å‰æ™¯

#### ä¸šåŠ¡ä»·å€¼åˆ†æ

1. **æ•ˆç‡æå‡**ï¼šæ™ºèƒ½æ„å›¾è¯†åˆ«å’Œæƒ…æ„Ÿåˆ†æï¼Œæå‡å®¢æœå“åº”æ•ˆç‡60%ä»¥ä¸Š
2. **æˆæœ¬é™ä½**ï¼šè‡ªåŠ¨åŒ–å¤„ç†å¸¸è§é—®é¢˜ï¼Œå‡å°‘äººå·¥å®¢æœæˆæœ¬40%
3. **ä½“éªŒä¼˜åŒ–**ï¼šå®æ—¶è´¨é‡è¯„ä¼°å’Œå»ºè®®ç”Ÿæˆï¼Œæå‡å®¢æˆ·æ»¡æ„åº¦25%
4. **æ•°æ®é©±åŠ¨**ï¼šå…¨é¢çš„ç›‘æ§å’Œåˆ†æä½“ç³»ï¼Œæ”¯æŒä¸šåŠ¡å†³ç­–ä¼˜åŒ–

#### æŠ€æœ¯åˆ›æ–°ç‚¹

1. **å¤šæ¨¡å‹èåˆ**ï¼šæ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„æœ‰æœºç»“åˆ
2. **å®æ—¶å¤„ç†**ï¼šæ¯«ç§’çº§å“åº”çš„é«˜æ€§èƒ½APIæœåŠ¡æ¶æ„
3. **æ™ºèƒ½è¿ç»´**ï¼šåŸºäºPrometheuså’ŒGrafanaçš„å…¨æ–¹ä½ç›‘æ§ä½“ç³»
4. **äº‘åŸç”Ÿéƒ¨ç½²**ï¼šå®¹å™¨åŒ–å’Œå¾®æœåŠ¡æ¶æ„çš„æœ€ä½³å®è·µ

#### æ‰©å±•åº”ç”¨åœºæ™¯

1. **é‡‘èæœåŠ¡**ï¼šé“¶è¡Œã€ä¿é™©ã€è¯åˆ¸ç­‰é‡‘èæœºæ„å®¢æœç³»ç»Ÿ
2. **ç”µå•†å¹³å°**ï¼šåœ¨çº¿è´­ç‰©ã€å”®åæœåŠ¡ã€ç”¨æˆ·å’¨è¯¢å¤„ç†
3. **æ•™è‚²åŸ¹è®­**ï¼šåœ¨çº¿æ•™è‚²å¹³å°çš„å­¦å‘˜æœåŠ¡å’Œç­”ç–‘ç³»ç»Ÿ
4. **åŒ»ç–—å¥åº·**ï¼šåŒ»ç–—å’¨è¯¢ã€é¢„çº¦æŒ‚å·ã€å¥åº·ç®¡ç†æœåŠ¡
5. **æ”¿åŠ¡æœåŠ¡**ï¼šæ”¿åºœéƒ¨é—¨çš„å…¬å…±æœåŠ¡å’Œæ°‘ç”Ÿå’¨è¯¢å¹³å°

é€šè¿‡æœ¬ç»¼åˆé¡¹ç›®æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†Trae AIå¼€å‘ç¯å¢ƒçš„å„é¡¹èƒ½åŠ›æ•´åˆåˆ°ä¸€ä¸ªå®Œæ•´çš„AIåº”ç”¨ç³»ç»Ÿä¸­ï¼Œä¸ºè¯»è€…æä¾›äº†ä»æ¦‚å¿µè®¾è®¡åˆ°ç”Ÿäº§éƒ¨ç½²çš„å…¨æµç¨‹å‚è€ƒæ–¹æ¡ˆã€‚è¿™ä¸ªé¡¹ç›®ä¸ä»…ä½“ç°äº†æŠ€æœ¯çš„å…ˆè¿›æ€§ï¼Œæ›´é‡è¦çš„æ˜¯å±•ç°äº†AIæŠ€æœ¯åœ¨å®é™…ä¸šåŠ¡åœºæ™¯ä¸­çš„åº”ç”¨ä»·å€¼å’Œå‘å±•æ½œåŠ›ã€‚