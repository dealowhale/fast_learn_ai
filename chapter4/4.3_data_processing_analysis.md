# 4.3 æ•°æ®å¤„ç†ä¸åˆ†æå®ä¾‹

åœ¨AIé¡¹ç›®å¼€å‘ä¸­ï¼Œæ•°æ®å¤„ç†ä¸åˆ†ææ˜¯è‡³å…³é‡è¦çš„ç¯èŠ‚ã€‚æœ¬èŠ‚å°†é€šè¿‡å®é™…æ¡ˆä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åœ¨Trae IDEä¸­è¿›è¡Œé«˜æ•ˆçš„æ•°æ®å¤„ç†ã€æ¢ç´¢æ€§åˆ†æå’Œå¯è§†åŒ–å·¥ä½œã€‚

## 4.3.1 æ•°æ®è·å–ä¸åŠ è½½

### å¤šæºæ•°æ®è·å–å·¥å…·

```python
# æ•°æ®è·å–å’ŒåŠ è½½å·¥å…·é›†
import pandas as pd
import numpy as np
import requests
import sqlite3
from pathlib import Path
import json
import yaml
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris, load_boston, load_wine
import warnings
warnings.filterwarnings('ignore')

class DataLoader:
    """å¤šæºæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir="./data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.loaded_datasets = {}
        
    def load_csv_data(self, file_path, **kwargs):
        """åŠ è½½CSVæ•°æ®"""
        print(f"=== åŠ è½½CSVæ•°æ®: {file_path} ===")
        
        try:
            # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding, **kwargs)
                    print(f"âœ… æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç åŠ è½½æ•°æ®")
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                raise ValueError("æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç ")
            
            # æ•°æ®åŸºæœ¬ä¿¡æ¯
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
            print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
            print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
            
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return None
    
    def load_sample_datasets(self):
        """åŠ è½½ç¤ºä¾‹æ•°æ®é›†"""
        print("=== åŠ è½½ç¤ºä¾‹æ•°æ®é›† ===")
        
        datasets = {}
        
        # 1. é¸¢å°¾èŠ±æ•°æ®é›†
        iris = load_iris()
        datasets['iris'] = pd.DataFrame(
            data=iris.data,
            columns=iris.feature_names
        )
        datasets['iris']['target'] = iris.target
        datasets['iris']['species'] = [iris.target_names[i] for i in iris.target]
        print(f"âœ… é¸¢å°¾èŠ±æ•°æ®é›†: {datasets['iris'].shape}")
        
        # 2. çº¢é…’æ•°æ®é›†
        wine = load_wine()
        datasets['wine'] = pd.DataFrame(
            data=wine.data,
            columns=wine.feature_names
        )
        datasets['wine']['target'] = wine.target
        datasets['wine']['wine_class'] = [wine.target_names[i] for i in wine.target]
        print(f"âœ… çº¢é…’æ•°æ®é›†: {datasets['wine'].shape}")
        
        # 3. ç”Ÿæˆæ¨¡æ‹Ÿç”µå•†æ•°æ®
        np.random.seed(42)
        n_samples = 1000
        
        ecommerce_data = {
            'user_id': range(1, n_samples + 1),
            'age': np.random.randint(18, 65, n_samples),
            'gender': np.random.choice(['M', 'F'], n_samples),
            'city': np.random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·'], n_samples),
            'purchase_amount': np.random.exponential(200, n_samples),
            'purchase_frequency': np.random.poisson(5, n_samples),
            'last_purchase_days': np.random.randint(1, 365, n_samples),
            'category_preference': np.random.choice(
                ['ç”µå­äº§å“', 'æœè£…', 'å®¶å±…', 'é£Ÿå“', 'å›¾ä¹¦'], n_samples
            )
        }
        
        datasets['ecommerce'] = pd.DataFrame(ecommerce_data)
        print(f"âœ… ç”µå•†æ•°æ®é›†: {datasets['ecommerce'].shape}")
        
        # 4. ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
        dates = pd.date_range('2023-01-01', periods=365, freq='D')
        ts_data = {
            'date': dates,
            'sales': 1000 + 200 * np.sin(2 * np.pi * np.arange(365) / 365) + 
                    np.random.normal(0, 50, 365),
            'temperature': 20 + 15 * np.sin(2 * np.pi * np.arange(365) / 365) + 
                          np.random.normal(0, 3, 365),
            'promotion': np.random.choice([0, 1], 365, p=[0.8, 0.2])
        }
        
        datasets['timeseries'] = pd.DataFrame(ts_data)
        print(f"âœ… æ—¶é—´åºåˆ—æ•°æ®é›†: {datasets['timeseries'].shape}")
        
        self.loaded_datasets = datasets
        return datasets
    
    def load_from_api(self, url, params=None, headers=None):
        """ä»APIåŠ è½½æ•°æ®"""
        print(f"=== ä»APIåŠ è½½æ•°æ®: {url} ===")
        
        try:
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            # å°è¯•è§£æJSON
            if 'application/json' in response.headers.get('content-type', ''):
                data = response.json()
                
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºDataFrame
                if isinstance(data, list):
                    df = pd.DataFrame(data)
                elif isinstance(data, dict):
                    # å¦‚æœæ˜¯åµŒå¥—å­—å…¸ï¼Œå°è¯•æå–æ•°æ®
                    if 'data' in data:
                        df = pd.DataFrame(data['data'])
                    else:
                        df = pd.json_normalize(data)
                else:
                    df = pd.DataFrame([data])
                
                print(f"âœ… APIæ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
                return df
            else:
                print("âŒ å“åº”ä¸æ˜¯JSONæ ¼å¼")
                return None
                
        except requests.RequestException as e:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"âŒ æ•°æ®è§£æå¤±è´¥: {e}")
            return None
    
    def save_dataset(self, df, name, format='csv'):
        """ä¿å­˜æ•°æ®é›†"""
        print(f"=== ä¿å­˜æ•°æ®é›†: {name} ===")
        
        file_path = self.data_dir / f"{name}.{format}"
        
        try:
            if format == 'csv':
                df.to_csv(file_path, index=False, encoding='utf-8')
            elif format == 'json':
                df.to_json(file_path, orient='records', force_ascii=False, indent=2)
            elif format == 'parquet':
                df.to_parquet(file_path, index=False)
            elif format == 'excel':
                df.to_excel(file_path, index=False)
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
            return file_path
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return None

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
data_loader = DataLoader()

# åŠ è½½ç¤ºä¾‹æ•°æ®é›†
sample_datasets = data_loader.load_sample_datasets()

# å±•ç¤ºæ•°æ®é›†æ¦‚è§ˆ
print("\n=== æ•°æ®é›†æ¦‚è§ˆ ===")
for name, df in sample_datasets.items():
    print(f"\nğŸ“Š {name} æ•°æ®é›†:")
    print(f"   å½¢çŠ¶: {df.shape}")
    print(f"   åˆ—å: {list(df.columns)[:5]}{'...' if len(df.columns) > 5 else ''}")
    print(f"   æ•°æ®ç±»å‹: {df.dtypes.value_counts().to_dict()}")
```

## 4.3.2 æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†

### æ™ºèƒ½æ•°æ®æ¸…æ´—å·¥å…·

```python
# æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†å·¥å…·
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
import re

class DataCleaner:
    """æ™ºèƒ½æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        self.cleaning_report = {}
        self.transformers = {}
    
    def analyze_data_quality(self, df, dataset_name="æ•°æ®é›†"):
        """åˆ†ææ•°æ®è´¨é‡"""
        print(f"=== {dataset_name} æ•°æ®è´¨é‡åˆ†æ ===")
        
        quality_report = {
            'basic_info': {
                'shape': df.shape,
                'memory_usage': f"{df.memory_usage(deep=True).sum() / 1024**2:.2f} MB"
            },
            'missing_values': {},
            'duplicates': df.duplicated().sum(),
            'data_types': df.dtypes.to_dict(),
            'numeric_stats': {},
            'categorical_stats': {}
        }
        
        # 1. ç¼ºå¤±å€¼åˆ†æ
        missing_counts = df.isnull().sum()
        missing_percentages = (missing_counts / len(df)) * 100
        
        for col in df.columns:
            if missing_counts[col] > 0:
                quality_report['missing_values'][col] = {
                    'count': int(missing_counts[col]),
                    'percentage': round(missing_percentages[col], 2)
                }
        
        print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ•°æ®å½¢çŠ¶: {quality_report['basic_info']['shape']}")
        print(f"   å†…å­˜ä½¿ç”¨: {quality_report['basic_info']['memory_usage']}")
        print(f"   é‡å¤è¡Œæ•°: {quality_report['duplicates']}")
        
        if quality_report['missing_values']:
            print(f"\nâš ï¸ ç¼ºå¤±å€¼æƒ…å†µ:")
            for col, info in quality_report['missing_values'].items():
                print(f"   {col}: {info['count']} ({info['percentage']}%)")
        else:
            print("\nâœ… æ— ç¼ºå¤±å€¼")
        
        # 2. æ•°å€¼å‹åˆ—ç»Ÿè®¡
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(f"\nğŸ“ˆ æ•°å€¼å‹åˆ—ç»Ÿè®¡:")
            for col in numeric_cols:
                stats = df[col].describe()
                quality_report['numeric_stats'][col] = {
                    'mean': round(stats['mean'], 2),
                    'std': round(stats['std'], 2),
                    'min': round(stats['min'], 2),
                    'max': round(stats['max'], 2),
                    'outliers': self._detect_outliers(df[col])
                }
                print(f"   {col}: å‡å€¼={stats['mean']:.2f}, æ ‡å‡†å·®={stats['std']:.2f}")
        
        # 3. åˆ†ç±»å‹åˆ—ç»Ÿè®¡
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            print(f"\nğŸ“‹ åˆ†ç±»å‹åˆ—ç»Ÿè®¡:")
            for col in categorical_cols:
                unique_count = df[col].nunique()
                most_common = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'N/A'
                quality_report['categorical_stats'][col] = {
                    'unique_count': unique_count,
                    'most_common': most_common
                }
                print(f"   {col}: {unique_count} ä¸ªå”¯ä¸€å€¼, æœ€å¸¸è§: {most_common}")
        
        self.cleaning_report[dataset_name] = quality_report
        return quality_report
    
    def _detect_outliers(self, series, method='iqr'):
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        if method == 'iqr':
            Q1 = series.quantile(0.25)
            Q3 = series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = series[(series < lower_bound) | (series > upper_bound)]
            return len(outliers)
        return 0
    
    def handle_missing_values(self, df, strategy='auto'):
        """å¤„ç†ç¼ºå¤±å€¼"""
        print("=== å¤„ç†ç¼ºå¤±å€¼ ===")
        
        df_cleaned = df.copy()
        missing_info = {}
        
        for col in df.columns:
            missing_count = df[col].isnull().sum()
            if missing_count > 0:
                missing_percentage = (missing_count / len(df)) * 100
                
                if strategy == 'auto':
                    # è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
                    if missing_percentage > 50:
                        # ç¼ºå¤±è¶…è¿‡50%ï¼Œè€ƒè™‘åˆ é™¤åˆ—
                        print(f"âš ï¸ {col} ç¼ºå¤± {missing_percentage:.1f}%ï¼Œå»ºè®®åˆ é™¤")
                        strategy_used = 'drop_column'
                    elif df[col].dtype in ['object', 'category']:
                        # åˆ†ç±»å˜é‡ç”¨ä¼—æ•°å¡«å……
                        mode_value = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'Unknown'
                        df_cleaned[col].fillna(mode_value, inplace=True)
                        strategy_used = f'mode: {mode_value}'
                    else:
                        # æ•°å€¼å˜é‡ç”¨ä¸­ä½æ•°å¡«å……
                        median_value = df[col].median()
                        df_cleaned[col].fillna(median_value, inplace=True)
                        strategy_used = f'median: {median_value:.2f}'
                else:
                    # ä½¿ç”¨æŒ‡å®šç­–ç•¥
                    if strategy == 'mean' and df[col].dtype in [np.number]:
                        mean_value = df[col].mean()
                        df_cleaned[col].fillna(mean_value, inplace=True)
                        strategy_used = f'mean: {mean_value:.2f}'
                    elif strategy == 'median' and df[col].dtype in [np.number]:
                        median_value = df[col].median()
                        df_cleaned[col].fillna(median_value, inplace=True)
                        strategy_used = f'median: {median_value:.2f}'
                    elif strategy == 'mode':
                        mode_value = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'Unknown'
                        df_cleaned[col].fillna(mode_value, inplace=True)
                        strategy_used = f'mode: {mode_value}'
                
                missing_info[col] = {
                    'original_missing': missing_count,
                    'percentage': missing_percentage,
                    'strategy': strategy_used
                }
                
                print(f"âœ… {col}: {missing_count} ä¸ªç¼ºå¤±å€¼ â†’ {strategy_used}")
        
        print(f"\nğŸ“Š å¤„ç†ç»“æœ: {len(missing_info)} åˆ—æœ‰ç¼ºå¤±å€¼è¢«å¤„ç†")
        return df_cleaned, missing_info
    
    def remove_duplicates(self, df, subset=None, keep='first'):
        """åˆ é™¤é‡å¤è¡Œ"""
        print("=== åˆ é™¤é‡å¤è¡Œ ===")
        
        original_shape = df.shape
        df_cleaned = df.drop_duplicates(subset=subset, keep=keep)
        duplicates_removed = original_shape[0] - df_cleaned.shape[0]
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {original_shape[0]} è¡Œ")
        print(f"ğŸ—‘ï¸ åˆ é™¤é‡å¤: {duplicates_removed} è¡Œ")
        print(f"âœ… æ¸…æ´—å: {df_cleaned.shape[0]} è¡Œ")
        
        return df_cleaned
    
    def standardize_text(self, df, text_columns):
        """æ ‡å‡†åŒ–æ–‡æœ¬æ•°æ®"""
        print("=== æ ‡å‡†åŒ–æ–‡æœ¬æ•°æ® ===")
        
        df_cleaned = df.copy()
        
        for col in text_columns:
            if col in df.columns:
                print(f"ğŸ”¤ å¤„ç†æ–‡æœ¬åˆ—: {col}")
                
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                df_cleaned[col] = df_cleaned[col].astype(str)
                
                # å»é™¤å‰åç©ºæ ¼
                df_cleaned[col] = df_cleaned[col].str.strip()
                
                # ç»Ÿä¸€å¤§å°å†™ï¼ˆå¦‚æœæ˜¯è‹±æ–‡ï¼‰
                if df_cleaned[col].str.contains(r'[a-zA-Z]').any():
                    df_cleaned[col] = df_cleaned[col].str.lower()
                
                # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ï¼‰
                df_cleaned[col] = df_cleaned[col].str.replace(
                    r'[^\w\s\u4e00-\u9fff]', '', regex=True
                )
                
                print(f"   âœ… {col} æ–‡æœ¬æ ‡å‡†åŒ–å®Œæˆ")
        
        return df_cleaned
    
    def encode_categorical(self, df, categorical_columns, method='label'):
        """ç¼–ç åˆ†ç±»å˜é‡"""
        print("=== ç¼–ç åˆ†ç±»å˜é‡ ===")
        
        df_encoded = df.copy()
        encoders = {}
        
        for col in categorical_columns:
            if col in df.columns:
                print(f"ğŸ·ï¸ ç¼–ç åˆ—: {col}")
                
                if method == 'label':
                    # æ ‡ç­¾ç¼–ç 
                    le = LabelEncoder()
                    df_encoded[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
                    encoders[col] = le
                    print(f"   âœ… æ ‡ç­¾ç¼–ç : {len(le.classes_)} ä¸ªç±»åˆ«")
                
                elif method == 'onehot':
                    # ç‹¬çƒ­ç¼–ç 
                    dummies = pd.get_dummies(df[col], prefix=col)
                    df_encoded = pd.concat([df_encoded, dummies], axis=1)
                    encoders[col] = list(dummies.columns)
                    print(f"   âœ… ç‹¬çƒ­ç¼–ç : {len(dummies.columns)} ä¸ªæ–°åˆ—")
        
        self.transformers['encoders'] = encoders
        return df_encoded, encoders
    
    def scale_features(self, df, numeric_columns, method='standard'):
        """ç‰¹å¾ç¼©æ”¾"""
        print("=== ç‰¹å¾ç¼©æ”¾ ===")
        
        df_scaled = df.copy()
        scalers = {}
        
        for col in numeric_columns:
            if col in df.columns:
                print(f"ğŸ“ ç¼©æ”¾åˆ—: {col}")
                
                if method == 'standard':
                    scaler = StandardScaler()
                    df_scaled[f'{col}_scaled'] = scaler.fit_transform(df[[col]])
                elif method == 'minmax':
                    scaler = MinMaxScaler()
                    df_scaled[f'{col}_scaled'] = scaler.fit_transform(df[[col]])
                
                scalers[col] = scaler
                
                # æ˜¾ç¤ºç¼©æ”¾å‰åçš„ç»Ÿè®¡ä¿¡æ¯
                original_stats = df[col].describe()
                scaled_stats = df_scaled[f'{col}_scaled'].describe()
                
                print(f"   åŸå§‹: å‡å€¼={original_stats['mean']:.2f}, æ ‡å‡†å·®={original_stats['std']:.2f}")
                print(f"   ç¼©æ”¾: å‡å€¼={scaled_stats['mean']:.2f}, æ ‡å‡†å·®={scaled_stats['std']:.2f}")
        
        self.transformers['scalers'] = scalers
        return df_scaled, scalers

# åˆ›å»ºæ•°æ®æ¸…æ´—å™¨
data_cleaner = DataCleaner()

# ä½¿ç”¨ç”µå•†æ•°æ®è¿›è¡Œæ¸…æ´—ç¤ºä¾‹
ecommerce_df = sample_datasets['ecommerce'].copy()

# äººå·¥æ·»åŠ ä¸€äº›æ•°æ®è´¨é‡é—®é¢˜ç”¨äºæ¼”ç¤º
np.random.seed(42)
# æ·»åŠ ç¼ºå¤±å€¼
ecommerce_df.loc[np.random.choice(ecommerce_df.index, 50), 'age'] = np.nan
ecommerce_df.loc[np.random.choice(ecommerce_df.index, 30), 'city'] = np.nan

# æ·»åŠ é‡å¤è¡Œ
ecommerce_df = pd.concat([ecommerce_df, ecommerce_df.iloc[:20]], ignore_index=True)

# åˆ†ææ•°æ®è´¨é‡
quality_report = data_cleaner.analyze_data_quality(ecommerce_df, "ç”µå•†æ•°æ®")

# å¤„ç†ç¼ºå¤±å€¼
ecommerce_cleaned, missing_info = data_cleaner.handle_missing_values(ecommerce_df)

# åˆ é™¤é‡å¤è¡Œ
ecommerce_cleaned = data_cleaner.remove_duplicates(ecommerce_cleaned)

# ç¼–ç åˆ†ç±»å˜é‡
categorical_cols = ['gender', 'city', 'category_preference']
ecommerce_encoded, encoders = data_cleaner.encode_categorical(
    ecommerce_cleaned, categorical_cols, method='label'
)

# ç‰¹å¾ç¼©æ”¾
numeric_cols = ['age', 'purchase_amount', 'purchase_frequency']
ecommerce_scaled, scalers = data_cleaner.scale_features(
    ecommerce_encoded, numeric_cols, method='standard'
)

print("\nğŸ‰ æ•°æ®æ¸…æ´—å®Œæˆï¼")
print(f"æœ€ç»ˆæ•°æ®å½¢çŠ¶: {ecommerce_scaled.shape}")
```

## 4.3.3 æ•°æ®å¯è§†åŒ–ä¸æ¢ç´¢æ€§åˆ†æ

### æ™ºèƒ½æ•°æ®å¯è§†åŒ–å·¥å…·

```python
# æ•°æ®å¯è§†åŒ–å’Œæ¢ç´¢æ€§åˆ†æå·¥å…·
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class DataVisualizer:
    """æ•°æ®å¯è§†åŒ–å·¥å…·ç±»"""
    
    def __init__(self, data):
        self.data = data
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
    
    def basic_info_plot(self):
        """åŸºç¡€ä¿¡æ¯å¯è§†åŒ–"""
        print("=== åŸºç¡€ä¿¡æ¯å¯è§†åŒ– ===")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. æ•°æ®ç±»å‹åˆ†å¸ƒ
        dtype_counts = self.data.dtypes.value_counts()
        axes[0, 0].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')
        axes[0, 0].set_title('æ•°æ®ç±»å‹åˆ†å¸ƒ')
        
        # 2. ç¼ºå¤±å€¼åˆ†å¸ƒ
        missing_data = self.data.isnull().sum()
        missing_data = missing_data[missing_data > 0]
        if len(missing_data) > 0:
            axes[0, 1].bar(range(len(missing_data)), missing_data.values)
            axes[0, 1].set_xticks(range(len(missing_data)))
            axes[0, 1].set_xticklabels(missing_data.index, rotation=45)
            axes[0, 1].set_title('ç¼ºå¤±å€¼åˆ†å¸ƒ')
        else:
            axes[0, 1].text(0.5, 0.5, 'æ— ç¼ºå¤±å€¼', ha='center', va='center', 
                           transform=axes[0, 1].transAxes, fontsize=14)
            axes[0, 1].set_title('ç¼ºå¤±å€¼åˆ†å¸ƒ')
        
        # 3. æ•°å€¼åˆ—åˆ†å¸ƒæ¦‚è§ˆ
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            # é€‰æ‹©å‰4ä¸ªæ•°å€¼åˆ—è¿›è¡Œå±•ç¤º
            cols_to_show = numeric_cols[:4]
            for i, col in enumerate(cols_to_show):
                if i < 4:  # æœ€å¤šæ˜¾ç¤º4ä¸ª
                    row, col_idx = divmod(i, 2)
                    if row == 0 and col_idx == 0:
                        continue  # è·³è¿‡å·²ä½¿ç”¨çš„ä½ç½®
                    axes[1, 0].hist(self.data[col].dropna(), bins=20, alpha=0.7, label=col)
            axes[1, 0].set_title('æ•°å€¼åˆ—åˆ†å¸ƒæ¦‚è§ˆ')
            axes[1, 0].legend()
        
        # 4. æ•°æ®é›†ä¿¡æ¯æ‘˜è¦
        info_text = f"æ•°æ®å½¢çŠ¶: {self.data.shape}\n"
        info_text += f"å†…å­˜ä½¿ç”¨: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n"
        info_text += f"æ•°å€¼åˆ—æ•°: {len(numeric_cols)}\n"
        info_text += f"åˆ†ç±»åˆ—æ•°: {len(self.data.select_dtypes(include=['object']).columns)}\n"
        info_text += f"æ€»ç¼ºå¤±å€¼: {self.data.isnull().sum().sum()}"
        
        axes[1, 1].text(0.1, 0.5, info_text, transform=axes[1, 1].transAxes, 
                        fontsize=12, verticalalignment='center')
        axes[1, 1].set_title('æ•°æ®é›†ä¿¡æ¯æ‘˜è¦')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()
        return fig
    
    def correlation_analysis(self):
        """ç›¸å…³æ€§åˆ†æ"""
        print("=== ç›¸å…³æ€§åˆ†æ ===")
        
        numeric_data = self.data.select_dtypes(include=[np.number])
        
        if numeric_data.shape[1] < 2:
            print("âŒ æ•°å€¼åˆ—ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ")
            return None
        
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        corr_matrix = numeric_data.corr()
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(12, 8))
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                   square=True, linewidths=0.5, cbar_kws={"shrink": .8})
        plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')
        plt.tight_layout()
        plt.show()
        
        # æ‰¾å‡ºé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    high_corr_pairs.append({
                        'feature1': corr_matrix.columns[i],
                        'feature2': corr_matrix.columns[j],
                        'correlation': corr_val
                    })
        
        if high_corr_pairs:
            print(f"\nâš ï¸ å‘ç° {len(high_corr_pairs)} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾ (|r| > 0.7):")
            for pair in high_corr_pairs:
                print(f"   {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}")
        else:
            print("\nâœ… æœªå‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹")
        
        return corr_matrix, high_corr_pairs
    
    def distribution_analysis(self, columns=None):
        """åˆ†å¸ƒåˆ†æ"""
        print("=== åˆ†å¸ƒåˆ†æ ===")
        
        if columns is None:
            columns = self.data.select_dtypes(include=[np.number]).columns[:4]
        
        if len(columns) == 0:
            print("âŒ æ²¡æœ‰æ•°å€¼åˆ—å¯ä¾›åˆ†æ")
            return None
        
        # åˆ›å»ºå­å›¾
        n_cols = min(len(columns), 2)
        n_rows = (len(columns) + 1) // 2
        
        fig, axes = plt.subplots(n_rows * 2, n_cols, figsize=(15, n_rows * 6))
        if len(columns) == 1:
            axes = axes.reshape(-1, 1)
        
        stats_results = {}
        
        for i, col in enumerate(columns):
            col_idx = i % n_cols
            
            # ç›´æ–¹å›¾
            axes[i * 2 // n_cols * 2, col_idx].hist(self.data[col].dropna(), 
                                                    bins=30, alpha=0.7, color='skyblue')
            axes[i * 2 // n_cols * 2, col_idx].set_title(f'{col} - åˆ†å¸ƒç›´æ–¹å›¾')
            axes[i * 2 // n_cols * 2, col_idx].set_xlabel(col)
            axes[i * 2 // n_cols * 2, col_idx].set_ylabel('é¢‘æ¬¡')
            
            # ç®±çº¿å›¾
            axes[i * 2 // n_cols * 2 + 1, col_idx].boxplot(self.data[col].dropna())
            axes[i * 2 // n_cols * 2 + 1, col_idx].set_title(f'{col} - ç®±çº¿å›¾')
            axes[i * 2 // n_cols * 2 + 1, col_idx].set_ylabel(col)
            
            # ç»Ÿè®¡æ£€éªŒ
            data_col = self.data[col].dropna()
            if len(data_col) > 3:
                try:
                    _, p_value = stats.normaltest(data_col)
                    stats_results[col] = {
                        'mean': data_col.mean(),
                        'std': data_col.std(),
                        'skewness': stats.skew(data_col),
                        'kurtosis': stats.kurtosis(data_col),
                        'is_normal': p_value > 0.05,
                        'p_value': p_value
                    }
                except:
                    stats_results[col] = {'error': 'ç»Ÿè®¡æ£€éªŒå¤±è´¥'}
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print("\nğŸ“Š åˆ†å¸ƒç»Ÿè®¡ç»“æœ:")
        for col, stats_info in stats_results.items():
            if 'error' not in stats_info:
                print(f"\n{col}:")
                print(f"   å‡å€¼: {stats_info['mean']:.3f}")
                print(f"   æ ‡å‡†å·®: {stats_info['std']:.3f}")
                print(f"   ååº¦: {stats_info['skewness']:.3f}")
                print(f"   å³°åº¦: {stats_info['kurtosis']:.3f}")
                print(f"   æ­£æ€åˆ†å¸ƒ: {'æ˜¯' if stats_info['is_normal'] else 'å¦'} (p={stats_info['p_value']:.4f})")
        
        return stats_results
    
    def outlier_detection(self, method='iqr'):
        """å¼‚å¸¸å€¼æ£€æµ‹"""
        print("=== å¼‚å¸¸å€¼æ£€æµ‹ ===")
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        outliers_info = {}
        
        for col in numeric_cols:
            data_col = self.data[col].dropna()
            
            if len(data_col) == 0:
                continue
            
            if method == 'iqr':
                Q1 = data_col.quantile(0.25)
                Q3 = data_col.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = data_col[(data_col < lower_bound) | (data_col > upper_bound)]
            
            elif method == 'zscore':
                z_scores = np.abs(stats.zscore(data_col))
                outliers = data_col[z_scores > 3]
            
            outliers_info[col] = {
                'count': len(outliers),
                'percentage': len(outliers) / len(data_col) * 100,
                'values': outliers.tolist()[:10],  # åªæ˜¾ç¤ºå‰10ä¸ª
                'bounds': {'lower': lower_bound if method == 'iqr' else None,
                          'upper': upper_bound if method == 'iqr' else None}
            }
        
        # å¯è§†åŒ–å¼‚å¸¸å€¼
        n_cols = min(4, len(numeric_cols))
        if n_cols > 0:
            fig, axes = plt.subplots(1, n_cols, figsize=(15, 4))
            if n_cols == 1:
                axes = [axes]
            
            for i, col in enumerate(numeric_cols[:n_cols]):
                sns.boxplot(y=self.data[col], ax=axes[i])
                axes[i].set_title(f'{col}\nå¼‚å¸¸å€¼: {outliers_info[col]["count"]}ä¸ª '
                                f'({outliers_info[col]["percentage"]:.1f}%)')
            
            plt.tight_layout()
            plt.show()
        
        # æ‰“å°å¼‚å¸¸å€¼ç»Ÿè®¡
        print("\nğŸ” å¼‚å¸¸å€¼æ£€æµ‹ç»“æœ:")
        for col, info in outliers_info.items():
            print(f"{col}: {info['count']}ä¸ªå¼‚å¸¸å€¼ ({info['percentage']:.1f}%)")
        
        return outliers_info
    
    def categorical_analysis(self):
        """åˆ†ç±»å˜é‡åˆ†æ"""
        print("=== åˆ†ç±»å˜é‡åˆ†æ ===")
        
        categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns
        
        if len(categorical_cols) == 0:
            print("âŒ æ²¡æœ‰åˆ†ç±»å˜é‡å¯ä¾›åˆ†æ")
            return None
        
        n_cols = min(2, len(categorical_cols))
        n_rows = (len(categorical_cols) + 1) // 2
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))
        if len(categorical_cols) == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = [axes]
        
        categorical_stats = {}
        
        for i, col in enumerate(categorical_cols):
            row = i // n_cols
            col_idx = i % n_cols
            
            # è·å–å½“å‰è½´
            if n_rows > 1:
                ax = axes[row][col_idx] if n_cols > 1 else axes[row]
            else:
                ax = axes[col_idx] if n_cols > 1 else axes
            
            # è®¡ç®—å€¼è®¡æ•°
            value_counts = self.data[col].value_counts()
            
            # ç»˜åˆ¶æ¡å½¢å›¾
            value_counts.plot(kind='bar', ax=ax)
            ax.set_title(f'{col} åˆ†å¸ƒ')
            ax.set_xlabel(col)
            ax.set_ylabel('è®¡æ•°')
            ax.tick_params(axis='x', rotation=45)
            
            # ç»Ÿè®¡ä¿¡æ¯
            categorical_stats[col] = {
                'unique_count': self.data[col].nunique(),
                'most_common': value_counts.index[0],
                'most_common_count': value_counts.iloc[0],
                'least_common': value_counts.index[-1],
                'least_common_count': value_counts.iloc[-1]
            }
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°åˆ†ç±»ç»Ÿè®¡
        print("\nğŸ“‹ åˆ†ç±»å˜é‡ç»Ÿè®¡:")
        for col, stats in categorical_stats.items():
            print(f"\n{col}:")
            print(f"   å”¯ä¸€å€¼æ•°é‡: {stats['unique_count']}")
            print(f"   æœ€å¸¸è§: {stats['most_common']} ({stats['most_common_count']}æ¬¡)")
            print(f"   æœ€å°‘è§: {stats['least_common']} ({stats['least_common_count']}æ¬¡)")
        
        return categorical_stats

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ä½¿ç”¨æ¸…æ´—åçš„ç”µå•†æ•°æ®è¿›è¡Œå¯è§†åŒ–åˆ†æ
    visualizer = DataVisualizer(ecommerce_cleaned)
    
    print("=== å¼€å§‹æ•°æ®å¯è§†åŒ–åˆ†æ ===")
    
    # 1. åŸºç¡€ä¿¡æ¯å¯è§†åŒ–
    visualizer.basic_info_plot()
    
    # 2. ç›¸å…³æ€§åˆ†æ
    corr_matrix, high_corr = visualizer.correlation_analysis()
    
    # 3. åˆ†å¸ƒåˆ†æ
    stats_results = visualizer.distribution_analysis(['age', 'purchase_amount', 'purchase_frequency'])
    
    # 4. å¼‚å¸¸å€¼æ£€æµ‹
    outliers = visualizer.outlier_detection(method='iqr')
    
    # 5. åˆ†ç±»å˜é‡åˆ†æ
     cat_stats = visualizer.categorical_analysis()
```

## 4.3.4 ç‰¹å¾å·¥ç¨‹ä¸é€‰æ‹©

### æ™ºèƒ½ç‰¹å¾å·¥ç¨‹å·¥å…·

```python
# ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©å·¥å…·
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import cross_val_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

class FeatureEngineer:
    """æ™ºèƒ½ç‰¹å¾å·¥ç¨‹å·¥å…·ç±»"""
    
    def __init__(self, data, target_column=None):
        self.data = data.copy()
        self.target_column = target_column
        self.feature_importance_scores = {}
        self.created_features = []
        self.transformers = {}
    
    def create_polynomial_features(self, columns, degree=2):
        """åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾"""
        print(f"=== åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾ (degree={degree}) ===")
        
        poly_features = []
        
        for col in columns:
            if col in self.data.columns and self.data[col].dtype in ['int64', 'float64']:
                print(f"ğŸ”¢ å¤„ç†åˆ—: {col}")
                
                # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
                poly = PolynomialFeatures(degree=degree, include_bias=False)
                col_data = self.data[[col]].fillna(0)  # å¡«å……ç¼ºå¤±å€¼
                poly_data = poly.fit_transform(col_data)
                feature_names = poly.get_feature_names_out([col])
                
                # æ·»åŠ æ–°ç‰¹å¾ï¼ˆè·³è¿‡åŸå§‹ç‰¹å¾ï¼‰
                for i, name in enumerate(feature_names[1:], 1):
                    new_col_name = f"{col}_poly_{i}"
                    self.data[new_col_name] = poly_data[:, i]
                    poly_features.append(new_col_name)
                    self.created_features.append(new_col_name)
                
                # ä¿å­˜è½¬æ¢å™¨
                self.transformers[f'{col}_poly'] = poly
                print(f"   âœ… åˆ›å»ºäº† {len(feature_names)-1} ä¸ªå¤šé¡¹å¼ç‰¹å¾")
        
        print(f"\nğŸ“Š æ€»è®¡åˆ›å»º {len(poly_features)} ä¸ªå¤šé¡¹å¼ç‰¹å¾")
        return poly_features
    
    def create_interaction_features(self, column_pairs):
        """åˆ›å»ºäº¤äº’ç‰¹å¾"""
        print("=== åˆ›å»ºäº¤äº’ç‰¹å¾ ===")
        
        interaction_features = []
        
        for col1, col2 in column_pairs:
            if col1 in self.data.columns and col2 in self.data.columns:
                print(f"ğŸ”— åˆ›å»ºäº¤äº’: {col1} Ã— {col2}")
                
                # ç¡®ä¿éƒ½æ˜¯æ•°å€¼ç±»å‹
                if (self.data[col1].dtype in ['int64', 'float64'] and 
                    self.data[col2].dtype in ['int64', 'float64']):
                    
                    # å¡«å……ç¼ºå¤±å€¼
                    data1 = self.data[col1].fillna(self.data[col1].median())
                    data2 = self.data[col2].fillna(self.data[col2].median())
                    
                    # 1. ä¹˜ç§¯ç‰¹å¾
                    mult_name = f"{col1}_x_{col2}"
                    self.data[mult_name] = data1 * data2
                    interaction_features.append(mult_name)
                    
                    # 2. æ¯”å€¼ç‰¹å¾ï¼ˆé¿å…é™¤é›¶ï¼‰
                    if not (data2 == 0).any():
                        ratio_name = f"{col1}_div_{col2}"
                        self.data[ratio_name] = data1 / data2
                        interaction_features.append(ratio_name)
                    
                    # 3. å·®å€¼ç‰¹å¾
                    diff_name = f"{col1}_minus_{col2}"
                    self.data[diff_name] = data1 - data2
                    interaction_features.append(diff_name)
                    
                    # 4. å’Œç‰¹å¾
                    sum_name = f"{col1}_plus_{col2}"
                    self.data[sum_name] = data1 + data2
                    interaction_features.append(sum_name)
                    
                    print(f"   âœ… åˆ›å»ºäº† 4 ä¸ªäº¤äº’ç‰¹å¾")
                else:
                    print(f"   âš ï¸ è·³è¿‡éæ•°å€¼åˆ—: {col1}, {col2}")
        
        self.created_features.extend(interaction_features)
        print(f"\nğŸ“Š æ€»è®¡åˆ›å»º {len(interaction_features)} ä¸ªäº¤äº’ç‰¹å¾")
        return interaction_features
    
    def create_binning_features(self, columns, n_bins=5, strategy='quantile'):
        """åˆ›å»ºåˆ†ç®±ç‰¹å¾"""
        print(f"=== åˆ›å»ºåˆ†ç®±ç‰¹å¾ (bins={n_bins}, strategy={strategy}) ===")
        
        binning_features = []
        
        for col in columns:
            if col in self.data.columns and self.data[col].dtype in ['int64', 'float64']:
                print(f"ğŸ“¦ åˆ†ç®±åˆ—: {col}")
                
                data_col = self.data[col].fillna(self.data[col].median())
                
                try:
                    if strategy == 'quantile':
                        # ç­‰é¢‘åˆ†ç®±
                        bin_name = f"{col}_qbin"
                        self.data[bin_name] = pd.qcut(data_col, q=n_bins, labels=False, duplicates='drop')
                        binning_features.append(bin_name)
                        
                        # åˆ†ç®±è¾¹ç•Œç‰¹å¾
                        boundary_name = f"{col}_qboundary"
                        bins = pd.qcut(data_col, q=n_bins, duplicates='drop')
                        self.data[boundary_name] = bins.apply(lambda x: x.right if pd.notna(x) else 0)
                        binning_features.append(boundary_name)
                    
                    elif strategy == 'uniform':
                        # ç­‰å®½åˆ†ç®±
                        bin_name = f"{col}_ubin"
                        self.data[bin_name] = pd.cut(data_col, bins=n_bins, labels=False)
                        binning_features.append(bin_name)
                    
                    print(f"   âœ… åˆ›å»ºäº† {2 if strategy == 'quantile' else 1} ä¸ªåˆ†ç®±ç‰¹å¾")
                    
                except Exception as e:
                    print(f"   âŒ åˆ†ç®±å¤±è´¥: {e}")
        
        self.created_features.extend(binning_features)
        print(f"\nğŸ“Š æ€»è®¡åˆ›å»º {len(binning_features)} ä¸ªåˆ†ç®±ç‰¹å¾")
        return binning_features
    
    def feature_selection_univariate(self, k=10, score_func=None):
        """å•å˜é‡ç‰¹å¾é€‰æ‹©"""
        print("=== å•å˜é‡ç‰¹å¾é€‰æ‹© ===")
        
        if self.target_column is None:
            print("âŒ éœ€è¦æŒ‡å®šç›®æ ‡åˆ—è¿›è¡Œç‰¹å¾é€‰æ‹©")
            return None
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]
        
        # åªé€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_features = X.select_dtypes(include=[np.number])
        
        if len(numeric_features.columns) == 0:
            print("âŒ æ²¡æœ‰æ•°å€¼ç‰¹å¾å¯ä¾›é€‰æ‹©")
            return None
        
        # å¡«å……ç¼ºå¤±å€¼
        numeric_features = numeric_features.fillna(numeric_features.median())
        
        # è‡ªåŠ¨é€‰æ‹©è¯„åˆ†å‡½æ•°
        if score_func is None:
            if y.dtype in ['int64', 'float64'] and y.nunique() > 10:
                # å›å½’ä»»åŠ¡
                from sklearn.feature_selection import f_regression
                score_func = f_regression
                task_type = "å›å½’"
            else:
                # åˆ†ç±»ä»»åŠ¡
                score_func = f_classif
                task_type = "åˆ†ç±»"
        else:
            task_type = "è‡ªå®šä¹‰"
        
        print(f"ğŸ“Š ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"ğŸ” ä» {len(numeric_features.columns)} ä¸ªç‰¹å¾ä¸­é€‰æ‹© {min(k, len(numeric_features.columns))} ä¸ª")
        
        # ç‰¹å¾é€‰æ‹©
        selector = SelectKBest(score_func=score_func, k=min(k, len(numeric_features.columns)))
        X_selected = selector.fit_transform(numeric_features, y)
        
        # è·å–é€‰ä¸­çš„ç‰¹å¾
        selected_features = numeric_features.columns[selector.get_support()]
        feature_scores = dict(zip(numeric_features.columns, selector.scores_))
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_scores = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)
        
        self.feature_importance_scores['univariate'] = feature_scores
        
        print(f"\nâœ… ç‰¹å¾é€‰æ‹©å®Œæˆ")
        print("ğŸ† Top 10 ç‰¹å¾åˆ†æ•°:")
        for feature, score in sorted_scores[:10]:
            selected = "âœ“" if feature in selected_features else "âœ—"
            print(f"   {selected} {feature}: {score:.3f}")
        
        return selected_features, feature_scores
    
    def feature_selection_tree_based(self, n_estimators=100, task_type='auto'):
        """åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©"""
        print("=== åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹© ===")
        
        if self.target_column is None:
            print("âŒ éœ€è¦æŒ‡å®šç›®æ ‡åˆ—è¿›è¡Œç‰¹å¾é€‰æ‹©")
            return None
        
        # å‡†å¤‡æ•°æ®
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]
        
        # åªä½¿ç”¨æ•°å€¼ç‰¹å¾
        numeric_features = X.select_dtypes(include=[np.number])
        
        if len(numeric_features.columns) == 0:
            print("âŒ æ²¡æœ‰æ•°å€¼ç‰¹å¾å¯ä¾›é€‰æ‹©")
            return None
        
        # å¡«å……ç¼ºå¤±å€¼
        numeric_features = numeric_features.fillna(numeric_features.median())
        
        # è‡ªåŠ¨åˆ¤æ–­ä»»åŠ¡ç±»å‹
        if task_type == 'auto':
            if y.dtype in ['int64', 'float64'] and y.nunique() > 10:
                model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)
                task_type = "å›å½’"
            else:
                model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
                task_type = "åˆ†ç±»"
        elif task_type == 'classification':
            model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
        else:
            model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)
        
        print(f"ğŸ“Š ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"ğŸŒ³ ä½¿ç”¨ {n_estimators} æ£µæ ‘è¿›è¡Œç‰¹å¾é‡è¦æ€§è¯„ä¼°")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(numeric_features, y)
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = dict(zip(numeric_features.columns, model.feature_importances_))
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        self.feature_importance_scores['tree_based'] = feature_importance
        
        print("\nğŸ† ç‰¹å¾é‡è¦æ€§æ’åº (Top 15):")
        for i, (feature, importance) in enumerate(sorted_features[:15], 1):
            print(f"   {i:2d}. {feature}: {importance:.4f}")
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        if len(sorted_features) > 0:
            top_features = sorted_features[:min(15, len(sorted_features))]
            features, importances = zip(*top_features)
            
            plt.figure(figsize=(12, 8))
            plt.barh(range(len(features)), importances)
            plt.yticks(range(len(features)), features)
            plt.xlabel('ç‰¹å¾é‡è¦æ€§')
            plt.title(f'åŸºäº{task_type}éšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.show()
        
        return sorted_features
    
    def dimensionality_reduction(self, n_components=0.95, method='pca'):
        """é™ç»´å¤„ç†"""
        print(f"=== é™ç»´å¤„ç† (method={method}) ===")
        
        # å‡†å¤‡æ•°å€¼ç‰¹å¾
        numeric_features = self.data.select_dtypes(include=[np.number])
        
        if len(numeric_features.columns) < 2:
            print("âŒ ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé™ç»´")
            return None
        
        # å¡«å……ç¼ºå¤±å€¼å¹¶æ ‡å‡†åŒ–
        numeric_features = numeric_features.fillna(numeric_features.median())
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(numeric_features)
        
        print(f"ğŸ“Š åŸå§‹ç‰¹å¾æ•°: {scaled_features.shape[1]}")
        
        if method == 'pca':
            # PCAé™ç»´
            pca = PCA(n_components=n_components)
            reduced_features = pca.fit_transform(scaled_features)
            
            # åˆ›å»ºPCAç‰¹å¾åˆ—
            pca_columns = [f'PCA_{i+1}' for i in range(reduced_features.shape[1])]
            pca_df = pd.DataFrame(reduced_features, columns=pca_columns, index=self.data.index)
            
            # æ·»åŠ åˆ°åŸæ•°æ®
            for col in pca_columns:
                self.data[col] = pca_df[col]
                self.created_features.append(col)
            
            # ä¿å­˜è½¬æ¢å™¨
            self.transformers['pca'] = pca
            self.transformers['pca_scaler'] = scaler
            
            print(f"âœ… PCAé™ç»´å®Œæˆ: {len(numeric_features.columns)} -> {len(pca_columns)} ç»´")
            print(f"ğŸ“ˆ ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_.sum():.3f}")
            
            # å¯è§†åŒ–è§£é‡Šæ–¹å·®æ¯”
            plt.figure(figsize=(12, 5))
            
            plt.subplot(1, 2, 1)
            plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), 
                    pca.explained_variance_ratio_, 'bo-')
            plt.xlabel('ä¸»æˆåˆ†')
            plt.ylabel('è§£é‡Šæ–¹å·®æ¯”')
            plt.title('å„ä¸»æˆåˆ†è§£é‡Šæ–¹å·®æ¯”')
            plt.grid(True)
            
            plt.subplot(1, 2, 2)
            plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), 
                    np.cumsum(pca.explained_variance_ratio_), 'ro-')
            plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
            plt.ylabel('ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”')
            plt.title('ç´¯è®¡è§£é‡Šæ–¹å·®æ¯”')
            plt.grid(True)
            
            plt.tight_layout()
            plt.show()
            
            return pca, pca_columns
        
        return None
    
    def get_feature_summary(self):
        """è·å–ç‰¹å¾å·¥ç¨‹æ€»ç»“"""
        print("=== ç‰¹å¾å·¥ç¨‹æ€»ç»“ ===")
        
        original_features = len(self.data.columns) - len(self.created_features)
        
        summary = {
            'original_features': original_features,
            'created_features': len(self.created_features),
            'total_features': len(self.data.columns),
            'feature_types': self.data.dtypes.value_counts().to_dict(),
            'created_feature_list': self.created_features,
            'importance_scores': self.feature_importance_scores,
            'transformers': list(self.transformers.keys())
        }
        
        print(f"ğŸ“Š ç‰¹å¾ç»Ÿè®¡:")
        print(f"   åŸå§‹ç‰¹å¾æ•°: {summary['original_features']}")
        print(f"   æ–°åˆ›å»ºç‰¹å¾æ•°: {summary['created_features']}")
        print(f"   æ€»ç‰¹å¾æ•°: {summary['total_features']}")
        print(f"   ç‰¹å¾å¢é•¿ç‡: {(summary['created_features'] / summary['original_features'] * 100):.1f}%")
        
        print(f"\nğŸ”§ ä½¿ç”¨çš„è½¬æ¢å™¨: {', '.join(summary['transformers'])}")
        
        print(f"\nğŸ“ˆ ç‰¹å¾ç±»å‹åˆ†å¸ƒ:")
        for dtype, count in summary['feature_types'].items():
            print(f"   {dtype}: {count} ä¸ª")
        
        # å¯è§†åŒ–ç‰¹å¾å·¥ç¨‹æ•ˆæœ
        if len(self.created_features) > 0:
            plt.figure(figsize=(12, 6))
            
            # ç‰¹å¾æ•°é‡å¯¹æ¯”
            plt.subplot(1, 2, 1)
            categories = ['åŸå§‹ç‰¹å¾', 'æ–°å¢ç‰¹å¾']
            counts = [summary['original_features'], summary['created_features']]
            colors = ['skyblue', 'lightcoral']
            plt.pie(counts, labels=categories, colors=colors, autopct='%1.1f%%')
            plt.title('ç‰¹å¾æ•°é‡åˆ†å¸ƒ')
            
            # ç‰¹å¾ç±»å‹åˆ†å¸ƒ
            plt.subplot(1, 2, 2)
            types = list(summary['feature_types'].keys())
            type_counts = list(summary['feature_types'].values())
            plt.bar(types, type_counts, color='lightgreen')
            plt.xlabel('æ•°æ®ç±»å‹')
            plt.ylabel('ç‰¹å¾æ•°é‡')
            plt.title('ç‰¹å¾ç±»å‹åˆ†å¸ƒ')
            plt.xticks(rotation=45)
            
            plt.tight_layout()
            plt.show()
        
        return summary

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ä½¿ç”¨æ¸…æ´—åçš„ç”µå•†æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹
    fe = FeatureEngineer(ecommerce_cleaned, target_column='purchase_frequency')
    
    print("=== å¼€å§‹ç‰¹å¾å·¥ç¨‹ ===")
    
    # 1. åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
    poly_features = fe.create_polynomial_features(['age', 'purchase_amount'], degree=2)
    
    # 2. åˆ›å»ºäº¤äº’ç‰¹å¾
    interaction_features = fe.create_interaction_features([
        ('age', 'purchase_amount'),
        ('age', 'last_purchase_days'),
        ('purchase_amount', 'last_purchase_days')
    ])
    
    # 3. åˆ›å»ºåˆ†ç®±ç‰¹å¾
    binning_features = fe.create_binning_features(['age', 'purchase_amount'], n_bins=5)
    
    # 4. å•å˜é‡ç‰¹å¾é€‰æ‹©
    selected_features, scores = fe.feature_selection_univariate(k=15)
    
    # 5. åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
    tree_features = fe.feature_selection_tree_based(n_estimators=100)
    
    # 6. PCAé™ç»´
    pca, pca_cols = fe.dimensionality_reduction(n_components=0.95)
    
    # 7. ç‰¹å¾å·¥ç¨‹æ€»ç»“
    summary = fe.get_feature_summary()
```

## 4.3.5 æœ¬èŠ‚æ€»ç»“

æœ¬èŠ‚å…¨é¢ä»‹ç»äº†åœ¨Trae IDEä¸­è¿›è¡Œæ•°æ®å¤„ç†ä¸åˆ†æçš„å®Œæ•´æµç¨‹ï¼Œä¸ºAIæ¨¡å‹å¼€å‘å¥ å®šäº†åšå®çš„æ•°æ®åŸºç¡€ã€‚

### ğŸ¯ æ ¸å¿ƒæˆæœ

#### 1. **å¤šæºæ•°æ®è·å–ç³»ç»Ÿ**
- âœ… æ”¯æŒCSVã€APIã€ç¤ºä¾‹æ•°æ®é›†ç­‰å¤šç§æ•°æ®æº
- âœ… æ™ºèƒ½ç¼–ç æ£€æµ‹å’Œæ•°æ®ç±»å‹æ¨æ–­
- âœ… æ ‡å‡†åŒ–çš„æ•°æ®ä¿å­˜å’Œç®¡ç†æœºåˆ¶
- âœ… å®Œæ•´çš„æ•°æ®åŠ è½½é”™è¯¯å¤„ç†

#### 2. **æ™ºèƒ½æ•°æ®æ¸…æ´—å·¥å…·**
- âœ… å…¨é¢çš„æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š
- âœ… è‡ªé€‚åº”ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
- âœ… é‡å¤æ•°æ®æ£€æµ‹å’Œæ¸…ç†
- âœ… æ–‡æœ¬æ ‡å‡†åŒ–å’Œåˆ†ç±»ç¼–ç 
- âœ… å¤šç§ç‰¹å¾ç¼©æ”¾æ–¹æ³•

#### 3. **æ•°æ®å¯è§†åŒ–åˆ†æç³»ç»Ÿ**
- âœ… åŸºç¡€ä¿¡æ¯å¯è§†åŒ–ï¼ˆæ•°æ®ç±»å‹ã€ç¼ºå¤±å€¼åˆ†å¸ƒï¼‰
- âœ… ç›¸å…³æ€§åˆ†æå’Œçƒ­åŠ›å›¾å±•ç¤º
- âœ… åˆ†å¸ƒåˆ†æå’Œç»Ÿè®¡æ£€éªŒ
- âœ… å¼‚å¸¸å€¼æ£€æµ‹å’Œå¯è§†åŒ–
- âœ… åˆ†ç±»å˜é‡ç»Ÿè®¡åˆ†æ

#### 4. **ç‰¹å¾å·¥ç¨‹å·¥å…·é›†**
- âœ… å¤šé¡¹å¼ç‰¹å¾è‡ªåŠ¨ç”Ÿæˆ
- âœ… äº¤äº’ç‰¹å¾åˆ›å»ºï¼ˆä¹˜ç§¯ã€æ¯”å€¼ã€å·®å€¼ã€å’Œï¼‰
- âœ… æ™ºèƒ½åˆ†ç®±å¤„ç†ï¼ˆç­‰é¢‘ã€ç­‰å®½ï¼‰
- âœ… å•å˜é‡å’Œæ ‘æ¨¡å‹ç‰¹å¾é€‰æ‹©
- âœ… PCAé™ç»´å’Œå¯è§†åŒ–

### ğŸ“Š æŠ€æœ¯ç‰¹è‰²

| ç‰¹æ€§ | æè¿° | ä»·å€¼ |
|------|------|------|
| **è‡ªåŠ¨åŒ–ç¨‹åº¦** | æ™ºèƒ½ç­–ç•¥é€‰æ‹©ï¼Œå‡å°‘æ‰‹å·¥é…ç½® | æå‡å¼€å‘æ•ˆç‡ |
| **å¯è§†åŒ–ä¸°å¯Œ** | å¤šç»´åº¦å›¾è¡¨å±•ç¤ºï¼Œç›´è§‚ç†è§£æ•°æ® | å¢å¼ºæ•°æ®æ´å¯Ÿ |
| **é”™è¯¯å¤„ç†** | å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæç¤ºæœºåˆ¶ | æé«˜ç³»ç»Ÿç¨³å®šæ€§ |
| **æ¨¡å—åŒ–è®¾è®¡** | ç‹¬ç«‹çš„åŠŸèƒ½æ¨¡å—ï¼Œæ˜“äºæ‰©å±• | ä¾¿äºå®šåˆ¶å’Œç»´æŠ¤ |
| **ç»Ÿè®¡ä¸¥è°¨** | åŸºäºç»Ÿè®¡å­¦åŸç†çš„åˆ†ææ–¹æ³• | ç¡®ä¿ç»“æœå¯é æ€§ |

### ğŸ”§ å®ç”¨å·¥å…·ç±»

1. **DataLoader**: å¤šæºæ•°æ®è·å–å’ŒåŠ è½½
2. **DataCleaner**: æ™ºèƒ½æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
3. **DataVisualizer**: æ•°æ®å¯è§†åŒ–å’Œæ¢ç´¢æ€§åˆ†æ
4. **FeatureEngineer**: ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©

### ğŸ’¡ æœ€ä½³å®è·µ

#### æ•°æ®å¤„ç†æµç¨‹
```
æ•°æ®è·å– â†’ è´¨é‡åˆ†æ â†’ æ•°æ®æ¸…æ´— â†’ æ¢ç´¢æ€§åˆ†æ â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ¨¡å‹å‡†å¤‡
```

#### å…³é”®æ£€æŸ¥ç‚¹
- âœ… æ•°æ®å®Œæ•´æ€§å’Œä¸€è‡´æ€§éªŒè¯
- âœ… ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼å¤„ç†ç­–ç•¥
- âœ… ç‰¹å¾ç›¸å…³æ€§å’Œé‡è¦æ€§åˆ†æ
- âœ… æ•°æ®åˆ†å¸ƒå’Œç»Ÿè®¡ç‰¹æ€§ç†è§£
- âœ… ç‰¹å¾å·¥ç¨‹æ•ˆæœè¯„ä¼°

### ğŸš€ åç»­åº”ç”¨

æœ¬èŠ‚å»ºç«‹çš„æ•°æ®å¤„ç†æ¡†æ¶å°†åœ¨åç»­ç« èŠ‚ä¸­æ”¯æŒï¼š
- **æ¨¡å‹è®­ç»ƒ**: ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹æä¾›é«˜è´¨é‡ç‰¹å¾
- **æ¨¡å‹ä¼˜åŒ–**: é€šè¿‡ç‰¹å¾é€‰æ‹©æå‡æ¨¡å‹æ€§èƒ½
- **æ¨¡å‹éƒ¨ç½²**: ç¡®ä¿ç”Ÿäº§ç¯å¢ƒæ•°æ®ä¸€è‡´æ€§
- **é¡¹ç›®å®æˆ˜**: åœ¨ç»¼åˆæ¡ˆä¾‹ä¸­åº”ç”¨å®Œæ•´æµç¨‹

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œè¯»è€…å·²ç»æŒæ¡äº†å®Œæ•´çš„æ•°æ®å¤„ç†å·¥å…·é“¾ï¼Œèƒ½å¤Ÿç‹¬ç«‹å®Œæˆä»åŸå§‹æ•°æ®åˆ°æ¨¡å‹å°±ç»ªç‰¹å¾çš„å…¨è¿‡ç¨‹ï¼Œä¸ºåç»­çš„AIæ¨¡å‹å¼€å‘å¥ å®šäº†åšå®åŸºç¡€ã€‚