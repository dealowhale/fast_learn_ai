# 4.3 数据处理与分析实例

在AI项目开发中，数据处理与分析是至关重要的环节。本节将通过实际案例，展示如何在Trae IDE中进行高效的数据处理、探索性分析和可视化工作。

## 4.3.1 数据获取与加载

### 多源数据获取工具

```python
# 数据获取和加载工具集
import pandas as pd
import numpy as np
import requests
import sqlite3
from pathlib import Path
import json
import yaml
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris, load_boston, load_wine
import warnings
warnings.filterwarnings('ignore')

class DataLoader:
    """多源数据加载器"""
    
    def __init__(self, data_dir="./data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.loaded_datasets = {}
        
    def load_csv_data(self, file_path, **kwargs):
        """加载CSV数据"""
        print(f"=== 加载CSV数据: {file_path} ===")
        
        try:
            # 自动检测编码
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding, **kwargs)
                    print(f"✅ 成功使用 {encoding} 编码加载数据")
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                raise ValueError("无法识别文件编码")
            
            # 数据基本信息
            print(f"📊 数据形状: {df.shape}")
            print(f"📋 列名: {list(df.columns)}")
            print(f"💾 内存使用: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
            
            return df
            
        except Exception as e:
            print(f"❌ 加载失败: {e}")
            return None
    
    def load_sample_datasets(self):
        """加载示例数据集"""
        print("=== 加载示例数据集 ===")
        
        datasets = {}
        
        # 1. 鸢尾花数据集
        iris = load_iris()
        datasets['iris'] = pd.DataFrame(
            data=iris.data,
            columns=iris.feature_names
        )
        datasets['iris']['target'] = iris.target
        datasets['iris']['species'] = [iris.target_names[i] for i in iris.target]
        print(f"✅ 鸢尾花数据集: {datasets['iris'].shape}")
        
        # 2. 红酒数据集
        wine = load_wine()
        datasets['wine'] = pd.DataFrame(
            data=wine.data,
            columns=wine.feature_names
        )
        datasets['wine']['target'] = wine.target
        datasets['wine']['wine_class'] = [wine.target_names[i] for i in wine.target]
        print(f"✅ 红酒数据集: {datasets['wine'].shape}")
        
        # 3. 生成模拟电商数据
        np.random.seed(42)
        n_samples = 1000
        
        ecommerce_data = {
            'user_id': range(1, n_samples + 1),
            'age': np.random.randint(18, 65, n_samples),
            'gender': np.random.choice(['M', 'F'], n_samples),
            'city': np.random.choice(['北京', '上海', '广州', '深圳', '杭州'], n_samples),
            'purchase_amount': np.random.exponential(200, n_samples),
            'purchase_frequency': np.random.poisson(5, n_samples),
            'last_purchase_days': np.random.randint(1, 365, n_samples),
            'category_preference': np.random.choice(
                ['电子产品', '服装', '家居', '食品', '图书'], n_samples
            )
        }
        
        datasets['ecommerce'] = pd.DataFrame(ecommerce_data)
        print(f"✅ 电商数据集: {datasets['ecommerce'].shape}")
        
        # 4. 生成时间序列数据
        dates = pd.date_range('2023-01-01', periods=365, freq='D')
        ts_data = {
            'date': dates,
            'sales': 1000 + 200 * np.sin(2 * np.pi * np.arange(365) / 365) + 
                    np.random.normal(0, 50, 365),
            'temperature': 20 + 15 * np.sin(2 * np.pi * np.arange(365) / 365) + 
                          np.random.normal(0, 3, 365),
            'promotion': np.random.choice([0, 1], 365, p=[0.8, 0.2])
        }
        
        datasets['timeseries'] = pd.DataFrame(ts_data)
        print(f"✅ 时间序列数据集: {datasets['timeseries'].shape}")
        
        self.loaded_datasets = datasets
        return datasets
    
    def load_from_api(self, url, params=None, headers=None):
        """从API加载数据"""
        print(f"=== 从API加载数据: {url} ===")
        
        try:
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            # 尝试解析JSON
            if 'application/json' in response.headers.get('content-type', ''):
                data = response.json()
                
                # 如果是列表，转换为DataFrame
                if isinstance(data, list):
                    df = pd.DataFrame(data)
                elif isinstance(data, dict):
                    # 如果是嵌套字典，尝试提取数据
                    if 'data' in data:
                        df = pd.DataFrame(data['data'])
                    else:
                        df = pd.json_normalize(data)
                else:
                    df = pd.DataFrame([data])
                
                print(f"✅ API数据加载成功: {df.shape}")
                return df
            else:
                print("❌ 响应不是JSON格式")
                return None
                
        except requests.RequestException as e:
            print(f"❌ API请求失败: {e}")
            return None
        except Exception as e:
            print(f"❌ 数据解析失败: {e}")
            return None
    
    def save_dataset(self, df, name, format='csv'):
        """保存数据集"""
        print(f"=== 保存数据集: {name} ===")
        
        file_path = self.data_dir / f"{name}.{format}"
        
        try:
            if format == 'csv':
                df.to_csv(file_path, index=False, encoding='utf-8')
            elif format == 'json':
                df.to_json(file_path, orient='records', force_ascii=False, indent=2)
            elif format == 'parquet':
                df.to_parquet(file_path, index=False)
            elif format == 'excel':
                df.to_excel(file_path, index=False)
            
            print(f"✅ 数据已保存到: {file_path}")
            return file_path
            
        except Exception as e:
            print(f"❌ 保存失败: {e}")
            return None

# 创建数据加载器
data_loader = DataLoader()

# 加载示例数据集
sample_datasets = data_loader.load_sample_datasets()

# 展示数据集概览
print("\n=== 数据集概览 ===")
for name, df in sample_datasets.items():
    print(f"\n📊 {name} 数据集:")
    print(f"   形状: {df.shape}")
    print(f"   列名: {list(df.columns)[:5]}{'...' if len(df.columns) > 5 else ''}")
    print(f"   数据类型: {df.dtypes.value_counts().to_dict()}")
```

## 4.3.2 数据清洗与预处理

### 智能数据清洗工具

```python
# 数据清洗和预处理工具
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
import re

class DataCleaner:
    """智能数据清洗器"""
    
    def __init__(self):
        self.cleaning_report = {}
        self.transformers = {}
    
    def analyze_data_quality(self, df, dataset_name="数据集"):
        """分析数据质量"""
        print(f"=== {dataset_name} 数据质量分析 ===")
        
        quality_report = {
            'basic_info': {
                'shape': df.shape,
                'memory_usage': f"{df.memory_usage(deep=True).sum() / 1024**2:.2f} MB"
            },
            'missing_values': {},
            'duplicates': df.duplicated().sum(),
            'data_types': df.dtypes.to_dict(),
            'numeric_stats': {},
            'categorical_stats': {}
        }
        
        # 1. 缺失值分析
        missing_counts = df.isnull().sum()
        missing_percentages = (missing_counts / len(df)) * 100
        
        for col in df.columns:
            if missing_counts[col] > 0:
                quality_report['missing_values'][col] = {
                    'count': int(missing_counts[col]),
                    'percentage': round(missing_percentages[col], 2)
                }
        
        print(f"\n📊 基本信息:")
        print(f"   数据形状: {quality_report['basic_info']['shape']}")
        print(f"   内存使用: {quality_report['basic_info']['memory_usage']}")
        print(f"   重复行数: {quality_report['duplicates']}")
        
        if quality_report['missing_values']:
            print(f"\n⚠️ 缺失值情况:")
            for col, info in quality_report['missing_values'].items():
                print(f"   {col}: {info['count']} ({info['percentage']}%)")
        else:
            print("\n✅ 无缺失值")
        
        # 2. 数值型列统计
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(f"\n📈 数值型列统计:")
            for col in numeric_cols:
                stats = df[col].describe()
                quality_report['numeric_stats'][col] = {
                    'mean': round(stats['mean'], 2),
                    'std': round(stats['std'], 2),
                    'min': round(stats['min'], 2),
                    'max': round(stats['max'], 2),
                    'outliers': self._detect_outliers(df[col])
                }
                print(f"   {col}: 均值={stats['mean']:.2f}, 标准差={stats['std']:.2f}")
        
        # 3. 分类型列统计
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            print(f"\n📋 分类型列统计:")
            for col in categorical_cols:
                unique_count = df[col].nunique()
                most_common = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'N/A'
                quality_report['categorical_stats'][col] = {
                    'unique_count': unique_count,
                    'most_common': most_common
                }
                print(f"   {col}: {unique_count} 个唯一值, 最常见: {most_common}")
        
        self.cleaning_report[dataset_name] = quality_report
        return quality_report
    
    def _detect_outliers(self, series, method='iqr'):
        """检测异常值"""
        if method == 'iqr':
            Q1 = series.quantile(0.25)
            Q3 = series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = series[(series < lower_bound) | (series > upper_bound)]
            return len(outliers)
        return 0
    
    def handle_missing_values(self, df, strategy='auto'):
        """处理缺失值"""
        print("=== 处理缺失值 ===")
        
        df_cleaned = df.copy()
        missing_info = {}
        
        for col in df.columns:
            missing_count = df[col].isnull().sum()
            if missing_count > 0:
                missing_percentage = (missing_count / len(df)) * 100
                
                if strategy == 'auto':
                    # 自动选择策略
                    if missing_percentage > 50:
                        # 缺失超过50%，考虑删除列
                        print(f"⚠️ {col} 缺失 {missing_percentage:.1f}%，建议删除")
                        strategy_used = 'drop_column'
                    elif df[col].dtype in ['object', 'category']:
                        # 分类变量用众数填充
                        mode_value = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'Unknown'
                        df_cleaned[col].fillna(mode_value, inplace=True)
                        strategy_used = f'mode: {mode_value}'
                    else:
                        # 数值变量用中位数填充
                        median_value = df[col].median()
                        df_cleaned[col].fillna(median_value, inplace=True)
                        strategy_used = f'median: {median_value:.2f}'
                else:
                    # 使用指定策略
                    if strategy == 'mean' and df[col].dtype in [np.number]:
                        mean_value = df[col].mean()
                        df_cleaned[col].fillna(mean_value, inplace=True)
                        strategy_used = f'mean: {mean_value:.2f}'
                    elif strategy == 'median' and df[col].dtype in [np.number]:
                        median_value = df[col].median()
                        df_cleaned[col].fillna(median_value, inplace=True)
                        strategy_used = f'median: {median_value:.2f}'
                    elif strategy == 'mode':
                        mode_value = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'Unknown'
                        df_cleaned[col].fillna(mode_value, inplace=True)
                        strategy_used = f'mode: {mode_value}'
                
                missing_info[col] = {
                    'original_missing': missing_count,
                    'percentage': missing_percentage,
                    'strategy': strategy_used
                }
                
                print(f"✅ {col}: {missing_count} 个缺失值 → {strategy_used}")
        
        print(f"\n📊 处理结果: {len(missing_info)} 列有缺失值被处理")
        return df_cleaned, missing_info
    
    def remove_duplicates(self, df, subset=None, keep='first'):
        """删除重复行"""
        print("=== 删除重复行 ===")
        
        original_shape = df.shape
        df_cleaned = df.drop_duplicates(subset=subset, keep=keep)
        duplicates_removed = original_shape[0] - df_cleaned.shape[0]
        
        print(f"📊 原始数据: {original_shape[0]} 行")
        print(f"🗑️ 删除重复: {duplicates_removed} 行")
        print(f"✅ 清洗后: {df_cleaned.shape[0]} 行")
        
        return df_cleaned
    
    def standardize_text(self, df, text_columns):
        """标准化文本数据"""
        print("=== 标准化文本数据 ===")
        
        df_cleaned = df.copy()
        
        for col in text_columns:
            if col in df.columns:
                print(f"🔤 处理文本列: {col}")
                
                # 转换为字符串
                df_cleaned[col] = df_cleaned[col].astype(str)
                
                # 去除前后空格
                df_cleaned[col] = df_cleaned[col].str.strip()
                
                # 统一大小写（如果是英文）
                if df_cleaned[col].str.contains(r'[a-zA-Z]').any():
                    df_cleaned[col] = df_cleaned[col].str.lower()
                
                # 移除特殊字符（保留中文、英文、数字）
                df_cleaned[col] = df_cleaned[col].str.replace(
                    r'[^\w\s\u4e00-\u9fff]', '', regex=True
                )
                
                print(f"   ✅ {col} 文本标准化完成")
        
        return df_cleaned
    
    def encode_categorical(self, df, categorical_columns, method='label'):
        """编码分类变量"""
        print("=== 编码分类变量 ===")
        
        df_encoded = df.copy()
        encoders = {}
        
        for col in categorical_columns:
            if col in df.columns:
                print(f"🏷️ 编码列: {col}")
                
                if method == 'label':
                    # 标签编码
                    le = LabelEncoder()
                    df_encoded[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
                    encoders[col] = le
                    print(f"   ✅ 标签编码: {len(le.classes_)} 个类别")
                
                elif method == 'onehot':
                    # 独热编码
                    dummies = pd.get_dummies(df[col], prefix=col)
                    df_encoded = pd.concat([df_encoded, dummies], axis=1)
                    encoders[col] = list(dummies.columns)
                    print(f"   ✅ 独热编码: {len(dummies.columns)} 个新列")
        
        self.transformers['encoders'] = encoders
        return df_encoded, encoders
    
    def scale_features(self, df, numeric_columns, method='standard'):
        """特征缩放"""
        print("=== 特征缩放 ===")
        
        df_scaled = df.copy()
        scalers = {}
        
        for col in numeric_columns:
            if col in df.columns:
                print(f"📏 缩放列: {col}")
                
                if method == 'standard':
                    scaler = StandardScaler()
                    df_scaled[f'{col}_scaled'] = scaler.fit_transform(df[[col]])
                elif method == 'minmax':
                    scaler = MinMaxScaler()
                    df_scaled[f'{col}_scaled'] = scaler.fit_transform(df[[col]])
                
                scalers[col] = scaler
                
                # 显示缩放前后的统计信息
                original_stats = df[col].describe()
                scaled_stats = df_scaled[f'{col}_scaled'].describe()
                
                print(f"   原始: 均值={original_stats['mean']:.2f}, 标准差={original_stats['std']:.2f}")
                print(f"   缩放: 均值={scaled_stats['mean']:.2f}, 标准差={scaled_stats['std']:.2f}")
        
        self.transformers['scalers'] = scalers
        return df_scaled, scalers

# 创建数据清洗器
data_cleaner = DataCleaner()

# 使用电商数据进行清洗示例
ecommerce_df = sample_datasets['ecommerce'].copy()

# 人工添加一些数据质量问题用于演示
np.random.seed(42)
# 添加缺失值
ecommerce_df.loc[np.random.choice(ecommerce_df.index, 50), 'age'] = np.nan
ecommerce_df.loc[np.random.choice(ecommerce_df.index, 30), 'city'] = np.nan

# 添加重复行
ecommerce_df = pd.concat([ecommerce_df, ecommerce_df.iloc[:20]], ignore_index=True)

# 分析数据质量
quality_report = data_cleaner.analyze_data_quality(ecommerce_df, "电商数据")

# 处理缺失值
ecommerce_cleaned, missing_info = data_cleaner.handle_missing_values(ecommerce_df)

# 删除重复行
ecommerce_cleaned = data_cleaner.remove_duplicates(ecommerce_cleaned)

# 编码分类变量
categorical_cols = ['gender', 'city', 'category_preference']
ecommerce_encoded, encoders = data_cleaner.encode_categorical(
    ecommerce_cleaned, categorical_cols, method='label'
)

# 特征缩放
numeric_cols = ['age', 'purchase_amount', 'purchase_frequency']
ecommerce_scaled, scalers = data_cleaner.scale_features(
    ecommerce_encoded, numeric_cols, method='standard'
)

print("\n🎉 数据清洗完成！")
print(f"最终数据形状: {ecommerce_scaled.shape}")
```

## 4.3.3 数据可视化与探索性分析

### 智能数据可视化工具

```python
# 数据可视化和探索性分析工具
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

class DataVisualizer:
    """数据可视化工具类"""
    
    def __init__(self, data):
        self.data = data
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        # 设置中文字体
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
    
    def basic_info_plot(self):
        """基础信息可视化"""
        print("=== 基础信息可视化 ===")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. 数据类型分布
        dtype_counts = self.data.dtypes.value_counts()
        axes[0, 0].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')
        axes[0, 0].set_title('数据类型分布')
        
        # 2. 缺失值分布
        missing_data = self.data.isnull().sum()
        missing_data = missing_data[missing_data > 0]
        if len(missing_data) > 0:
            axes[0, 1].bar(range(len(missing_data)), missing_data.values)
            axes[0, 1].set_xticks(range(len(missing_data)))
            axes[0, 1].set_xticklabels(missing_data.index, rotation=45)
            axes[0, 1].set_title('缺失值分布')
        else:
            axes[0, 1].text(0.5, 0.5, '无缺失值', ha='center', va='center', 
                           transform=axes[0, 1].transAxes, fontsize=14)
            axes[0, 1].set_title('缺失值分布')
        
        # 3. 数值列分布概览
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            # 选择前4个数值列进行展示
            cols_to_show = numeric_cols[:4]
            for i, col in enumerate(cols_to_show):
                if i < 4:  # 最多显示4个
                    row, col_idx = divmod(i, 2)
                    if row == 0 and col_idx == 0:
                        continue  # 跳过已使用的位置
                    axes[1, 0].hist(self.data[col].dropna(), bins=20, alpha=0.7, label=col)
            axes[1, 0].set_title('数值列分布概览')
            axes[1, 0].legend()
        
        # 4. 数据集信息摘要
        info_text = f"数据形状: {self.data.shape}\n"
        info_text += f"内存使用: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n"
        info_text += f"数值列数: {len(numeric_cols)}\n"
        info_text += f"分类列数: {len(self.data.select_dtypes(include=['object']).columns)}\n"
        info_text += f"总缺失值: {self.data.isnull().sum().sum()}"
        
        axes[1, 1].text(0.1, 0.5, info_text, transform=axes[1, 1].transAxes, 
                        fontsize=12, verticalalignment='center')
        axes[1, 1].set_title('数据集信息摘要')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.show()
        return fig
    
    def correlation_analysis(self):
        """相关性分析"""
        print("=== 相关性分析 ===")
        
        numeric_data = self.data.select_dtypes(include=[np.number])
        
        if numeric_data.shape[1] < 2:
            print("❌ 数值列不足，无法进行相关性分析")
            return None
        
        # 计算相关系数矩阵
        corr_matrix = numeric_data.corr()
        
        # 创建热力图
        plt.figure(figsize=(12, 8))
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                   square=True, linewidths=0.5, cbar_kws={"shrink": .8})
        plt.title('特征相关性热力图')
        plt.tight_layout()
        plt.show()
        
        # 找出高相关性特征对
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    high_corr_pairs.append({
                        'feature1': corr_matrix.columns[i],
                        'feature2': corr_matrix.columns[j],
                        'correlation': corr_val
                    })
        
        if high_corr_pairs:
            print(f"\n⚠️ 发现 {len(high_corr_pairs)} 对高相关性特征 (|r| > 0.7):")
            for pair in high_corr_pairs:
                print(f"   {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}")
        else:
            print("\n✅ 未发现高相关性特征对")
        
        return corr_matrix, high_corr_pairs
    
    def distribution_analysis(self, columns=None):
        """分布分析"""
        print("=== 分布分析 ===")
        
        if columns is None:
            columns = self.data.select_dtypes(include=[np.number]).columns[:4]
        
        if len(columns) == 0:
            print("❌ 没有数值列可供分析")
            return None
        
        # 创建子图
        n_cols = min(len(columns), 2)
        n_rows = (len(columns) + 1) // 2
        
        fig, axes = plt.subplots(n_rows * 2, n_cols, figsize=(15, n_rows * 6))
        if len(columns) == 1:
            axes = axes.reshape(-1, 1)
        
        stats_results = {}
        
        for i, col in enumerate(columns):
            col_idx = i % n_cols
            
            # 直方图
            axes[i * 2 // n_cols * 2, col_idx].hist(self.data[col].dropna(), 
                                                    bins=30, alpha=0.7, color='skyblue')
            axes[i * 2 // n_cols * 2, col_idx].set_title(f'{col} - 分布直方图')
            axes[i * 2 // n_cols * 2, col_idx].set_xlabel(col)
            axes[i * 2 // n_cols * 2, col_idx].set_ylabel('频次')
            
            # 箱线图
            axes[i * 2 // n_cols * 2 + 1, col_idx].boxplot(self.data[col].dropna())
            axes[i * 2 // n_cols * 2 + 1, col_idx].set_title(f'{col} - 箱线图')
            axes[i * 2 // n_cols * 2 + 1, col_idx].set_ylabel(col)
            
            # 统计检验
            data_col = self.data[col].dropna()
            if len(data_col) > 3:
                try:
                    _, p_value = stats.normaltest(data_col)
                    stats_results[col] = {
                        'mean': data_col.mean(),
                        'std': data_col.std(),
                        'skewness': stats.skew(data_col),
                        'kurtosis': stats.kurtosis(data_col),
                        'is_normal': p_value > 0.05,
                        'p_value': p_value
                    }
                except:
                    stats_results[col] = {'error': '统计检验失败'}
        
        plt.tight_layout()
        plt.show()
        
        # 打印统计结果
        print("\n📊 分布统计结果:")
        for col, stats_info in stats_results.items():
            if 'error' not in stats_info:
                print(f"\n{col}:")
                print(f"   均值: {stats_info['mean']:.3f}")
                print(f"   标准差: {stats_info['std']:.3f}")
                print(f"   偏度: {stats_info['skewness']:.3f}")
                print(f"   峰度: {stats_info['kurtosis']:.3f}")
                print(f"   正态分布: {'是' if stats_info['is_normal'] else '否'} (p={stats_info['p_value']:.4f})")
        
        return stats_results
    
    def outlier_detection(self, method='iqr'):
        """异常值检测"""
        print("=== 异常值检测 ===")
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        outliers_info = {}
        
        for col in numeric_cols:
            data_col = self.data[col].dropna()
            
            if len(data_col) == 0:
                continue
            
            if method == 'iqr':
                Q1 = data_col.quantile(0.25)
                Q3 = data_col.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = data_col[(data_col < lower_bound) | (data_col > upper_bound)]
            
            elif method == 'zscore':
                z_scores = np.abs(stats.zscore(data_col))
                outliers = data_col[z_scores > 3]
            
            outliers_info[col] = {
                'count': len(outliers),
                'percentage': len(outliers) / len(data_col) * 100,
                'values': outliers.tolist()[:10],  # 只显示前10个
                'bounds': {'lower': lower_bound if method == 'iqr' else None,
                          'upper': upper_bound if method == 'iqr' else None}
            }
        
        # 可视化异常值
        n_cols = min(4, len(numeric_cols))
        if n_cols > 0:
            fig, axes = plt.subplots(1, n_cols, figsize=(15, 4))
            if n_cols == 1:
                axes = [axes]
            
            for i, col in enumerate(numeric_cols[:n_cols]):
                sns.boxplot(y=self.data[col], ax=axes[i])
                axes[i].set_title(f'{col}\n异常值: {outliers_info[col]["count"]}个 '
                                f'({outliers_info[col]["percentage"]:.1f}%)')
            
            plt.tight_layout()
            plt.show()
        
        # 打印异常值统计
        print("\n🔍 异常值检测结果:")
        for col, info in outliers_info.items():
            print(f"{col}: {info['count']}个异常值 ({info['percentage']:.1f}%)")
        
        return outliers_info
    
    def categorical_analysis(self):
        """分类变量分析"""
        print("=== 分类变量分析 ===")
        
        categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns
        
        if len(categorical_cols) == 0:
            print("❌ 没有分类变量可供分析")
            return None
        
        n_cols = min(2, len(categorical_cols))
        n_rows = (len(categorical_cols) + 1) // 2
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))
        if len(categorical_cols) == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = [axes]
        
        categorical_stats = {}
        
        for i, col in enumerate(categorical_cols):
            row = i // n_cols
            col_idx = i % n_cols
            
            # 获取当前轴
            if n_rows > 1:
                ax = axes[row][col_idx] if n_cols > 1 else axes[row]
            else:
                ax = axes[col_idx] if n_cols > 1 else axes
            
            # 计算值计数
            value_counts = self.data[col].value_counts()
            
            # 绘制条形图
            value_counts.plot(kind='bar', ax=ax)
            ax.set_title(f'{col} 分布')
            ax.set_xlabel(col)
            ax.set_ylabel('计数')
            ax.tick_params(axis='x', rotation=45)
            
            # 统计信息
            categorical_stats[col] = {
                'unique_count': self.data[col].nunique(),
                'most_common': value_counts.index[0],
                'most_common_count': value_counts.iloc[0],
                'least_common': value_counts.index[-1],
                'least_common_count': value_counts.iloc[-1]
            }
        
        plt.tight_layout()
        plt.show()
        
        # 打印分类统计
        print("\n📋 分类变量统计:")
        for col, stats in categorical_stats.items():
            print(f"\n{col}:")
            print(f"   唯一值数量: {stats['unique_count']}")
            print(f"   最常见: {stats['most_common']} ({stats['most_common_count']}次)")
            print(f"   最少见: {stats['least_common']} ({stats['least_common_count']}次)")
        
        return categorical_stats

# 使用示例
if __name__ == "__main__":
    # 使用清洗后的电商数据进行可视化分析
    visualizer = DataVisualizer(ecommerce_cleaned)
    
    print("=== 开始数据可视化分析 ===")
    
    # 1. 基础信息可视化
    visualizer.basic_info_plot()
    
    # 2. 相关性分析
    corr_matrix, high_corr = visualizer.correlation_analysis()
    
    # 3. 分布分析
    stats_results = visualizer.distribution_analysis(['age', 'purchase_amount', 'purchase_frequency'])
    
    # 4. 异常值检测
    outliers = visualizer.outlier_detection(method='iqr')
    
    # 5. 分类变量分析
     cat_stats = visualizer.categorical_analysis()
```

## 4.3.4 特征工程与选择

### 智能特征工程工具

```python
# 特征工程和选择工具
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import cross_val_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

class FeatureEngineer:
    """智能特征工程工具类"""
    
    def __init__(self, data, target_column=None):
        self.data = data.copy()
        self.target_column = target_column
        self.feature_importance_scores = {}
        self.created_features = []
        self.transformers = {}
    
    def create_polynomial_features(self, columns, degree=2):
        """创建多项式特征"""
        print(f"=== 创建多项式特征 (degree={degree}) ===")
        
        poly_features = []
        
        for col in columns:
            if col in self.data.columns and self.data[col].dtype in ['int64', 'float64']:
                print(f"🔢 处理列: {col}")
                
                # 创建多项式特征
                poly = PolynomialFeatures(degree=degree, include_bias=False)
                col_data = self.data[[col]].fillna(0)  # 填充缺失值
                poly_data = poly.fit_transform(col_data)
                feature_names = poly.get_feature_names_out([col])
                
                # 添加新特征（跳过原始特征）
                for i, name in enumerate(feature_names[1:], 1):
                    new_col_name = f"{col}_poly_{i}"
                    self.data[new_col_name] = poly_data[:, i]
                    poly_features.append(new_col_name)
                    self.created_features.append(new_col_name)
                
                # 保存转换器
                self.transformers[f'{col}_poly'] = poly
                print(f"   ✅ 创建了 {len(feature_names)-1} 个多项式特征")
        
        print(f"\n📊 总计创建 {len(poly_features)} 个多项式特征")
        return poly_features
    
    def create_interaction_features(self, column_pairs):
        """创建交互特征"""
        print("=== 创建交互特征 ===")
        
        interaction_features = []
        
        for col1, col2 in column_pairs:
            if col1 in self.data.columns and col2 in self.data.columns:
                print(f"🔗 创建交互: {col1} × {col2}")
                
                # 确保都是数值类型
                if (self.data[col1].dtype in ['int64', 'float64'] and 
                    self.data[col2].dtype in ['int64', 'float64']):
                    
                    # 填充缺失值
                    data1 = self.data[col1].fillna(self.data[col1].median())
                    data2 = self.data[col2].fillna(self.data[col2].median())
                    
                    # 1. 乘积特征
                    mult_name = f"{col1}_x_{col2}"
                    self.data[mult_name] = data1 * data2
                    interaction_features.append(mult_name)
                    
                    # 2. 比值特征（避免除零）
                    if not (data2 == 0).any():
                        ratio_name = f"{col1}_div_{col2}"
                        self.data[ratio_name] = data1 / data2
                        interaction_features.append(ratio_name)
                    
                    # 3. 差值特征
                    diff_name = f"{col1}_minus_{col2}"
                    self.data[diff_name] = data1 - data2
                    interaction_features.append(diff_name)
                    
                    # 4. 和特征
                    sum_name = f"{col1}_plus_{col2}"
                    self.data[sum_name] = data1 + data2
                    interaction_features.append(sum_name)
                    
                    print(f"   ✅ 创建了 4 个交互特征")
                else:
                    print(f"   ⚠️ 跳过非数值列: {col1}, {col2}")
        
        self.created_features.extend(interaction_features)
        print(f"\n📊 总计创建 {len(interaction_features)} 个交互特征")
        return interaction_features
    
    def create_binning_features(self, columns, n_bins=5, strategy='quantile'):
        """创建分箱特征"""
        print(f"=== 创建分箱特征 (bins={n_bins}, strategy={strategy}) ===")
        
        binning_features = []
        
        for col in columns:
            if col in self.data.columns and self.data[col].dtype in ['int64', 'float64']:
                print(f"📦 分箱列: {col}")
                
                data_col = self.data[col].fillna(self.data[col].median())
                
                try:
                    if strategy == 'quantile':
                        # 等频分箱
                        bin_name = f"{col}_qbin"
                        self.data[bin_name] = pd.qcut(data_col, q=n_bins, labels=False, duplicates='drop')
                        binning_features.append(bin_name)
                        
                        # 分箱边界特征
                        boundary_name = f"{col}_qboundary"
                        bins = pd.qcut(data_col, q=n_bins, duplicates='drop')
                        self.data[boundary_name] = bins.apply(lambda x: x.right if pd.notna(x) else 0)
                        binning_features.append(boundary_name)
                    
                    elif strategy == 'uniform':
                        # 等宽分箱
                        bin_name = f"{col}_ubin"
                        self.data[bin_name] = pd.cut(data_col, bins=n_bins, labels=False)
                        binning_features.append(bin_name)
                    
                    print(f"   ✅ 创建了 {2 if strategy == 'quantile' else 1} 个分箱特征")
                    
                except Exception as e:
                    print(f"   ❌ 分箱失败: {e}")
        
        self.created_features.extend(binning_features)
        print(f"\n📊 总计创建 {len(binning_features)} 个分箱特征")
        return binning_features
    
    def feature_selection_univariate(self, k=10, score_func=None):
        """单变量特征选择"""
        print("=== 单变量特征选择 ===")
        
        if self.target_column is None:
            print("❌ 需要指定目标列进行特征选择")
            return None
        
        # 准备特征和目标
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]
        
        # 只选择数值特征
        numeric_features = X.select_dtypes(include=[np.number])
        
        if len(numeric_features.columns) == 0:
            print("❌ 没有数值特征可供选择")
            return None
        
        # 填充缺失值
        numeric_features = numeric_features.fillna(numeric_features.median())
        
        # 自动选择评分函数
        if score_func is None:
            if y.dtype in ['int64', 'float64'] and y.nunique() > 10:
                # 回归任务
                from sklearn.feature_selection import f_regression
                score_func = f_regression
                task_type = "回归"
            else:
                # 分类任务
                score_func = f_classif
                task_type = "分类"
        else:
            task_type = "自定义"
        
        print(f"📊 任务类型: {task_type}")
        print(f"🔍 从 {len(numeric_features.columns)} 个特征中选择 {min(k, len(numeric_features.columns))} 个")
        
        # 特征选择
        selector = SelectKBest(score_func=score_func, k=min(k, len(numeric_features.columns)))
        X_selected = selector.fit_transform(numeric_features, y)
        
        # 获取选中的特征
        selected_features = numeric_features.columns[selector.get_support()]
        feature_scores = dict(zip(numeric_features.columns, selector.scores_))
        
        # 按分数排序
        sorted_scores = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)
        
        self.feature_importance_scores['univariate'] = feature_scores
        
        print(f"\n✅ 特征选择完成")
        print("🏆 Top 10 特征分数:")
        for feature, score in sorted_scores[:10]:
            selected = "✓" if feature in selected_features else "✗"
            print(f"   {selected} {feature}: {score:.3f}")
        
        return selected_features, feature_scores
    
    def feature_selection_tree_based(self, n_estimators=100, task_type='auto'):
        """基于树模型的特征选择"""
        print("=== 基于树模型的特征选择 ===")
        
        if self.target_column is None:
            print("❌ 需要指定目标列进行特征选择")
            return None
        
        # 准备数据
        X = self.data.drop(columns=[self.target_column])
        y = self.data[self.target_column]
        
        # 只使用数值特征
        numeric_features = X.select_dtypes(include=[np.number])
        
        if len(numeric_features.columns) == 0:
            print("❌ 没有数值特征可供选择")
            return None
        
        # 填充缺失值
        numeric_features = numeric_features.fillna(numeric_features.median())
        
        # 自动判断任务类型
        if task_type == 'auto':
            if y.dtype in ['int64', 'float64'] and y.nunique() > 10:
                model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)
                task_type = "回归"
            else:
                model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
                task_type = "分类"
        elif task_type == 'classification':
            model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
        else:
            model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)
        
        print(f"📊 任务类型: {task_type}")
        print(f"🌳 使用 {n_estimators} 棵树进行特征重要性评估")
        
        # 训练模型
        model.fit(numeric_features, y)
        
        # 获取特征重要性
        feature_importance = dict(zip(numeric_features.columns, model.feature_importances_))
        
        # 按重要性排序
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        self.feature_importance_scores['tree_based'] = feature_importance
        
        print("\n🏆 特征重要性排序 (Top 15):")
        for i, (feature, importance) in enumerate(sorted_features[:15], 1):
            print(f"   {i:2d}. {feature}: {importance:.4f}")
        
        # 可视化特征重要性
        if len(sorted_features) > 0:
            top_features = sorted_features[:min(15, len(sorted_features))]
            features, importances = zip(*top_features)
            
            plt.figure(figsize=(12, 8))
            plt.barh(range(len(features)), importances)
            plt.yticks(range(len(features)), features)
            plt.xlabel('特征重要性')
            plt.title(f'基于{task_type}随机森林的特征重要性')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.show()
        
        return sorted_features
    
    def dimensionality_reduction(self, n_components=0.95, method='pca'):
        """降维处理"""
        print(f"=== 降维处理 (method={method}) ===")
        
        # 准备数值特征
        numeric_features = self.data.select_dtypes(include=[np.number])
        
        if len(numeric_features.columns) < 2:
            print("❌ 特征数量不足，无法进行降维")
            return None
        
        # 填充缺失值并标准化
        numeric_features = numeric_features.fillna(numeric_features.median())
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(numeric_features)
        
        print(f"📊 原始特征数: {scaled_features.shape[1]}")
        
        if method == 'pca':
            # PCA降维
            pca = PCA(n_components=n_components)
            reduced_features = pca.fit_transform(scaled_features)
            
            # 创建PCA特征列
            pca_columns = [f'PCA_{i+1}' for i in range(reduced_features.shape[1])]
            pca_df = pd.DataFrame(reduced_features, columns=pca_columns, index=self.data.index)
            
            # 添加到原数据
            for col in pca_columns:
                self.data[col] = pca_df[col]
                self.created_features.append(col)
            
            # 保存转换器
            self.transformers['pca'] = pca
            self.transformers['pca_scaler'] = scaler
            
            print(f"✅ PCA降维完成: {len(numeric_features.columns)} -> {len(pca_columns)} 维")
            print(f"📈 累计解释方差比: {pca.explained_variance_ratio_.sum():.3f}")
            
            # 可视化解释方差比
            plt.figure(figsize=(12, 5))
            
            plt.subplot(1, 2, 1)
            plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), 
                    pca.explained_variance_ratio_, 'bo-')
            plt.xlabel('主成分')
            plt.ylabel('解释方差比')
            plt.title('各主成分解释方差比')
            plt.grid(True)
            
            plt.subplot(1, 2, 2)
            plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), 
                    np.cumsum(pca.explained_variance_ratio_), 'ro-')
            plt.xlabel('主成分数量')
            plt.ylabel('累计解释方差比')
            plt.title('累计解释方差比')
            plt.grid(True)
            
            plt.tight_layout()
            plt.show()
            
            return pca, pca_columns
        
        return None
    
    def get_feature_summary(self):
        """获取特征工程总结"""
        print("=== 特征工程总结 ===")
        
        original_features = len(self.data.columns) - len(self.created_features)
        
        summary = {
            'original_features': original_features,
            'created_features': len(self.created_features),
            'total_features': len(self.data.columns),
            'feature_types': self.data.dtypes.value_counts().to_dict(),
            'created_feature_list': self.created_features,
            'importance_scores': self.feature_importance_scores,
            'transformers': list(self.transformers.keys())
        }
        
        print(f"📊 特征统计:")
        print(f"   原始特征数: {summary['original_features']}")
        print(f"   新创建特征数: {summary['created_features']}")
        print(f"   总特征数: {summary['total_features']}")
        print(f"   特征增长率: {(summary['created_features'] / summary['original_features'] * 100):.1f}%")
        
        print(f"\n🔧 使用的转换器: {', '.join(summary['transformers'])}")
        
        print(f"\n📈 特征类型分布:")
        for dtype, count in summary['feature_types'].items():
            print(f"   {dtype}: {count} 个")
        
        # 可视化特征工程效果
        if len(self.created_features) > 0:
            plt.figure(figsize=(12, 6))
            
            # 特征数量对比
            plt.subplot(1, 2, 1)
            categories = ['原始特征', '新增特征']
            counts = [summary['original_features'], summary['created_features']]
            colors = ['skyblue', 'lightcoral']
            plt.pie(counts, labels=categories, colors=colors, autopct='%1.1f%%')
            plt.title('特征数量分布')
            
            # 特征类型分布
            plt.subplot(1, 2, 2)
            types = list(summary['feature_types'].keys())
            type_counts = list(summary['feature_types'].values())
            plt.bar(types, type_counts, color='lightgreen')
            plt.xlabel('数据类型')
            plt.ylabel('特征数量')
            plt.title('特征类型分布')
            plt.xticks(rotation=45)
            
            plt.tight_layout()
            plt.show()
        
        return summary

# 使用示例
if __name__ == "__main__":
    # 使用清洗后的电商数据进行特征工程
    fe = FeatureEngineer(ecommerce_cleaned, target_column='purchase_frequency')
    
    print("=== 开始特征工程 ===")
    
    # 1. 创建多项式特征
    poly_features = fe.create_polynomial_features(['age', 'purchase_amount'], degree=2)
    
    # 2. 创建交互特征
    interaction_features = fe.create_interaction_features([
        ('age', 'purchase_amount'),
        ('age', 'last_purchase_days'),
        ('purchase_amount', 'last_purchase_days')
    ])
    
    # 3. 创建分箱特征
    binning_features = fe.create_binning_features(['age', 'purchase_amount'], n_bins=5)
    
    # 4. 单变量特征选择
    selected_features, scores = fe.feature_selection_univariate(k=15)
    
    # 5. 基于树模型的特征选择
    tree_features = fe.feature_selection_tree_based(n_estimators=100)
    
    # 6. PCA降维
    pca, pca_cols = fe.dimensionality_reduction(n_components=0.95)
    
    # 7. 特征工程总结
    summary = fe.get_feature_summary()
```

## 4.3.5 本节总结

本节全面介绍了在Trae IDE中进行数据处理与分析的完整流程，为AI模型开发奠定了坚实的数据基础。

### 🎯 核心成果

#### 1. **多源数据获取系统**
- ✅ 支持CSV、API、示例数据集等多种数据源
- ✅ 智能编码检测和数据类型推断
- ✅ 标准化的数据保存和管理机制
- ✅ 完整的数据加载错误处理

#### 2. **智能数据清洗工具**
- ✅ 全面的数据质量分析报告
- ✅ 自适应缺失值处理策略
- ✅ 重复数据检测和清理
- ✅ 文本标准化和分类编码
- ✅ 多种特征缩放方法

#### 3. **数据可视化分析系统**
- ✅ 基础信息可视化（数据类型、缺失值分布）
- ✅ 相关性分析和热力图展示
- ✅ 分布分析和统计检验
- ✅ 异常值检测和可视化
- ✅ 分类变量统计分析

#### 4. **特征工程工具集**
- ✅ 多项式特征自动生成
- ✅ 交互特征创建（乘积、比值、差值、和）
- ✅ 智能分箱处理（等频、等宽）
- ✅ 单变量和树模型特征选择
- ✅ PCA降维和可视化

### 📊 技术特色

| 特性 | 描述 | 价值 |
|------|------|------|
| **自动化程度** | 智能策略选择，减少手工配置 | 提升开发效率 |
| **可视化丰富** | 多维度图表展示，直观理解数据 | 增强数据洞察 |
| **错误处理** | 完善的异常处理和提示机制 | 提高系统稳定性 |
| **模块化设计** | 独立的功能模块，易于扩展 | 便于定制和维护 |
| **统计严谨** | 基于统计学原理的分析方法 | 确保结果可靠性 |

### 🔧 实用工具类

1. **DataLoader**: 多源数据获取和加载
2. **DataCleaner**: 智能数据清洗和预处理
3. **DataVisualizer**: 数据可视化和探索性分析
4. **FeatureEngineer**: 特征工程和选择

### 💡 最佳实践

#### 数据处理流程
```
数据获取 → 质量分析 → 数据清洗 → 探索性分析 → 特征工程 → 模型准备
```

#### 关键检查点
- ✅ 数据完整性和一致性验证
- ✅ 缺失值和异常值处理策略
- ✅ 特征相关性和重要性分析
- ✅ 数据分布和统计特性理解
- ✅ 特征工程效果评估

### 🚀 后续应用

本节建立的数据处理框架将在后续章节中支持：
- **模型训练**: 为机器学习模型提供高质量特征
- **模型优化**: 通过特征选择提升模型性能
- **模型部署**: 确保生产环境数据一致性
- **项目实战**: 在综合案例中应用完整流程

通过本节的学习，读者已经掌握了完整的数据处理工具链，能够独立完成从原始数据到模型就绪特征的全过程，为后续的AI模型开发奠定了坚实基础。