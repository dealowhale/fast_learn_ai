# 4.4 æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–å®ä¾‹

åœ¨å®Œæˆæ•°æ®å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹åï¼Œæœ¬èŠ‚å°†æ·±å…¥æ¢è®¨åœ¨Trae IDEä¸­è¿›è¡Œæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–çš„å®Œæ•´æµç¨‹ã€‚æˆ‘ä»¬å°†æ„å»ºä¸€å¥—æ™ºèƒ½åŒ–çš„æ¨¡å‹è®­ç»ƒç³»ç»Ÿï¼Œæ¶µç›–æ¨¡å‹é€‰æ‹©ã€è®­ç»ƒç®¡ç†ã€è¶…å‚æ•°ä¼˜åŒ–å’Œæ€§èƒ½è¯„ä¼°ç­‰å…³é”®ç¯èŠ‚ã€‚

## 4.4.1 æ™ºèƒ½æ¨¡å‹é€‰æ‹©ä¸ç®¡ç†

### æ¨¡å‹ç®¡ç†å™¨

```python
# æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œç®¡ç†ç³»ç»Ÿ
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.svm import SVC, SVR
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import joblib
import warnings
warnings.filterwarnings('ignore')

class ModelManager:
    """æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, task_type='auto'):
        self.task_type = task_type
        self.models = {}
        self.trained_models = {}
        self.model_results = {}
        self.best_model = None
        self.scaler = None
        self.label_encoder = None
        
        # åˆå§‹åŒ–æ¨¡å‹åº“
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹åº“"""
        print("=== åˆå§‹åŒ–æ¨¡å‹åº“ ===")
        
        # åˆ†ç±»æ¨¡å‹
        self.classification_models = {
            'RandomForest': RandomForestClassifier(random_state=42),
            'GradientBoosting': GradientBoostingClassifier(random_state=42),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
            'SVM': SVC(random_state=42, probability=True),
            'KNN': KNeighborsClassifier(),
            'NaiveBayes': GaussianNB(),
            'DecisionTree': DecisionTreeClassifier(random_state=42),
            'MLP': MLPClassifier(random_state=42, max_iter=500)
        }
        
        # å›å½’æ¨¡å‹
        self.regression_models = {
            'RandomForest': RandomForestRegressor(random_state=42),
            'GradientBoosting': GradientBoostingRegressor(random_state=42),
            'LinearRegression': LinearRegression(),
            'Ridge': Ridge(random_state=42),
            'Lasso': Lasso(random_state=42),
            'SVR': SVR(),
            'KNN': KNeighborsRegressor(),
            'DecisionTree': DecisionTreeRegressor(random_state=42),
            'MLP': MLPRegressor(random_state=42, max_iter=500)
        }
        
        print(f"âœ… åˆ†ç±»æ¨¡å‹: {len(self.classification_models)} ä¸ª")
        print(f"âœ… å›å½’æ¨¡å‹: {len(self.regression_models)} ä¸ª")
    
    def auto_detect_task_type(self, y):
        """è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹"""
        print("=== è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹ ===")
        
        if self.task_type != 'auto':
            print(f"ğŸ“Š æ‰‹åŠ¨æŒ‡å®šä»»åŠ¡ç±»å‹: {self.task_type}")
            return self.task_type
        
        # æ£€æµ‹ç›®æ ‡å˜é‡ç‰¹å¾
        unique_values = y.nunique()
        data_type = y.dtype
        
        print(f"ğŸ” ç›®æ ‡å˜é‡åˆ†æ:")
        print(f"   æ•°æ®ç±»å‹: {data_type}")
        print(f"   å”¯ä¸€å€¼æ•°é‡: {unique_values}")
        print(f"   æ ·æœ¬æ•°é‡: {len(y)}")
        
        # åˆ¤æ–­é€»è¾‘
        if data_type in ['object', 'category'] or unique_values <= 20:
            detected_type = 'classification'
            print(f"âœ… æ£€æµ‹ä¸ºåˆ†ç±»ä»»åŠ¡ (å”¯ä¸€å€¼â‰¤20 æˆ– åˆ†ç±»æ•°æ®ç±»å‹)")
        elif data_type in ['int64', 'float64'] and unique_values > 20:
            detected_type = 'regression'
            print(f"âœ… æ£€æµ‹ä¸ºå›å½’ä»»åŠ¡ (æ•°å€¼ç±»å‹ä¸”å”¯ä¸€å€¼>20)")
        else:
            # é»˜è®¤åˆ†ç±»
            detected_type = 'classification'
            print(f"âš ï¸ æ— æ³•æ˜ç¡®åˆ¤æ–­ï¼Œé»˜è®¤ä¸ºåˆ†ç±»ä»»åŠ¡")
        
        self.task_type = detected_type
        return detected_type
    
    def prepare_data(self, X, y, test_size=0.2, random_state=42):
        """æ•°æ®é¢„å¤„ç†å’Œåˆ†å‰²"""
        print("=== æ•°æ®é¢„å¤„ç†å’Œåˆ†å‰² ===")
        
        # è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹
        task_type = self.auto_detect_task_type(y)
        
        # å¤„ç†ç¼ºå¤±å€¼
        print(f"ğŸ”§ å¤„ç†ç¼ºå¤±å€¼...")
        X_processed = X.copy()
        
        # æ•°å€¼ç‰¹å¾å¡«å……ä¸­ä½æ•°
        numeric_features = X_processed.select_dtypes(include=[np.number])
        if len(numeric_features.columns) > 0:
            X_processed[numeric_features.columns] = numeric_features.fillna(numeric_features.median())
            print(f"   âœ… æ•°å€¼ç‰¹å¾: {len(numeric_features.columns)} ä¸ªï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……")
        
        # åˆ†ç±»ç‰¹å¾å¡«å……ä¼—æ•°
        categorical_features = X_processed.select_dtypes(include=['object', 'category'])
        if len(categorical_features.columns) > 0:
            for col in categorical_features.columns:
                X_processed[col] = X_processed[col].fillna(X_processed[col].mode()[0] if len(X_processed[col].mode()) > 0 else 'Unknown')
            print(f"   âœ… åˆ†ç±»ç‰¹å¾: {len(categorical_features.columns)} ä¸ªï¼Œä½¿ç”¨ä¼—æ•°å¡«å……")
        
        # ç¼–ç åˆ†ç±»ç‰¹å¾
        if len(categorical_features.columns) > 0:
            print(f"ğŸ”¤ ç¼–ç åˆ†ç±»ç‰¹å¾...")
            for col in categorical_features.columns:
                le = LabelEncoder()
                X_processed[col] = le.fit_transform(X_processed[col].astype(str))
            print(f"   âœ… å®Œæˆ {len(categorical_features.columns)} ä¸ªåˆ†ç±»ç‰¹å¾ç¼–ç ")
        
        # å¤„ç†ç›®æ ‡å˜é‡
        y_processed = y.copy()
        if task_type == 'classification' and y_processed.dtype == 'object':
            print(f"ğŸ¯ ç¼–ç ç›®æ ‡å˜é‡...")
            self.label_encoder = LabelEncoder()
            y_processed = self.label_encoder.fit_transform(y_processed)
            print(f"   âœ… ç›®æ ‡å˜é‡ç¼–ç å®Œæˆ: {len(self.label_encoder.classes_)} ä¸ªç±»åˆ«")
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        print(f"ğŸ“ ç‰¹å¾æ ‡å‡†åŒ–...")
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_processed)
        X_scaled = pd.DataFrame(X_scaled, columns=X_processed.columns, index=X_processed.index)
        print(f"   âœ… æ ‡å‡†åŒ–å®Œæˆ: {X_scaled.shape[1]} ä¸ªç‰¹å¾")
        
        # æ•°æ®åˆ†å‰²
        print(f"âœ‚ï¸ æ•°æ®åˆ†å‰² (æµ‹è¯•é›†æ¯”ä¾‹: {test_size})...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_processed, test_size=test_size, random_state=random_state, 
            stratify=y_processed if task_type == 'classification' else None
        )
        
        print(f"   âœ… è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
        print(f"   âœ… æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        self.X_train, self.X_test = X_train, X_test
        self.y_train, self.y_test = y_train, y_test
        
        return X_train, X_test, y_train, y_test
    
    def get_model_library(self):
        """è·å–é€‚åˆå½“å‰ä»»åŠ¡çš„æ¨¡å‹åº“"""
        if self.task_type == 'classification':
            return self.classification_models
        else:
            return self.regression_models
    
    def train_single_model(self, model_name, model=None, cv_folds=5):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"\n=== è®­ç»ƒæ¨¡å‹: {model_name} ===")
        
        if model is None:
            model_library = self.get_model_library()
            if model_name not in model_library:
                print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
                return None
            model = model_library[model_name]
        
        start_time = datetime.now()
        
        try:
            # äº¤å‰éªŒè¯
            print(f"ğŸ”„ è¿›è¡Œ {cv_folds} æŠ˜äº¤å‰éªŒè¯...")
            
            if self.task_type == 'classification':
                cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=cv_folds, scoring='accuracy')
                scoring_name = 'accuracy'
            else:
                cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=cv_folds, scoring='r2')
                scoring_name = 'r2'
            
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            print(f"   ğŸ“Š äº¤å‰éªŒè¯ {scoring_name}: {cv_mean:.4f} (Â±{cv_std:.4f})")
            
            # è®­ç»ƒæ¨¡å‹
            print(f"ğŸ‹ï¸ è®­ç»ƒæ¨¡å‹...")
            model.fit(self.X_train, self.y_train)
            
            # é¢„æµ‹
            y_train_pred = model.predict(self.X_train)
            y_test_pred = model.predict(self.X_test)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            if self.task_type == 'classification':
                metrics = self._calculate_classification_metrics(y_train_pred, y_test_pred)
            else:
                metrics = self._calculate_regression_metrics(y_train_pred, y_test_pred)
            
            # è®­ç»ƒæ—¶é—´
            training_time = (datetime.now() - start_time).total_seconds()
            
            # ä¿å­˜ç»“æœ
            result = {
                'model': model,
                'cv_scores': cv_scores,
                'cv_mean': cv_mean,
                'cv_std': cv_std,
                'metrics': metrics,
                'training_time': training_time,
                'predictions': {
                    'train': y_train_pred,
                    'test': y_test_pred
                }
            }
            
            self.trained_models[model_name] = model
            self.model_results[model_name] = result
            
            print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ (è€—æ—¶: {training_time:.2f}ç§’)")
            self._print_model_metrics(model_name, metrics)
            
            return result
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return None
    
    def _calculate_classification_metrics(self, y_train_pred, y_test_pred):
        """è®¡ç®—åˆ†ç±»æŒ‡æ ‡"""
        metrics = {}
        
        # è®­ç»ƒé›†æŒ‡æ ‡
        metrics['train'] = {
            'accuracy': accuracy_score(self.y_train, y_train_pred),
            'precision': precision_score(self.y_train, y_train_pred, average='weighted', zero_division=0),
            'recall': recall_score(self.y_train, y_train_pred, average='weighted', zero_division=0),
            'f1': f1_score(self.y_train, y_train_pred, average='weighted', zero_division=0)
        }
        
        # æµ‹è¯•é›†æŒ‡æ ‡
        metrics['test'] = {
            'accuracy': accuracy_score(self.y_test, y_test_pred),
            'precision': precision_score(self.y_test, y_test_pred, average='weighted', zero_division=0),
            'recall': recall_score(self.y_test, y_test_pred, average='weighted', zero_division=0),
            'f1': f1_score(self.y_test, y_test_pred, average='weighted', zero_division=0)
        }
        
        return metrics
    
    def _calculate_regression_metrics(self, y_train_pred, y_test_pred):
        """è®¡ç®—å›å½’æŒ‡æ ‡"""
        metrics = {}
        
        # è®­ç»ƒé›†æŒ‡æ ‡
        metrics['train'] = {
            'mse': mean_squared_error(self.y_train, y_train_pred),
            'mae': mean_absolute_error(self.y_train, y_train_pred),
            'r2': r2_score(self.y_train, y_train_pred),
            'rmse': np.sqrt(mean_squared_error(self.y_train, y_train_pred))
        }
        
        # æµ‹è¯•é›†æŒ‡æ ‡
        metrics['test'] = {
            'mse': mean_squared_error(self.y_test, y_test_pred),
            'mae': mean_absolute_error(self.y_test, y_test_pred),
            'r2': r2_score(self.y_test, y_test_pred),
            'rmse': np.sqrt(mean_squared_error(self.y_test, y_test_pred))
        }
        
        return metrics
    
    def _print_model_metrics(self, model_name, metrics):
        """æ‰“å°æ¨¡å‹è¯„ä¼°æŒ‡æ ‡"""
        print(f"\nğŸ“ˆ {model_name} æ€§èƒ½æŒ‡æ ‡:")
        
        if self.task_type == 'classification':
            print(f"   è®­ç»ƒé›† - Accuracy: {metrics['train']['accuracy']:.4f}, F1: {metrics['train']['f1']:.4f}")
            print(f"   æµ‹è¯•é›† - Accuracy: {metrics['test']['accuracy']:.4f}, F1: {metrics['test']['f1']:.4f}")
        else:
            print(f"   è®­ç»ƒé›† - RÂ²: {metrics['train']['r2']:.4f}, RMSE: {metrics['train']['rmse']:.4f}")
            print(f"   æµ‹è¯•é›† - RÂ²: {metrics['test']['r2']:.4f}, RMSE: {metrics['test']['rmse']:.4f}")
    
    def train_all_models(self, cv_folds=5):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("=== æ‰¹é‡è®­ç»ƒæ‰€æœ‰æ¨¡å‹ ===")
        
        model_library = self.get_model_library()
        total_models = len(model_library)
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {total_models} ä¸ªæ¨¡å‹...")
        
        results_summary = []
        
        for i, (model_name, model) in enumerate(model_library.items(), 1):
            print(f"\n[{i}/{total_models}] è®­ç»ƒ {model_name}...")
            
            result = self.train_single_model(model_name, model, cv_folds)
            
            if result:
                # æå–å…³é”®æŒ‡æ ‡ç”¨äºæ¯”è¾ƒ
                if self.task_type == 'classification':
                    key_metric = result['metrics']['test']['accuracy']
                    metric_name = 'accuracy'
                else:
                    key_metric = result['metrics']['test']['r2']
                    metric_name = 'r2'
                
                results_summary.append({
                    'model': model_name,
                    'cv_mean': result['cv_mean'],
                    'cv_std': result['cv_std'],
                    f'test_{metric_name}': key_metric,
                    'training_time': result['training_time']
                })
        
        # åˆ›å»ºç»“æœå¯¹æ¯”è¡¨
        if results_summary:
            results_df = pd.DataFrame(results_summary)
            results_df = results_df.sort_values(f'test_{metric_name}', ascending=False)
            
            print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ:")
            print(results_df.to_string(index=False, float_format='%.4f'))
            
            # é€‰æ‹©æœ€ä½³æ¨¡å‹
            best_model_name = results_df.iloc[0]['model']
            self.best_model = self.trained_models[best_model_name]
            
            print(f"\nğŸ¥‡ æœ€ä½³æ¨¡å‹: {best_model_name}")
            print(f"   æµ‹è¯•é›† {metric_name}: {results_df.iloc[0][f'test_{metric_name}']:.4f}")
            
            return results_df
        
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‡è®¾æˆ‘ä»¬æœ‰å¤„ç†å¥½çš„æ•°æ®
    # X, y = load_your_data()
    
    # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
    model_manager = ModelManager(task_type='auto')
    
    # æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test = model_manager.prepare_data(X, y, test_size=0.2)
    
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    results = model_manager.train_all_models(cv_folds=5)
    
    print("\n=== æ¨¡å‹è®­ç»ƒå®Œæˆ ===")
```

## 4.4.2 è¶…å‚æ•°ä¼˜åŒ–ç³»ç»Ÿ

### æ™ºèƒ½è¶…å‚æ•°è°ƒä¼˜

```python
# è¶…å‚æ•°ä¼˜åŒ–ç³»ç»Ÿ
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.metrics import make_scorer
from scipy.stats import randint, uniform
import optuna
from datetime import datetime
import json

class HyperparameterOptimizer:
    """æ™ºèƒ½è¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.optimization_results = {}
        self.best_params = {}
        
        # é¢„å®šä¹‰å‚æ•°ç©ºé—´
        self._initialize_param_grids()
    
    def _initialize_param_grids(self):
        """åˆå§‹åŒ–å‚æ•°æœç´¢ç©ºé—´"""
        print("=== åˆå§‹åŒ–å‚æ•°æœç´¢ç©ºé—´ ===")
        
        # åˆ†ç±»æ¨¡å‹å‚æ•°ç©ºé—´
        self.classification_param_grids = {
            'RandomForest': {
                'n_estimators': [50, 100, 200, 300],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None]
            },
            'GradientBoosting': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            },
            'LogisticRegression': {
                'C': [0.001, 0.01, 0.1, 1, 10, 100],
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear', 'saga']
            },
            'SVM': {
                'C': [0.1, 1, 10, 100],
                'kernel': ['rbf', 'poly', 'sigmoid'],
                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]
            },
            'KNN': {
                'n_neighbors': [3, 5, 7, 9, 11, 15],
                'weights': ['uniform', 'distance'],
                'metric': ['euclidean', 'manhattan', 'minkowski']
            },
            'DecisionTree': {
                'max_depth': [None, 5, 10, 15, 20],
                'min_samples_split': [2, 5, 10, 20],
                'min_samples_leaf': [1, 2, 5, 10],
                'criterion': ['gini', 'entropy']
            },
            'MLP': {
                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],
                'activation': ['relu', 'tanh'],
                'alpha': [0.0001, 0.001, 0.01],
                'learning_rate': ['constant', 'adaptive']
            }
        }
        
        # å›å½’æ¨¡å‹å‚æ•°ç©ºé—´
        self.regression_param_grids = {
            'RandomForest': {
                'n_estimators': [50, 100, 200, 300],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None]
            },
            'GradientBoosting': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            },
            'Ridge': {
                'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]
            },
            'Lasso': {
                'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]
            },
            'SVR': {
                'C': [0.1, 1, 10, 100],
                'kernel': ['rbf', 'poly', 'sigmoid'],
                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]
            },
            'KNN': {
                'n_neighbors': [3, 5, 7, 9, 11, 15],
                'weights': ['uniform', 'distance'],
                'metric': ['euclidean', 'manhattan', 'minkowski']
            },
            'DecisionTree': {
                'max_depth': [None, 5, 10, 15, 20],
                'min_samples_split': [2, 5, 10, 20],
                'min_samples_leaf': [1, 2, 5, 10],
                'criterion': ['squared_error', 'absolute_error']
            },
            'MLP': {
                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],
                'activation': ['relu', 'tanh'],
                'alpha': [0.0001, 0.001, 0.01],
                'learning_rate': ['constant', 'adaptive']
            }
        }
        
        print(f"âœ… åˆ†ç±»æ¨¡å‹å‚æ•°ç©ºé—´: {len(self.classification_param_grids)} ä¸ªæ¨¡å‹")
        print(f"âœ… å›å½’æ¨¡å‹å‚æ•°ç©ºé—´: {len(self.regression_param_grids)} ä¸ªæ¨¡å‹")
    
    def get_param_grid(self, model_name):
        """è·å–æŒ‡å®šæ¨¡å‹çš„å‚æ•°ç½‘æ ¼"""
        if self.model_manager.task_type == 'classification':
            return self.classification_param_grids.get(model_name, {})
        else:
            return self.regression_param_grids.get(model_name, {})
    
    def grid_search_optimization(self, model_name, cv_folds=5, n_jobs=-1):
        """ç½‘æ ¼æœç´¢ä¼˜åŒ–"""
        print(f"\n=== ç½‘æ ¼æœç´¢ä¼˜åŒ–: {model_name} ===")
        
        # è·å–æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼
        model_library = self.model_manager.get_model_library()
        if model_name not in model_library:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return None
        
        model = model_library[model_name]
        param_grid = self.get_param_grid(model_name)
        
        if not param_grid:
            print(f"âŒ æ¨¡å‹ {model_name} æ²¡æœ‰é¢„å®šä¹‰å‚æ•°ç½‘æ ¼")
            return None
        
        print(f"ğŸ” å‚æ•°æœç´¢ç©ºé—´: {param_grid}")
        
        # è®¡ç®—æœç´¢ç©ºé—´å¤§å°
        search_space_size = 1
        for param_values in param_grid.values():
            search_space_size *= len(param_values)
        print(f"ğŸ“Š æœç´¢ç©ºé—´å¤§å°: {search_space_size} ç§ç»„åˆ")
        
        start_time = datetime.now()
        
        try:
            # é€‰æ‹©è¯„åˆ†æŒ‡æ ‡
            if self.model_manager.task_type == 'classification':
                scoring = 'accuracy'
            else:
                scoring = 'r2'
            
            # ç½‘æ ¼æœç´¢
            print(f"ğŸ”„ å¼€å§‹ç½‘æ ¼æœç´¢ (CV={cv_folds}, scoring={scoring})...")
            
            grid_search = GridSearchCV(
                estimator=model,
                param_grid=param_grid,
                cv=cv_folds,
                scoring=scoring,
                n_jobs=n_jobs,
                verbose=1
            )
            
            grid_search.fit(self.model_manager.X_train, self.model_manager.y_train)
            
            optimization_time = (datetime.now() - start_time).total_seconds()
            
            # ä¿å­˜ç»“æœ
            result = {
                'method': 'GridSearch',
                'best_params': grid_search.best_params_,
                'best_score': grid_search.best_score_,
                'best_estimator': grid_search.best_estimator_,
                'cv_results': grid_search.cv_results_,
                'optimization_time': optimization_time,
                'search_space_size': search_space_size
            }
            
            self.optimization_results[f'{model_name}_grid'] = result
            self.best_params[model_name] = grid_search.best_params_
            
            print(f"âœ… ç½‘æ ¼æœç´¢å®Œæˆ (è€—æ—¶: {optimization_time:.2f}ç§’)")
            print(f"ğŸ† æœ€ä½³å‚æ•°: {grid_search.best_params_}")
            print(f"ğŸ“ˆ æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")
            
            return result
            
        except Exception as e:
            print(f"âŒ ç½‘æ ¼æœç´¢å¤±è´¥: {e}")
            return None
    
    def random_search_optimization(self, model_name, n_iter=100, cv_folds=5, n_jobs=-1):
        """éšæœºæœç´¢ä¼˜åŒ–"""
        print(f"\n=== éšæœºæœç´¢ä¼˜åŒ–: {model_name} ===")
        
        # è·å–æ¨¡å‹å’Œå‚æ•°åˆ†å¸ƒ
        model_library = self.model_manager.get_model_library()
        if model_name not in model_library:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return None
        
        model = model_library[model_name]
        param_distributions = self._get_param_distributions(model_name)
        
        if not param_distributions:
            print(f"âŒ æ¨¡å‹ {model_name} æ²¡æœ‰é¢„å®šä¹‰å‚æ•°åˆ†å¸ƒ")
            return None
        
        print(f"ğŸ² å‚æ•°åˆ†å¸ƒ: {param_distributions}")
        print(f"ğŸ”¢ éšæœºé‡‡æ ·æ¬¡æ•°: {n_iter}")
        
        start_time = datetime.now()
        
        try:
            # é€‰æ‹©è¯„åˆ†æŒ‡æ ‡
            if self.model_manager.task_type == 'classification':
                scoring = 'accuracy'
            else:
                scoring = 'r2'
            
            # éšæœºæœç´¢
            print(f"ğŸ¯ å¼€å§‹éšæœºæœç´¢ (CV={cv_folds}, scoring={scoring})...")
            
            random_search = RandomizedSearchCV(
                estimator=model,
                param_distributions=param_distributions,
                n_iter=n_iter,
                cv=cv_folds,
                scoring=scoring,
                n_jobs=n_jobs,
                random_state=42,
                verbose=1
            )
            
            random_search.fit(self.model_manager.X_train, self.model_manager.y_train)
            
            optimization_time = (datetime.now() - start_time).total_seconds()
            
            # ä¿å­˜ç»“æœ
            result = {
                'method': 'RandomSearch',
                'best_params': random_search.best_params_,
                'best_score': random_search.best_score_,
                'best_estimator': random_search.best_estimator_,
                'cv_results': random_search.cv_results_,
                'optimization_time': optimization_time,
                'n_iter': n_iter
            }
            
            self.optimization_results[f'{model_name}_random'] = result
            self.best_params[model_name] = random_search.best_params_
            
            print(f"âœ… éšæœºæœç´¢å®Œæˆ (è€—æ—¶: {optimization_time:.2f}ç§’)")
            print(f"ğŸ† æœ€ä½³å‚æ•°: {random_search.best_params_}")
            print(f"ğŸ“ˆ æœ€ä½³å¾—åˆ†: {random_search.best_score_:.4f}")
            
            return result
            
        except Exception as e:
            print(f"âŒ éšæœºæœç´¢å¤±è´¥: {e}")
            return None
    
    def _get_param_distributions(self, model_name):
        """è·å–å‚æ•°åˆ†å¸ƒï¼ˆç”¨äºéšæœºæœç´¢ï¼‰"""
        # å°†ç¦»æ•£å‚æ•°ç½‘æ ¼è½¬æ¢ä¸ºåˆ†å¸ƒ
        param_grid = self.get_param_grid(model_name)
        param_distributions = {}
        
        for param, values in param_grid.items():
            if isinstance(values[0], (int, float)):
                # æ•°å€¼å‚æ•°ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
                param_distributions[param] = uniform(min(values), max(values) - min(values))
            else:
                # åˆ†ç±»å‚æ•°ä¿æŒåŸæ ·
                param_distributions[param] = values
        
        return param_distributions
    
    def optuna_optimization(self, model_name, n_trials=100, timeout=3600):
        """Optunaè´å¶æ–¯ä¼˜åŒ–"""
        print(f"\n=== Optunaè´å¶æ–¯ä¼˜åŒ–: {model_name} ===")
        
        # è·å–æ¨¡å‹
        model_library = self.model_manager.get_model_library()
        if model_name not in model_library:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return None
        
        print(f"ğŸ§  è´å¶æ–¯ä¼˜åŒ– (trials={n_trials}, timeout={timeout}s)")
        
        start_time = datetime.now()
        
        try:
            # åˆ›å»ºOptuna study
            study = optuna.create_study(direction='maximize')
            
            # å®šä¹‰ç›®æ ‡å‡½æ•°
            def objective(trial):
                params = self._suggest_params(trial, model_name)
                
                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                model_class = type(model_library[model_name])
                model = model_class(**params, random_state=42)
                
                # äº¤å‰éªŒè¯
                if self.model_manager.task_type == 'classification':
                    scores = cross_val_score(model, self.model_manager.X_train, 
                                           self.model_manager.y_train, cv=5, scoring='accuracy')
                else:
                    scores = cross_val_score(model, self.model_manager.X_train, 
                                           self.model_manager.y_train, cv=5, scoring='r2')
                
                return scores.mean()
            
            # ä¼˜åŒ–
            study.optimize(objective, n_trials=n_trials, timeout=timeout)
            
            optimization_time = (datetime.now() - start_time).total_seconds()
            
            # è®­ç»ƒæœ€ä½³æ¨¡å‹
            best_params = study.best_params
            model_class = type(model_library[model_name])
            best_model = model_class(**best_params, random_state=42)
            best_model.fit(self.model_manager.X_train, self.model_manager.y_train)
            
            # ä¿å­˜ç»“æœ
            result = {
                'method': 'Optuna',
                'best_params': best_params,
                'best_score': study.best_value,
                'best_estimator': best_model,
                'study': study,
                'optimization_time': optimization_time,
                'n_trials': len(study.trials)
            }
            
            self.optimization_results[f'{model_name}_optuna'] = result
            self.best_params[model_name] = best_params
            
            print(f"âœ… Optunaä¼˜åŒ–å®Œæˆ (è€—æ—¶: {optimization_time:.2f}ç§’)")
            print(f"ğŸ† æœ€ä½³å‚æ•°: {best_params}")
            print(f"ğŸ“ˆ æœ€ä½³å¾—åˆ†: {study.best_value:.4f}")
            print(f"ğŸ”¢ å®Œæˆè¯•éªŒ: {len(study.trials)} æ¬¡")
            
            return result
            
        except Exception as e:
            print(f"âŒ Optunaä¼˜åŒ–å¤±è´¥: {e}")
            return None
    
    def _suggest_params(self, trial, model_name):
        """ä¸ºOptunaè¯•éªŒå»ºè®®å‚æ•°"""
        params = {}
        
        if model_name == 'RandomForest':
            params['n_estimators'] = trial.suggest_int('n_estimators', 50, 300)
            params['max_depth'] = trial.suggest_int('max_depth', 5, 30)
            params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 20)
            params['min_samples_leaf'] = trial.suggest_int('min_samples_leaf', 1, 10)
            params['max_features'] = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])
            
        elif model_name == 'GradientBoosting':
            params['n_estimators'] = trial.suggest_int('n_estimators', 50, 300)
            params['learning_rate'] = trial.suggest_float('learning_rate', 0.01, 0.3)
            params['max_depth'] = trial.suggest_int('max_depth', 3, 10)
            params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 20)
            params['min_samples_leaf'] = trial.suggest_int('min_samples_leaf', 1, 10)
            
        elif model_name == 'LogisticRegression':
            params['C'] = trial.suggest_float('C', 0.001, 100, log=True)
            params['penalty'] = trial.suggest_categorical('penalty', ['l1', 'l2'])
            params['solver'] = trial.suggest_categorical('solver', ['liblinear', 'saga'])
            params['max_iter'] = 1000
            
        elif model_name == 'SVM':
            params['C'] = trial.suggest_float('C', 0.1, 100, log=True)
            params['kernel'] = trial.suggest_categorical('kernel', ['rbf', 'poly', 'sigmoid'])
            params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])
            if self.model_manager.task_type == 'classification':
                params['probability'] = True
            
        elif model_name == 'KNN':
            params['n_neighbors'] = trial.suggest_int('n_neighbors', 3, 20)
            params['weights'] = trial.suggest_categorical('weights', ['uniform', 'distance'])
            params['metric'] = trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski'])
            
        elif model_name == 'DecisionTree':
            params['max_depth'] = trial.suggest_int('max_depth', 5, 30)
            params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 20)
            params['min_samples_leaf'] = trial.suggest_int('min_samples_leaf', 1, 10)
            if self.model_manager.task_type == 'classification':
                params['criterion'] = trial.suggest_categorical('criterion', ['gini', 'entropy'])
            else:
                params['criterion'] = trial.suggest_categorical('criterion', ['squared_error', 'absolute_error'])
                
        elif model_name == 'MLP':
            n_layers = trial.suggest_int('n_layers', 1, 3)
            hidden_sizes = []
            for i in range(n_layers):
                hidden_sizes.append(trial.suggest_int(f'layer_{i}_size', 50, 200))
            params['hidden_layer_sizes'] = tuple(hidden_sizes)
            params['activation'] = trial.suggest_categorical('activation', ['relu', 'tanh'])
            params['alpha'] = trial.suggest_float('alpha', 0.0001, 0.01, log=True)
            params['learning_rate'] = trial.suggest_categorical('learning_rate', ['constant', 'adaptive'])
            params['max_iter'] = 500
            
        elif model_name in ['Ridge', 'Lasso']:
            params['alpha'] = trial.suggest_float('alpha', 0.001, 1000, log=True)
        
        return params
    
    def compare_optimization_methods(self, model_name):
        """æ¯”è¾ƒä¸åŒä¼˜åŒ–æ–¹æ³•çš„æ•ˆæœ"""
        print(f"\n=== ä¼˜åŒ–æ–¹æ³•å¯¹æ¯”: {model_name} ===")
        
        methods = ['grid', 'random', 'optuna']
        comparison_data = []
        
        for method in methods:
            key = f'{model_name}_{method}'
            if key in self.optimization_results:
                result = self.optimization_results[key]
                comparison_data.append({
                    'method': result['method'],
                    'best_score': result['best_score'],
                    'optimization_time': result['optimization_time'],
                    'search_efficiency': result['best_score'] / (result['optimization_time'] / 60)  # åˆ†æ•°/åˆ†é’Ÿ
                })
        
        if comparison_data:
            comparison_df = pd.DataFrame(comparison_data)
            comparison_df = comparison_df.sort_values('best_score', ascending=False)
            
            print("ğŸ“Š ä¼˜åŒ–æ–¹æ³•å¯¹æ¯”ç»“æœ:")
            print(comparison_df.to_string(index=False, float_format='%.4f'))
            
            # å¯è§†åŒ–å¯¹æ¯”
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))
            
            # æœ€ä½³å¾—åˆ†å¯¹æ¯”
            axes[0].bar(comparison_df['method'], comparison_df['best_score'])
            axes[0].set_title('æœ€ä½³å¾—åˆ†å¯¹æ¯”')
            axes[0].set_ylabel('å¾—åˆ†')
            
            # ä¼˜åŒ–æ—¶é—´å¯¹æ¯”
            axes[1].bar(comparison_df['method'], comparison_df['optimization_time'])
            axes[1].set_title('ä¼˜åŒ–æ—¶é—´å¯¹æ¯”')
            axes[1].set_ylabel('æ—¶é—´ (ç§’)')
            
            # æœç´¢æ•ˆç‡å¯¹æ¯”
            axes[2].bar(comparison_df['method'], comparison_df['search_efficiency'])
            axes[2].set_title('æœç´¢æ•ˆç‡å¯¹æ¯”')
            axes[2].set_ylabel('å¾—åˆ†/åˆ†é’Ÿ')
            
            plt.tight_layout()
            plt.show()
            
            return comparison_df
        
        return None
    
    def get_optimization_summary(self):
        """è·å–ä¼˜åŒ–æ€»ç»“"""
        print("=== è¶…å‚æ•°ä¼˜åŒ–æ€»ç»“ ===")
        
        if not self.optimization_results:
            print("âŒ æ²¡æœ‰ä¼˜åŒ–ç»“æœ")
            return None
        
        summary_data = []
        
        for key, result in self.optimization_results.items():
            model_name = key.rsplit('_', 1)[0]
            summary_data.append({
                'model': model_name,
                'method': result['method'],
                'best_score': result['best_score'],
                'optimization_time': result['optimization_time'],
                'best_params': str(result['best_params'])
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        print("ğŸ“ˆ ä¼˜åŒ–ç»“æœæ±‡æ€»:")
        print(summary_df[['model', 'method', 'best_score', 'optimization_time']].to_string(index=False, float_format='%.4f'))
        
        # æ‰¾å‡ºæ¯ä¸ªæ¨¡å‹çš„æœ€ä½³ä¼˜åŒ–æ–¹æ³•
        best_results = summary_df.loc[summary_df.groupby('model')['best_score'].idxmax()]
        
        print("\nğŸ† å„æ¨¡å‹æœ€ä½³ä¼˜åŒ–ç»“æœ:")
        print(best_results[['model', 'method', 'best_score']].to_string(index=False, float_format='%.4f'))
        
        return summary_df

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºè¶…å‚æ•°ä¼˜åŒ–å™¨
    optimizer = HyperparameterOptimizer(model_manager)
    
    # å¯¹RandomForestè¿›è¡Œä¸åŒæ–¹æ³•çš„ä¼˜åŒ–
    model_name = 'RandomForest'
    
    # 1. ç½‘æ ¼æœç´¢
    grid_result = optimizer.grid_search_optimization(model_name, cv_folds=5)
    
    # 2. éšæœºæœç´¢
    random_result = optimizer.random_search_optimization(model_name, n_iter=50, cv_folds=5)
    
    # 3. Optunaä¼˜åŒ–
    optuna_result = optimizer.optuna_optimization(model_name, n_trials=50)
    
    # 4. æ¯”è¾ƒä¼˜åŒ–æ–¹æ³•
    comparison = optimizer.compare_optimization_methods(model_name)
    
    # 5. è·å–ä¼˜åŒ–æ€»ç»“
    summary = optimizer.get_optimization_summary()
```

## 4.4.3 æ¨¡å‹æ€§èƒ½è¯„ä¼°ä¸å¯è§†åŒ–

### ç»¼åˆæ€§èƒ½è¯„ä¼°ç³»ç»Ÿ

```python
# æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œå¯è§†åŒ–ç³»ç»Ÿ
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.model_selection import learning_curve, validation_curve
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import numpy as np

class ModelEvaluator:
    """æ¨¡å‹æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.evaluation_results = {}
    
    def comprehensive_evaluation(self, model_name):
        """ç»¼åˆæ€§èƒ½è¯„ä¼°"""
        print(f"\n=== ç»¼åˆæ€§èƒ½è¯„ä¼°: {model_name} ===")
        
        if model_name not in self.model_manager.trained_models:
            print(f"âŒ æ¨¡å‹ {model_name} æœªè®­ç»ƒ")
            return None
        
        model = self.model_manager.trained_models[model_name]
        
        # è·å–é¢„æµ‹ç»“æœ
        y_train_pred = model.predict(self.model_manager.X_train)
        y_test_pred = model.predict(self.model_manager.X_test)
        
        evaluation_result = {
            'model_name': model_name,
            'task_type': self.model_manager.task_type
        }
        
        if self.model_manager.task_type == 'classification':
            evaluation_result.update(self._evaluate_classification(model, y_train_pred, y_test_pred))
        else:
            evaluation_result.update(self._evaluate_regression(model, y_train_pred, y_test_pred))
        
        self.evaluation_results[model_name] = evaluation_result
        
        return evaluation_result
    
    def _evaluate_classification(self, model, y_train_pred, y_test_pred):
        """åˆ†ç±»æ¨¡å‹è¯„ä¼°"""
        print("ğŸ“Š åˆ†ç±»æ¨¡å‹æ€§èƒ½åˆ†æ...")
        
        evaluation = {}
        
        # åŸºç¡€æŒ‡æ ‡
        train_accuracy = accuracy_score(self.model_manager.y_train, y_train_pred)
        test_accuracy = accuracy_score(self.model_manager.y_test, y_test_pred)
        
        evaluation['metrics'] = {
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'overfitting_score': train_accuracy - test_accuracy
        }
        
        print(f"   è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f}")
        print(f"   æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
        print(f"   è¿‡æ‹Ÿåˆç¨‹åº¦: {train_accuracy - test_accuracy:.4f}")
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.model_manager.y_test, y_test_pred)
        evaluation['confusion_matrix'] = cm
        
        # åˆ†ç±»æŠ¥å‘Š
        if hasattr(self.model_manager, 'label_encoder') and self.model_manager.label_encoder:
            target_names = self.model_manager.label_encoder.classes_
        else:
            target_names = None
        
        report = classification_report(self.model_manager.y_test, y_test_pred, 
                                     target_names=target_names, output_dict=True)
        evaluation['classification_report'] = report
        
        # ROCæ›²çº¿ï¼ˆäºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»ï¼‰
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(self.model_manager.X_test)
            evaluation['probabilities'] = y_proba
            
            # è®¡ç®—ROC AUC
            n_classes = len(np.unique(self.model_manager.y_test))
            if n_classes == 2:
                # äºŒåˆ†ç±»
                fpr, tpr, _ = roc_curve(self.model_manager.y_test, y_proba[:, 1])
                roc_auc = auc(fpr, tpr)
                evaluation['roc_data'] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}
                print(f"   ROC AUC: {roc_auc:.4f}")
            else:
                # å¤šåˆ†ç±»
                evaluation['roc_data'] = self._calculate_multiclass_roc(y_proba)
        
        return evaluation
    
    def _evaluate_regression(self, model, y_train_pred, y_test_pred):
        """å›å½’æ¨¡å‹è¯„ä¼°"""
        print("ğŸ“ˆ å›å½’æ¨¡å‹æ€§èƒ½åˆ†æ...")
        
        evaluation = {}
        
        # åŸºç¡€æŒ‡æ ‡
        train_r2 = r2_score(self.model_manager.y_train, y_train_pred)
        test_r2 = r2_score(self.model_manager.y_test, y_test_pred)
        train_rmse = np.sqrt(mean_squared_error(self.model_manager.y_train, y_train_pred))
        test_rmse = np.sqrt(mean_squared_error(self.model_manager.y_test, y_test_pred))
        train_mae = mean_absolute_error(self.model_manager.y_train, y_train_pred)
        test_mae = mean_absolute_error(self.model_manager.y_test, y_test_pred)
        
        evaluation['metrics'] = {
            'train_r2': train_r2,
            'test_r2': test_r2,
            'train_rmse': train_rmse,
            'test_rmse': test_rmse,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'overfitting_score': train_r2 - test_r2
        }
        
        print(f"   è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
        print(f"   æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
        print(f"   æµ‹è¯•é›† RMSE: {test_rmse:.4f}")
        print(f"   æµ‹è¯•é›† MAE: {test_mae:.4f}")
        print(f"   è¿‡æ‹Ÿåˆç¨‹åº¦: {train_r2 - test_r2:.4f}")
        
        # æ®‹å·®åˆ†æ
        residuals = self.model_manager.y_test - y_test_pred
        evaluation['residuals'] = residuals
        
        # é¢„æµ‹å€¼åˆ†æ
        evaluation['predictions'] = {
            'y_true': self.model_manager.y_test,
            'y_pred': y_test_pred
        }
        
        return evaluation
    
    def _calculate_multiclass_roc(self, y_proba):
        """è®¡ç®—å¤šåˆ†ç±»ROC"""
        from sklearn.preprocessing import label_binarize
        from sklearn.metrics import roc_curve, auc
        from itertools import cycle
        
        # äºŒå€¼åŒ–æ ‡ç­¾
        y_test_bin = label_binarize(self.model_manager.y_test, 
                                   classes=np.unique(self.model_manager.y_test))
        n_classes = y_test_bin.shape[1]
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROC
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        
        # è®¡ç®—micro-average ROC
        fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_proba.ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
        
        return {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc, 'n_classes': n_classes}
    
    def plot_confusion_matrix(self, model_name, figsize=(8, 6)):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        if model_name not in self.evaluation_results:
            print(f"âŒ æ¨¡å‹ {model_name} æœªè¯„ä¼°")
            return
        
        if self.model_manager.task_type != 'classification':
            print("âŒ åªæœ‰åˆ†ç±»æ¨¡å‹æ‰æœ‰æ··æ·†çŸ©é˜µ")
            return
        
        cm = self.evaluation_results[model_name]['confusion_matrix']
        
        plt.figure(figsize=figsize)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'{model_name} - æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.show()
    
    def plot_roc_curve(self, model_name, figsize=(8, 6)):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        if model_name not in self.evaluation_results:
            print(f"âŒ æ¨¡å‹ {model_name} æœªè¯„ä¼°")
            return
        
        if self.model_manager.task_type != 'classification':
            print("âŒ åªæœ‰åˆ†ç±»æ¨¡å‹æ‰æœ‰ROCæ›²çº¿")
            return
        
        roc_data = self.evaluation_results[model_name].get('roc_data')
        if not roc_data:
            print("âŒ æ²¡æœ‰ROCæ•°æ®")
            return
        
        plt.figure(figsize=figsize)
        
        if 'n_classes' in roc_data:  # å¤šåˆ†ç±»
            colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])
            
            for i, color in zip(range(roc_data['n_classes']), colors):
                plt.plot(roc_data['fpr'][i], roc_data['tpr'][i], color=color, lw=2,
                        label=f'ç±»åˆ« {i} (AUC = {roc_data["auc"][i]:.2f})')
            
            plt.plot(roc_data['fpr']["micro"], roc_data['tpr']["micro"], 
                    color='deeppink', linestyle=':', linewidth=4,
                    label=f'Micro-average (AUC = {roc_data["auc"]["micro"]:.2f})')
        else:  # äºŒåˆ†ç±»
            plt.plot(roc_data['fpr'], roc_data['tpr'], color='darkorange', lw=2,
                    label=f'ROC curve (AUC = {roc_data["auc"]:.2f})')
        
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('å‡æ­£ç‡ (FPR)')
        plt.ylabel('çœŸæ­£ç‡ (TPR)')
        plt.title(f'{model_name} - ROCæ›²çº¿')
        plt.legend(loc="lower right")
        plt.show()
    
    def plot_regression_analysis(self, model_name, figsize=(15, 5)):
        """ç»˜åˆ¶å›å½’åˆ†æå›¾"""
        if model_name not in self.evaluation_results:
            print(f"âŒ æ¨¡å‹ {model_name} æœªè¯„ä¼°")
            return
        
        if self.model_manager.task_type != 'regression':
            print("âŒ åªæœ‰å›å½’æ¨¡å‹æ‰æœ‰å›å½’åˆ†æå›¾")
            return
        
        evaluation = self.evaluation_results[model_name]
        y_true = evaluation['predictions']['y_true']
        y_pred = evaluation['predictions']['y_pred']
        residuals = evaluation['residuals']
        
        fig, axes = plt.subplots(1, 3, figsize=figsize)
        
        # é¢„æµ‹å€¼ vs çœŸå®å€¼
        axes[0].scatter(y_true, y_pred, alpha=0.6)
        axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
        axes[0].set_xlabel('çœŸå®å€¼')
        axes[0].set_ylabel('é¢„æµ‹å€¼')
        axes[0].set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')
        
        # æ®‹å·®å›¾
        axes[1].scatter(y_pred, residuals, alpha=0.6)
        axes[1].axhline(y=0, color='r', linestyle='--')
        axes[1].set_xlabel('é¢„æµ‹å€¼')
        axes[1].set_ylabel('æ®‹å·®')
        axes[1].set_title('æ®‹å·®å›¾')
        
        # æ®‹å·®åˆ†å¸ƒ
        axes[2].hist(residuals, bins=30, alpha=0.7, edgecolor='black')
        axes[2].set_xlabel('æ®‹å·®')
        axes[2].set_ylabel('é¢‘æ•°')
        axes[2].set_title('æ®‹å·®åˆ†å¸ƒ')
        
        plt.suptitle(f'{model_name} - å›å½’åˆ†æ')
        plt.tight_layout()
        plt.show()
    
    def plot_learning_curve(self, model_name, cv=5, figsize=(10, 6)):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        if model_name not in self.model_manager.trained_models:
            print(f"âŒ æ¨¡å‹ {model_name} æœªè®­ç»ƒ")
            return
        
        model = self.model_manager.trained_models[model_name]
        
        # è®¡ç®—å­¦ä¹ æ›²çº¿
        train_sizes = np.linspace(0.1, 1.0, 10)
        
        if self.model_manager.task_type == 'classification':
            scoring = 'accuracy'
        else:
            scoring = 'r2'
        
        train_sizes, train_scores, val_scores = learning_curve(
            model, self.model_manager.X_train, self.model_manager.y_train,
            train_sizes=train_sizes, cv=cv, scoring=scoring, n_jobs=-1
        )
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        val_mean = np.mean(val_scores, axis=1)
        val_std = np.std(val_scores, axis=1)
        
        # ç»˜å›¾
        plt.figure(figsize=figsize)
        plt.plot(train_sizes, train_mean, 'o-', color='blue', label='è®­ç»ƒé›†')
        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
        
        plt.plot(train_sizes, val_mean, 'o-', color='red', label='éªŒè¯é›†')
        plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
        
        plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
        plt.ylabel(f'{scoring.upper()}')
        plt.title(f'{model_name} - å­¦ä¹ æ›²çº¿')
        plt.legend()
        plt.grid(True)
        plt.show()
    
    def compare_models_performance(self, model_names=None, figsize=(12, 8)):
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½"""
        if model_names is None:
            model_names = list(self.evaluation_results.keys())
        
        if not model_names:
            print("âŒ æ²¡æœ‰å¯æ¯”è¾ƒçš„æ¨¡å‹")
            return
        
        comparison_data = []
        
        for model_name in model_names:
            if model_name in self.evaluation_results:
                evaluation = self.evaluation_results[model_name]
                metrics = evaluation['metrics']
                
                if self.model_manager.task_type == 'classification':
                    comparison_data.append({
                        'model': model_name,
                        'test_accuracy': metrics['test_accuracy'],
                        'overfitting': metrics['overfitting_score']
                    })
                else:
                    comparison_data.append({
                        'model': model_name,
                        'test_r2': metrics['test_r2'],
                        'test_rmse': metrics['test_rmse'],
                        'overfitting': metrics['overfitting_score']
                    })
        
        if not comparison_data:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°æ•°æ®")
            return
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # å¯è§†åŒ–æ¯”è¾ƒ
        if self.model_manager.task_type == 'classification':
            fig, axes = plt.subplots(1, 2, figsize=figsize)
            
            # å‡†ç¡®ç‡æ¯”è¾ƒ
            axes[0].bar(comparison_df['model'], comparison_df['test_accuracy'])
            axes[0].set_title('æµ‹è¯•é›†å‡†ç¡®ç‡æ¯”è¾ƒ')
            axes[0].set_ylabel('å‡†ç¡®ç‡')
            axes[0].tick_params(axis='x', rotation=45)
            
            # è¿‡æ‹Ÿåˆç¨‹åº¦æ¯”è¾ƒ
            axes[1].bar(comparison_df['model'], comparison_df['overfitting'])
            axes[1].set_title('è¿‡æ‹Ÿåˆç¨‹åº¦æ¯”è¾ƒ')
            axes[1].set_ylabel('è¿‡æ‹Ÿåˆåˆ†æ•°')
            axes[1].tick_params(axis='x', rotation=45)
            axes[1].axhline(y=0, color='r', linestyle='--', alpha=0.7)
            
        else:
            fig, axes = plt.subplots(1, 3, figsize=figsize)
            
            # RÂ²æ¯”è¾ƒ
            axes[0].bar(comparison_df['model'], comparison_df['test_r2'])
            axes[0].set_title('æµ‹è¯•é›† RÂ² æ¯”è¾ƒ')
            axes[0].set_ylabel('RÂ²')
            axes[0].tick_params(axis='x', rotation=45)
            
            # RMSEæ¯”è¾ƒ
            axes[1].bar(comparison_df['model'], comparison_df['test_rmse'])
            axes[1].set_title('æµ‹è¯•é›† RMSE æ¯”è¾ƒ')
            axes[1].set_ylabel('RMSE')
            axes[1].tick_params(axis='x', rotation=45)
            
            # è¿‡æ‹Ÿåˆç¨‹åº¦æ¯”è¾ƒ
            axes[2].bar(comparison_df['model'], comparison_df['overfitting'])
            axes[2].set_title('è¿‡æ‹Ÿåˆç¨‹åº¦æ¯”è¾ƒ')
            axes[2].set_ylabel('è¿‡æ‹Ÿåˆåˆ†æ•°')
            axes[2].tick_params(axis='x', rotation=45)
            axes[2].axhline(y=0, color='r', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°æ¯”è¾ƒè¡¨æ ¼
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
        print(comparison_df.to_string(index=False, float_format='%.4f'))
        
        return comparison_df

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(model_manager)
    
    # è¯„ä¼°æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    for model_name in model_manager.trained_models.keys():
        evaluation = evaluator.comprehensive_evaluation(model_name)
        
        # ç»˜åˆ¶ç›¸åº”çš„å›¾è¡¨
        if model_manager.task_type == 'classification':
            evaluator.plot_confusion_matrix(model_name)
            evaluator.plot_roc_curve(model_name)
        else:
            evaluator.plot_regression_analysis(model_name)
        
        evaluator.plot_learning_curve(model_name)
    
    # æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹
    comparison = evaluator.compare_models_performance()
```

## 4.4.4 æ¨¡å‹é›†æˆä¸èåˆ

### é›†æˆå­¦ä¹ ç³»ç»Ÿ

```python
# æ¨¡å‹é›†æˆå’Œèåˆç³»ç»Ÿ
from sklearn.ensemble import VotingClassifier, VotingRegressor
from sklearn.ensemble import BaggingClassifier, BaggingRegressor
from sklearn.ensemble import StackingClassifier, StackingRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import cross_val_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

class ModelEnsemble:
    """æ¨¡å‹é›†æˆå™¨"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.ensemble_models = {}
        self.ensemble_results = {}
    
    def create_voting_ensemble(self, model_names, voting='soft', weights=None):
        """åˆ›å»ºæŠ•ç¥¨é›†æˆæ¨¡å‹"""
        print(f"\n=== åˆ›å»ºæŠ•ç¥¨é›†æˆæ¨¡å‹ ===")
        print(f"ğŸ“Š é›†æˆæ¨¡å‹: {model_names}")
        print(f"ğŸ—³ï¸ æŠ•ç¥¨æ–¹å¼: {voting}")
        
        # å‡†å¤‡åŸºç¡€æ¨¡å‹
        estimators = []
        for model_name in model_names:
            if model_name in self.model_manager.trained_models:
                model = self.model_manager.trained_models[model_name]
                estimators.append((model_name, model))
            else:
                print(f"âš ï¸ æ¨¡å‹ {model_name} æœªè®­ç»ƒï¼Œè·³è¿‡")
        
        if len(estimators) < 2:
            print("âŒ éœ€è¦è‡³å°‘2ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹")
            return None
        
        # åˆ›å»ºé›†æˆæ¨¡å‹
        if self.model_manager.task_type == 'classification':
            ensemble = VotingClassifier(
                estimators=estimators,
                voting=voting,
                weights=weights
            )
        else:
            ensemble = VotingRegressor(
                estimators=estimators,
                weights=weights
            )
        
        # è®­ç»ƒé›†æˆæ¨¡å‹
        print("ğŸ”„ è®­ç»ƒæŠ•ç¥¨é›†æˆæ¨¡å‹...")
        start_time = datetime.now()
        
        ensemble.fit(self.model_manager.X_train, self.model_manager.y_train)
        
        training_time = (datetime.now() - start_time).total_seconds()
        
        # è¯„ä¼°æ€§èƒ½
        train_score = ensemble.score(self.model_manager.X_train, self.model_manager.y_train)
        test_score = ensemble.score(self.model_manager.X_test, self.model_manager.y_test)
        
        ensemble_name = f"Voting_{'_'.join(model_names)}"
        
        result = {
            'type': 'voting',
            'base_models': model_names,
            'voting_type': voting,
            'weights': weights,
            'model': ensemble,
            'train_score': train_score,
            'test_score': test_score,
            'training_time': training_time
        }
        
        self.ensemble_models[ensemble_name] = ensemble
        self.ensemble_results[ensemble_name] = result
        
        print(f"âœ… æŠ•ç¥¨é›†æˆå®Œæˆ (è€—æ—¶: {training_time:.2f}ç§’)")
        print(f"ğŸ“ˆ è®­ç»ƒå¾—åˆ†: {train_score:.4f}")
        print(f"ğŸ“Š æµ‹è¯•å¾—åˆ†: {test_score:.4f}")
        
        return result
    
    def create_bagging_ensemble(self, base_model_name, n_estimators=10, max_samples=1.0, max_features=1.0):
        """åˆ›å»ºBaggingé›†æˆæ¨¡å‹"""
        print(f"\n=== åˆ›å»ºBaggingé›†æˆæ¨¡å‹ ===")
        print(f"ğŸ¯ åŸºç¡€æ¨¡å‹: {base_model_name}")
        print(f"ğŸ”¢ é›†æˆæ•°é‡: {n_estimators}")
        
        if base_model_name not in self.model_manager.trained_models:
            print(f"âŒ åŸºç¡€æ¨¡å‹ {base_model_name} æœªè®­ç»ƒ")
            return None
        
        base_model = self.model_manager.trained_models[base_model_name]
        
        # åˆ›å»ºBaggingé›†æˆ
        if self.model_manager.task_type == 'classification':
            ensemble = BaggingClassifier(
                base_estimator=base_model,
                n_estimators=n_estimators,
                max_samples=max_samples,
                max_features=max_features,
                random_state=42
            )
        else:
            ensemble = BaggingRegressor(
                base_estimator=base_model,
                n_estimators=n_estimators,
                max_samples=max_samples,
                max_features=max_features,
                random_state=42
            )
        
        # è®­ç»ƒé›†æˆæ¨¡å‹
        print("ğŸ”„ è®­ç»ƒBaggingé›†æˆæ¨¡å‹...")
        start_time = datetime.now()
        
        ensemble.fit(self.model_manager.X_train, self.model_manager.y_train)
        
        training_time = (datetime.now() - start_time).total_seconds()
        
        # è¯„ä¼°æ€§èƒ½
        train_score = ensemble.score(self.model_manager.X_train, self.model_manager.y_train)
        test_score = ensemble.score(self.model_manager.X_test, self.model_manager.y_test)
        
        ensemble_name = f"Bagging_{base_model_name}"
        
        result = {
            'type': 'bagging',
            'base_model': base_model_name,
            'n_estimators': n_estimators,
            'model': ensemble,
            'train_score': train_score,
            'test_score': test_score,
            'training_time': training_time
        }
        
        self.ensemble_models[ensemble_name] = ensemble
        self.ensemble_results[ensemble_name] = result
        
        print(f"âœ… Baggingé›†æˆå®Œæˆ (è€—æ—¶: {training_time:.2f}ç§’)")
        print(f"ğŸ“ˆ è®­ç»ƒå¾—åˆ†: {train_score:.4f}")
        print(f"ğŸ“Š æµ‹è¯•å¾—åˆ†: {test_score:.4f}")
        
        return result
    
    def create_stacking_ensemble(self, base_model_names, meta_learner=None, cv=5):
        """åˆ›å»ºStackingé›†æˆæ¨¡å‹"""
        print(f"\n=== åˆ›å»ºStackingé›†æˆæ¨¡å‹ ===")
        print(f"ğŸ—ï¸ åŸºç¡€æ¨¡å‹: {base_model_names}")
        
        # å‡†å¤‡åŸºç¡€æ¨¡å‹
        estimators = []
        for model_name in base_model_names:
            if model_name in self.model_manager.trained_models:
                model = self.model_manager.trained_models[model_name]
                estimators.append((model_name, model))
            else:
                print(f"âš ï¸ æ¨¡å‹ {model_name} æœªè®­ç»ƒï¼Œè·³è¿‡")
        
        if len(estimators) < 2:
            print("âŒ éœ€è¦è‡³å°‘2ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹")
            return None
        
        # è®¾ç½®å…ƒå­¦ä¹ å™¨
        if meta_learner is None:
            if self.model_manager.task_type == 'classification':
                meta_learner = LogisticRegression(random_state=42)
            else:
                meta_learner = LinearRegression()
        
        print(f"ğŸ§  å…ƒå­¦ä¹ å™¨: {type(meta_learner).__name__}")
        
        # åˆ›å»ºStackingé›†æˆ
        if self.model_manager.task_type == 'classification':
            ensemble = StackingClassifier(
                estimators=estimators,
                final_estimator=meta_learner,
                cv=cv
            )
        else:
            ensemble = StackingRegressor(
                estimators=estimators,
                final_estimator=meta_learner,
                cv=cv
            )
        
        # è®­ç»ƒé›†æˆæ¨¡å‹
        print("ğŸ”„ è®­ç»ƒStackingé›†æˆæ¨¡å‹...")
        start_time = datetime.now()
        
        ensemble.fit(self.model_manager.X_train, self.model_manager.y_train)
        
        training_time = (datetime.now() - start_time).total_seconds()
        
        # è¯„ä¼°æ€§èƒ½
        train_score = ensemble.score(self.model_manager.X_train, self.model_manager.y_train)
        test_score = ensemble.score(self.model_manager.X_test, self.model_manager.y_test)
        
        ensemble_name = f"Stacking_{'_'.join(base_model_names)}"
        
        result = {
            'type': 'stacking',
            'base_models': base_model_names,
            'meta_learner': type(meta_learner).__name__,
            'cv': cv,
            'model': ensemble,
            'train_score': train_score,
            'test_score': test_score,
            'training_time': training_time
        }
        
        self.ensemble_models[ensemble_name] = ensemble
        self.ensemble_results[ensemble_name] = result
        
        print(f"âœ… Stackingé›†æˆå®Œæˆ (è€—æ—¶: {training_time:.2f}ç§’)")
        print(f"ğŸ“ˆ è®­ç»ƒå¾—åˆ†: {train_score:.4f}")
        print(f"ğŸ“Š æµ‹è¯•å¾—åˆ†: {test_score:.4f}")
        
        return result
    
    def compare_ensemble_methods(self, figsize=(15, 10)):
        """æ¯”è¾ƒä¸åŒé›†æˆæ–¹æ³•çš„æ€§èƒ½"""
        print("\n=== é›†æˆæ–¹æ³•æ€§èƒ½å¯¹æ¯” ===")
        
        if not self.ensemble_results:
            print("âŒ æ²¡æœ‰é›†æˆç»“æœ")
            return None
        
        # å‡†å¤‡å¯¹æ¯”æ•°æ®
        comparison_data = []
        
        # æ·»åŠ å•ä¸ªæ¨¡å‹çš„æ€§èƒ½ä½œä¸ºåŸºå‡†
        for model_name, model in self.model_manager.trained_models.items():
            train_score = model.score(self.model_manager.X_train, self.model_manager.y_train)
            test_score = model.score(self.model_manager.X_test, self.model_manager.y_test)
            
            comparison_data.append({
                'model': model_name,
                'type': 'single',
                'train_score': train_score,
                'test_score': test_score,
                'overfitting': train_score - test_score
            })
        
        # æ·»åŠ é›†æˆæ¨¡å‹çš„æ€§èƒ½
        for ensemble_name, result in self.ensemble_results.items():
            comparison_data.append({
                'model': ensemble_name,
                'type': result['type'],
                'train_score': result['train_score'],
                'test_score': result['test_score'],
                'overfitting': result['train_score'] - result['test_score']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values('test_score', ascending=False)
        
        # å¯è§†åŒ–å¯¹æ¯”
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # æµ‹è¯•å¾—åˆ†å¯¹æ¯”
        colors = ['red' if t == 'single' else 'blue' if t == 'voting' else 'green' if t == 'bagging' else 'orange' 
                 for t in comparison_df['type']]
        
        axes[0, 0].bar(range(len(comparison_df)), comparison_df['test_score'], color=colors)
        axes[0, 0].set_title('æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯”')
        axes[0, 0].set_ylabel('å¾—åˆ†')
        axes[0, 0].set_xticks(range(len(comparison_df)))
        axes[0, 0].set_xticklabels(comparison_df['model'], rotation=45, ha='right')
        
        # è¿‡æ‹Ÿåˆç¨‹åº¦å¯¹æ¯”
        axes[0, 1].bar(range(len(comparison_df)), comparison_df['overfitting'], color=colors)
        axes[0, 1].set_title('è¿‡æ‹Ÿåˆç¨‹åº¦å¯¹æ¯”')
        axes[0, 1].set_ylabel('è¿‡æ‹Ÿåˆåˆ†æ•°')
        axes[0, 1].set_xticks(range(len(comparison_df)))
        axes[0, 1].set_xticklabels(comparison_df['model'], rotation=45, ha='right')
        axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.7)
        
        # æŒ‰ç±»å‹åˆ†ç»„çš„æ€§èƒ½
        type_performance = comparison_df.groupby('type')['test_score'].agg(['mean', 'std']).reset_index()
        
        axes[1, 0].bar(type_performance['type'], type_performance['mean'], 
                      yerr=type_performance['std'], capsize=5)
        axes[1, 0].set_title('ä¸åŒé›†æˆæ–¹æ³•å¹³å‡æ€§èƒ½')
        axes[1, 0].set_ylabel('å¹³å‡å¾—åˆ†')
        
        # æ€§èƒ½æå‡åˆ†æ
        single_models = comparison_df[comparison_df['type'] == 'single']
        ensemble_models = comparison_df[comparison_df['type'] != 'single']
        
        if not single_models.empty and not ensemble_models.empty:
            baseline_score = single_models['test_score'].max()
            ensemble_improvements = ensemble_models['test_score'] - baseline_score
            
            axes[1, 1].bar(range(len(ensemble_improvements)), ensemble_improvements, 
                          color=['blue' if t == 'voting' else 'green' if t == 'bagging' else 'orange' 
                                for t in ensemble_models['type']])
            axes[1, 1].set_title('ç›¸å¯¹æœ€ä½³å•æ¨¡å‹çš„æ€§èƒ½æå‡')
            axes[1, 1].set_ylabel('æ€§èƒ½æå‡')
            axes[1, 1].set_xticks(range(len(ensemble_improvements)))
            axes[1, 1].set_xticklabels(ensemble_models['model'], rotation=45, ha='right')
            axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°è¯¦ç»†å¯¹æ¯”è¡¨æ ¼
        print("ğŸ“Š è¯¦ç»†æ€§èƒ½å¯¹æ¯”:")
        print(comparison_df.to_string(index=False, float_format='%.4f'))
        
        # åˆ†ææœ€ä½³é›†æˆæ–¹æ³•
        best_ensemble = comparison_df[comparison_df['type'] != 'single'].iloc[0] if not comparison_df[comparison_df['type'] != 'single'].empty else None
        best_single = comparison_df[comparison_df['type'] == 'single'].iloc[0] if not comparison_df[comparison_df['type'] == 'single'].empty else None
        
        if best_ensemble is not None and best_single is not None:
            improvement = best_ensemble['test_score'] - best_single['test_score']
            print(f"\nğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {best_ensemble['model']} ({best_ensemble['type']})")
            print(f"ğŸ“ˆ æ€§èƒ½æå‡: {improvement:.4f} ({improvement/best_single['test_score']*100:.2f}%)")
        
        return comparison_df
    
    def get_ensemble_summary(self):
        """è·å–é›†æˆæ¨¡å‹æ€»ç»“"""
        print("\n=== é›†æˆæ¨¡å‹æ€»ç»“ ===")
        
        if not self.ensemble_results:
            print("âŒ æ²¡æœ‰é›†æˆç»“æœ")
            return None
        
        summary_data = []
        
        for ensemble_name, result in self.ensemble_results.items():
            summary_data.append({
                'ensemble': ensemble_name,
                'type': result['type'],
                'test_score': result['test_score'],
                'training_time': result['training_time'],
                'base_models': result.get('base_models', result.get('base_model', 'N/A'))
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_df = summary_df.sort_values('test_score', ascending=False)
        
        print("ğŸ“ˆ é›†æˆæ¨¡å‹æ±‡æ€»:")
        print(summary_df[['ensemble', 'type', 'test_score', 'training_time']].to_string(index=False, float_format='%.4f'))
        
        return summary_df

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºé›†æˆå™¨
    ensemble = ModelEnsemble(model_manager)
    
    # å‡è®¾å·²ç»è®­ç»ƒäº†å¤šä¸ªæ¨¡å‹
    available_models = ['RandomForest', 'GradientBoosting', 'LogisticRegression']
    
    # 1. åˆ›å»ºæŠ•ç¥¨é›†æˆ
    voting_result = ensemble.create_voting_ensemble(available_models, voting='soft')
    
    # 2. åˆ›å»ºBaggingé›†æˆ
    bagging_result = ensemble.create_bagging_ensemble('RandomForest', n_estimators=20)
    
    # 3. åˆ›å»ºStackingé›†æˆ
    stacking_result = ensemble.create_stacking_ensemble(available_models)
    
    # 4. æ¯”è¾ƒé›†æˆæ–¹æ³•
    comparison = ensemble.compare_ensemble_methods()
    
    # 5. è·å–é›†æˆæ€»ç»“
    summary = ensemble.get_ensemble_summary()
```

## 4.4.5 æœ¬èŠ‚æ€»ç»“

æœ¬èŠ‚è¯¦ç»†ä»‹ç»äº†Traeç¯å¢ƒä¸‹çš„æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–å®ä¾‹ï¼Œæ„å»ºäº†å®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ã€‚é€šè¿‡å››ä¸ªæ ¸å¿ƒç³»ç»Ÿçš„å®ç°ï¼Œä¸ºAIå¼€å‘è€…æä¾›äº†ä»æ¨¡å‹é€‰æ‹©åˆ°é›†æˆä¼˜åŒ–çš„å…¨å¥—è§£å†³æ–¹æ¡ˆã€‚

### æ ¸å¿ƒæˆæœæ€»ç»“

| ç³»ç»Ÿæ¨¡å— | ä¸»è¦åŠŸèƒ½ | æŠ€æœ¯ç‰¹è‰² | å®ç”¨ä»·å€¼ |
|---------|---------|---------|----------|
| **æ™ºèƒ½æ¨¡å‹ç®¡ç†** | è‡ªåŠ¨æ¨¡å‹é€‰æ‹©ã€è®­ç»ƒã€è¯„ä¼° | ä»»åŠ¡ç±»å‹è‡ªåŠ¨è¯†åˆ«ã€å¤šæ¨¡å‹å¹¶è¡Œè®­ç»ƒ | é™ä½æ¨¡å‹é€‰æ‹©é—¨æ§›ï¼Œæé«˜å¼€å‘æ•ˆç‡ |
| **è¶…å‚æ•°ä¼˜åŒ–** | ç½‘æ ¼æœç´¢ã€éšæœºæœç´¢ã€è´å¶æ–¯ä¼˜åŒ– | Optunaé›†æˆã€å¤šç­–ç•¥å¯¹æ¯”åˆ†æ | è‡ªåŠ¨åŒ–å‚æ•°è°ƒä¼˜ï¼Œæå‡æ¨¡å‹æ€§èƒ½ |
| **æ€§èƒ½è¯„ä¼°å¯è§†åŒ–** | ç»¼åˆè¯„ä¼°ã€å¤šç»´åº¦åˆ†æ | åˆ†ç±»/å›å½’ä¸“ç”¨è¯„ä¼°ã€å­¦ä¹ æ›²çº¿åˆ†æ | æ·±å…¥ç†è§£æ¨¡å‹è¡Œä¸ºï¼ŒæŒ‡å¯¼ä¼˜åŒ–æ–¹å‘ |
| **æ¨¡å‹é›†æˆèåˆ** | æŠ•ç¥¨ã€Baggingã€Stackingé›†æˆ | å¤šç­–ç•¥é›†æˆã€æ€§èƒ½å¯¹æ¯”åˆ†æ | æå‡é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¢å¼ºæ¨¡å‹é²æ£’æ€§ |

### æŠ€æœ¯äº®ç‚¹

```python
# æ ¸å¿ƒæŠ€æœ¯ç‰¹è‰²å±•ç¤º
technical_highlights = {
    "æ™ºèƒ½åŒ–ç¨‹åº¦": {
        "è‡ªåŠ¨ä»»åŠ¡è¯†åˆ«": "æ ¹æ®ç›®æ ‡å˜é‡ç±»å‹è‡ªåŠ¨é€‰æ‹©åˆ†ç±»/å›å½’ä»»åŠ¡",
        "æ™ºèƒ½æ¨¡å‹æ¨è": "åŸºäºæ•°æ®ç‰¹å¾æ¨èæœ€é€‚åˆçš„ç®—æ³•ç»„åˆ",
        "è‡ªé€‚åº”å‚æ•°ç©ºé—´": "æ ¹æ®æ¨¡å‹ç±»å‹åŠ¨æ€è°ƒæ•´è¶…å‚æ•°æœç´¢èŒƒå›´"
    },
    
    "ä¼˜åŒ–ç­–ç•¥": {
        "å¤šå±‚æ¬¡ä¼˜åŒ–": "ä»å•æ¨¡å‹åˆ°é›†æˆæ¨¡å‹çš„æ¸è¿›å¼ä¼˜åŒ–",
        "è´å¶æ–¯ä¼˜åŒ–": "ä½¿ç”¨Optunaå®ç°é«˜æ•ˆçš„è¶…å‚æ•°æœç´¢",
        "äº¤å‰éªŒè¯": "ç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å¯é è¯„ä¼°"
    },
    
    "å¯è§†åŒ–åˆ†æ": {
        "å¤šç»´åº¦è¯„ä¼°": "å‡†ç¡®ç‡ã€ROCæ›²çº¿ã€å­¦ä¹ æ›²çº¿ç­‰å…¨æ–¹ä½åˆ†æ",
        "å¯¹æ¯”åˆ†æ": "å•æ¨¡å‹vsé›†æˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”å¯è§†åŒ–",
        "è¯Šæ–­å·¥å…·": "è¿‡æ‹Ÿåˆæ£€æµ‹ã€æ®‹å·®åˆ†æç­‰è¯Šæ–­åŠŸèƒ½"
    },
    
    "å·¥ç¨‹å®è·µ": {
        "æ¨¡å—åŒ–è®¾è®¡": "å„åŠŸèƒ½æ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºæ‰©å±•å’Œç»´æŠ¤",
        "å¼‚å¸¸å¤„ç†": "å®Œå–„çš„é”™è¯¯å¤„ç†å’Œç”¨æˆ·å‹å¥½çš„æç¤ºä¿¡æ¯",
        "æ€§èƒ½ç›‘æ§": "è®­ç»ƒæ—¶é—´ã€å†…å­˜ä½¿ç”¨ç­‰æ€§èƒ½æŒ‡æ ‡ç›‘æ§"
    }
}

# å¯è§†åŒ–æŠ€æœ¯äº®ç‚¹
import matplotlib.pyplot as plt
import numpy as np

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# æ™ºèƒ½åŒ–ç¨‹åº¦é›·è¾¾å›¾
categories = list(technical_highlights.keys())
values = [len(technical_highlights[cat]) for cat in categories]

angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
values += values[:1]  # é—­åˆå›¾å½¢
angles = np.concatenate((angles, [angles[0]]))

axes[0, 0].plot(angles, values, 'o-', linewidth=2, color='blue')
axes[0, 0].fill(angles, values, alpha=0.25, color='blue')
axes[0, 0].set_xticks(angles[:-1])
axes[0, 0].set_xticklabels(categories)
axes[0, 0].set_title('æŠ€æœ¯ç‰¹è‰²åˆ†å¸ƒ')

# ä¼˜åŒ–æ–¹æ³•æ•ˆæœå¯¹æ¯”
optimization_methods = ['ç½‘æ ¼æœç´¢', 'éšæœºæœç´¢', 'Optunaä¼˜åŒ–']
efficiency_scores = [0.7, 0.8, 0.95]
time_costs = [1.0, 0.6, 0.4]

axes[0, 1].bar(optimization_methods, efficiency_scores, alpha=0.7, label='ä¼˜åŒ–æ•ˆæœ')
axes[0, 1].set_ylabel('ä¼˜åŒ–æ•ˆæœ', color='blue')
axes[0, 1].tick_params(axis='y', labelcolor='blue')

ax2 = axes[0, 1].twinx()
ax2.plot(optimization_methods, time_costs, 'ro-', label='æ—¶é—´æˆæœ¬')
ax2.set_ylabel('ç›¸å¯¹æ—¶é—´æˆæœ¬', color='red')
ax2.tick_params(axis='y', labelcolor='red')

axes[0, 1].set_title('è¶…å‚æ•°ä¼˜åŒ–æ–¹æ³•å¯¹æ¯”')
axes[0, 1].legend(loc='upper left')
ax2.legend(loc='upper right')

# é›†æˆæ–¹æ³•æ€§èƒ½æå‡
ensemble_methods = ['æŠ•ç¥¨é›†æˆ', 'Bagging', 'Stacking']
performance_improvement = [0.03, 0.05, 0.08]
model_complexity = [1.2, 1.5, 2.0]

axes[1, 0].bar(ensemble_methods, performance_improvement, color=['skyblue', 'lightgreen', 'orange'])
axes[1, 0].set_ylabel('æ€§èƒ½æå‡')
axes[1, 0].set_title('é›†æˆæ–¹æ³•æ€§èƒ½æå‡å¯¹æ¯”')

# æ¨¡å‹è¯„ä¼°ç»´åº¦
evaluation_aspects = ['å‡†ç¡®æ€§', 'é²æ£’æ€§', 'å¯è§£é‡Šæ€§', 'æ•ˆç‡', 'æ³›åŒ–èƒ½åŠ›']
single_model_scores = [0.75, 0.70, 0.85, 0.90, 0.72]
ensemble_scores = [0.88, 0.85, 0.65, 0.75, 0.85]

x = np.arange(len(evaluation_aspects))
width = 0.35

axes[1, 1].bar(x - width/2, single_model_scores, width, label='å•æ¨¡å‹', alpha=0.8)
axes[1, 1].bar(x + width/2, ensemble_scores, width, label='é›†æˆæ¨¡å‹', alpha=0.8)
axes[1, 1].set_ylabel('è¯„åˆ†')
axes[1, 1].set_title('å•æ¨¡å‹ vs é›†æˆæ¨¡å‹å¯¹æ¯”')
axes[1, 1].set_xticks(x)
axes[1, 1].set_xticklabels(evaluation_aspects, rotation=45)
axes[1, 1].legend()

plt.tight_layout()
plt.show()
```

### å®è·µä»·å€¼ä¸åº”ç”¨åœºæ™¯

**1. å¿«é€ŸåŸå‹å¼€å‘**
- è‡ªåŠ¨åŒ–æ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒæµç¨‹
- å‡å°‘é‡å¤æ€§ç¼–ç å·¥ä½œ
- å¿«é€ŸéªŒè¯ç®—æ³•å¯è¡Œæ€§

**2. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**
- å®Œæ•´çš„æ¨¡å‹è¯„ä¼°å’ŒéªŒè¯ä½“ç³»
- é›†æˆå­¦ä¹ æå‡é¢„æµ‹ç¨³å®šæ€§
- æ€§èƒ½ç›‘æ§å’Œè¯Šæ–­å·¥å…·

**3. æ•™å­¦å’Œç ”ç©¶**
- ç›´è§‚çš„å¯è§†åŒ–åˆ†æå·¥å…·
- å¤šç§ä¼˜åŒ–ç­–ç•¥çš„å¯¹æ¯”å­¦ä¹ 
- å®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ç¤ºä¾‹

### æœ€ä½³å®è·µå»ºè®®

1. **æ¨¡å‹é€‰æ‹©ç­–ç•¥**ï¼šä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å°è¯•å¤æ‚ç®—æ³•
2. **è¶…å‚æ•°ä¼˜åŒ–**ï¼šä¼˜å…ˆä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼Œæé«˜æœç´¢æ•ˆç‡
3. **æ€§èƒ½è¯„ä¼°**ï¼šå…³æ³¨å¤šä¸ªæŒ‡æ ‡ï¼Œé¿å…å•ä¸€æŒ‡æ ‡çš„å±€é™æ€§
4. **é›†æˆå­¦ä¹ **ï¼šåœ¨æ€§èƒ½è¦æ±‚é«˜çš„åœºæ™¯ä¸‹ä½¿ç”¨Stackingæ–¹æ³•
5. **è®¡ç®—èµ„æº**ï¼šæ ¹æ®æ•°æ®è§„æ¨¡å’Œæ—¶é—´è¦æ±‚é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥

é€šè¿‡æœ¬èŠ‚çš„å­¦ä¹ ï¼Œå¼€å‘è€…å¯ä»¥æŒæ¡å®Œæ•´çš„æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–æµç¨‹ï¼Œä¸ºåç»­çš„æ¨¡å‹éƒ¨ç½²å’Œåº”ç”¨å¥ å®šåšå®åŸºç¡€ã€‚ä¸‹ä¸€èŠ‚å°†ä»‹ç»æ¨¡å‹éƒ¨ç½²ä¸åº”ç”¨å®ä¾‹ï¼Œå±•ç¤ºå¦‚ä½•å°†è®­ç»ƒå¥½çš„æ¨¡å‹æŠ•å…¥å®é™…ä½¿ç”¨ã€‚
```