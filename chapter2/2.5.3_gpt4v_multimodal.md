# 2.5.3 GPT-4Vï¼šå¤šæ¨¡æ€ç†è§£çš„æ–°é«˜åº¦

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. **ç†è§£GPT-4Vçš„çªç ´æ€§æ„ä¹‰**ï¼šæŒæ¡å¤§è¯­è¨€æ¨¡å‹è§†è§‰èƒ½åŠ›çš„æŠ€æœ¯å®ç°
2. **è®¤è¯†å¤šæ¨¡æ€ç»Ÿä¸€æ¶æ„**ï¼šäº†è§£å¦‚ä½•åœ¨å•ä¸€æ¨¡å‹ä¸­èåˆæ–‡æœ¬å’Œè§†è§‰ç†è§£
3. **æŒæ¡è§†è§‰æ¨ç†èƒ½åŠ›**ï¼šç†è§£AIå¦‚ä½•è¿›è¡Œå¤æ‚çš„è§†è§‰åˆ†æå’Œæ¨ç†
4. **ä½“éªŒå¤šæ¨¡æ€åº”ç”¨**ï¼šä½¿ç”¨Traeå®ç°GPT-4Vé£æ ¼çš„å¤šæ¨¡æ€åº”ç”¨

## GPT-4Vçš„å†å²èƒŒæ™¯

### ä»å•æ¨¡æ€åˆ°å¤šæ¨¡æ€çš„æ¼”è¿›

**ä¼ ç»Ÿæ¨¡å‹çš„å±€é™**ï¼š
```mermaid
graph LR
    subgraph "å•æ¨¡æ€æ—¶ä»£"
        A[GPT-3] --> B[çº¯æ–‡æœ¬ç†è§£]
        C[CLIP] --> D[å›¾æ–‡åŒ¹é…]
        E[DALL-E] --> F[æ–‡æœ¬ç”Ÿæˆå›¾åƒ]
    end
    
    subgraph "å¤šæ¨¡æ€ç»Ÿä¸€"
        G[GPT-4V] --> H[æ–‡æœ¬+è§†è§‰ç†è§£]
        H --> I[å¤æ‚æ¨ç†]
        H --> J[å¤šæ¨¡æ€å¯¹è¯]
        H --> K[è§†è§‰é—®ç­”]
    end
```

**GPT-4Vçš„åˆ›æ–°ç‚¹**ï¼š
- **ç»Ÿä¸€æ¶æ„**ï¼šå•ä¸€æ¨¡å‹å¤„ç†æ–‡æœ¬å’Œå›¾åƒ
- **æ·±åº¦ç†è§£**ï¼šä¸ä»…è¯†åˆ«ï¼Œæ›´èƒ½æ¨ç†å’Œåˆ†æ
- **è‡ªç„¶äº¤äº’**ï¼šæ”¯æŒå›¾æ–‡æ··åˆçš„è‡ªç„¶å¯¹è¯
- **å¹¿æ³›åº”ç”¨**ï¼šä»å­¦æœ¯ç ”ç©¶åˆ°å®é™…åº”ç”¨çš„å…¨è¦†ç›–

### æŠ€æœ¯å‘å±•è„‰ç»œ

```mermaid
timeline
    title å¤šæ¨¡æ€å¤§æ¨¡å‹å‘å±•
    
    2021 : CLIPå‘å¸ƒ
         : å›¾æ–‡å¯¹æ¯”å­¦ä¹ 
         : é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›
    
    2022 : Flamingo
         : å°‘æ ·æœ¬å¤šæ¨¡æ€å­¦ä¹ 
         : ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›
    
    2022 : BLIPç³»åˆ—
         : ç»Ÿä¸€è§†è§‰-è¯­è¨€ç†è§£
         : å›¾åƒæè¿°ç”Ÿæˆ
    
    2023å¹´3æœˆ : GPT-4å‘å¸ƒ
              : å¼ºå¤§çš„æ–‡æœ¬èƒ½åŠ›
              : å¤šæ¨¡æ€èƒ½åŠ›é¢„å‘Š
    
    2023å¹´9æœˆ : GPT-4Væ­£å¼å‘å¸ƒ
              : è§†è§‰ç†è§£èƒ½åŠ›
              : å¤æ‚æ¨ç†èƒ½åŠ›
              : å®ç”¨åŒ–åº”ç”¨
```

## GPT-4Vçš„æ ¸å¿ƒæ¶æ„

### ç»Ÿä¸€çš„Transformeræ¶æ„

**æ¶æ„è®¾è®¡ç†å¿µ**ï¼š
```python
class GPT4VArchitecture:
    def __init__(self):
        # ç»Ÿä¸€çš„Transformerä¸»å¹²
        self.transformer_backbone = TransformerLM(
            layers=96,  # æ›´æ·±çš„ç½‘ç»œ
            hidden_size=12288,  # æ›´å¤§çš„éšè—å±‚
            attention_heads=96,  # æ›´å¤šæ³¨æ„åŠ›å¤´
            vocab_size=100000  # æ‰©å±•è¯æ±‡è¡¨
        )
        
        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = VisionTransformer(
            patch_size=14,
            embed_dim=1024,
            depth=24,
            num_heads=16
        )
        
        # è·¨æ¨¡æ€èåˆ
        self.cross_modal_adapter = CrossModalAdapter(
            vision_dim=1024,
            text_dim=12288
        )
    
    def process_multimodal_input(self, text_tokens, images):
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        # 1. æ–‡æœ¬ç¼–ç 
        text_embeddings = self.transformer_backbone.embed_tokens(text_tokens)
        
        # 2. å›¾åƒç¼–ç 
        if images is not None:
            image_features = self.vision_encoder(images)
            # å°†å›¾åƒç‰¹å¾è½¬æ¢ä¸ºtokenåºåˆ—
            image_tokens = self.cross_modal_adapter.vision_to_text(image_features)
        else:
            image_tokens = None
        
        # 3. åºåˆ—èåˆ
        if image_tokens is not None:
            # æ’å…¥ç‰¹æ®Šçš„å›¾åƒåˆ†éš”ç¬¦
            combined_sequence = self.interleave_modalities(
                text_embeddings, image_tokens
            )
        else:
            combined_sequence = text_embeddings
        
        # 4. ç»Ÿä¸€å¤„ç†
        output = self.transformer_backbone.forward_embeddings(combined_sequence)
        
        return output
```

### è§†è§‰ç¼–ç å™¨è®¾è®¡

**Vision Transformeré€‚é…**ï¼š
```python
class GPT4VisionEncoder(nn.Module):
    def __init__(self, 
                 image_size=336,  # é«˜åˆ†è¾¨ç‡è¾“å…¥
                 patch_size=14,
                 embed_dim=1024,
                 depth=24,
                 num_heads=16):
        super().__init__()
        
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        
        # å›¾åƒåˆ†å—åµŒå…¥
        self.patch_embed = PatchEmbed(
            img_size=image_size,
            patch_size=patch_size,
            embed_dim=embed_dim
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_embed = nn.Parameter(
            torch.zeros(1, self.num_patches + 1, embed_dim)
        )
        
        # CLS token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # Transformerå±‚
        self.blocks = nn.ModuleList([
            VisionTransformerBlock(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=4.0
            ) for _ in range(depth)
        ])
        
        # å±‚å½’ä¸€åŒ–
        self.norm = nn.LayerNorm(embed_dim)
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        self.multi_scale_adapter = MultiScaleAdapter(embed_dim)
    
    def forward(self, images):
        B = images.shape[0]
        
        # 1. å›¾åƒåˆ†å—
        x = self.patch_embed(images)  # [B, num_patches, embed_dim]
        
        # 2. æ·»åŠ CLS token
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # 3. ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        
        # 4. Transformerå¤„ç†
        intermediate_features = []
        for i, block in enumerate(self.blocks):
            x = block(x)
            # æ”¶é›†ä¸­é—´å±‚ç‰¹å¾ç”¨äºå¤šå°ºåº¦ç†è§£
            if i in [7, 15, 23]:  # é€‰æ‹©ç‰¹å®šå±‚
                intermediate_features.append(x)
        
        x = self.norm(x)
        
        # 5. å¤šå°ºåº¦ç‰¹å¾èåˆ
        multi_scale_features = self.multi_scale_adapter(
            x, intermediate_features
        )
        
        return {
            'global_features': x[:, 0],  # CLS token
            'patch_features': x[:, 1:],  # å›¾åƒå—ç‰¹å¾
            'multi_scale_features': multi_scale_features
        }
```

### è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶

**è§†è§‰-æ–‡æœ¬äº¤äº’**ï¼š
```python
class CrossModalAttention(nn.Module):
    def __init__(self, 
                 text_dim=12288,
                 vision_dim=1024,
                 num_heads=16):
        super().__init__()
        
        self.num_heads = num_heads
        self.head_dim = text_dim // num_heads
        
        # æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±
        self.q_proj = nn.Linear(text_dim, text_dim)
        self.k_proj = nn.Linear(vision_dim, text_dim)
        self.v_proj = nn.Linear(vision_dim, text_dim)
        
        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(text_dim, text_dim)
        
        # é—¨æ§æœºåˆ¶
        self.gate = nn.Sequential(
            nn.Linear(text_dim + vision_dim, text_dim),
            nn.Sigmoid()
        )
    
    def forward(self, text_features, vision_features, attention_mask=None):
        B, T, _ = text_features.shape
        B, V, _ = vision_features.shape
        
        # 1. è®¡ç®—æ³¨æ„åŠ›
        Q = self.q_proj(text_features)  # [B, T, text_dim]
        K = self.k_proj(vision_features)  # [B, V, text_dim]
        V = self.v_proj(vision_features)  # [B, V, text_dim]
        
        # 2. å¤šå¤´æ³¨æ„åŠ›
        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(B, V, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(B, V, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 3. æ³¨æ„åŠ›è®¡ç®—
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if attention_mask is not None:
            attention_scores += attention_mask
        
        attention_probs = F.softmax(attention_scores, dim=-1)
        
        # 4. åŠ æƒæ±‚å’Œ
        context = torch.matmul(attention_probs, V)
        context = context.transpose(1, 2).contiguous().view(B, T, -1)
        
        # 5. è¾“å‡ºæŠ•å½±
        output = self.out_proj(context)
        
        # 6. é—¨æ§èåˆ
        gate_input = torch.cat([
            text_features,
            output
        ], dim=-1)
        gate_weights = self.gate(gate_input)
        
        # æ®‹å·®è¿æ¥ + é—¨æ§
        final_output = text_features + gate_weights * output
        
        return final_output, attention_probs
```

## GPT-4Vçš„æ ¸å¿ƒèƒ½åŠ›

### è§†è§‰ç†è§£èƒ½åŠ›

**å¤šå±‚æ¬¡è§†è§‰ç†è§£**ï¼š
```python
class VisualUnderstanding:
    def __init__(self, model):
        self.model = model
        
    def analyze_image_content(self, image, query_type="comprehensive"):
        """å¤šå±‚æ¬¡å›¾åƒå†…å®¹åˆ†æ"""
        
        analysis_prompts = {
            'objects': "What objects can you see in this image? List them with their locations.",
            'scene': "Describe the overall scene and setting of this image.",
            'people': "Describe any people in the image, including their actions and expressions.",
            'text': "Is there any text visible in the image? If so, what does it say?",
            'emotions': "What emotions or mood does this image convey?",
            'composition': "Analyze the composition, lighting, and artistic elements.",
            'comprehensive': "Provide a detailed analysis of this image, including objects, people, scene, mood, and any notable details."
        }
        
        prompt = analysis_prompts.get(query_type, analysis_prompts['comprehensive'])
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
    
    def visual_reasoning(self, image, question):
        """è§†è§‰æ¨ç†"""
        reasoning_prompt = f"""
        Look at this image carefully and answer the following question with detailed reasoning:
        
        Question: {question}
        
        Please:
        1. Describe what you observe in the image
        2. Explain your reasoning process
        3. Provide your final answer
        """
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": reasoning_prompt}
                    ]
                }
            ]
        )
        
        return response
    
    def compare_images(self, image1, image2, comparison_aspect="general"):
        """å›¾åƒæ¯”è¾ƒåˆ†æ"""
        comparison_prompts = {
            'general': "Compare these two images. What are the similarities and differences?",
            'objects': "Compare the objects present in these two images.",
            'style': "Compare the artistic style and composition of these images.",
            'quality': "Compare the quality, resolution, and technical aspects of these images.",
            'content': "Compare the content and subject matter of these images."
        }
        
        prompt = comparison_prompts.get(comparison_aspect, comparison_prompts['general'])
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Image 1:"},
                        {"type": "image", "image": image1},
                        {"type": "text", "text": "Image 2:"},
                        {"type": "image", "image": image2},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
```

### æ–‡æ¡£ç†è§£èƒ½åŠ›

**ç»“æ„åŒ–æ–‡æ¡£åˆ†æ**ï¼š
```python
class DocumentUnderstanding:
    def __init__(self, model):
        self.model = model
    
    def extract_text_from_document(self, document_image):
        """ä»æ–‡æ¡£å›¾åƒä¸­æå–æ–‡æœ¬"""
        prompt = """
        Please extract all the text content from this document image. 
        Maintain the original structure and formatting as much as possible.
        If there are tables, preserve the table structure.
        """
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": document_image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
    
    def analyze_document_structure(self, document_image):
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        prompt = """
        Analyze the structure of this document. Please identify:
        1. Document type (report, form, invoice, etc.)
        2. Main sections and headings
        3. Key information fields
        4. Tables, charts, or diagrams
        5. Overall layout and organization
        """
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": document_image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
    
    def extract_key_information(self, document_image, fields_of_interest):
        """æå–å…³é”®ä¿¡æ¯"""
        fields_str = ", ".join(fields_of_interest)
        
        prompt = f"""
        From this document, please extract the following specific information:
        {fields_str}
        
        For each field, provide:
        - The extracted value
        - The confidence level (high/medium/low)
        - The location in the document where you found it
        
        If a field is not found, please indicate "Not found".
        """
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": document_image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
    
    def summarize_document(self, document_image, summary_length="medium"):
        """æ–‡æ¡£æ‘˜è¦"""
        length_prompts = {
            'short': "Provide a brief 2-3 sentence summary",
            'medium': "Provide a comprehensive paragraph summary",
            'long': "Provide a detailed summary with key points and sections"
        }
        
        length_instruction = length_prompts.get(summary_length, length_prompts['medium'])
        
        prompt = f"""
        Please read this document and {length_instruction} of its content.
        Focus on the main points, key findings, and important information.
        """
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": document_image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
```

### ä»£ç ç†è§£èƒ½åŠ›

**ä»£ç åˆ†æå’Œè§£é‡Š**ï¼š
```python
class CodeUnderstanding:
    def __init__(self, model):
        self.model = model
    
    def analyze_code_screenshot(self, code_image):
        """åˆ†æä»£ç æˆªå›¾"""
        prompt = """
        Please analyze this code image and provide:
        1. Programming language identification
        2. Code structure and organization
        3. Main functions or classes
        4. Code logic explanation
        5. Potential issues or improvements
        6. Overall code quality assessment
        """
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": code_image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
    
    def debug_code_from_image(self, code_image, error_description=None):
        """ä»å›¾åƒè°ƒè¯•ä»£ç """
        base_prompt = """
        Please help debug this code. Analyze the code in the image and:
        1. Identify any syntax errors
        2. Look for logical errors
        3. Suggest improvements
        4. Provide corrected code if needed
        """
        
        if error_description:
            prompt = f"{base_prompt}\n\nSpecific error reported: {error_description}"
        else:
            prompt = base_prompt
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": code_image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
    
    def explain_algorithm_from_diagram(self, algorithm_image):
        """è§£é‡Šç®—æ³•å›¾è§£"""
        prompt = """
        This image shows an algorithm or data structure diagram. Please:
        1. Identify what algorithm or data structure is being illustrated
        2. Explain how it works step by step
        3. Describe the time and space complexity
        4. Provide use cases and applications
        5. If possible, provide pseudocode or implementation hints
        """
        
        response = self.model.generate(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": algorithm_image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
        )
        
        return response
```

## Traeå®è·µï¼šæ„å»ºGPT-4Vé£æ ¼åº”ç”¨

### ç¯å¢ƒå‡†å¤‡

```python
# å®‰è£…å¿…è¦çš„åº“
!pip install openai
!pip install pillow
!pip install gradio
!pip install base64
!pip install requests
!pip install streamlit

# å¯¼å…¥åº“
import openai
from PIL import Image
import gradio as gr
import base64
import io
import requests
from typing import List, Dict, Any, Optional
import json
```

### å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿ

```python
class MultimodalChatbot:
    def __init__(self, api_key: str, model="gpt-4-vision-preview"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.conversation_history = []
    
    def encode_image(self, image: Image.Image) -> str:
        """å°†PILå›¾åƒç¼–ç ä¸ºbase64å­—ç¬¦ä¸²"""
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        return f"data:image/png;base64,{img_str}"
    
    def add_message(self, role: str, content: List[Dict[str, Any]]):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        self.conversation_history.append({
            "role": role,
            "content": content
        })
    
    def chat_with_image(self, 
                       text: str, 
                       image: Optional[Image.Image] = None,
                       max_tokens: int = 1000) -> str:
        """ä¸å›¾åƒè¿›è¡Œå¯¹è¯"""
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        content = []
        
        if image is not None:
            # æ·»åŠ å›¾åƒ
            image_data = self.encode_image(image)
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": image_data,
                    "detail": "high"  # é«˜è´¨é‡åˆ†æ
                }
            })
        
        # æ·»åŠ æ–‡æœ¬
        content.append({
            "type": "text",
            "text": text
        })
        
        # æ·»åŠ åˆ°å¯¹è¯å†å²
        self.add_message("user", content)
        
        try:
            # è°ƒç”¨API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.conversation_history,
                max_tokens=max_tokens,
                temperature=0.7
            )
            
            # è·å–å›å¤
            assistant_message = response.choices[0].message.content
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.add_message("assistant", [{
                "type": "text",
                "text": assistant_message
            }])
            
            return assistant_message
            
        except Exception as e:
            return f"é”™è¯¯: {str(e)}"
    
    def analyze_image(self, 
                     image: Image.Image, 
                     analysis_type: str = "comprehensive") -> str:
        """å›¾åƒåˆ†æ"""
        
        analysis_prompts = {
            "comprehensive": "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾åƒï¼ŒåŒ…æ‹¬å…¶ä¸­çš„å¯¹è±¡ã€åœºæ™¯ã€äººç‰©ã€æ–‡å­—ã€æƒ…æ„Ÿå’Œä»»ä½•å€¼å¾—æ³¨æ„çš„ç»†èŠ‚ã€‚",
            "objects": "è¯·è¯†åˆ«å¹¶åˆ—å‡ºå›¾åƒä¸­çš„æ‰€æœ‰å¯¹è±¡ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„ä½ç½®å’Œç‰¹å¾ã€‚",
            "text": "è¯·æå–å›¾åƒä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œä¿æŒåŸæœ‰æ ¼å¼ã€‚",
            "people": "è¯·æè¿°å›¾åƒä¸­çš„äººç‰©ï¼ŒåŒ…æ‹¬ä»–ä»¬çš„å¤–è§‚ã€åŠ¨ä½œã€è¡¨æƒ…å’Œäº’åŠ¨ã€‚",
            "scene": "è¯·æè¿°å›¾åƒçš„æ•´ä½“åœºæ™¯ã€ç¯å¢ƒå’Œæ°›å›´ã€‚",
            "technical": "è¯·ä»æŠ€æœ¯è§’åº¦åˆ†æè¿™å¼ å›¾åƒï¼ŒåŒ…æ‹¬æ„å›¾ã€å…‰çº¿ã€è‰²å½©å’Œæ‹æ‘„æŠ€å·§ã€‚"
        }
        
        prompt = analysis_prompts.get(analysis_type, analysis_prompts["comprehensive"])
        
        return self.chat_with_image(prompt, image)
    
    def visual_qa(self, image: Image.Image, question: str) -> str:
        """è§†è§‰é—®ç­”"""
        prompt = f"è¯·ä»”ç»†è§‚å¯Ÿè¿™å¼ å›¾åƒå¹¶å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
        return self.chat_with_image(prompt, image)
    
    def compare_images(self, image1: Image.Image, image2: Image.Image, aspect: str = "general") -> str:
        """æ¯”è¾ƒä¸¤å¼ å›¾åƒ"""
        # æ³¨æ„ï¼šGPT-4Vç›®å‰ä¸æ”¯æŒåœ¨å•ä¸ªæ¶ˆæ¯ä¸­å¤„ç†å¤šå¼ å›¾åƒ
        # è¿™é‡Œæˆ‘ä»¬åˆ†åˆ«åˆ†æä¸¤å¼ å›¾åƒï¼Œç„¶åè¿›è¡Œæ¯”è¾ƒ
        
        analysis1 = self.analyze_image(image1, "comprehensive")
        analysis2 = self.analyze_image(image2, "comprehensive")
        
        comparison_prompt = f"""
        åŸºäºä»¥ä¸‹ä¸¤å¼ å›¾åƒçš„åˆ†æç»“æœï¼Œè¯·è¿›è¡Œæ¯”è¾ƒï¼š
        
        å›¾åƒ1åˆ†æï¼š{analysis1}
        
        å›¾åƒ2åˆ†æï¼š{analysis2}
        
        è¯·æ¯”è¾ƒè¿™ä¸¤å¼ å›¾åƒçš„{aspect}æ–¹é¢ï¼ŒæŒ‡å‡ºç›¸ä¼¼ç‚¹å’Œä¸åŒç‚¹ã€‚
        """
        
        return self.chat_with_image(comparison_prompt)
    
    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.conversation_history = []
    
    def get_conversation_summary(self) -> str:
        """è·å–å¯¹è¯æ‘˜è¦"""
        if not self.conversation_history:
            return "æš‚æ— å¯¹è¯å†å²"
        
        summary_prompt = "è¯·æ€»ç»“æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œçªå‡ºä¸»è¦è®¨è®ºçš„å›¾åƒå’Œè¯é¢˜ã€‚"
        return self.chat_with_image(summary_prompt)

# ä½¿ç”¨ç¤ºä¾‹
# chatbot = MultimodalChatbot(api_key="your-openai-api-key")
# response = chatbot.chat_with_image("è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ", image)
```

### æ–‡æ¡£å¤„ç†åº”ç”¨

```python
class DocumentProcessor:
    def __init__(self, api_key: str):
        self.chatbot = MultimodalChatbot(api_key)
    
    def extract_text(self, document_image: Image.Image) -> Dict[str, Any]:
        """æå–æ–‡æ¡£æ–‡æœ¬"""
        prompt = """
        è¯·ä»è¿™ä¸ªæ–‡æ¡£å›¾åƒä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹ã€‚è¯·ï¼š
        1. ä¿æŒåŸæœ‰çš„æ ¼å¼å’Œç»“æ„
        2. å¦‚æœæœ‰è¡¨æ ¼ï¼Œè¯·ä¿æŒè¡¨æ ¼æ ¼å¼
        3. æ ‡æ³¨ä¸åŒçš„ç« èŠ‚å’Œæ ‡é¢˜
        4. å¦‚æœæ–‡æœ¬ä¸æ¸…æ™°ï¼Œè¯·æ ‡æ³¨"[ä¸æ¸…æ™°]"
        """
        
        extracted_text = self.chatbot.chat_with_image(prompt, document_image)
        
        return {
            "extracted_text": extracted_text,
            "processing_time": "completed",
            "confidence": "high"  # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè¯„ä¼°
        }
    
    def analyze_document_type(self, document_image: Image.Image) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£ç±»å‹"""
        prompt = """
        è¯·åˆ†æè¿™ä¸ªæ–‡æ¡£çš„ç±»å‹å’Œç»“æ„ã€‚è¯·è¯†åˆ«ï¼š
        1. æ–‡æ¡£ç±»å‹ï¼ˆå¦‚ï¼šæŠ¥å‘Šã€å‘ç¥¨ã€åˆåŒã€è¡¨å•ç­‰ï¼‰
        2. ä¸»è¦ç« èŠ‚å’Œæ ‡é¢˜
        3. å…³é”®ä¿¡æ¯å­—æ®µ
        4. æ–‡æ¡£çš„æ•´ä½“å¸ƒå±€å’Œç»„ç»‡æ–¹å¼
        5. ä»»ä½•ç‰¹æ®Šçš„æ ¼å¼æˆ–å…ƒç´ 
        """
        
        analysis = self.chatbot.chat_with_image(prompt, document_image)
        
        return {
            "document_analysis": analysis,
            "suggested_fields": self._extract_suggested_fields(analysis)
        }
    
    def extract_structured_data(self, 
                              document_image: Image.Image, 
                              fields: List[str]) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–æ•°æ®"""
        fields_str = "ã€".join(fields)
        
        prompt = f"""
        è¯·ä»è¿™ä¸ªæ–‡æ¡£ä¸­æå–ä»¥ä¸‹ç‰¹å®šä¿¡æ¯ï¼š{fields_str}
        
        å¯¹äºæ¯ä¸ªå­—æ®µï¼Œè¯·æä¾›ï¼š
        - æå–çš„å€¼
        - ç½®ä¿¡åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
        - åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®æè¿°
        
        å¦‚æœæŸä¸ªå­—æ®µæœªæ‰¾åˆ°ï¼Œè¯·æ ‡æ³¨"æœªæ‰¾åˆ°"ã€‚
        è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚
        """
        
        result = self.chatbot.chat_with_image(prompt, document_image)
        
        try:
            # å°è¯•è§£æJSON
            structured_data = json.loads(result)
        except:
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬
            structured_data = {"raw_response": result}
        
        return structured_data
    
    def _extract_suggested_fields(self, analysis: str) -> List[str]:
        """ä»åˆ†æç»“æœä¸­æå–å»ºè®®çš„å­—æ®µ"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯æ¥æå–å­—æ®µ
        # ç®€å•å®ç°ï¼šæŸ¥æ‰¾å¸¸è§çš„å­—æ®µå…³é”®è¯
        common_fields = [
            "å§“å", "æ—¥æœŸ", "é‡‘é¢", "åœ°å€", "ç”µè¯", "é‚®ç®±", 
            "å…¬å¸åç§°", "å‘ç¥¨å·", "åˆåŒå·", "ç­¾å"
        ]
        
        suggested = []
        for field in common_fields:
            if field in analysis:
                suggested.append(field)
        
        return suggested
    
    def summarize_document(self, 
                         document_image: Image.Image, 
                         summary_type: str = "comprehensive") -> str:
        """æ–‡æ¡£æ‘˜è¦"""
        summary_prompts = {
            "brief": "è¯·ç”¨2-3å¥è¯ç®€è¦æ€»ç»“è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹ã€‚",
            "comprehensive": "è¯·æä¾›è¿™ä¸ªæ–‡æ¡£çš„è¯¦ç»†æ‘˜è¦ï¼ŒåŒ…æ‹¬ä¸»è¦å†…å®¹ã€å…³é”®ä¿¡æ¯å’Œé‡è¦ç»†èŠ‚ã€‚",
            "key_points": "è¯·åˆ—å‡ºè¿™ä¸ªæ–‡æ¡£çš„å…³é”®è¦ç‚¹å’Œé‡è¦ä¿¡æ¯ã€‚"
        }
        
        prompt = summary_prompts.get(summary_type, summary_prompts["comprehensive"])
        
        return self.chatbot.chat_with_image(prompt, document_image)

# ä½¿ç”¨ç¤ºä¾‹
# processor = DocumentProcessor(api_key="your-openai-api-key")
# text_result = processor.extract_text(document_image)
# structured_data = processor.extract_structured_data(document_image, ["å§“å", "æ—¥æœŸ", "é‡‘é¢"])
```

### äº¤äº’å¼Webåº”ç”¨

```python
class GPT4VWebApp:
    def __init__(self, api_key: str):
        self.chatbot = MultimodalChatbot(api_key)
        self.document_processor = DocumentProcessor(api_key)
    
    def chat_interface(self, message, image, history):
        """èŠå¤©ç•Œé¢"""
        if image is None and not message.strip():
            return history, ""
        
        try:
            # å¤„ç†å›¾åƒ
            pil_image = None
            if image is not None:
                if isinstance(image, str):  # æ–‡ä»¶è·¯å¾„
                    pil_image = Image.open(image)
                else:  # PILå›¾åƒ
                    pil_image = image
            
            # è·å–å›å¤
            response = self.chatbot.chat_with_image(message, pil_image)
            
            # æ›´æ–°å†å²
            history.append([message, response])
            
            return history, ""
            
        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            history.append([message, error_msg])
            return history, ""
    
    def analyze_interface(self, image, analysis_type):
        """åˆ†æç•Œé¢"""
        if image is None:
            return "è¯·ä¸Šä¼ å›¾åƒ"
        
        try:
            pil_image = Image.open(image) if isinstance(image, str) else image
            result = self.chatbot.analyze_image(pil_image, analysis_type)
            return result
        except Exception as e:
            return f"åˆ†æé”™è¯¯: {str(e)}"
    
    def document_interface(self, document_image, operation, custom_fields):
        """æ–‡æ¡£å¤„ç†ç•Œé¢"""
        if document_image is None:
            return "è¯·ä¸Šä¼ æ–‡æ¡£å›¾åƒ"
        
        try:
            pil_image = Image.open(document_image) if isinstance(document_image, str) else document_image
            
            if operation == "æå–æ–‡æœ¬":
                result = self.document_processor.extract_text(pil_image)
                return result["extracted_text"]
            
            elif operation == "åˆ†ææ–‡æ¡£ç±»å‹":
                result = self.document_processor.analyze_document_type(pil_image)
                return result["document_analysis"]
            
            elif operation == "æå–ç»“æ„åŒ–æ•°æ®":
                if not custom_fields.strip():
                    return "è¯·è¾“å…¥è¦æå–çš„å­—æ®µï¼ˆç”¨é€—å·åˆ†éš”ï¼‰"
                
                fields = [f.strip() for f in custom_fields.split(',')]
                result = self.document_processor.extract_structured_data(pil_image, fields)
                return json.dumps(result, ensure_ascii=False, indent=2)
            
            elif operation == "æ–‡æ¡£æ‘˜è¦":
                result = self.document_processor.summarize_document(pil_image)
                return result
            
        except Exception as e:
            return f"å¤„ç†é”™è¯¯: {str(e)}"
    
    def launch_app(self):
        """å¯åŠ¨Gradioåº”ç”¨"""
        
        with gr.Blocks(title="GPT-4Vå¤šæ¨¡æ€åŠ©æ‰‹") as app:
            gr.Markdown("# ğŸ¤– GPT-4Vå¤šæ¨¡æ€AIåŠ©æ‰‹")
            gr.Markdown("æ”¯æŒå›¾åƒç†è§£ã€æ–‡æ¡£å¤„ç†ã€è§†è§‰é—®ç­”ç­‰åŠŸèƒ½")
            
            with gr.Tab("ğŸ’¬ å¤šæ¨¡æ€å¯¹è¯"):
                with gr.Row():
                    with gr.Column(scale=1):
                        chat_image = gr.Image(
                            label="ä¸Šä¼ å›¾åƒï¼ˆå¯é€‰ï¼‰",
                            type="pil"
                        )
                        
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å†å²")
                    
                    with gr.Column(scale=2):
                        chatbot_display = gr.Chatbot(
                            label="å¯¹è¯å†å²",
                            height=400
                        )
                        
                        with gr.Row():
                            msg_input = gr.Textbox(
                                label="è¾“å…¥æ¶ˆæ¯",
                                placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æè¿°...",
                                scale=4
                            )
                            send_btn = gr.Button("ğŸ“¤ å‘é€", scale=1)
                
                # ç»‘å®šäº‹ä»¶
                send_btn.click(
                    fn=self.chat_interface,
                    inputs=[msg_input, chat_image, chatbot_display],
                    outputs=[chatbot_display, msg_input]
                )
                
                msg_input.submit(
                    fn=self.chat_interface,
                    inputs=[msg_input, chat_image, chatbot_display],
                    outputs=[chatbot_display, msg_input]
                )
                
                clear_btn.click(
                    fn=lambda: ([], None),
                    outputs=[chatbot_display, chat_image]
                )
            
            with gr.Tab("ğŸ” å›¾åƒåˆ†æ"):
                with gr.Row():
                    with gr.Column():
                        analyze_image = gr.Image(
                            label="ä¸Šä¼ è¦åˆ†æçš„å›¾åƒ",
                            type="pil"
                        )
                        
                        analysis_type = gr.Dropdown(
                            choices=[
                                "comprehensive", "objects", "text", 
                                "people", "scene", "technical"
                            ],
                            value="comprehensive",
                            label="åˆ†æç±»å‹"
                        )
                        
                        analyze_btn = gr.Button("ğŸ” å¼€å§‹åˆ†æ")
                    
                    with gr.Column():
                        analysis_result = gr.Textbox(
                            label="åˆ†æç»“æœ",
                            lines=15,
                            max_lines=20
                        )
                
                analyze_btn.click(
                    fn=self.analyze_interface,
                    inputs=[analyze_image, analysis_type],
                    outputs=[analysis_result]
                )
            
            with gr.Tab("ğŸ“„ æ–‡æ¡£å¤„ç†"):
                with gr.Row():
                    with gr.Column():
                        doc_image = gr.Image(
                            label="ä¸Šä¼ æ–‡æ¡£å›¾åƒ",
                            type="pil"
                        )
                        
                        doc_operation = gr.Dropdown(
                            choices=[
                                "æå–æ–‡æœ¬", "åˆ†ææ–‡æ¡£ç±»å‹", 
                                "æå–ç»“æ„åŒ–æ•°æ®", "æ–‡æ¡£æ‘˜è¦"
                            ],
                            value="æå–æ–‡æœ¬",
                            label="å¤„ç†æ“ä½œ"
                        )
                        
                        custom_fields = gr.Textbox(
                            label="è‡ªå®šä¹‰å­—æ®µï¼ˆç”¨é€—å·åˆ†éš”ï¼‰",
                            placeholder="å§“å,æ—¥æœŸ,é‡‘é¢,åœ°å€",
                            visible=False
                        )
                        
                        process_btn = gr.Button("ğŸ“„ å¼€å§‹å¤„ç†")
                    
                    with gr.Column():
                        doc_result = gr.Textbox(
                            label="å¤„ç†ç»“æœ",
                            lines=15,
                            max_lines=20
                        )
                
                # æ˜¾ç¤º/éšè—è‡ªå®šä¹‰å­—æ®µè¾“å…¥
                def toggle_fields(operation):
                    return gr.update(visible=(operation == "æå–ç»“æ„åŒ–æ•°æ®"))
                
                doc_operation.change(
                    fn=toggle_fields,
                    inputs=[doc_operation],
                    outputs=[custom_fields]
                )
                
                process_btn.click(
                    fn=self.document_interface,
                    inputs=[doc_image, doc_operation, custom_fields],
                    outputs=[doc_result]
                )
            
            with gr.Tab("â„¹ï¸ ä½¿ç”¨è¯´æ˜"):
                gr.Markdown("""
                ## ğŸš€ åŠŸèƒ½ä»‹ç»
                
                ### ğŸ’¬ å¤šæ¨¡æ€å¯¹è¯
                - ä¸Šä¼ å›¾åƒå¹¶ä¸AIè¿›è¡Œè‡ªç„¶å¯¹è¯
                - æ”¯æŒå›¾åƒå†…å®¹è¯¢é—®ã€è§†è§‰æ¨ç†ç­‰
                - ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒè¿ç»­äº¤äº’
                
                ### ğŸ” å›¾åƒåˆ†æ
                - **ç»¼åˆåˆ†æ**: å…¨é¢åˆ†æå›¾åƒå†…å®¹
                - **å¯¹è±¡è¯†åˆ«**: è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“
                - **æ–‡å­—æå–**: æå–å›¾åƒä¸­çš„æ–‡æœ¬
                - **äººç‰©åˆ†æ**: åˆ†æäººç‰©ç‰¹å¾å’ŒåŠ¨ä½œ
                - **åœºæ™¯æè¿°**: æè¿°æ•´ä½“åœºæ™¯å’Œç¯å¢ƒ
                - **æŠ€æœ¯åˆ†æ**: åˆ†ææ„å›¾ã€å…‰çº¿ç­‰æŠ€æœ¯è¦ç´ 
                
                ### ğŸ“„ æ–‡æ¡£å¤„ç†
                - **æ–‡æœ¬æå–**: ä»æ–‡æ¡£å›¾åƒä¸­æå–æ‰€æœ‰æ–‡å­—
                - **ç±»å‹åˆ†æ**: è¯†åˆ«æ–‡æ¡£ç±»å‹å’Œç»“æ„
                - **ç»“æ„åŒ–æå–**: æå–ç‰¹å®šå­—æ®µçš„ä¿¡æ¯
                - **æ–‡æ¡£æ‘˜è¦**: ç”Ÿæˆæ–‡æ¡£å†…å®¹æ‘˜è¦
                
                ## ğŸ’¡ ä½¿ç”¨æŠ€å·§
                
                1. **å›¾åƒè´¨é‡**: ä¸Šä¼ æ¸…æ™°ã€é«˜åˆ†è¾¨ç‡çš„å›¾åƒè·å¾—æ›´å¥½æ•ˆæœ
                2. **å…·ä½“é—®é¢˜**: æå‡ºå…·ä½“ã€æ˜ç¡®çš„é—®é¢˜
                3. **ä¸Šä¸‹æ–‡**: åœ¨å¯¹è¯ä¸­æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                4. **å¤šè§’åº¦**: å°è¯•ä»ä¸åŒè§’åº¦åˆ†æåŒä¸€å¼ å›¾åƒ
                
                ## âš ï¸ æ³¨æ„äº‹é¡¹
                
                - è¯·ç¡®ä¿ä¸Šä¼ çš„å›¾åƒå†…å®¹åˆè§„
                - å¤„ç†å¤§å‹å›¾åƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
                - APIè°ƒç”¨å¯èƒ½äº§ç”Ÿè´¹ç”¨ï¼Œè¯·åˆç†ä½¿ç”¨
                """)
        
        return app

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    # è¯·æ›¿æ¢ä¸ºæ‚¨çš„OpenAI APIå¯†é’¥
    API_KEY = "your-openai-api-key"
    
    app = GPT4VWebApp(API_KEY)
    interface = app.launch_app()
    interface.launch(
        share=True,
        server_name="0.0.0.0",
        server_port=7860
    )
```

## å­¦ä¹ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **GPT-4Vçš„çªç ´**ï¼š
   - ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¶æ„
   - å¼ºå¤§çš„è§†è§‰ç†è§£èƒ½åŠ›
   - è‡ªç„¶çš„å›¾æ–‡äº¤äº’ä½“éªŒ
   - å¹¿æ³›çš„åº”ç”¨åœºæ™¯è¦†ç›–

2. **æŠ€æœ¯åˆ›æ–°**ï¼š
   - **Vision Transformer**ï¼šé«˜æ•ˆçš„å›¾åƒç¼–ç 
   - **è·¨æ¨¡æ€æ³¨æ„åŠ›**ï¼šæ·±åº¦çš„å›¾æ–‡èåˆ
   - **ç»Ÿä¸€è®­ç»ƒ**ï¼šç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€å­¦ä¹ 
   - **æŒ‡ä»¤è°ƒä¼˜**ï¼šæ›´å¥½çš„äººç±»å¯¹é½

3. **æ ¸å¿ƒèƒ½åŠ›**ï¼š
   - å›¾åƒå†…å®¹ç†è§£å’Œæè¿°
   - è§†è§‰æ¨ç†å’Œé—®ç­”
   - æ–‡æ¡£åˆ†æå’Œä¿¡æ¯æå–
   - ä»£ç ç†è§£å’Œè°ƒè¯•
   - å¤šæ¨¡æ€å¯¹è¯äº¤äº’

4. **åº”ç”¨ä»·å€¼**ï¼š
   - æ•™è‚²è¾…åŠ©å’Œå­¦ä¹ æ”¯æŒ
   - æ–‡æ¡£å¤„ç†å’Œä¿¡æ¯æå–
   - å†…å®¹åˆ›ä½œå’Œåˆ†æ
   - è¾…åŠ©å†³ç­–å’Œé—®é¢˜è§£å†³

### å®è·µæŠ€èƒ½

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ åº”è¯¥æŒæ¡ï¼š

- [ ] ç†è§£GPT-4Vçš„æŠ€æœ¯æ¶æ„å’Œæ ¸å¿ƒåŸç†
- [ ] ä½¿ç”¨GPT-4V APIè¿›è¡Œå¤šæ¨¡æ€åº”ç”¨å¼€å‘
- [ ] æ„å»ºå›¾åƒåˆ†æå’Œæ–‡æ¡£å¤„ç†åº”ç”¨
- [ ] è®¾è®¡æœ‰æ•ˆçš„å¤šæ¨¡æ€äº¤äº’ç•Œé¢
- [ ] ä¼˜åŒ–å¤šæ¨¡æ€åº”ç”¨çš„ç”¨æˆ·ä½“éªŒ
- [ ] ç†è§£å¤šæ¨¡æ€AIçš„å‘å±•è¶‹åŠ¿å’Œåº”ç”¨å‰æ™¯

### æ€è€ƒé¢˜

1. **æŠ€æœ¯å±€é™**ï¼šGPT-4Våœ¨è§†è§‰ç†è§£æ–¹é¢è¿˜æœ‰å“ªäº›å±€é™æ€§ï¼Ÿ

2. **åº”ç”¨åœºæ™¯**ï¼šåœ¨å“ªäº›åœºæ™¯ä¸‹å¤šæ¨¡æ€AIæ¯”å•æ¨¡æ€AIæ›´æœ‰ä¼˜åŠ¿ï¼Ÿ

3. **éšç§å®‰å…¨**ï¼šä½¿ç”¨å¤šæ¨¡æ€AIå¤„ç†å›¾åƒæ—¶éœ€è¦æ³¨æ„å“ªäº›éšç§å’Œå®‰å…¨é—®é¢˜ï¼Ÿ

4. **æˆæœ¬æ•ˆç›Š**ï¼šå¦‚ä½•åœ¨åŠŸèƒ½éœ€æ±‚å’ŒAPIæˆæœ¬ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Ÿ

5. **æœªæ¥å‘å±•**ï¼šæ‚¨è®¤ä¸ºå¤šæ¨¡æ€AIæŠ€æœ¯ä¼šå¦‚ä½•è¿›ä¸€æ­¥å‘å±•ï¼Ÿ

### æ‰©å±•é˜…è¯»

- **GPT-4VæŠ€æœ¯æŠ¥å‘Š**ï¼šOpenAIå®˜æ–¹æŠ€æœ¯æ–‡æ¡£
- **å¤šæ¨¡æ€å­¦ä¹ ç»¼è¿°**ï¼šæœ€æ–°çš„å¤šæ¨¡æ€AIç ”ç©¶è¿›å±•
- **Vision Transformer**ï¼šè§†è§‰Transformerçš„åŸç†å’Œåº”ç”¨
- **åº”ç”¨æ¡ˆä¾‹é›†**ï¼šGPT-4Våœ¨å„è¡Œä¸šçš„åº”ç”¨å®ä¾‹

---

**ä¸Šä¸€èŠ‚**ï¼š[2.5.2 DALL-Eï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„é©å‘½](2.5.2_dalle_text_to_image.md)

**ä¸‹ä¸€èŠ‚**ï¼š[2.6 ç« èŠ‚æ€»ç»“](2.6_chapter_summary.md)

**æœ¬èŠ‚æ€»ç»“**ï¼šGPT-4Vä»£è¡¨äº†å¤šæ¨¡æ€AIå‘å±•çš„æ–°é«˜åº¦ï¼Œå®ƒå°†å¼ºå¤§çš„è¯­è¨€ç†è§£èƒ½åŠ›ä¸ç²¾ç¡®çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å®Œç¾ç»“åˆï¼Œå¼€åˆ›äº†äººæœºäº¤äº’çš„æ–°æ¨¡å¼ã€‚é€šè¿‡ç»Ÿä¸€çš„æ¶æ„è®¾è®¡å’Œæ·±åº¦çš„è·¨æ¨¡æ€èåˆï¼ŒGPT-4Vä¸ä»…èƒ½å¤Ÿç†è§£å›¾åƒå†…å®¹ï¼Œæ›´èƒ½è¿›è¡Œå¤æ‚çš„è§†è§‰æ¨ç†ï¼Œä¸ºAIåº”ç”¨å¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„å¯èƒ½æ€§ã€‚