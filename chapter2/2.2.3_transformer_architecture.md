# 2.2.3 Transformer架构详解

## 学习目标

通过本节学习，你将能够：
- 深入理解Transformer的完整架构设计
- 掌握编码器和解码器的具体实现细节
- 理解残差连接和层归一化的作用机制
- 分析前馈网络的设计原理
- 实现完整的Transformer模型

## Transformer整体架构

### 架构概览

Transformer采用经典的编码器-解码器（Encoder-Decoder）架构，但完全基于注意力机制：

```
输入序列 → 编码器 → 上下文表示 → 解码器 → 输出序列
```

**核心特点**：
- 编码器和解码器都由多个相同层堆叠而成
- 每层都包含多头注意力和前馈网络
- 使用残差连接和层归一化
- 完全并行化，无循环结构

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import MultiheadAttention
import math
from typing import Optional, Tuple
import copy

class TransformerArchitecture:
    """
    Transformer架构详细分析和实现
    """
    
    def __init__(self):
        self.model_configs = {
            "Base": {
                "d_model": 512,
                "n_heads": 8,
                "n_layers": 6,
                "d_ff": 2048,
                "dropout": 0.1,
                "max_seq_len": 5000
            },
            "Big": {
                "d_model": 1024,
                "n_heads": 16,
                "n_layers": 6,
                "d_ff": 4096,
                "dropout": 0.3,
                "max_seq_len": 5000
            }
        }
    
    def visualize_architecture(self):
        """可视化Transformer完整架构"""
        print("=== Transformer完整架构可视化 ===")
        print()
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 12))
        
        # 编码器架构
        self._draw_encoder(ax1)
        ax1.set_title('Transformer编码器', fontsize=16, fontweight='bold')
        
        # 解码器架构
        self._draw_decoder(ax2)
        ax2.set_title('Transformer解码器', fontsize=16, fontweight='bold')
        
        plt.tight_layout()
        plt.show()
        
        # 打印架构参数
        print("\nTransformer模型配置:")
        print("-" * 50)
        print(f"{'配置':<10} {'d_model':<8} {'heads':<6} {'layers':<7} {'d_ff':<6} {'参数量':<10}")
        print("-" * 50)
        
        for config_name, config in self.model_configs.items():
            params = self._calculate_parameters(config)
            print(f"{config_name:<10} {config['d_model']:<8} {config['n_heads']:<6} "
                  f"{config['n_layers']:<7} {config['d_ff']:<6} {params:<10}")
    
    def _draw_encoder(self, ax):
        """绘制编码器架构"""
        layers = 6
        layer_height = 1.5
        
        for i in range(layers):
            y_base = i * layer_height
            
            # 多头注意力
            rect1 = plt.Rectangle((0, y_base), 3, 0.6, 
                                facecolor='lightblue', edgecolor='black', linewidth=2)
            ax.add_patch(rect1)
            ax.text(1.5, y_base + 0.3, 'Multi-Head\nSelf-Attention', 
                   ha='center', va='center', fontsize=10, fontweight='bold')
            
            # Add & Norm
            rect2 = plt.Rectangle((0, y_base + 0.7), 3, 0.2, 
                                facecolor='yellow', edgecolor='black')
            ax.add_patch(rect2)
            ax.text(1.5, y_base + 0.8, 'Add & Norm', 
                   ha='center', va='center', fontsize=8)
            
            # 前馈网络
            rect3 = plt.Rectangle((0, y_base + 1.0), 3, 0.4, 
                                facecolor='lightgreen', edgecolor='black', linewidth=2)
            ax.add_patch(rect3)
            ax.text(1.5, y_base + 1.2, 'Feed Forward', 
                   ha='center', va='center', fontsize=10, fontweight='bold')
            
            # Add & Norm
            rect4 = plt.Rectangle((0, y_base + 1.45), 3, 0.2, 
                                facecolor='yellow', edgecolor='black')
            ax.add_patch(rect4)
            ax.text(1.5, y_base + 1.55, 'Add & Norm', 
                   ha='center', va='center', fontsize=8)
            
            # 层标签
            ax.text(-0.5, y_base + 0.75, f'Layer\n{i+1}', 
                   ha='center', va='center', fontsize=9, fontweight='bold')
        
        # 输入嵌入
        rect_input = plt.Rectangle((0, -1), 3, 0.5, 
                                 facecolor='lightcoral', edgecolor='black', linewidth=2)
        ax.add_patch(rect_input)
        ax.text(1.5, -0.75, 'Input Embedding\n+ Positional Encoding', 
               ha='center', va='center', fontsize=10, fontweight='bold')
        
        # 连接线
        for i in range(layers + 1):
            y = i * layer_height - 0.5
            if i < layers:
                ax.arrow(1.5, y, 0, 0.4, head_width=0.1, head_length=0.05, 
                        fc='black', ec='black')
        
        ax.set_xlim(-1, 4)
        ax.set_ylim(-1.5, layers * layer_height + 0.5)
        ax.set_aspect('equal')
        ax.axis('off')
    
    def _draw_decoder(self, ax):
        """绘制解码器架构"""
        layers = 6
        layer_height = 2.0
        
        for i in range(layers):
            y_base = i * layer_height
            
            # 掩码多头自注意力
            rect1 = plt.Rectangle((0, y_base), 3, 0.5, 
                                facecolor='lightcoral', edgecolor='black', linewidth=2)
            ax.add_patch(rect1)
            ax.text(1.5, y_base + 0.25, 'Masked Multi-Head\nSelf-Attention', 
                   ha='center', va='center', fontsize=9, fontweight='bold')
            
            # Add & Norm
            rect2 = plt.Rectangle((0, y_base + 0.55), 3, 0.15, 
                                facecolor='yellow', edgecolor='black')
            ax.add_patch(rect2)
            ax.text(1.5, y_base + 0.625, 'Add & Norm', 
                   ha='center', va='center', fontsize=7)
            
            # 编码器-解码器注意力
            rect3 = plt.Rectangle((0, y_base + 0.8), 3, 0.5, 
                                facecolor='lightyellow', edgecolor='black', linewidth=2)
            ax.add_patch(rect3)
            ax.text(1.5, y_base + 1.05, 'Multi-Head\nCross-Attention', 
                   ha='center', va='center', fontsize=9, fontweight='bold')
            
            # Add & Norm
            rect4 = plt.Rectangle((0, y_base + 1.35), 3, 0.15, 
                                facecolor='yellow', edgecolor='black')
            ax.add_patch(rect4)
            ax.text(1.5, y_base + 1.425, 'Add & Norm', 
                   ha='center', va='center', fontsize=7)
            
            # 前馈网络
            rect5 = plt.Rectangle((0, y_base + 1.6), 3, 0.3, 
                                facecolor='lightgreen', edgecolor='black', linewidth=2)
            ax.add_patch(rect5)
            ax.text(1.5, y_base + 1.75, 'Feed Forward', 
                   ha='center', va='center', fontsize=9, fontweight='bold')
            
            # Add & Norm
            rect6 = plt.Rectangle((0, y_base + 1.95), 3, 0.15, 
                                facecolor='yellow', edgecolor='black')
            ax.add_patch(rect6)
            ax.text(1.5, y_base + 2.025, 'Add & Norm', 
                   ha='center', va='center', fontsize=7)
            
            # 层标签
            ax.text(-0.5, y_base + 1.0, f'Layer\n{i+1}', 
                   ha='center', va='center', fontsize=9, fontweight='bold')
            
            # 编码器输入箭头
            ax.arrow(3.5, y_base + 1.05, 0.8, 0, head_width=0.05, head_length=0.1, 
                    fc='red', ec='red', alpha=0.7)
            ax.text(4.5, y_base + 1.2, 'Encoder\nOutput', 
                   ha='center', va='center', fontsize=8, color='red')
        
        # 输出嵌入
        rect_output = plt.Rectangle((0, -1), 3, 0.5, 
                                  facecolor='lightcoral', edgecolor='black', linewidth=2)
        ax.add_patch(rect_output)
        ax.text(1.5, -0.75, 'Output Embedding\n+ Positional Encoding', 
               ha='center', va='center', fontsize=10, fontweight='bold')
        
        # 线性层和Softmax
        rect_linear = plt.Rectangle((0, layers * layer_height + 0.2), 3, 0.3, 
                                  facecolor='orange', edgecolor='black', linewidth=2)
        ax.add_patch(rect_linear)
        ax.text(1.5, layers * layer_height + 0.35, 'Linear & Softmax', 
               ha='center', va='center', fontsize=10, fontweight='bold')
        
        # 连接线
        for i in range(layers + 2):
            y = i * layer_height - 0.5
            if i <= layers:
                ax.arrow(1.5, y, 0, 0.3, head_width=0.1, head_length=0.05, 
                        fc='black', ec='black')
        
        ax.set_xlim(-1, 5.5)
        ax.set_ylim(-1.5, layers * layer_height + 1)
        ax.set_aspect('equal')
        ax.axis('off')
    
    def _calculate_parameters(self, config):
        """计算模型参数量"""
        d_model = config['d_model']
        n_heads = config['n_heads']
        n_layers = config['n_layers']
        d_ff = config['d_ff']
        
        # 多头注意力参数
        attention_params = 4 * d_model * d_model  # Q, K, V, O矩阵
        
        # 前馈网络参数
        ffn_params = d_model * d_ff + d_ff * d_model  # 两个线性层
        
        # 每层参数
        layer_params = attention_params + ffn_params
        
        # 编码器+解码器参数
        total_params = 2 * n_layers * layer_params
        
        # 转换为M单位
        return f"{total_params / 1e6:.1f}M"

class PositionalEncoding(nn.Module):
    """位置编码实现"""
    
    def __init__(self, d_model: int, max_seq_len: int = 5000):
        super().__init__()
        self.d_model = d_model
        
        # 创建位置编码矩阵
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        
        # 计算除数项
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        # 应用sin和cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # 添加batch维度并注册为buffer
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor, shape [seq_len, batch_size, embedding_dim]
        """
        x = x + self.pe[:x.size(0), :]
        return x

class MultiHeadAttention(nn.Module):
    """多头注意力机制实现"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # 线性变换层
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = query.size(0)
        seq_len = query.size(1)
        
        # 1. 线性变换并重塑为多头
        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # 2. 计算注意力
        attention_output, attention_weights = self._scaled_dot_product_attention(
            Q, K, V, mask)
        
        # 3. 拼接多头
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model)
        
        # 4. 最终线性变换
        output = self.w_o(attention_output)
        
        return output, attention_weights
    
    def _scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, 
                                    V: torch.Tensor, mask: Optional[torch.Tensor] = None
                                    ) -> Tuple[torch.Tensor, torch.Tensor]:
        # 计算注意力分数
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # 应用掩码
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmax归一化
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # 加权求和
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights

class PositionwiseFeedForward(nn.Module):
    """位置前馈网络实现"""
    
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.w_2(self.dropout(F.relu(self.w_1(x))))

class EncoderLayer(nn.Module):
    """Transformer编码器层"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
               ) -> torch.Tensor:
        # 多头自注意力 + 残差连接 + 层归一化
        attn_output, _ = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 前馈网络 + 残差连接 + 层归一化
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class DecoderLayer(nn.Module):
    """Transformer解码器层"""
    
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # 掩码多头自注意力 + 残差连接 + 层归一化
        self_attn_output, _ = self.self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(self_attn_output))
        
        # 编码器-解码器注意力 + 残差连接 + 层归一化
        cross_attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # 前馈网络 + 残差连接 + 层归一化
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x

class TransformerEncoder(nn.Module):
    """Transformer编码器"""
    
    def __init__(self, encoder_layer: EncoderLayer, n_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(n_layers)])
        self.n_layers = n_layers
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
               ) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x, mask)
        return x

class TransformerDecoder(nn.Module):
    """Transformer解码器"""
    
    def __init__(self, decoder_layer: DecoderLayer, n_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(n_layers)])
        self.n_layers = n_layers
        
    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)
        return x

class Transformer(nn.Module):
    """完整的Transformer模型"""
    
    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 512,
                 n_heads: int = 8, n_layers: int = 6, d_ff: int = 2048,
                 max_seq_len: int = 5000, dropout: float = 0.1):
        super().__init__()
        
        self.d_model = d_model
        
        # 嵌入层
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # 位置编码
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # 编码器和解码器
        encoder_layer = EncoderLayer(d_model, n_heads, d_ff, dropout)
        decoder_layer = DecoderLayer(d_model, n_heads, d_ff, dropout)
        
        self.encoder = TransformerEncoder(encoder_layer, n_layers)
        self.decoder = TransformerDecoder(decoder_layer, n_layers)
        
        # 输出投影层
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        
        # 初始化参数
        self._init_parameters()
        
    def _init_parameters(self):
        """初始化模型参数"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, src: torch.Tensor, tgt: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # 嵌入 + 位置编码
        src_embedded = self.dropout(self.pos_encoding(
            self.src_embedding(src) * math.sqrt(self.d_model)))
        tgt_embedded = self.dropout(self.pos_encoding(
            self.tgt_embedding(tgt) * math.sqrt(self.d_model)))
        
        # 编码器
        encoder_output = self.encoder(src_embedded, src_mask)
        
        # 解码器
        decoder_output = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)
        
        # 输出投影
        output = self.output_projection(decoder_output)
        
        return output
    
    def encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None
              ) -> torch.Tensor:
        """仅编码器前向传播"""
        src_embedded = self.dropout(self.pos_encoding(
            self.src_embedding(src) * math.sqrt(self.d_model)))
        return self.encoder(src_embedded, src_mask)
    
    def decode(self, tgt: torch.Tensor, encoder_output: torch.Tensor,
               src_mask: Optional[torch.Tensor] = None,
               tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """仅解码器前向传播"""
        tgt_embedded = self.dropout(self.pos_encoding(
            self.tgt_embedding(tgt) * math.sqrt(self.d_model)))
        decoder_output = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)
        return self.output_projection(decoder_output)

class TransformerAnalyzer:
    """Transformer架构分析器"""
    
    def __init__(self):
        self.model = None
        
    def analyze_attention_patterns(self):
        """分析注意力模式"""
        print("=== 注意力模式分析 ===")
        print()
        
        # 创建示例模型
        model = Transformer(src_vocab_size=1000, tgt_vocab_size=1000, 
                          d_model=512, n_heads=8, n_layers=2)
        model.eval()
        
        # 创建示例数据
        torch.manual_seed(42)
        batch_size, seq_len = 1, 10
        src = torch.randint(1, 100, (batch_size, seq_len))
        tgt = torch.randint(1, 100, (batch_size, seq_len))
        
        # 获取注意力权重
        with torch.no_grad():
            # 修改模型以返回注意力权重
            src_embedded = model.dropout(model.pos_encoding(
                model.src_embedding(src) * math.sqrt(model.d_model)))
            
            # 获取第一层编码器的注意力权重
            encoder_layer = model.encoder.layers[0]
            attn_output, attn_weights = encoder_layer.self_attention(
                src_embedded, src_embedded, src_embedded)
        
        # 可视化注意力权重
        self._visualize_attention_heads(attn_weights[0].numpy(), 
                                      [f"Token_{i}" for i in range(seq_len)])
        
        return attn_weights
    
    def _visualize_attention_heads(self, attention_weights, tokens):
        """可视化多头注意力权重"""
        n_heads = attention_weights.shape[0]
        
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.flatten()
        
        for head in range(n_heads):
            ax = axes[head]
            
            # 创建热力图
            sns.heatmap(attention_weights[head], 
                       xticklabels=tokens, 
                       yticklabels=tokens,
                       cmap='Blues', 
                       annot=True, 
                       fmt='.2f',
                       ax=ax,
                       cbar=False)
            
            ax.set_title(f'Head {head + 1}', fontweight='bold')
            ax.set_xlabel('Key')
            ax.set_ylabel('Query')
        
        plt.suptitle('多头注意力权重可视化', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
    def analyze_layer_outputs(self):
        """分析各层输出"""
        print("\n=== 各层输出分析 ===")
        print()
        
        # 创建示例模型
        model = Transformer(src_vocab_size=1000, tgt_vocab_size=1000, 
                          d_model=512, n_heads=8, n_layers=6)
        model.eval()
        
        # 创建示例数据
        torch.manual_seed(42)
        batch_size, seq_len = 1, 10
        src = torch.randint(1, 100, (batch_size, seq_len))
        
        layer_outputs = []
        
        with torch.no_grad():
            # 嵌入层输出
            x = model.dropout(model.pos_encoding(
                model.src_embedding(src) * math.sqrt(model.d_model)))
            layer_outputs.append(x.clone())
            
            # 各编码器层输出
            for i, layer in enumerate(model.encoder.layers):
                x = layer(x)
                layer_outputs.append(x.clone())
        
        # 分析输出统计
        self._analyze_output_statistics(layer_outputs)
        
        return layer_outputs
    
    def _analyze_output_statistics(self, layer_outputs):
        """分析输出统计信息"""
        print("各层输出统计:")
        print("-" * 60)
        print(f"{'层':<10} {'均值':<10} {'标准差':<10} {'最大值':<10} {'最小值':<10}")
        print("-" * 60)
        
        layer_names = ['Embedding'] + [f'Encoder-{i+1}' for i in range(len(layer_outputs)-1)]
        
        means = []
        stds = []
        
        for i, (name, output) in enumerate(zip(layer_names, layer_outputs)):
            mean_val = output.mean().item()
            std_val = output.std().item()
            max_val = output.max().item()
            min_val = output.min().item()
            
            means.append(mean_val)
            stds.append(std_val)
            
            print(f"{name:<10} {mean_val:<10.4f} {std_val:<10.4f} {max_val:<10.4f} {min_val:<10.4f}")
        
        # 可视化统计信息
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 均值变化
        ax1.plot(range(len(means)), means, 'bo-', linewidth=2, markersize=8)
        ax1.set_xlabel('层')
        ax1.set_ylabel('输出均值')
        ax1.set_title('各层输出均值变化')
        ax1.set_xticks(range(len(layer_names)))
        ax1.set_xticklabels(layer_names, rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # 标准差变化
        ax2.plot(range(len(stds)), stds, 'ro-', linewidth=2, markersize=8)
        ax2.set_xlabel('层')
        ax2.set_ylabel('输出标准差')
        ax2.set_title('各层输出标准差变化')
        ax2.set_xticks(range(len(layer_names)))
        ax2.set_xticklabels(layer_names, rotation=45)
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def analyze_parameter_distribution(self):
        """分析参数分布"""
        print("\n=== 参数分布分析 ===")
        print()
        
        # 创建示例模型
        model = Transformer(src_vocab_size=1000, tgt_vocab_size=1000, 
                          d_model=512, n_heads=8, n_layers=6)
        
        # 收集不同类型的参数
        param_types = {
            'Embedding': [],
            'Attention': [],
            'FeedForward': [],
            'LayerNorm': [],
            'Output': []
        }
        
        for name, param in model.named_parameters():
            if 'embedding' in name:
                param_types['Embedding'].extend(param.flatten().tolist())
            elif 'attention' in name or 'w_q' in name or 'w_k' in name or 'w_v' in name or 'w_o' in name:
                param_types['Attention'].extend(param.flatten().tolist())
            elif 'feed_forward' in name or 'w_1' in name or 'w_2' in name:
                param_types['FeedForward'].extend(param.flatten().tolist())
            elif 'norm' in name:
                param_types['LayerNorm'].extend(param.flatten().tolist())
            elif 'output_projection' in name:
                param_types['Output'].extend(param.flatten().tolist())
        
        # 可视化参数分布
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for i, (param_type, params) in enumerate(param_types.items()):
            if params:  # 如果参数列表不为空
                ax = axes[i]
                ax.hist(params, bins=50, alpha=0.7, color=colors[i], density=True)
                ax.set_title(f'{param_type} 参数分布')
                ax.set_xlabel('参数值')
                ax.set_ylabel('密度')
                ax.grid(True, alpha=0.3)
                
                # 添加统计信息
                mean_val = np.mean(params)
                std_val = np.std(params)
                ax.axvline(mean_val, color='red', linestyle='--', 
                          label=f'均值: {mean_val:.4f}')
                ax.axvline(mean_val + std_val, color='orange', linestyle='--', alpha=0.7,
                          label=f'±标准差: {std_val:.4f}')
                ax.axvline(mean_val - std_val, color='orange', linestyle='--', alpha=0.7)
                ax.legend()
        
        # 删除多余的子图
        if len(param_types) < len(axes):
            for i in range(len(param_types), len(axes)):
                fig.delaxes(axes[i])
        
        plt.tight_layout()
        plt.show()
        
        # 打印参数统计
        print("参数统计信息:")
        print("-" * 70)
        print(f"{'参数类型':<15} {'数量':<10} {'均值':<12} {'标准差':<12} {'最大值':<12} {'最小值':<12}")
        print("-" * 70)
        
        for param_type, params in param_types.items():
            if params:
                count = len(params)
                mean_val = np.mean(params)
                std_val = np.std(params)
                max_val = np.max(params)
                min_val = np.min(params)
                
                print(f"{param_type:<15} {count:<10} {mean_val:<12.6f} {std_val:<12.6f} "
                      f"{max_val:<12.6f} {min_val:<12.6f}")

# 使用示例
if __name__ == "__main__":
    print("Transformer架构详细分析")
    print("=" * 50)
    
    # 创建架构分析器
    arch_analyzer = TransformerArchitecture()
    
    print("\n1. 架构可视化")
    arch_analyzer.visualize_architecture()
    
    # 创建模型分析器
    model_analyzer = TransformerAnalyzer()
    
    print("\n2. 注意力模式分析")
    model_analyzer.analyze_attention_patterns()
    
    print("\n3. 各层输出分析")
    model_analyzer.analyze_layer_outputs()
    
    print("\n4. 参数分布分析")
    model_analyzer.analyze_parameter_distribution()
```

## 编码器详解

### 编码器层结构

每个编码器层包含两个主要子层：

1. **多头自注意力机制**
2. **位置前馈网络**

每个子层都使用残差连接和层归一化：

```
LayerNorm(x + Sublayer(x))
```

### 多头自注意力

**工作原理**：
- 输入序列中的每个位置都可以关注所有位置
- 通过多个注意力头捕获不同类型的依赖关系
- 并行计算，效率高

**数学表示**：
```
MultiHead(Q,K,V) = Concat(head₁,...,headₕ)W^O
where headᵢ = Attention(QWᵢ^Q, KWᵢ^K, VWᵢ^V)
```

### 位置前馈网络

**结构**：
```
FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
```

**特点**：
- 两层全连接网络
- 中间使用ReLU激活函数
- 维度变化：d_model → d_ff → d_model
- 对每个位置独立应用

## 解码器详解

### 解码器层结构

每个解码器层包含三个主要子层：

1. **掩码多头自注意力机制**
2. **编码器-解码器注意力机制**
3. **位置前馈网络**

### 掩码自注意力

**目的**：防止解码器在生成当前位置时看到未来的信息

**实现**：
```python
# 创建下三角掩码矩阵
mask = torch.tril(torch.ones(seq_len, seq_len))
scores = scores.masked_fill(mask == 0, -1e9)
```

**效果**：
- 位置i只能关注位置0到i
- 保证自回归生成的因果性
- 训练时可以并行计算

### 编码器-解码器注意力

**交叉注意力机制**：
- Query来自解码器的前一层
- Key和Value来自编码器的输出
- 实现解码器对源序列的关注

**作用**：
- 建立源序列和目标序列的对应关系
- 类似于传统seq2seq中的注意力机制
- 提供上下文信息指导生成

## 残差连接和层归一化

### 残差连接

**公式**：
```
output = x + F(x)
```

**优势**：
- 缓解梯度消失问题
- 加速训练收敛
- 支持更深的网络
- 提供恒等映射的快捷路径

### 层归一化

**公式**：
```
LayerNorm(x) = γ * (x - μ) / σ + β
```

其中：
- μ：层内均值
- σ：层内标准差
- γ, β：可学习参数

**优势**：
- 稳定训练过程
- 减少内部协变量偏移
- 提高模型泛化能力
- 对批次大小不敏感

## 位置编码详解

### 正弦余弦位置编码

**公式**：
```
PE(pos,2i) = sin(pos/10000^(2i/d_model))
PE(pos,2i+1) = cos(pos/10000^(2i/d_model))
```

**设计原理**：
- 不同维度使用不同频率的正弦余弦函数
- 低维度频率高，高维度频率低
- 可以表示相对位置关系
- 支持序列长度的外推

### 位置编码的性质

1. **唯一性**：每个位置都有唯一的编码
2. **有界性**：编码值在[-1, 1]范围内
3. **周期性**：不同维度有不同的周期
4. **相对位置**：PE(pos+k)可以表示为PE(pos)的线性函数

## 训练技巧

### 学习率调度

**Warmup策略**：
```
lrate = d_model^(-0.5) × min(step^(-0.5), step × warmup_steps^(-1.5))
```

**阶段**：
1. **Warmup阶段**：学习率线性增加
2. **衰减阶段**：学习率按幂律衰减

### 正则化技术

1. **Dropout**：
   - 注意力权重dropout
   - 残差连接前dropout
   - 嵌入层dropout

2. **标签平滑**：
   - 软化目标分布
   - 提高模型泛化能力
   - 减少过拟合

### 参数初始化

**Xavier初始化**：
```python
for p in model.parameters():
    if p.dim() > 1:
        nn.init.xavier_uniform_(p)
```

**目的**：
- 保持激活值的方差
- 避免梯度消失/爆炸
- 加速训练收敛

## 计算复杂度分析

### 时间复杂度

**自注意力**：O(n²·d)
- n：序列长度
- d：模型维度

**前馈网络**：O(n·d²)
- 对每个位置独立计算

**总复杂度**：O(n²·d + n·d²)

### 空间复杂度

**注意力矩阵**：O(n²)
- 存储所有位置对的注意力权重

**激活值**：O(n·d)
- 存储中间层的激活值

### 并行化分析

**优势**：
- 自注意力完全并行化
- 前馈网络位置独立
- 编码器层间可以流水线并行

**限制**：
- 解码器的自回归性质
- 内存带宽限制
- 通信开销

## 模型变体

### 编码器模型

**BERT类模型**：
- 只使用编码器部分
- 双向自注意力
- 适合理解任务

### 解码器模型

**GPT类模型**：
- 只使用解码器部分
- 掩码自注意力
- 适合生成任务

### 编码器-解码器模型

**T5类模型**：
- 保持完整架构
- 适合序列到序列任务
- 统一的文本到文本框架

## 优化技术

### 注意力优化

1. **稀疏注意力**：
   - 减少注意力计算
   - 保持长距离建模能力
   - 降低复杂度到O(n√n)

2. **线性注意力**：
   - 将复杂度降低到O(n)
   - 使用核技巧近似
   - 适合超长序列

3. **Flash Attention**：
   - 内存高效的注意力计算
   - 减少内存访问次数
   - 提高计算效率

### 模型压缩

1. **知识蒸馏**：
   - 大模型指导小模型
   - 保持性能的同时减少参数

2. **模型剪枝**：
   - 移除不重要的参数
   - 结构化或非结构化剪枝

3. **量化技术**：
   - 降低参数精度
   - INT8或更低精度
   - 硬件友好

## 思考题

1. **架构设计**：为什么Transformer选择残差连接而不是其他连接方式？

2. **层归一化**：层归一化相比批归一化在Transformer中有什么优势？

3. **位置编码**：如何设计更好的位置编码方案？

4. **计算效率**：如何在保持性能的同时降低Transformer的计算复杂度？

5. **模型深度**：更深的Transformer一定更好吗？如何确定最优深度？

## 本节小结

Transformer架构是现代深度学习的重要里程碑：

**核心组件**：
- **多头注意力**：捕获不同类型的依赖关系
- **位置前馈网络**：提供非线性变换能力
- **残差连接**：缓解梯度消失，支持深层网络
- **层归一化**：稳定训练过程
- **位置编码**：为模型提供位置信息

**设计优势**：
- **并行化**：大幅提升训练效率
- **长距离依赖**：直接建模任意位置间的关系
- **可扩展性**：支持大规模模型训练
- **通用性**：适用于多种NLP任务

**技术创新**：
- 完全基于注意力机制
- 编码器-解码器统一设计
- 有效的正则化和优化技术
- 灵活的模型变体支持

Transformer不仅在技术上实现了突破，更重要的是为后续的大模型发展奠定了坚实基础。它的设计思想和技术细节至今仍在指导着AI领域的发展方向。

---

**Trae实践建议**：
1. 从零实现完整的Transformer模型
2. 实验不同的超参数配置
3. 可视化注意力权重和层输出
4. 对比不同优化技术的效果
5. 尝试实现模型的变体和改进